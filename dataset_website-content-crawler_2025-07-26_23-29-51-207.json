[{
  "url": "https://docs.comfy.org/",
  "markdown": "# ComfyUI Official Documentation - ComfyUI\n\nGet Started\n\nComfyUI Official Documentation"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/create-text-to-image-prompt",
  "markdown": "# Create Text to Image Prompt\n\n#### Body\n\nHide child attributes\n\ninference\\_params.add\\_quality\\_guidance\n\nWhether to add quality guidance\n\ninference\\_params.caching\\_coefficient\n\nCaching coefficient for optimization\n\ninference\\_params.caching\\_cooldown\n\nNumber of caching cooldown steps\n\ninference\\_params.caching\\_warmup\n\nNumber of caching warmup steps\n\ninference\\_params.clip\\_value\n\nCLIP value for generation control\n\ninference\\_params.conditioning\\_frame\\_index\n\nIndex of the conditioning frame\n\ninference\\_params.cooldown\\_steps\n\nNumber of cooldown steps (calculated based on num\\_frames)\n\nFrames per second of the generated video\n\ninference\\_params.guidance\\_scale\n\nGuidance scale for generation control\n\nHeight of the generated video in pixels\n\ninference\\_params.negative\\_prompt\n\ninference\\_params.num\\_frames\n\nNumber of frames to generate\n\nRandom seed for generation (default: random)\n\ninference\\_params.shift\\_value\n\nShift value for generation control\n\nNumber of denoising steps\n\ninference\\_params.use\\_guidance\\_schedule\n\nWhether to use guidance scheduling\n\ninference\\_params.use\\_negative\\_prompts\n\nWhether to use negative prompts\n\ninference\\_params.use\\_timestep\\_transform\n\nWhether to use timestep transformation\n\ninference\\_params.warmup\\_steps\n\nNumber of warmup steps (calculated based on num\\_frames)\n\nWidth of the generated video in pixels\n\n#### Response\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/create-text-to-video-prompt",
  "markdown": "# Create Text to Video Prompt\n\n#### Body\n\nHide child attributes\n\ninference\\_params.add\\_quality\\_guidance\n\nWhether to add quality guidance\n\ninference\\_params.caching\\_coefficient\n\nCaching coefficient for optimization\n\ninference\\_params.caching\\_cooldown\n\nNumber of caching cooldown steps\n\ninference\\_params.caching\\_warmup\n\nNumber of caching warmup steps\n\ninference\\_params.clip\\_value\n\nCLIP value for generation control\n\ninference\\_params.conditioning\\_frame\\_index\n\nIndex of the conditioning frame\n\ninference\\_params.cooldown\\_steps\n\nNumber of cooldown steps (calculated based on num\\_frames)\n\nFrames per second of the generated video\n\ninference\\_params.guidance\\_scale\n\nGuidance scale for generation control\n\nHeight of the generated video in pixels\n\ninference\\_params.negative\\_prompt\n\ninference\\_params.num\\_frames\n\nNumber of frames to generate\n\nRandom seed for generation (default: random)\n\ninference\\_params.shift\\_value\n\nShift value for generation control\n\nNumber of denoising steps\n\ninference\\_params.use\\_guidance\\_schedule\n\nWhether to use guidance scheduling\n\ninference\\_params.use\\_negative\\_prompts\n\nWhether to use negative prompts\n\ninference\\_params.use\\_timestep\\_transform\n\nWhether to use timestep transformation\n\ninference\\_params.warmup\\_steps\n\nNumber of warmup steps (calculated based on num\\_frames)\n\nWidth of the generated video in pixels\n\n#### Response\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/create-image-to-video-prompt",
  "markdown": "# Create Image to Video Prompt\n\n#### Body\n\nHide child attributes\n\ninference\\_params.add\\_quality\\_guidance\n\nWhether to add quality guidance\n\ninference\\_params.caching\\_coefficient\n\nCaching coefficient for optimization\n\ninference\\_params.caching\\_cooldown\n\nNumber of caching cooldown steps\n\ninference\\_params.caching\\_warmup\n\nNumber of caching warmup steps\n\ninference\\_params.clip\\_value\n\nCLIP value for generation control\n\ninference\\_params.conditioning\\_frame\\_index\n\nIndex of the conditioning frame\n\ninference\\_params.cooldown\\_steps\n\nNumber of cooldown steps (calculated based on num\\_frames)\n\nFrames per second of the generated video\n\ninference\\_params.guidance\\_scale\n\nGuidance scale for generation control\n\nHeight of the generated video in pixels\n\ninference\\_params.negative\\_prompt\n\ninference\\_params.num\\_frames\n\nNumber of frames to generate\n\nRandom seed for generation (default: random)\n\ninference\\_params.shift\\_value\n\nShift value for generation control\n\nNumber of denoising steps\n\ninference\\_params.use\\_guidance\\_schedule\n\nWhether to use guidance scheduling\n\ninference\\_params.use\\_negative\\_prompts\n\nWhether to use negative prompts\n\ninference\\_params.use\\_timestep\\_transform\n\nWhether to use timestep transformation\n\ninference\\_params.warmup\\_steps\n\nNumber of warmup steps (calculated based on num\\_frames)\n\nWidth of the generated video in pixels\n\n#### Response\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/get_started/first_generation",
  "markdown": "# Getting Started with AI Image Generation\n\nThis guide aims to help you understand ComfyUI’s basic operations and complete your first image generation. We’ll cover:\n\n1.  Loading example workflows\n    *   Loading from ComfyUI’s workflow templates\n    *   Loading from images with workflow metadata\n2.  Model installation guidance\n    *   Automatic model installation\n    *   Manual model installation\n    *   Using ComfyUI Manager for model installation\n3.  Completing your first text-to-image generation\n\n## About Text-to-Image\n\nText-to-Image is a fundamental AI drawing feature that generates images from text descriptions. It’s one of the most commonly used functions in AI art generation. You can think of the process as telling your requirements (positive and negative prompts) to an artist (the drawing model), who will then create what you want. Detailed explanations about text-to-image will be covered in the [Text to Image](https://docs.comfy.org/tutorials/basic/text-to-image) chapter.\n\n## ComfyUI Text-to-Image Workflow Tutorial\n\n### 1\\. Launch ComfyUI\n\nMake sure you’ve followed the installation guide to start ComfyUI and can successfully enter the ComfyUI interface. ![ComfyUI Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/comfyui-boot-screen.jpg) If you have not installed ComfyUI, please choose a suitable version to install based on your device.\n\n### 2\\. Load Default Text-to-Image Workflow\n\nComfyUI usually loads the default text-to-image workflow automatically when launched. However, you can try different methods to load workflows to familiarize yourself with ComfyUI’s basic operations:\n\n ![ComfyUI Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1.jpg) Follow the numbered steps in the image:\n\n1.  Click the **Fit View** button in the bottom right to ensure any loaded workflow isn’t hidden\n2.  Click the **folder icon (workflows)** in the sidebar\n3.  Click the **Browse example workflows** button at the top of the Workflows panel\n\nContinue with: ![Load Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg) \n\n4.  Select the first default workflow **Image Generation** to load it\n\nAlternatively, you can select **Browse workflow templates** from the workflow menu ![ComfyUI Menu - Browse Workflow Templates](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg) \n\n### 3\\. Model Installation\n\nMost ComfyUI installations don’t include base models by default. After loading the workflow, if you don’t have the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model installed, you’ll see this prompt: ![Missing Models](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg) All models are stored in `<your ComfyUI installation>/ComfyUI/models/` with subfolders like `checkpoints`, `embeddings`, `vae`, `lora`, `upscale_model`, etc. ComfyUI detects models in these folders and paths configured in `extra_model_paths.yaml` at startup. ![ComfyUI Models Folder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg) You can install models through:\n\nAfter you click the **Download** button, ComfyUI will execute the download, and different behaviors will be performed depending on the version you are using.\n\nThe desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.![Model Download Progress](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg)If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.\n\n### 4\\. Load Model and Generate Your First Image\n\nAfter installing the model: ![Image Generation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)\n\n1.  In the **Load Checkpoint** node, ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected\n2.  Click `Queue` or press `Ctrl + Enter` to generate\n\nThe result will appear in the **Save Image** node. Right-click to save locally. ![ComfyUI First Image Generation Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg) For detailed text-to-image instructions, see our comprehensive guide:\n\n[\n\n## ComfyUI Text-to-Image Workflow Guide\n\nClick here for detailed text-to-image workflow instructions\n\n\n\n](https://docs.comfy.org/tutorials/basic/text-to-image)\n\n## Troubleshooting\n\n### Model Loading Issues\n\nIf the `Load Checkpoint` node shows no models or displays “null”, verify your model installation location and try refreshing or restarting ComfyUI."
},
{
  "url": "https://docs.comfy.org/changelog",
  "markdown": "# Changelog - ComfyUI\n\nTrack ComfyUI’s latest features, improvements, and bug fixes\n\n#### 0 reactions\n\n[Sign in](https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fchangelog) to add your reaction."
},
{
  "url": "https://docs.comfy.org/development/core-concepts/workflow",
  "markdown": "# Workflow - ComfyUI\n\n## A graph of nodes\n\nComfyUI is an environment for building and running generative content _**workflows**_. In this context, a workflow is defined as a collection of program objects called _**nodes**_ that are connected to each other, forming a network. This network is also known as a _**graph**_. A ComfyUI workflow can generate any type of media: image, video, audio, AI model, AI agent, and so on.\n\n## Sample workflows\n\nTo get started, try out some of the [official workflows](https://comfyanonymous.github.io/ComfyUI_examples). These use only the Core nodes included in the ComfyUI installation. A thriving community of developers has created a rich [ecosystem](https://registry.comfy.org/) of custom nodes to extend the functionality of ComfyUI.\n\n### Simple Example\n\n![simple workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/simple_workflow.jpeg)\n\n## Visual programming\n\nA node-based computer program like ComfyUI provides a level of power and flexibility that can’t be achieved with traditional menu- and button-driven applications. The ComfyUI node graph is not limited by the tools provided in a traditional computer application. It’s a high-level _**visual programming environment**_ allowing users to design complex systems without needing to write program code or understand advanced mathematics. Many other computer applications use this same node graph paradigm. Examples include the compositing application called Nuke, the 3D programs Maya and Blender, the Unreal real-time graphics engine, and the interactive media authoring program called Max.\n\n### More Complex Example\n\n![complex workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/complex_workflow.jpeg)\n\n## Procedural framework\n\nAnother term used to describe a node-based application is _**procedural framework**_. Procedural means generative: some procedure or algorithm is employed to generate content such as a 3D model or a musical composition. ComfyUI is all of these things: a node graph, a visual programming environment, and a procedural framework. What makes ComfyUI different (and amazing!) is that its radically open structure allows us to generate any type of media asset such as picture, movie, sound, 3D model, AI model, etc. In the context of ComfyUI, the term _**workflow**_ is a synonym for the node network or graph. It corresponds to the _**scene graph**_ in a 3D or multimedia program: the network of all of the nodes within a particular disk file. 3D programs call this a _**scene file**_. Video editing, compositing, and multimedia programs usually call it a _**project file**_.\n\n## Saving workflows\n\nThe ComfyUI workflow is automatically saved in the metadata of any generated image, allowing users to open and use the graph that generated the image. A workflow can also be stored in a human-readable text file that follows the JSON data format. This is necessary for media formats that don’t support metadata. ComfyUI workflows stored as JSON files are very small, allowing convenient versioning, archiving, and sharing of graphs, independently of any generated media.\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/nodes",
  "markdown": "# Nodes - ComfyUI\n\nIn ComfyUI, nodes are the fundamental building blocks for executing tasks. Each node is an independently built module, whether it’s a **Comfy Core** node or a **Custom Node**, with its own unique functionality. Nodes connect to each other through links, allowing us to build complex functionality like assembling LEGO blocks. The combinations of different nodes create the unlimited possibilities of ComfyUI. ![Comfy Core K-Sampler Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg) For example, in the K-Sampler node, you can see it has multiple inputs and outputs, and also includes multiple parameter settings. These parameters determine the logic of node execution. Behind each node is well-written Python logic, allowing you to achieve corresponding functionality without having to write code yourself.\n\n## Nodes perform operations\n\nIn computer science, a _**node**_ is a container for information, usually including programmed instructions to perform some task. Nodes almost never exist in isolation, they’re almost always connected to other nodes in a networked graph. In ComfyUI, nodes take the visual form of boxes that are connected to each other. ComfyUI nodes are usually _**function operators**_. This means that they operate on some data to perform a function. A function is a process that accepts input data, performs some operation on it, and produces output data. In other words, nodes do some work, contributing to the completion of a task such as generating an image. So ComfyUI nodes almost always have at least one input or output, and usually have multiple inputs and outputs.\n\n## Different Node States\n\n![Node States](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/status.jpg) In ComfyUI, nodes have multiple states. Here are some common node states:\n\n1.  **Normal State**: The default state\n2.  **Running State**: The running state, typically displayed when a node is executing after you start running the workflow\n3.  **Error State**: Node error, typically displayed after running the workflow if there’s a problem with the node’s input, indicated by red marking of the erroneous input node. You need to fix the problematic input to ensure the workflow runs correctly\n4.  **Missing State**: This state usually appears after importing workflows, with two possibilities:\n    *   Comfy Core native node missing: This usually happens because ComfyUI has been updated, but you’re using an older version of ComfyUI. You need to update ComfyUI to resolve this issue\n    *   Custom node missing: The workflow uses custom nodes developed by third-party authors, but your local ComfyUI version doesn’t have these custom nodes installed. You can use [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to find and install the missing custom nodes\n\n## Connections Between Nodes\n\nIn ComfyUI, nodes are connected through [links](https://docs.comfy.org/development/core-concepts/links), allowing data of the same type to flow between different processing units to achieve the final result. ![ComfyUI Node Links](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/inpaint.jpg) Each node receives some input, processes it through its module, and converts it to corresponding output. Connections between different nodes must conform to the data type requirements. In ComfyUI, we use different colors to distinguish node data types. Below are some basic data types: ![ComfyUI Node Data Types](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/data_type.jpg)\n\n| Data type | Color |\n| --- | --- |\n| diffusion model | lavender |\n| CLIP model | yellow |\n| VAE model | rose |\n| conditioning | orange |\n| latent image | pink |\n| pixel image | blue |\n| mask | green |\n| number (integer or float) | light green |\n| mesh | bright green |\n\nAs ComfyUI evolves, we may expand to more data types to meet the needs of more scenarios.\n\n### Connecting and Disconnecting Nodes\n\n![ComfyUI Node Connecting](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/link_nodes.gif) **Connecting**: Drag from the output point of one node to the input of the same color on another node to connect them **Disconnecting**: Click on the input endpoint and drag the mouse left button to disconnect, or cancel the connection through the midpoint menu of the link\n\n## Node Appearance\n\n![Node Appearance](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/index/node.jpg) We provide various style settings for you to customize the appearance of nodes:\n\n*   Modify styles\n*   Double-click the node title to modify the node name\n*   Switch node inputs between input sockets and widgets through the context menu\n*   Resize the node using the bottom right corner\n\n### Node Badges\n\n![Node Badges](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/badge.jpg) We provide multiple node badge display features, such as:\n\n*   Node ID\n*   Node source\n\nCurrently, **Comfy Core nodes** use a fox icon for display, while custom nodes use their names. This way you can quickly understand which node package a node comes from. You can set the corresponding display in the menu: ![Badge Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/badge_setting.jpg)\n\nNode context menus are mainly divided into two types:\n\n*   Context menu for the node itself\n*   Context menu for inputs/outputs\n\nBy right-clicking on a node, you can expand the corresponding node context menu: ![Node Context Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/context_menus_1.jpg) In the node’s right-click context menu, you can:\n\n*   Adjust the node’s color style\n*   Modify the title\n*   Clone, copy, or delete the node\n*   Set the node’s mode\n\nIn this menu, besides appearance-related settings, the following menu operations are important:\n\n*   **Mode**: Set the node’s mode: Always, Never, Bypass\n*   **Toggle between Widget and Input mode for node inputs**: Switch between widget and input mode for node inputs\n\n#### Mode\n\nFor modes, you may notice that we currently provide: Always, Never, On Event, On Trigger - four modes, but actually only **Always** and **Never** are effective. **On Event** and **On Trigger** are currently ineffective as we haven’t fully implemented this feature. Additionally, you can understand **Bypass** as a mode. Below is an explanation of the available modes:\n\n*   **Always**: The default node mode. The node will execute whenever it runs for the first time or when any of its inputs change since the last execution\n*   **Never**: The node will never execute under any circumstances, as if it’s been deleted. Subsequent nodes cannot read or receive any data from it\n*   **Bypass**: The node will never execute under any circumstances, but subsequent nodes can still try to obtain data that hasn’t been processed by this node\n\nBelow is a comparison of the `Never` and `Bypass` modes: ![Never vs Bypass Mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/never_vs_bypass.jpg) In this comparison example, you can see that both workflows apply two LoRA models simultaneously, with the difference being that one `Load LoRA` node is set to `Never` mode while the other is set to `Bypass` mode.\n\n*   The node set to `Never` mode causes subsequent nodes to show errors because they don’t receive any input data\n*   The node set to `Bypass` mode still allows subsequent nodes to receive unprocessed data, so they load the output data from the first `Load LoRA` node, allowing the subsequent workflow to continue running normally\n\n#### Switching Between Widget and Input Mode for Node Inputs\n\nIn some cases, we need to use output results from other nodes as input. In this case, we can switch between widget and input mode for node inputs. Here’s a very simple example: ![Switch Widget and Input Mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/switch_widget.jpg) By switching the K-Sampler’s Seed from widget to input mode, multiple nodes can share the same seed, achieving variable uniformity across multiple samplers. Comparing the first node with the subsequent two nodes, you can see that the seed in the latter two nodes is in input mode. You can also convert it back to widget mode: ![Convert Input Mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/convert_input.jpg)\n\nThis context menu is mainly related to the data type of the corresponding input/output: ![Node Input/Output Context Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/context_menus_2.jpg) When dragging the input/output of a node, if a connection appears but you haven’t connected to another node’s input or output, releasing the mouse will pop up a context menu for the input/output, used to quickly add related types of nodes. You can adjust the number of node suggestions in the settings: ![Node Suggestion Count](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/node_suggestions.jpg)\n\nThe **Node Selection Toolbox** is a floating tool that provides quick operations for nodes. When you select a node, it hovers above the selected node. Through this toolbox, you can:\n\n*   Change the node’s color\n*   Quickly set the node to Bypass mode (not execute during runtime)\n*   Lock the node\n*   Delete the node\n\nOf course, these functions can also be found in the right-click menu of the corresponding node. The node selection toolbox just provides a shortcut operation. If you want to disable this feature, you can turn it off in the settings. ![Disable Node Selection Toolbox](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/setting_selection_toolbox.jpg)\n\n## Node Groups\n\nIn ComfyUI, you can select multiple parts of a workflow simultaneously, then use the right-click menu to merge them into a node group, making that part a reusable module that can be repeatedly called in your ComfyUI.\n\n## Custom Nodes\n\nComfyUI includes many powerful nodes in the base installation package. These are known as **Comfy Core** nodes. Additionally, the ComfyUI community has created an amazing array of [_**custom nodes**_](https://registry.comfy.org/) to perform a wide variety of functions.\n\n## ComfyUI Manager\n\n![ComfyUI Manager interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png) The **ComfyUI Manager** window makes it easy to perform custom node management tasks such as search, install, update, disable, and uninstall. The Manager is included in the ComfyUI desktop application, but not in the ComfyUI server application.\n\n### Installing the ComfyUI Manager\n\nIf you’re running the ComfyUI server application, you need to install the Manager. If ComfyUI is running, shut it down before proceeding. The first step is to install Git, a command-line application for software version control. Git will download the ComfyUI Manager from [github.com](https://github.com/). Download Git from [git-scm.com](https://git-scm.com/) and install it. Once Git is installed, navigate to the ComfyUI server program directory, to the folder labeled **custom\\_nodes**. Open up a command window or terminal. Make sure that the command line displays the current directory path as **custom\\_nodes**. Enter the following command. This will download the Manager. Technically, this is known as _cloning a Git repository_.\n\n```\ngit clone https://github.com/ltdrdata/ComfyUI-Manager.git\n```\n\nFor details or special cases, see [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation).\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/custom-nodes",
  "markdown": "# Custom Nodes - ComfyUI\n\n## About Custom Nodes\n\nAfter installing ComfyUI, you’ll discover that it includes many built-in nodes. These native nodes are called **Comfy Core** nodes, which are officially maintained by ComfyUI. Additionally, there are numerous [**custom nodes**](https://registry.comfy.org/) created by various authors from the ComfyUI community. These custom nodes bring extensive functionality to ComfyUI, greatly expanding its capabilities and feature boundaries. In this guide, we’ll cover various operations related to custom nodes, including installation, updates, disabling, uninstalling, and dependency installation. Anyone can develop their own custom extensions for ComfyUI and share them with others. You can find many community custom nodes [here](https://registry.comfy.org/). If you want to develop your own custom nodes, visit the section below to get started:\n\n[\n\n## Start Developing Custom Nodes\n\nLearn how to start developing a custom node\n\n\n\n](https://docs.comfy.org/custom-nodes/overview)\n\n## Custom Node Management\n\nIn this section we will cover:\n\n*   Installing custom nodes\n*   Installing node dependencies\n*   Custom node version control\n*   Uninstalling custom nodes\n*   Temporarily disabling custom nodes\n*   Handling custom node dependency conflicts\n\n### 1\\. Installing Custom Nodes\n\nCurrently, ComfyUI supports installing custom nodes through multiple methods, including:\n\n*   [Install via ComfyUI Manager (Recommended)](#install-via-comfyui-manager)\n*   Install via Git\n*   Manual installation\n\nWe recommend installing custom nodes through **ComfyUI Manager**, which is a highly significant tool in the ComfyUI custom node ecosystem. It makes custom node management (such as searching, installing, updating, disabling, and uninstalling) simple - you just need to search for the node you want to install in ComfyUI Manager and click install. However, since all custom nodes are currently stored on GitHub, for regions that cannot access GitHub normally, we have written detailed instructions for different custom node installation methods in this guide. Additionally, since we recommend using **ComfyUI Manager** for plugin management, we recommend using this tool for plugin management. You can find its source code [here](https://github.com/Comfy-Org/ComfyUI-Manager). Therefore, in this documentation, we will use installing ComfyUI Manager as a custom node installation example, and supplement how to use it for node management in the relevant introduction sections.\n\n### 2\\. Installing Node Dependencies\n\nCustom nodes all require the installation of related dependencies. For example, for ComfyUI-Manager, you can visit the [requirements.txt](https://github.com/Comfy-Org/ComfyUI-Manager/blob/main/requirements.txt) file to view the dependency package requirements. In the previous steps, we only cloned the custom node code locally and did not install the corresponding dependencies, so next we need to install the corresponding dependencies.\n\nIn the [Dependencies](https://docs.comfy.org/development/core-concepts/dependencies) chapter, we introduced the relevant content about dependencies in ComfyUI. ComfyUI is a **Python**\\-based project, and we built an independent **Python** runtime environment for running ComfyUI. All related dependencies need to be installed in this independent **Python** runtime environment. If you run `pip install -r requirements.txt` directly in the system-level terminal, the corresponding dependencies may be installed in the system-level **Python** environment, which will cause the dependencies to still be missing in ComfyUI’s environment, preventing the corresponding custom nodes from running normally. So next we need to use ComfyUI’s independent Python runtime environment to complete the dependency installation. Depending on different ComfyUI versions, we will use different methods to install the corresponding dependencies:\n\nFor ComfyUI Portable version, it uses an embedded Python located in the `\\ComfyUI_windows_portable\\python_embeded` directory. We need to use this Python to complete the dependency installation.First, start the terminal in the portable version directory, or use the `cd` command to navigate to the `\\ComfyUI_windows_portable\\` directory after starting the terminal.![Start Terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/custom_nodes/open_terminal.jpg)Ensure that the terminal directory is `\\ComfyUI_windows_portable\\`, as shown below for `D:\\ComfyUI_windows_portable\\`![Terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/custom_nodes/terminal.jpg)Then use `python_embeded\\python.exe` to complete the dependency installation:\n\n```\npython_embeded\\python.exe -m pip install -r ComfyUI\\custom_nodes\\ComfyUI-Manager\\requirements.txt\n```\n\nOf course, you can replace ComfyUI-Manager with the name of the custom node you actually installed, but make sure that a `requirements.txt` file exists in the corresponding node directory.\n\n### Custom Node Version Control\n\nCustom node version control is actually based on Git version control. You can manage node versions through Git, but ComfyUI Manager has already integrated this version management functionality very well. Many thanks to [@Dr.Lt.Data](https://github.com/ltdrdata) for bringing us such a convenient tool. In this section, we will still explain these two different plugin version management methods for you, but if you use ZIP packages for manual installation, the corresponding git version history information will be lost, making it impossible to perform version management.\n\n### Uninstalling Custom Nodes\n\nTo be updated\n\n### Temporarily Disabling Custom Nodes\n\nTo be updated\n\n### Custom Node Dependency Conflicts\n\nTo be updated\n\n## ComfyUI Manager\n\n![ComfyUI Manager Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png) This tool is currently included by default in the [Desktop version](https://docs.comfy.org/installation/desktop/windows), while in the [Portable version](https://docs.comfy.org/installation/comfyui_portable_windows), you need to refer to the installation instructions in the [Install Manager](#installing-custom-nodes) section of this document.\n\n### Installing the Manager\n\nIf you are running the ComfyUI server application, you need to install the manager. If ComfyUI is running, please close it before continuing. The first step is to install Git, which is a command-line application for software version control. Git will download the ComfyUI manager from [github.com](https://github.com/). Download and install Git from [git-scm.com](https://git-scm.com/). After installing Git, navigate to the ComfyUI server program directory and enter the folder labeled **custom\\_nodes**. Open a command window or terminal. Make sure the command line shows the current directory path as **custom\\_nodes**. Enter the following command. This will download the manager. Technically, this is called _cloning a Git repository_.\n\n### Detecting Missing Nodes\n\nAfter installing the manager, you can detect missing nodes in the manager. ![ComfyUI Manager Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png)\n\n## Developing a Custom Node\n\nIf you have some development capabilities, please start with the documentation below to learn how to begin developing a custom node.\n\n[\n\n## Start Developing Custom Nodes\n\nLearn how to start developing a custom node\n\n\n\n](https://docs.comfy.org/custom-nodes/overview)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/properties",
  "markdown": "# Properties - ComfyUI\n\n## Nodes are containers for properties\n\nNodes usually have _**properties**_. Also known as _**parameters**_ or _**attributes**_, node properties are variables that can be changed. Some properties can be adjusted manually by the user, using a data entry field called a _**widget**_. Other properties can be driven automatically by other nodes connected to the property _**input slot**_ or port. Usually, a property can be converted from widget to input and vice versa, allowing users to control property values manually or automatically. Properties can take many forms and hold many different types of information. For example, a **Load Checkpoint** node has a single property:  the file path to the generative model checkpoint file. A **KSampler** node has multiple properties such as the number of sampling **steps**, **CFG** scale, **sampler\\_name**, etc. ![node properties](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_properties.png)\n\n## Data types\n\nInformation can come in many different forms, called _**data types**_. For example, alphanumeric text is known as a _**string**_, a whole number is an _**integer**_, and a number with a decimal point is known as a _**floating point**_ number or _**float**_. New data types are always being added to ComfyUI. ComfyUI is written in the Python scripting language, which is very forgiving about data types. By contrast, the ComfyUI environment is very _**strongly typed**_. This means that different data types can’t be mixed up. For example, we can’t connect an image output to an integer input. This is a huge benefit to users, guiding them to proper workflow construction and preventing program errors.\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/links",
  "markdown": "# Links - ComfyUI\n\n## Links connect nodes\n\nIn the terminology of ComfyUI, the lines or curves between nodes are called _**links**_. They’re also known as _**connections**_ or wires. Links can be displayed in several ways, such as curves, right angles, straight lines, or completely hidden. ![Link styles](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/link_styles.jpg) You can modify the link style in **Setup Menu** —> **Display (Lite Graph)** —> **Graph** —> **Link Render Mode**. ![Canvas Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/render_mode.jpg) You can also temporarily hide links in the **Canvas Menu**. ![Canvas Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/canvas_menu.jpg) Link display is crucial. Depending on the situation, it may be necessary to see all links. Especially when learning, sharing, or even just understanding workflows, the visibility of links enables users to follow the flow of data through the graph. For packaged workflows that aren’t intended to be altered, it might make sense to hide the links to reduce clutter.\n\n### Reroute node\n\nIf legibility of the graph structure is important, then link wires can be manually routed in the 2D space of the graph with a tiny node called **Reroute**. Its purpose is to position the beginning and/or end points of link wires to ensure visibility. We can design a workflow so that link wires don’t pass behind nodes, don’t cross other link wires, and so on. ![ComfyUI Reroute node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/reroute.jpg) We are also continuously improving the native reroute functionality in litegraph. We recommend using this feature in the future to reorganize connections. ![ComfyUI Native Reroute](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/native_reroute.jpg)\n\n## Color-coding\n\nThe data type of node properties is indicated by color coding of input/output ports and link connection wires. We can always tell which inputs and outputs can be connected to one another by their color. Ports can only be connected to other ports of the same color to ensure matching data types. Common data types: ![ComfyUI Node Data Types](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/data_type.jpg)\n\n| Data type | Color |\n| --- | --- |\n| diffusion model | lavender |\n| CLIP model | yellow |\n| VAE model | rose |\n| conditioning | orange |\n| latent image | pink |\n| pixel image | blue |\n| mask | green |\n| number (integer or float) | light green |\n| mesh | bright green |\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/models",
  "markdown": "# Models - ComfyUI\n\n## Models are essential\n\nModels are essential building blocks for media generation workflows. They can be combined and mixed to achieve different creative effects. The word _**model**_ has many different meanings. Here, it means a data file carrying information that is required for a node graph to do its work. Specifically, it’s a data structure that _models_ some function. As a verb, to model something means to represent it or provide an example. The primary example of a model data file in ComfyUI is an AI _**diffusion model**_. This is a large set of data that represents the complex relationships among text strings and images, making it possible to translate words into pictures or vice versa. Other examples of common models used for image generation are multimodal vision and language models such as CLIP, and upscaling models such as RealESRGAN.\n\n## Model files\n\nModel files are indispensable for generative media production. Without them, workflows cannot proceed effectively. Models are not included in the ComfyUI installation, but ComfyUI can often automatically download and install missing model files. Many models can be downloaded and installed from the **ComfyUI Manager** window. Models can also be found at websites such as [huggingface.co](https://huggingface.co/), [civitai.green](https://civitai.green/), and [github.com](https://github.com/).\n\n### Using Models in ComfyUI\n\n1.  Download and place them in the ComfyUI program directory\n    1.  Within the **models** folder, you’ll find subfolders for various types of models, such as **checkpoints**\n    2.  The **ComfyUI Manager** helps to automate the process of searching, downloading, and installing\n    3.  Restart ComfyUI if it’s running\n2.  In your workflow, create the node appropriate to the model type, e.g. **Load Checkpoint**, **Load LoRA**, **Load VAE**\n3.  In the loader node, choose the model you wish to use\n4.  Connect the loader node to other nodes in your workflow\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI’s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 lora/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n### File size\n\nModels can be extremely large files relative to image files. A typical uncompressed image may require a few megabytes of disk storage. Generative AI models can be tens of thousands of times larger, up to tens of gigabytes per model. They take up a great deal of disk space and take a long time to transfer over a network.\n\n## Model training and refinement\n\nA generative AI model is created by training a machine learning program on a very large set of data, such as pairs of images and text descriptions. An AI model doesn’t store the training data explicitly, but rather it stores the correlations that are implicit within the data. Organizations and companies such as Stability AI and Black Forest Labs release “base” models that carry large amounts of generic information. These are general purpose generative AI models. Commonly, the base models need to be _**refined**_ in order to get high quality generative outputs. A dedicated community of people work to refine the base models. The new, refined models produce better output, provide new or different functionality, and/or use fewer resources. Refined models can usually be run on systems with less computing power and/or memory.\n\n## Auxiliary models\n\nModel functionality can be extended with auxiliary models. For example, art directing a text-to-image workflow to achieve a specific result may be difficult or impossible using a diffusion model alone. Additional models can refine a diffusion model within the workflow graph to produce desired results. Examples include **LoRA** (Low Rank Adaptation), a small model that is trained on a specific subject; **ControlNet**, a model that helps control composition using a guide image; and **Inpainting**, a model that allows certain diffusion models to generate new content within an existing image. ![auxiliary models](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_auxiliary-model.png)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/dependencies",
  "markdown": "# Dependencies - ComfyUI\n\n## A workflow file depends on other files\n\nWe often obtain various workflow files from the community, but frequently find that the workflow cannot run directly after loading. This is because a workflow file depends on other files besides the workflow itself, such as media asset inputs, models, custom nodes, related Python dependencies, etc. ComfyUI workflows can only run normally when all relevant dependencies are satisfied. ComfyUI workflow dependencies mainly fall into the following categories:\n\n*   Assets (media files including audio, video, images, and other inputs)\n*   Custom nodes\n*   Python dependencies\n*   Models (such as Stable Diffusion models, etc.)\n\n## Assets\n\nAn AI model is an example of an _**asset**_. In media production, an asset is some media file that supplies input data. For example, a video editing program operates on movie files stored on disk. The editing program’s project file holds links to these movie file assets, allowing non-destructive editing that doesn’t alter the original movie files. ComfyUI works the same way. A workflow can only run if all of the required assets are found and loaded. Generative AI models, images, movies, and sounds are some examples of assets that a workflow might depend upon. These are therefore known as _**dependent assets**_ or _**asset dependencies**_.\n\n## Custom Nodes\n\nCustom nodes are an important component of ComfyUI that extend its functionality. They are created by the community and can be installed to add new capabilities to your workflows.\n\n## Python Dependencies\n\nComfyUI is a Python-based project. We build a standalone Python environment to run ComfyUI, and all related dependencies are installed in this isolated Python environment.\n\n### ComfyUI Dependencies\n\nYou can view ComfyUI’s current dependencies in the [requirements.txt](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) file:\n\n```\ncomfyui-frontend-package==1.14.5\ntorch\ntorchsde\ntorchvision\ntorchaudio\nnumpy>=1.25.0\neinops\ntransformers>=4.28.1\ntokenizers>=0.13.3\nsentencepiece\nsafetensors>=0.4.2\naiohttp>=3.11.8\nyarl>=1.18.0\npyyaml\nPillow\nscipy\ntqdm\npsutil\n\n#non essential dependencies:\nkornia>=0.7.1\nspandrel\nsoundfile\nav\n```\n\nAs ComfyUI evolves, we may adjust dependencies accordingly, such as adding new dependencies or removing ones that are no longer needed. So if you use Git to update ComfyUI, you need to run the following command in the corresponding environment after pulling the latest updates:\n\n```\npip install -r requirements.txt\n```\n\nThis ensures that ComfyUI’s dependencies are up to date for proper operation. You can also modify specific package dependency versions to upgrade or downgrade certain dependencies. Additionally, ComfyUI’s frontend [ComfyUI\\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is currently maintained as a separate project. We update the `comfyui-frontend-package` dependency version after the corresponding version stabilizes. If you need to switch to a different frontend version, you can check the version information [here](https://pypi.org/project/comfyui-frontend-package/#history).\n\n### Custom Node Dependencies\n\nThanks to the efforts of many authors in the ComfyUI community, we can extend ComfyUI’s functionality by using different custom nodes, enabling impressive creativity. Typically, each custom node has its own dependencies and a separate `requirements.txt` file. If you use [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) to install custom nodes, ComfyUI Manager will usually automatically install the corresponding dependencies. There are also cases where you need to install dependencies manually. Currently, all custom nodes are installed in the `ComfyUI/custom_nodes` directory. You need to navigate to the corresponding plugin directory in your ComfyUI Python environment and run `pip install -r requirements.txt` to install the dependencies. If you’re using the [Windows Portable version](https://docs.comfy.org/installation/comfyui_portable_windows), you can use the following command in the `ComfyUI_windows_portable` directory:\n\n```\npython_embeded\\python.exe -m pip install -r ComfyUI\\custom_nodes\\<custom_node_name>\\requirements.txt\n```\n\nto install the dependencies for the corresponding node.\n\n### Dependency Conflicts\n\nDependency conflicts are a common issue when using ComfyUI. You might find that after installing or updating a custom node, previously installed custom nodes can no longer be found in ComfyUI’s node library, or error pop-ups appear. One possible reason is dependency conflicts. There can be many reasons for dependency conflicts, such as:\n\n1.  Custom node version locking\n\nSome plugins may fix the exact version of a dependency library (e.g., `open_clip_torch==2.26.1`), while other plugins may require a higher version (e.g., `open_clip_torch>=2.29.0`), making it impossible to satisfy both version requirements simultaneously. **Solution**: You can try changing the fixed version dependency to a range constraint, such as `open_clip_torch>=2.26.1`, and then reinstall the dependencies to resolve these issues.\n\n2.  Environment pollution\n\nDuring the installation of custom node dependencies, it may overwrite versions of libraries already installed by other plugins. For example, multiple plugins may depend on `PyTorch` but require different CUDA versions, and the later installed plugin will break the existing environment. **Solutions**:\n\n*   You can try manually installing specific versions of dependencies in the Python virtual environment to resolve such issues.\n*   Or create different Python virtual environments for different plugins to resolve these issues.\n*   Try installing plugins one by one, restarting ComfyUI after each installation to observe if dependency conflicts occur.\n\n3.  Custom node dependency versions incompatible with ComfyUI dependency versions\n\nThese types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues. **Solution**: These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.\n\n## Models\n\nModels are a significant asset dependency for ComfyUI. Various custom nodes and workflows are built around specific models, such as the Stable Diffusion series, Flux series, Ltxv, and others. These models are an essential foundation for creation with ComfyUI, so we need to ensure that the models we use are properly available. Typically, our models are saved in the corresponding directory under `ComfyUI/models/`. Of course, you can also create an [extra\\_model\\_paths.yaml](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) by modifying the template to make additional model paths recognized by ComfyUI. This allows multiple ComfyUI instances to share the same model library, reducing disk usage.\n\n## Software\n\nAn advanced application like ComfyUI also has _**software dependencies**_. These are libraries of programming code and data that are required for the application to run. Custom nodes are examples of software dependencies. On an even more fundamental level, the Python programming environment is the ultimate dependency for ComfyUI. The correct version of Python is required to run a particular version of ComfyUI. Updates to Python, ComfyUI, and custom nodes can all be handled from the **ComfyUI Manager** window. ![ComfyUI Custom Nodes Manager](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_dependecies_custom-nodes-manager.png)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/interface/overview",
  "markdown": "# ComfyUI Interface Overview - ComfyUI\n\nThe visual interface is currently the way most users utilize ComfyUI to call the [ComfyUI Server](https://docs.comfy.org/development/comfyui-server/comms_overview) to generate corresponding media resources. It provides a visual interface for users to operate and organize workflows, debug workflows, and create amazing works. Typically, when you start the ComfyUI server, you will see an interface like this: ![ComfyUI Basic Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui_new_interface.jpg) If you are an earlier user, you may have seen the previous menu interface like this: ![ComfyUI Old Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui_old_interface.jpg) Currently, the [ComfyUI frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is a separate project, released and maintained as an independent pip package. If you want to contribute, you can fork this [repository](https://github.com/Comfy-Org/ComfyUI_frontend) and submit a pull request.\n\n## Localization Support\n\nCurrently, ComfyUI supports: English, Chinese, Russian, French, Japanese, and Korean. If you need to switch the interface language to your preferred language, you can click the **Settings gear icon** and then select your desired language under `Comfy` —> `Locale`. ![ComfyUI Localization Support](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/locale.jpg)\n\n### Workspace Areas\n\nBelow are the main interface areas of ComfyUI and a brief introduction to each part. ![ComfyUI Workspace](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui-new-interface-main.png) Currently, apart from the main workflow interface, the ComfyUI interface is mainly divided into the following parts:\n\n1.  Menu Bar: Provides workflow, editing, help menus, workflow execution, ComfyUI Manager entry, etc.\n2.  Sidebar Panel Switch Buttons: Used to switch between workflow history queue, node library, model library, local user workflow browsing, etc.\n3.  Theme Switch Button: Quickly switch between ComfyUI’s default dark theme and light theme\n4.  Settings: Click to open the settings button\n5.  Canvas Menu: Provides zoom in, zoom out, and auto-fit operations for the ComfyUI canvas\n\n![ComfyUI Workspace](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui-new-interface-menu-bar.png) The image above shows the corresponding functions of the top menu bar, including common features, which we will explain in detail in the specific function usage section.\n\n![ComfyUI Sidebar Panel](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/side-panel.png) In the current ComfyUI, we provide four side panels with the following functions:\n\n1.  Workflow History Queue (Queue): All queue information for ComfyUI executing media content generation\n2.  Node Library: All nodes in ComfyUI, including `Comfy Core` and your installed custom nodes, can be found here\n3.  Model Library: Models in your local `ComfyUI/models` directory can be found here\n4.  Local User Workflows (Workflows): Your locally saved workflows can be found here\n\nCurrently, ComfyUI enables the new interface by default. If you prefer to use the old interface, you can click the **Settings gear icon** and then set `Use new menu` to `disabled` under `Comfy` —> `Menu` to switch to the old menu version.\n\nThe function annotations for the old menu interface are explained below: ![ComfyUI Old Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui-old-menu.png)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/interface/maskeditor",
  "markdown": "# Mask Editor - Create and Edit Masks in ComfyUI\n\nThe Mask Editor is a very useful feature in ComfyUI that allows users to create and edit masks within images without needing to use other applications. The Mask Editor is currently triggered through the `Load Image` node. After uploading an image, you can right-click on the node and select `Open in MaskEditor` from the menu to open the Mask Editor. ![ComfyUI Mask Editor](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/maskeditor/maskeditor.jpg) ![ComfyUI Mask Editor](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/maskeditor/maskeditor_ui.jpg) You can then click with your mouse on the image to create and edit masks.\n\n## Video Tutorial\n\nYour browser does not support the video tag.\n\n#### 0 reactions\n\nOn this page\n\n*   [Video Tutorial](#video-tutorial)"
},
{
  "url": "https://docs.comfy.org/troubleshooting/overview",
  "markdown": "# How to Troubleshoot and Solve ComfyUI Issues\n\n[\n\n## Custom Node Troubleshooting Guide\n\nCheck how to troubleshoot issues caused by custom nodes.\n\n\n\n](https://docs.comfy.org/troubleshooting/custom-node-issues)\n\n## Common Issues & Quick Fixes\n\nBefore diving into detailed troubleshooting, try these common solutions:\n\n### ComfyUI Won’t Start\n\n**Symptoms:** Application crashes on startup, black screen, or fails to load **Quick fixes:**\n\n1.  **Check system requirements** - Ensure your system meets the [minimum requirements](https://docs.comfy.org/installation/system_requirements)\n2.  **Update GPU drivers** - Download latest drivers from NVIDIA/AMD/Intel\n\n### Generation Fails or Produces Errors\n\n**Symptoms:** “Prompt execution failed” dialog with “Show report” button, workflow stops executing **Quick fixes:**\n\n1.  **Click “Show report”** - Read the detailed error message to identify the specific issue\n2.  **Check if it’s a custom node issue** - [Follow our custom node troubleshooting guide](https://docs.comfy.org/troubleshooting/custom-node-issues)\n3.  **Verify model files** - See [Models documentation](https://docs.comfy.org/development/core-concepts/models) for model setup\n4.  **Check VRAM usage** - Close other applications using GPU memory\n\n### Slow Performance\n\n**Symptoms:** Very slow generation times, system freezing, out of memory errors **Quick fixes:**\n\n1.  **Lower resolution/batch size** - Reduce image size or number of images\n2.  **Use memory optimization flags** - See performance optimization section below\n3.  **Close unnecessary applications** - Free up RAM and VRAM\n4.  **Check CPU/GPU usage** - Use Task Manager to identify bottlenecks\n\n**Performance Optimization Commands:** For low VRAM systems:\n\n```\n# Low VRAM mode (uses cpu for text encoder)\npython main.py --lowvram\n\n# CPU mode (very slow but works with any hardware, only use as absolute last resort)\npython main.py --cpu\n```\n\nFor better performance:\n\n```\n# Disable previews (saves VRAM and processing)\npython main.py --preview-method none\n\n# Use optimized attention mechanisms\npython main.py --use-pytorch-cross-attention\npython main.py --use-flash-attention\n\n# Async weight offloading\npython main.py --async-offload\n```\n\nFor memory management:\n\n```\n# Reserve specific VRAM amount for OS (in GB)\npython main.py --reserve-vram 2\n\n# Disable smart memory management\npython main.py --disable-smart-memory\n\n# Use different caching strategies\npython main.py --cache-none      # Less RAM usage, but slower\npython main.py --cache-lru 10    # Cache 10 results, faster\npython main.py --cache-classic   # Use the old style (aggressive) caching. \n```\n\n## Installation-Specific Issues\n\n### Desktop App Issues\n\nFor comprehensive desktop installation troubleshooting, see the [Desktop Installation Guide](https://docs.comfy.org/installation/desktop/windows).\n\n*   **Unsupported device**: ComfyUI Desktop Windows only supports NVIDIA GPUs with CUDA. Use [ComfyUI Portable](https://docs.comfy.org/installation/comfyui_portable_windows) or [manual installation](https://docs.comfy.org/installation/manual_install) for other GPUs\n*   **Installation fails**: Run installer as administrator, ensure at least 15GB disk space\n*   **Maintenance page**: Check [mirror settings](https://docs.comfy.org/installation/desktop/windows#mirror-settings) if downloads fail\n*   **Missing models**: Models are not copied during migration, only linked. Verify model paths\n\n### Manual Installation Issues\n\n**Python version conflicts:**\n\n```\n# Check Python version (3.9+ required, 3.12 recommended)\npython --version\n\n# Use virtual environment (recommended)\npython -m venv comfyui_env\nsource comfyui_env/bin/activate  # Linux/Mac\ncomfyui_env\\Scripts\\activate     # Windows\n```\n\n**Package installation failures:**\n\n```\n# Update pip first\npython -m pip install --upgrade pip\n\n# Install dependencies\npip install -r requirements.txt\n\n# For NVIDIA GPUs (CUDA 12.8)\npip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128\n\n# For AMD GPUs (Linux only - ROCm 6.3)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3\n```\n\n### Linux-Specific Issues\n\n**LD\\_LIBRARY\\_PATH errors:** Common symptoms:\n\n*   “libcuda.so.1: cannot open shared object file”\n*   “libnccl.so: cannot open shared object file”\n*   “ImportError: libnvinfer.so.X: cannot open shared object file”\n\n**Solutions:**\n\n1.  **Modern PyTorch installations (most common):**\n\n```\n# For virtual environments with NVIDIA packages\nexport LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# For conda environments\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# Or find your Python site-packages automatically\nPYTHON_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# You may also need other NVIDIA libraries\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cuda_runtime/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cublas/lib:$LD_LIBRARY_PATH\n```\n\n2.  **Find what libraries you have:**\n\n```\n# Check installed NVIDIA packages\npython -c \"import site; import os; nvidia_path=os.path.join(site.getsitepackages()[0], 'nvidia'); print('NVIDIA libs:', [d for d in os.listdir(nvidia_path) if os.path.isdir(os.path.join(nvidia_path, d))] if os.path.exists(nvidia_path) else 'Not found')\"\n\n# Find missing libraries that PyTorch needs\npython -c \"import torch; print(torch.__file__)\"\nldd $(python -c \"import torch; print(torch.__file__.replace('__init__.py', 'lib/libtorch_cuda.so'))\")\n```\n\n3.  **Set permanently for your environment:**\n\n```\n# For virtual environments, add to activation script\necho 'export LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> $VIRTUAL_ENV/bin/activate\n\n# For conda environments\nconda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# For global bashrc (adjust Python version as needed)\necho 'export LD_LIBRARY_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> ~/.bashrc\n```\n\n4.  **Alternative: Use ldconfig:**\n\n```\n# Check current library cache\nldconfig -p | grep cuda\nldconfig -p | grep nccl\n\n# If missing, add library paths (requires root)\nsudo echo \"/usr/local/cuda/lib64\" > /etc/ld.so.conf.d/cuda.conf\nsudo ldconfig\n```\n\n5.  **Debug library loading:**\n\n```\n# Verbose library loading to see what's missing\nLD_DEBUG=libs python main.py 2>&1 | grep \"looking for\"\n\n# Check PyTorch CUDA availability\npython -c \"import torch; print('CUDA available:', torch.cuda.is_available()); print('CUDA version:', torch.version.cuda)\"\n```\n\nFor comprehensive model troubleshooting including architecture mismatches, missing models, and loading errors, see the dedicated [Model Issues](https://docs.comfy.org/troubleshooting/model-issues) page.\n\n## Network & API Issues\n\n### API Nodes Not Working\n\n**Symptoms:** API calls fail, timeout errors, quota exceeded **Solutions:**\n\n1.  **Check API key validity** - Verify keys in [user settings](https://docs.comfy.org/interface/user)\n2.  **Check account credits** - Ensure sufficient [API credits](https://docs.comfy.org/interface/credits)\n3.  **Verify internet connection** - Test with other online services\n4.  **Check service status** - Provider may be experiencing downtime\n\n### Connection Issues\n\n**Symptoms:** “Failed to connect to server”, timeout errors **Solutions:**\n\n1.  **Check firewall settings** - Allow ComfyUI through firewall\n2.  **Try different port** - Default is 8188, try 8189 or 8190\n3.  **Disable VPN temporarily** - VPN may be blocking connections\n4.  **Check proxy settings** - Disable proxy if not required\n\n### Frontend Issues\n\n**“Frontend or Templates Package Not Updated”:**\n\n```\n# After updating ComfyUI via Git, update frontend dependencies\npip install -r requirements.txt\n```\n\n**“Can’t Find Custom Node”:**\n\n*   Disable node validation in ComfyUI settings\n\n**“Error Toast About Workflow Failing Validation”:**\n\n*   Disable workflow validation in settings temporarily\n*   Report the issue to the ComfyUI team\n\n**Login Issues When Not on Localhost:**\n\n*   Normal login only works when accessing from localhost\n*   For LAN/remote access: Generate API key at [platform.comfy.org/login](https://platform.comfy.org/login)\n*   Use API key in login dialog or with `--api-key` command line argument\n\n## Hardware-Specific Issues\n\n### NVIDIA GPU Issues\n\n**“Torch not compiled with CUDA enabled” error:**\n\n```\n# First uninstall torch\npip uninstall torch\n\n# Install stable PyTorch with CUDA 12.8\npip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128\n\n# For nightly builds (might have performance improvements)\npip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n\n# Verify CUDA support\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\n**GPU not detected:**\n\n```\n# Check if GPU is visible\nnvidia-smi\n\n# Check driver version and CUDA compatibility\nnvidia-smi --query-gpu=driver_version --format=csv\n```\n\n### AMD GPU Issues\n\n**ROCm support (Linux only):**\n\n```\n# Install stable ROCm PyTorch (6.3.1 at the time of writing)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3\n\n# For nightly builds (ROCm 6.4 at the time of writing), which might have performance improvements)\npip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4\n```\n\n**Unsupported AMD GPUs:**\n\n```\n# For RDNA2 or older (6700, 6600)\nHSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py\n\n# For RDNA3 cards (7600)\nHSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py\n```\n\n**Performance optimization:**\n\n```\n# Enable experimental memory efficient attention (no longer necessary with PyTorch 2.4)\nTORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention\n\n# Enable tunable operations (slow first run, but faster subsequent runs)\nPYTORCH_TUNABLEOP_ENABLED=1 python main.py\n```\n\n### Apple Silicon (M1/M2/M3) Issues\n\n**MPS backend setup:**\n\n```\n# Install PyTorch nightly for Apple Silicon\n# Follow Apple's guide: https://developer.apple.com/metal/pytorch/\n\n# Check MPS availability\npython -c \"import torch; print(torch.backends.mps.is_available())\"\n\n# Launch ComfyUI\npython main.py\n```\n\n**If MPS causes issues:**\n\n```\n# Force CPU mode\npython main.py --cpu\n\n# With memory optimization\npython main.py --force-fp16 --cpu\n```\n\n### Intel GPU Issues\n\n**Option 1: Native PyTorch XPU support (Windows/Linux):**\n\n```\n# Install PyTorch nightly with XPU support\npip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu\n\n# Launch ComfyUI\npython main.py\n```\n\n**Option 2: Intel Extension for PyTorch (IPEX):**\n\n```\n# For Intel Arc A-Series Graphics\nconda install libuv\npip install torch==2.3.1.post0+cxx11.abi torchvision==0.18.1.post0+cxx11.abi torchaudio==2.3.1.post0+cxx11.abi intel-extension-for-pytorch==2.3.110.post0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n```\n\n## Getting Help & Reporting Bugs\n\n### Before Reporting a Bug\n\n1.  **Check if it’s a known issue:**\n    *   Search [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n    *   Check [ComfyUI Forum](https://forum.comfy.org/)\n    *   Review [Discord discussions](https://discord.com/invite/comfyorg)\n2.  **Try basic troubleshooting:**\n    *   Test with [default workflow](https://docs.comfy.org/get_started/first_generation)\n    *   Disable all custom nodes (see [custom node troubleshooting](https://docs.comfy.org/troubleshooting/custom-node-issues))\n    *   Check console/terminal for error messages\n    *   If using comfy-cli, try updating: `comfy node update all`\n\n### How to Report Bugs Effectively\n\n#### For ComfyUI Core Issues\n\n**Where to report:** [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n\n#### For Desktop App Issues\n\n**Where to report:** [Desktop GitHub Issues](https://github.com/Comfy-Org/desktop/issues)\n\n#### For Frontend Issues\n\n**Where to report:** [Frontend GitHub Issues](https://github.com/Comfy-Org/ComfyUI_frontend/issues)\n\n#### For Custom Node Issues\n\n**Where to report:** Contact the specific custom node developer\n\n### Required Information\n\nWhen reporting any issue, include:\n\n*   **Official Forum:** [forum.comfy.org](https://forum.comfy.org/)\n*   **Discord:** [ComfyUI Discord Server](https://discord.com/invite/comfyorg)\n*   **Reddit:** [r/comfyui](https://reddit.com/r/comfyui)\n*   **YouTube:** [ComfyUI Tutorials](https://www.youtube.com/@comfyorg)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/troubleshooting/model-issues",
  "markdown": "# How to Troubleshoot and Solve ComfyUI Model Issues\n\n## Model Architecture Mismatch\n\n**Symptoms:** Tensor dimension errors during generation, especially during VAE decode stage **Common error messages:**\n\n*   `Given groups=1, weight of size [64, 4, 3, 3], expected input[1, 16, 128, 128] to have 4 channels, but got 16 channels instead`\n*   `Given groups=1, weight of size [4, 4, 1, 1], expected input[1, 16, 144, 112] to have 4 channels, but got 16 channels instead`\n*   `Given groups=1, weight of size [320, 4, 3, 3], expected input[2, 16, 192, 128] to have 4 channels, but got 16 channels instead`\n*   `The size of tensor a (49) must match the size of tensor b (16) at non-singleton dimension 1`\n*   `Tensors must have same number of dimensions: got 2 and 3`\n*   `mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)`\n\n**Root cause:** Using models from different architecture families together\n\n### Solutions\n\n1.  **Verify model family compatibility:**\n    *   **Flux models** use 16-channel latent space with dual text encoder conditioning (CLIP-L + T5-XXL)\n    *   **SD1.5 models** use 4-channel latent space with single CLIP ViT-L/14 text encoder\n    *   **SDXL models** use 4-channel latent space with dual text encoders (CLIP ViT-L/14 + OpenCLIP ViT-bigG/14)\n    *   **SD3 models** use 16-channel latent space with triple text encoder conditioning (CLIP-L + OpenCLIP bigG + T5-XXL)\n    *   **ControlNet models** must match the architecture of the base checkpoint (SD1.5 ControlNets only work with SD1.5 checkpoints, SDXL ControlNets only work with SDXL checkpoints, etc.)\n2.  **Common mismatch scenarios and fixes:** **Flux + wrong VAE:**\n    \n    ```\n    Problem: Using taesd or sdxl_vae.safetensors with Flux checkpoint\n    Fix: Use ae.safetensors (Flux VAE) from Hugging Face Flux releases\n    ```\n    \n    **Flux + incorrect CLIP configuration:**\n    \n    ```\n    Problem: Using t5xxl_fp8_e4m3fn.safetensors in both CLIP slots of DualClipLoader\n    Fix: Use t5xxl_fp8_e4m3fn.safetensors in one slot and clip_l.safetensors in the other\n    ```\n    \n    **ControlNet architecture mismatch:**\n    \n    ```\n    Problem: SD1.5 ControlNet with SDXL checkpoint (or vice versa)\n    Error: \"mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)\"\n    Fix: Use ControlNet models designed for your checkpoint architecture\n         - SD1.5 checkpoints require SD1.5 ControlNets\n         - SDXL checkpoints require SDXL ControlNets\n    ```\n    \n3.  **Quick diagnostics:**\n    \n    ```\n    # Check if error occurs at VAE decode stage\n    # Look for \"expected input[X, Y, Z] to have N channels, but got M channels\"\n    # Y value indicates channel count: 4 = SD models, 16 = Flux models\n    ```\n    \n4.  **Prevention strategies:**\n    *   Keep all workflow models within the same architecture family\n    *   Download complete model packages from same source/release (often all in a Hugging Face repo)\n    *   When trying new models, start with the template workflows or official ComfyUI workflow examples before customizing\n\n## Missing Models Error\n\n**Example error message:**\n\n```\nPrompt execution failed\nPrompt outputs failed validation:\nCheckpointLoaderSimple:\n- Value not in list: ckpt_name: 'model-name.safetensors' not in []\n```\n\n### Solutions\n\n1.  **Download required models:**\n    *   Use ComfyUI Manager to auto-download models\n    *   Verify models are in correct subfolders\n2.  **Check model paths:**\n    *   **Checkpoints**: `models/checkpoints/`\n    *   **VAE**: `models/vae/`\n    *   **LoRA**: `models/loras/`\n    *   **ControlNet**: `models/controlnet/`\n    *   **Embeddings**: `models/embeddings/`\n3.  **Share models between UIs or use custom paths:**\n    *   See [ComfyUI Model Sharing and Custom Model Directory Configuration](https://docs.comfy.org/installation/comfyui_portable_windows#2-comfyui-model-sharing-and-custom-model-directory-configuration) for detailed instructions\n    *   Edit `extra_model_paths.yaml` file to add custom model directories\n\n### Model Search Path Configuration\n\nIf you have models in custom locations, see the detailed guide for [ComfyUI Model Sharing and Custom Model Directory Configuration](https://docs.comfy.org/installation/comfyui_portable_windows#2-comfyui-model-sharing-and-custom-model-directory-configuration) to configure ComfyUI to find them.\n\n## Model Loading Errors\n\n**Error message:** “Error while deserializing header”\n\n### Solutions\n\n1.  **Re-download the model** - File may be corrupted during download\n2.  **Check available disk space** - Ensure enough space for model loading (models can be 2-15GB+)\n3.  **Check file permissions** - Ensure ComfyUI can read the model files\n4.  **Test with different model** - Verify if issue is model-specific or system-wide\n\n## Model Performance Issues\n\n### Slow Model Loading\n\n**Symptoms:** Long delays when switching models or starting generation **Solutions:**\n\n1.  **Use faster storage:**\n    *   Move models to SSD if using HDD\n    *   Use NVMe SSD for best performance\n2.  **Adjust caching settings:**\n    \n    ```\n    python main.py --cache-classic       # Use the old style (aggressive) caching. \n    python main.py --cache-lru 10         # Increase size of LRU cache\n    ```\n    \n\n### Memory Issues with Large Models\n\n**“RuntimeError: CUDA out of memory”:**\n\n```\n# Progressive memory reduction\npython main.py --lowvram          # First try\npython main.py --novram           # If lowvram insufficient  \npython main.py --cpu              # Last resort\n```\n\n**Model-specific memory optimization:**\n\n```\n# Force lower precision\npython main.py --force-fp16\n\n# Reduce attention memory usage\npython main.py --use-pytorch-cross-attention\n```"
},
{
  "url": "https://docs.comfy.org/troubleshooting/custom-node-issues",
  "markdown": "# How to Troubleshoot and Solve ComfyUI Issues\n\nHere is the overall approach for troubleshooting custom node issues:\n\n## How to disable all custom nodes?\n\nStart ComfyUI Desktop with custom nodes disabled from the settings menu ![Settings menu - Disable custom nodes](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/troubleshooting/desktop-diable-custom-node.jpg) or run the server manually:\n\n```\ncd path/to/your/comfyui\npython main.py --disable-all-custom-nodes\n```\n\n**Results:**\n\n*   ✅ **Issue disappears**: A custom node is causing the problem → Continue to Step 2\n*   ❌ **Issue persists**: Not a custom node issue → [Report the issue](#reporting-issues)\n\n## What is Binary Search?\n\nIn this document, we will introduce the binary search approach for troubleshooting custom node issues, which involves checking half of the custom nodes at a time until we locate the problematic node. Please refer to the flowchart below for the specific approach - enable half of the currently disabled nodes each time and check if the issue appears, until we identify which custom node is causing the issue\n\n## Two Troubleshooting Methods\n\nIn this document, we categorize custom nodes into two types for troubleshooting: ![Custom node types](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/troubleshooting/custom_node_type.jpg)\n\n*   A: Custom nodes with frontend extensions\n*   B: Regular custom nodes\n\nLet’s first understand the potential issues and causes for different types of custom nodes:\n\nFor custom nodes, we can prioritize troubleshooting those with frontend extensions, as they cause the most issues. Their main conflicts arise from incompatibilities with ComfyUI frontend version updates.Common issues include:\n\n*   Workflows not executing\n*   Some nodes can’t show preview images(such as save image node)\n*   Misaligned UI elements\n*   Unable to access ComfyUI frontend\n*   Completely broken UI or blank screen\n*   Unable to communicate normally with ComfyUI backend\n*   Node connections not working properly\n*   And more\n\nCommon causes for these issues:\n\n*   Frontend modifications during updates that custom nodes haven’t adapted to yet\n*   Users updating ComfyUI without synchronously upgrading custom nodes, even though authors have released compatible versions\n*   Authors stopping maintenance, leading to incompatibility between custom node extensions and the ComfyUI frontend\n\n## Using Binary Search for Troubleshooting\n\nAmong these two different types of custom node issues, conflicts between custom node frontend extensions and ComfyUI are more common. We’ll prioritize troubleshooting these nodes first. Here’s the overall troubleshooting approach:\n\n### 1\\. Troubleshooting the Custom Nodes’ Frontend Extensions\n\nUsing this method, you don’t need to restart ComfyUI multiple times - just reload ComfyUI after enabling/disabling custom node frontend extensions. Plus, your troubleshooting scope is limited to nodes with frontend extensions, which greatly narrows down the search range.\n\n### 2\\. General Custom Node Troubleshooting\n\n## How to Fix the Issue\n\nOnce you’ve identified the problematic custom node:\n\n### Option 1: Update the Node\n\n1.  Check if there’s an update available in ComfyUI Manager\n2.  Update the node and test again\n\n### Option 2: Replace the Node\n\n1.  Look for alternative custom nodes with similar functionality\n2.  Check the [ComfyUI Registry](https://registry.comfy.org/) for alternatives\n\n### Option 3: Report the Issue\n\nContact the custom node developer:\n\n1.  Find the node’s GitHub repository\n2.  Create an issue with:\n    *   Your ComfyUI version\n    *   Error messages/logs\n    *   Steps to reproduce\n    *   Your operating system\n\n### Option 4: Remove or Disable the Node\n\nIf no fix is available and you don’t need the functionality:\n\n1.  Remove the problematic node from `custom_nodes/` or disable it in the ComfyUI Manager interface\n2.  Restart ComfyUI\n\n## Reporting Issues\n\nIf the issue isn’t caused by custom nodes, refer to the general [troubleshooting overview](https://docs.comfy.org/troubleshooting/overview) for other common problems.\n\n### For Custom Node-Specific Issues\n\nContact the custom node developer:\n\n*   Find the node’s GitHub repository\n*   Create an issue with your ComfyUI version, error messages, reproduction steps, and OS\n*   Check the node’s documentation and Issues page for known issues\n\n### For ComfyUI Core Issues\n\n*   **GitHub**: [ComfyUI Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n*   **Forum**: [Official ComfyUI Forum](https://forum.comfy.org/)\n\n### For Desktop App Issues\n\n*   **GitHub**: [ComfyUI Desktop Issues](https://github.com/Comfy-Org/desktop/issues)\n\n### For Frontend Issues\n\n*   **GitHub**: [ComfyUI Frontend Issues](https://github.com/Comfy-Org/ComfyUI_frontend/issues)"
},
{
  "url": "https://docs.comfy.org/community/contributing",
  "markdown": "# Contributing - ComfyUI\n\n##### Get Started\n\n*   [](https://docs.comfy.org/)\n\n*   [](https://docs.comfy.org/get_started/first_generation)\n*   [](https://docs.comfy.org/changelog)\n\n##### Basic Concepts\n\n*   [](https://docs.comfy.org/development/core-concepts/workflow)\n*   [](https://docs.comfy.org/development/core-concepts/nodes)\n*   [](https://docs.comfy.org/development/core-concepts/custom-nodes)\n*   [](https://docs.comfy.org/development/core-concepts/properties)\n*   [](https://docs.comfy.org/development/core-concepts/links)\n*   [](https://docs.comfy.org/development/core-concepts/models)\n*   [](https://docs.comfy.org/development/core-concepts/dependencies)\n\n##### Interface Guide\n\n*   [](https://docs.comfy.org/interface/overview)\n*   [](https://docs.comfy.org/interface/maskeditor)\n\n##### Tutorials\n\n##### Troubleshooting\n\n*   [](https://docs.comfy.org/troubleshooting/overview)\n*   [](https://docs.comfy.org/troubleshooting/model-issues)\n*   [](https://docs.comfy.org/troubleshooting/custom-node-issues)\n\n##### Community\n\n*   [](https://docs.comfy.org/community/contributing)\n\n### Create a PR\n\nFork the [repo](https://github.com/comfyanonymous/ComfyUI), and create a PR."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/overview",
  "markdown": "# ComfyUI Built-in Nodes - ComfyUI\n\nBuilt-in nodes are ComfyUI’s default nodes. They are core functionalities of ComfyUI that you can use without installing any third-party custom node packages.\n\n## About built-in node document\n\nWe have now added built-in node help documentation, so the content of this section is periodically synced from [this repo](https://github.com/Comfy-Org/embedded-docs). We will update the content manually once a week.\n\n## Contribute\n\nIf you find any errors in the content, or want to contribute missing content, please submit an issue or PR in [this repo](https://github.com/Comfy-Org/embedded-docs) to help us improve."
},
{
  "url": "https://docs.comfy.org/development/overview",
  "markdown": "# Overview - ComfyUI\n\nComfyUI is a powerful GenAI inference engine that can be used to run AI models locally, create workflows, develop custom nodes, and be deployed as a server. ComfyUI’s key capabilities are:\n\n*   **[Creating Workflows](https://docs.comfy.org/development/core-concepts/workflow)**: Workflows are a way to orchestrate AI models and automate tasks. They are a series of nodes that are connected together to form a pipeline.\n*   **[Custom Nodes](https://docs.comfy.org/custom-nodes/overview)**: Custom nodes can be written by anyone to extend the capabilities of ComfyUI for your own use. Nodes are written in Python and are published by the community.\n*   **Extensions**: Extensions are 3rd party applications that improve the UI of ComfyUI.\n*   **[Deployment](https://docs.comfy.org/development/comfyui-server/comms_overview)**: ComfyUI can be deployed in your own environment as an API endpoint. \\[Learn more\\]"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/text-to-image",
  "markdown": "# ComfyUI Text to Image Workflow\n\nThis guide aims to introduce you to ComfyUI’s text-to-image workflow and help you understand the functionality and usage of various ComfyUI nodes. In this document, we will:\n\n*   Complete a text-to-image workflow\n*   Gain a basic understanding of diffusion model principles\n*   Learn about the functions and roles of workflow nodes\n*   Get an initial understanding of the SD1.5 model\n\nWe’ll start by running a text-to-image workflow, followed by explanations of related concepts. Please choose the relevant sections based on your needs.\n\n## About Text to Image\n\n**Text to Image** is a fundamental process in AI art generation that creates images from text descriptions, with **diffusion models** at its core. The text-to-image process requires the following elements:\n\n*   **Artist:** The image generation model\n*   **Canvas:** The latent space\n*   **Image Requirements (Prompts):** Including positive prompts (elements you want in the image) and negative prompts (elements you don’t want)\n\nThis text-to-image generation process can be simply understood as telling your requirements (positive and negative prompts) to an **artist (the image model)**, who then creates what you want based on these requirements.\n\n## ComfyUI Text to Image Workflow Example Guide\n\n### 1\\. Preparation\n\nEnsure you have at least one SD1.5 model file in your `ComfyUI/models/checkpoints` folder, such as [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) If you haven’t installed it yet, please refer to the model installation section in [Getting Started with ComfyUI AI Art Generation](https://docs.comfy.org/get_started/first_generation).\n\n### 2\\. Loading the Text to Image Workflow\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![ComfyUI-Text to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/text-to-image-workflow.png)\n\n### 3\\. Loading the Model and Generating Your First Image\n\nAfter installing the image model, follow the steps in the image below to load the model and generate your first image ![Image Generation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg) Follow these steps according to the image numbers:\n\n1.  In the **Load Checkpoint** node, use the arrows or click the text area to ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected, and the left/right arrows don’t show **null** text\n2.  Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute image generation\n\nAfter the process completes, you should see the resulting image in the **Save Image** node interface, which you can right-click to save locally ![ComfyUI First Image Generation Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)\n\n### 4\\. Start Experimenting\n\nTry modifying the text in the **CLIP Text Encoder** ![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg) The `Positive` connection to the KSampler node represents positive prompts, while the `Negative` connection represents negative prompts Here are some basic prompting principles for the SD1.5 model:\n\n*   Use English whenever possible\n*   Separate prompts with English commas `,`\n*   Use phrases rather than long sentences\n*   Use specific descriptions\n*   Use expressions like `(golden hour:1.2)` to increase the weight of specific keywords, making them more likely to appear in the image. `1.2` is the weight, `golden hour` is the keyword\n*   Use keywords like `masterpiece, best quality, 4k` to improve generation quality\n\nHere are several prompt examples you can try, or use your own prompts for generation: **1\\. Anime Style** Positive prompts:\n\n```\nanime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details\n\nmasterpiece, best quality, 4k\n```\n\nNegative prompts:\n\n```\nlow quality, blurry, deformed hands, extra fingers\n```\n\n**2\\. Realistic Style** Positive prompts:\n\n```\n(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), \nfull body, soft cinematic lighting, (golden hour:1.2), \n(fujifilm XT4:1.1), shallow depth of field, \n(skin texture details:1.3), (film grain:1.1), \ngentle wind flow, warm color grading, (perfect facial symmetry:1.3)\n```\n\nNegative prompts:\n\n```\n(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)\n```\n\n**3\\. Specific Artist Style** Positive prompts:\n\n```\nfantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style\n```\n\nNegative prompts:\n\n```\nblurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting\n```\n\n## Text to Image Working Principles\n\nThe entire text-to-image process can be understood as a **reverse diffusion process**. The [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) we downloaded is a pre-trained model that can **generate target images from pure Gaussian noise**. We only need to input our prompts, and it can generate target images through denoising random noise.\n\nWe need to understand two concepts:\n\n1.  **Latent Space:** Latent Space is an abstract data representation method in diffusion models. Converting images from pixel space to latent space reduces storage space and makes it easier to train diffusion models and reduce denoising complexity. It’s like architects using blueprints (latent space) for design rather than designing directly on the building (pixel space), maintaining structural features while significantly reducing modification costs\n2.  **Pixel Space:** Pixel Space is the storage space for images, which is the final image we see, used to store pixel values.\n\nIf you want to learn more about diffusion models, you can read these papers:\n\n*   [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)\n*   [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)\n*   [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)\n\n## ComfyUI Text to Image Workflow Node Explanation\n\n![ComfyUI Text to Image Workflow Explanation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/text-image-workflow.jpg)\n\n### A. Load Checkpoint Node\n\n![Load Checkpoint](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_checkpoint.jpg) This node is typically used to load the image generation model. A `checkpoint` usually contains three components: `MODEL (UNet)`, `CLIP`, and `VAE`\n\n*   `MODEL (UNet)`: The UNet model responsible for noise prediction and image generation during the diffusion process\n*   `CLIP`: The text encoder that converts our text prompts into vectors that the model can understand, as the model cannot directly understand text prompts\n*   `VAE`: The Variational AutoEncoder that converts images between pixel space and latent space, as diffusion models work in latent space while our images are in pixel space\n\n### B. Empty Latent Image Node\n\n![Empty Latent Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/empty_latent_image.jpg) Defines a latent space that outputs to the KSampler node. The Empty Latent Image node constructs a **pure noise latent space** You can think of its function as defining the canvas size, which determines the dimensions of our final generated image\n\n### C. CLIP Text Encoder Node\n\n![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg) Used to encode prompts, which are your requirements for the image\n\n*   The `Positive` condition input connected to the KSampler node represents positive prompts (elements you want in the image)\n*   The `Negative` condition input connected to the KSampler node represents negative prompts (elements you don’t want in the image)\n\nThe prompts are encoded into semantic vectors by the `CLIP` component from the `Load Checkpoint` node and output as conditions to the KSampler node\n\n### D. KSampler Node\n\n![KSampler](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg) The **KSampler** is the core of the entire workflow, where the entire noise denoising process occurs, ultimately outputting a latent space image\n\nHere’s an explanation of the KSampler node parameters:\n\n| Parameter Name | Description | Function |\n| --- | --- | --- |\n| **model** | Diffusion model used for denoising | Determines the style and quality of generated images |\n| **positive** | Positive prompt condition encoding | Guides generation to include specified elements |\n| **negative** | Negative prompt condition encoding | Suppresses unwanted content |\n| **latent\\_image** | Latent space image to be denoised | Serves as the input carrier for noise initialization |\n| **seed** | Random seed for noise generation | Controls generation randomness |\n| **control\\_after\\_generate** | Seed control mode after generation | Determines seed variation pattern in batch generation |\n| **steps** | Number of denoising iterations | More steps mean finer details but longer processing time |\n| **cfg** | Classifier-free guidance scale | Controls prompt constraint strength (too high leads to overfitting) |\n| **sampler\\_name** | Sampling algorithm name | Determines the mathematical method for denoising path |\n| **scheduler** | Scheduler type | Controls noise decay rate and step size allocation |\n| **denoise** | Denoising strength coefficient | Controls noise strength added to latent space, 0.0 preserves original input features, 1.0 is complete noise |\n\nIn the KSampler node, the latent space uses `seed` as an initialization parameter to construct random noise, and semantic vectors `Positive` and `Negative` are input as conditions to the diffusion model Then, based on the number of denoising steps specified by the `steps` parameter, denoising is performed. Each denoising step uses the denoising strength coefficient specified by the `denoise` parameter to denoise the latent space and generate a new latent space image\n\n### E. VAE Decode Node\n\n![VAE Decode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/vae_decode.jpg) Converts the latent space image output from the **KSampler** into a pixel space image\n\n### F. Save Image Node\n\n![Save Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/save_image.jpg) Previews and saves the decoded image from latent space to the local `ComfyUI/output` folder\n\n## Introduction to SD1.5 Model\n\n**SD1.5 (Stable Diffusion 1.5)** is an AI image generation model developed by [Stability AI](https://stability.ai/). It’s the foundational version of the Stable Diffusion series, trained on **512×512** resolution images, making it particularly good at generating images at this resolution. With a size of about 4GB, it runs smoothly on **consumer-grade GPUs (e.g., 6GB VRAM)**. Currently, SD1.5 has a rich ecosystem, supporting various plugins (like ControlNet, LoRA) and optimization tools. As a milestone model in AI art generation, SD1.5 remains the best entry-level choice thanks to its open-source nature, lightweight architecture, and rich ecosystem. Although newer versions like SDXL/SD3 have been released, its value for consumer-grade hardware remains unmatched.\n\n### Basic Information\n\n*   **Release Date**: October 2022\n*   **Core Architecture**: Based on Latent Diffusion Model (LDM)\n*   **Training Data**: LAION-Aesthetics v2.5 dataset (approximately 590M training steps)\n*   **Open Source Features**: Fully open-source model/code/training data\n\n### Advantages and Limitations\n\nModel Advantages:\n\n*   Lightweight: Small size, only about 4GB, runs smoothly on consumer GPUs\n*   Low Entry Barrier: Supports a wide range of plugins and optimization tools\n*   Mature Ecosystem: Extensive plugin and tool support\n*   Fast Generation: Smooth operation on consumer GPUs\n\nModel Limitations:\n\n*   Detail Handling: Hands/complex lighting prone to distortion\n*   Resolution Limits: Quality degrades for direct 1024x1024 generation\n*   Prompt Dependency: Requires precise English descriptions for control"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher",
  "markdown": "# Retrieve permissions the user has for a given publisher\n\n#### Path Parameters\n\n#### Response\n\nThe response is of type `object`.\n\nRetrieve permissions the user has for a given publisher"
},
{
  "url": "https://docs.comfy.org/installation/system_requirements",
  "markdown": "# System Requirements - ComfyUI\n\nIn this guide, we will introduce the system requirements for installing ComfyUI. Due to frequent updates of ComfyUI, this document may not be updated in a timely manner. Please refer to the relevant instructions in [ComfyUI](https://github.com/comfyanonymous/ComfyUI). Regardless of which version of ComfyUI you use, it runs in a separate Python environment.\n\n### System Requirements\n\nCurrently, we support the following operating systems:\n\n*   Windows\n*   Linux\n*   macOS (supports Apple Silicon M1/M2)\n\nYou can refer to the following sections to learn about the installation methods for different systems and versions of ComfyUI. In the installation of different versions, we have simply described the system requirements.\n\n### Python Version\n\n*   Recommended Python 3.12\n*   Supports Python 3.13 (some custom nodes may not be compatible)\n\n### Supported Hardware\n\n*   NVIDIA GPU\n*   AMD GPU\n*   Intel GPU (includes Arc series, supports IPEX)\n*   Apple Silicon (M1/M2)\n*   Ascend NPU\n*   Cambricon MLU\n*   CPU (can use the —cpu parameter, slower)\n\nPlease refer to the [ComfyUI Windows and Linux manual installation section](https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#manual-install-windows-linux) for detailed installation steps.\n\n### Dependencies\n\n*   Install PyTorch\n*   Install all dependencies in the requirements.txt of ComfyUI\n\n[\n\n## Manual Installation\n\nPlease refer to the manual installation section for detailed installation steps.\n\n\n\n](https://docs.comfy.org/installation/manual_install)"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/publish-a-new-version-of-a-node",
  "markdown": "# Publish a new version of a node\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/publishers/{publisherId}/nodes/{nodeId}/versions \\\n  --header 'Authorization: Bearer <token>' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n  \"node\": {\n    \"author\": \"<string>\",\n    \"banner_url\": \"<string>\",\n    \"category\": \"<string>\",\n    \"created_at\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"downloads\": 123,\n    \"github_stars\": 123,\n    \"icon\": \"<string>\",\n    \"id\": \"<string>\",\n    \"latest_version\": {\n      \"changelog\": \"<string>\",\n      \"comfy_node_extract_status\": \"<string>\",\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"dependencies\": [\n        \"<string>\"\n      ],\n      \"deprecated\": true,\n      \"downloadUrl\": \"<string>\",\n      \"id\": \"<string>\",\n      \"node_id\": \"<string>\",\n      \"status\": \"NodeVersionStatusActive\",\n      \"status_reason\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"version\": \"<string>\"\n    },\n    \"license\": \"<string>\",\n    \"name\": \"<string>\",\n    \"preempted_comfy_node_names\": [\n      \"<string>\"\n    ],\n    \"publisher\": {\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"id\": \"<string>\",\n      \"logo\": \"<string>\",\n      \"members\": [\n        {\n          \"id\": \"<string>\",\n          \"role\": \"<string>\",\n          \"user\": {\n            \"email\": \"<string>\",\n            \"id\": \"<string>\",\n            \"name\": \"<string>\"\n          }\n        }\n      ],\n      \"name\": \"<string>\",\n      \"source_code_repo\": \"<string>\",\n      \"status\": \"PublisherStatusActive\",\n      \"support\": \"<string>\",\n      \"website\": \"<string>\"\n    },\n    \"rating\": 123,\n    \"repository\": \"<string>\",\n    \"search_ranking\": 123,\n    \"status\": \"NodeStatusActive\",\n    \"status_detail\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"tags\": [\n      \"<string>\"\n    ],\n    \"translations\": {}\n  },\n  \"node_version\": {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  },\n  \"personal_access_token\": \"<string>\"\n}'\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/update-changelog-and-deprecation-status-of-a-node-version",
  "markdown": "# Update changelog and deprecation status of a node version\n\n```\n{\n  \"changelog\": \"<string>\",\n  \"comfy_node_extract_status\": \"<string>\",\n  \"createdAt\": \"2023-11-07T05:31:56Z\",\n  \"dependencies\": [\n    \"<string>\"\n  ],\n  \"deprecated\": true,\n  \"downloadUrl\": \"<string>\",\n  \"id\": \"<string>\",\n  \"node_id\": \"<string>\",\n  \"status\": \"NodeVersionStatusActive\",\n  \"status_reason\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"version\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/unpublish-delete-a-specific-version-of-a-node",
  "markdown": "# Unpublish (delete) a specific version of a node\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Response\n\nVersion unpublished (deleted) successfully\n\n#### 0 reactions\n\nUnpublish (delete) a specific version of a node"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher-1",
  "markdown": "# Retrieve permissions the user has for a given publisher\n\n#### Path Parameters\n\n#### Response\n\n#### 0 reactions\n\n[Sign in](https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fretrieve-permissions-the-user-has-for-a-given-publisher-1) to add your reaction.\n\nRetrieve permissions the user has for a given publisher"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/delete-a-specific-personal-access-token",
  "markdown": "# Delete a specific personal access token\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Response\n\nToken deleted successfully\n\nDelete a specific personal access token"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/get-information-about-the-calling-user",
  "markdown": "# Get information about the calling user.\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Response\n\nThe response is of type `object`.\n\nGet information about the calling user."
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-all-publishers-for-a-given-user",
  "markdown": "# Retrieve all publishers for a given user\n\n```\n[\n  {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  }\n]\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-a-new-personal-access-token",
  "markdown": "# Create a new personal access token\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n\\[Output Only\\]The date and time the token was created.\n\nOptional. A more detailed description of the token's intended use.\n\nUnique identifier for the GitCommit\n\nRequired. The name of the token. Can be a simple description.\n\n\\[Output Only\\]. The personal access token. Only returned during creation.\n\n#### Response\n\nToken created successfully\n\nThe newly created personal access token.\n\n#### 0 reactions\n\nCreate a new personal access token"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/list-all-node-versions-given-some-filters",
  "markdown": "# List all node versions given some filters.\n\n```\n{\n  \"page\": 123,\n  \"pageSize\": 123,\n  \"total\": 123,\n  \"totalPages\": 123,\n  \"versions\": [\n    {\n      \"changelog\": \"<string>\",\n      \"comfy_node_extract_status\": \"<string>\",\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"dependencies\": [\n        \"<string>\"\n      ],\n      \"deprecated\": true,\n      \"downloadUrl\": \"<string>\",\n      \"id\": \"<string>\",\n      \"node_id\": \"<string>\",\n      \"status\": \"NodeVersionStatusActive\",\n      \"status_reason\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"version\": \"<string>\"\n    }\n  ]\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/create-video-to-video-prompt",
  "markdown": "# Create Video to Video Prompt\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/proxy/moonvalley/prompts/video-to-video \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n  \"control_type\": \"motion_control\",\n  \"inference_params\": {\n    \"add_quality_guidance\": true,\n    \"caching_coefficient\": 0.3,\n    \"caching_cooldown\": 3,\n    \"caching_warmup\": 3,\n    \"clip_value\": 3,\n    \"conditioning_frame_index\": 0,\n    \"cooldown_steps\": 36,\n    \"guidance_scale\": 15,\n    \"negative_prompt\": \"<string>\",\n    \"seed\": 123,\n    \"shift_value\": 3,\n    \"steps\": 80,\n    \"use_guidance_schedule\": true,\n    \"use_negative_prompts\": false,\n    \"use_timestep_transform\": true,\n    \"warmup_steps\": 24\n  },\n  \"prompt_text\": \"<string>\",\n  \"video_url\": \"<string>\",\n  \"webhook_url\": \"<string>\"\n}'\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/resize-a-video",
  "markdown": "# Resize a video - ComfyUI\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/proxy/moonvalley/prompts/video-to-video/resize \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n  \"control_type\": \"motion_control\",\n  \"inference_params\": {\n    \"add_quality_guidance\": true,\n    \"caching_coefficient\": 0.3,\n    \"caching_cooldown\": 3,\n    \"caching_warmup\": 3,\n    \"clip_value\": 3,\n    \"conditioning_frame_index\": 0,\n    \"cooldown_steps\": 36,\n    \"guidance_scale\": 15,\n    \"negative_prompt\": \"<string>\",\n    \"seed\": 123,\n    \"shift_value\": 3,\n    \"steps\": 80,\n    \"use_guidance_schedule\": true,\n    \"use_negative_prompts\": false,\n    \"use_timestep_transform\": true,\n    \"warmup_steps\": 24\n  },\n  \"prompt_text\": \"<string>\",\n  \"video_url\": \"<string>\",\n  \"webhook_url\": \"<string>\",\n  \"frame_position\": [\n    123\n  ],\n  \"frame_resolution\": [\n    123\n  ],\n  \"scale\": [\n    123\n  ]\n}'\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/get-prompt-details",
  "markdown": "# Get Prompt Details - ComfyUI\n\n```\n{\n  \"error\": {},\n  \"frame_conditioning\": {},\n  \"id\": \"<string>\",\n  \"inference_params\": {},\n  \"meta\": {},\n  \"model_params\": {},\n  \"output_url\": \"<string>\",\n  \"prompt_text\": \"<string>\",\n  \"status\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/releases/get-release-notes",
  "markdown": "# Get release notes - ComfyUI\n\n#### Query Parameters\n\nThe project to get release notes for\n\nAvailable options:\n\n`comfyui`,\n\n`comfyui_frontend`,\n\n`desktop`\n\nThe current version to filter release notes\n\nThe locale for the release notes\n\nAvailable options:\n\n`en`,\n\n`es`,\n\n`fr`,\n\n`ja`,\n\n`ko`,\n\n`ru`,\n\n`zh`\n\nThe platform requesting the release notes\n\n#### Response\n\nRelease notes retrieved successfully\n\nThe response is of type `object[]`."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fcreate-text-to-image-prompt",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-multiple-node-versions-in-a-single-request",
  "markdown": "# Retrieve multiple node versions in a single request\n\n```\n{\n  \"node_versions\": [\n    {\n      \"error_message\": \"<string>\",\n      \"identifier\": {\n        \"node_id\": \"<string>\",\n        \"version\": \"<string>\"\n      },\n      \"node_version\": {\n        \"changelog\": \"<string>\",\n        \"comfy_node_extract_status\": \"<string>\",\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"dependencies\": [\n          \"<string>\"\n        ],\n        \"deprecated\": true,\n        \"downloadUrl\": \"<string>\",\n        \"id\": \"<string>\",\n        \"node_id\": \"<string>\",\n        \"status\": \"NodeVersionStatusActive\",\n        \"status_reason\": \"<string>\",\n        \"supported_accelerators\": [\n          \"<string>\"\n        ],\n        \"supported_comfyui_frontend_version\": \"<string>\",\n        \"supported_comfyui_version\": \"<string>\",\n        \"supported_os\": [\n          \"<string>\"\n        ],\n        \"version\": \"<string>\"\n      },\n      \"status\": \"success\"\n    }\n  ]\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-a-node-by-comfyui-node-name",
  "markdown": "# Retrieve a node by ComfyUI node name\n\n```\n{\n  \"author\": \"<string>\",\n  \"banner_url\": \"<string>\",\n  \"category\": \"<string>\",\n  \"created_at\": \"2023-11-07T05:31:56Z\",\n  \"description\": \"<string>\",\n  \"downloads\": 123,\n  \"github_stars\": 123,\n  \"icon\": \"<string>\",\n  \"id\": \"<string>\",\n  \"latest_version\": {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  },\n  \"license\": \"<string>\",\n  \"name\": \"<string>\",\n  \"preempted_comfy_node_names\": [\n    \"<string>\"\n  ],\n  \"publisher\": {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  },\n  \"rating\": 123,\n  \"repository\": \"<string>\",\n  \"search_ranking\": 123,\n  \"status\": \"NodeStatusActive\",\n  \"status_detail\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"tags\": [\n    \"<string>\"\n  ],\n  \"translations\": {}\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/upload-files",
  "markdown": "# Upload Files - ComfyUI\n\n[ComfyUI home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo/dark.svg)](https://docs.comfy.org/)\n\n##### Registry\n\n*   [POST\n    \n    Retrieve multiple node versions in a single request\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-multiple-node-versions-in-a-single-request)\n*   [GET\n    \n    Retrieve a node by ComfyUI node name\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-a-node-by-comfyui-node-name)\n*   [GET\n    \n    Retrieves a list of nodes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes)\n*   [GET\n    \n    Retrieves a list of nodes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes-1)\n*   [GET\n    \n    Retrieve a specific node by ID\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-a-specific-node-by-id)\n*   [GET\n    \n    Returns a node version to be installed.\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/returns-a-node-version-to-be-installed)\n*   [POST\n    \n    Add review to a specific version of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/add-review-to-a-specific-version-of-a-node)\n*   [POST\n    \n    Create Node Translations\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-node-translations)\n*   [GET\n    \n    List all versions of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/list-all-versions-of-a-node)\n*   [GET\n    \n    Retrieve a specific version of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-a-specific-version-of-a-node)\n*   [GET\n    \n    list comfy-nodes for certain node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/list-comfy-nodes-for-certain-node)\n*   [POST\n    \n    create comfy-nodes for certain node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-comfy-nodes-for-certain-node)\n*   [GET\n    \n    get specify comfy-node based on its id\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/get-specify-comfy-node-based-on-its-id)\n*   [GET\n    \n    Retrieve all publishers\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-all-publishers)\n*   [POST\n    \n    Create a new publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-a-new-publisher)\n*   [GET\n    \n    Validate if a publisher username is available\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/validate-if-a-publisher-username-is-available)\n*   [GET\n    \n    Retrieve a publisher by ID\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-a-publisher-by-id)\n*   [PUT\n    \n    Update a publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/update-a-publisher)\n*   [DEL\n    \n    Delete a publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/delete-a-publisher)\n*   [GET\n    \n    Retrieve all nodes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-all-nodes)\n*   [POST\n    \n    Create a new custom node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-a-new-custom-node)\n*   [GET\n    \n    Retrieve all nodes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-all-nodes-1)\n*   [PUT\n    \n    Update a specific node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/update-a-specific-node)\n*   [DEL\n    \n    Delete a specific node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/delete-a-specific-node)\n*   [POST\n    \n    Claim nodeId into publisherId for the authenticated publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/claim-nodeid-into-publisherid-for-the-authenticated-publisher)\n*   [GET\n    \n    Retrieve permissions the user has for a given publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher)\n*   [POST\n    \n    Publish a new version of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/publish-a-new-version-of-a-node)\n*   [PUT\n    \n    Update changelog and deprecation status of a node version\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/update-changelog-and-deprecation-status-of-a-node-version)\n*   [DEL\n    \n    Unpublish (delete) a specific version of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/unpublish-delete-a-specific-version-of-a-node)\n*   [GET\n    \n    Retrieve permissions the user has for a given publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher-1)\n*   [POST\n    \n    Create a new personal access token\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-a-new-personal-access-token)\n*   [DEL\n    \n    Delete a specific personal access token\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/delete-a-specific-personal-access-token)\n*   [GET\n    \n    Get information about the calling user.\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/get-information-about-the-calling-user)\n*   [GET\n    \n    Retrieve all publishers for a given user\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-all-publishers-for-a-given-user)\n*   [GET\n    \n    List all node versions given some filters.\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/list-all-node-versions-given-some-filters)\n\n##### API Nodes\n\n*   [POST\n    \n    Create Image to Video Prompt\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/create-image-to-video-prompt)\n*   [POST\n    \n    Create Text to Image Prompt\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/create-text-to-image-prompt)\n*   [POST\n    \n    Create Text to Video Prompt\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/create-text-to-video-prompt)\n*   [POST\n    \n    Create Video to Video Prompt\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/create-video-to-video-prompt)\n*   [POST\n    \n    Resize a video\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/resize-a-video)\n*   [GET\n    \n    Get Prompt Details\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/get-prompt-details)\n*   [POST\n    \n    Upload Files\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/upload-files)\n\n##### Releases\n\n*   [GET\n    \n    Get release notes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/releases/get-release-notes)\n\n[ComfyUI home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo/dark.svg)](https://docs.comfy.org/)\n\nAPI Nodes\n\nPOST\n\n/\n\nproxy\n\n/\n\nmoonvalley\n\n/\n\nuploads\n\nUpload Files\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/proxy/moonvalley/uploads \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@example-file.txt\n```\n\n```\n{\n  \"access_url\": \"<string>\"\n}\n```\n\n#### Body\n\nmultipart/form-data\n\n[​](#body-file)\n\nfile\n\nfile\n\n#### Response\n\n200 - application/json\n\nFile uploaded successfully\n\n[​](#response-access-url)\n\naccess\\_url\n\nstring\n\n#### 0 reactions\n\n[Sign in](https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fupload-files) to add your reaction.\n\n#### 0 comments\n\n[Sign in with GitHub](https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fupload-files)\n\nWas this page helpful?\n\n[Suggest edits](https://github.com/comfy-org/docs/edit/main/api-reference/api-nodes/upload-files.mdx)\n\n[Previous](https://docs.comfy.org/api-reference/api-nodes/get-prompt-details)\n\n[\n\nGet release notesFetch release notes from Strapi with caching\n\nNext\n\n\n\n](https://docs.comfy.org/api-reference/releases/get-release-notes)\n\nUpload Files\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/proxy/moonvalley/uploads \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@example-file.txt\n```\n\n```\n{\n  \"access_url\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes-1",
  "markdown": "# Retrieves a list of nodes\n\n```\n{\n  \"limit\": 123,\n  \"nodes\": [\n    {\n      \"author\": \"<string>\",\n      \"banner_url\": \"<string>\",\n      \"category\": \"<string>\",\n      \"created_at\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"downloads\": 123,\n      \"github_stars\": 123,\n      \"icon\": \"<string>\",\n      \"id\": \"<string>\",\n      \"latest_version\": {\n        \"changelog\": \"<string>\",\n        \"comfy_node_extract_status\": \"<string>\",\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"dependencies\": [\n          \"<string>\"\n        ],\n        \"deprecated\": true,\n        \"downloadUrl\": \"<string>\",\n        \"id\": \"<string>\",\n        \"node_id\": \"<string>\",\n        \"status\": \"NodeVersionStatusActive\",\n        \"status_reason\": \"<string>\",\n        \"supported_accelerators\": [\n          \"<string>\"\n        ],\n        \"supported_comfyui_frontend_version\": \"<string>\",\n        \"supported_comfyui_version\": \"<string>\",\n        \"supported_os\": [\n          \"<string>\"\n        ],\n        \"version\": \"<string>\"\n      },\n      \"license\": \"<string>\",\n      \"name\": \"<string>\",\n      \"preempted_comfy_node_names\": [\n        \"<string>\"\n      ],\n      \"publisher\": {\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"description\": \"<string>\",\n        \"id\": \"<string>\",\n        \"logo\": \"<string>\",\n        \"members\": [\n          {\n            \"id\": \"<string>\",\n            \"role\": \"<string>\",\n            \"user\": {\n              \"email\": \"<string>\",\n              \"id\": \"<string>\",\n              \"name\": \"<string>\"\n            }\n          }\n        ],\n        \"name\": \"<string>\",\n        \"source_code_repo\": \"<string>\",\n        \"status\": \"PublisherStatusActive\",\n        \"support\": \"<string>\",\n        \"website\": \"<string>\"\n      },\n      \"rating\": 123,\n      \"repository\": \"<string>\",\n      \"search_ranking\": 123,\n      \"status\": \"NodeStatusActive\",\n      \"status_detail\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"tags\": [\n        \"<string>\"\n      ],\n      \"translations\": {}\n    }\n  ],\n  \"page\": 123,\n  \"total\": 123,\n  \"totalPages\": 123\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes",
  "markdown": "# Retrieves a list of nodes\n\n```\n{\n  \"limit\": 123,\n  \"nodes\": [\n    {\n      \"author\": \"<string>\",\n      \"banner_url\": \"<string>\",\n      \"category\": \"<string>\",\n      \"created_at\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"downloads\": 123,\n      \"github_stars\": 123,\n      \"icon\": \"<string>\",\n      \"id\": \"<string>\",\n      \"latest_version\": {\n        \"changelog\": \"<string>\",\n        \"comfy_node_extract_status\": \"<string>\",\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"dependencies\": [\n          \"<string>\"\n        ],\n        \"deprecated\": true,\n        \"downloadUrl\": \"<string>\",\n        \"id\": \"<string>\",\n        \"node_id\": \"<string>\",\n        \"status\": \"NodeVersionStatusActive\",\n        \"status_reason\": \"<string>\",\n        \"supported_accelerators\": [\n          \"<string>\"\n        ],\n        \"supported_comfyui_frontend_version\": \"<string>\",\n        \"supported_comfyui_version\": \"<string>\",\n        \"supported_os\": [\n          \"<string>\"\n        ],\n        \"version\": \"<string>\"\n      },\n      \"license\": \"<string>\",\n      \"name\": \"<string>\",\n      \"preempted_comfy_node_names\": [\n        \"<string>\"\n      ],\n      \"publisher\": {\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"description\": \"<string>\",\n        \"id\": \"<string>\",\n        \"logo\": \"<string>\",\n        \"members\": [\n          {\n            \"id\": \"<string>\",\n            \"role\": \"<string>\",\n            \"user\": {\n              \"email\": \"<string>\",\n              \"id\": \"<string>\",\n              \"name\": \"<string>\"\n            }\n          }\n        ],\n        \"name\": \"<string>\",\n        \"source_code_repo\": \"<string>\",\n        \"status\": \"PublisherStatusActive\",\n        \"support\": \"<string>\",\n        \"website\": \"<string>\"\n      },\n      \"rating\": 123,\n      \"repository\": \"<string>\",\n      \"search_ranking\": 123,\n      \"status\": \"NodeStatusActive\",\n      \"status_detail\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"tags\": [\n        \"<string>\"\n      ],\n      \"translations\": {}\n    }\n  ],\n  \"page\": 123,\n  \"total\": 123,\n  \"totalPages\": 123\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-a-specific-node-by-id",
  "markdown": "# Retrieve a specific node by ID\n\n```\n{\n  \"author\": \"<string>\",\n  \"banner_url\": \"<string>\",\n  \"category\": \"<string>\",\n  \"created_at\": \"2023-11-07T05:31:56Z\",\n  \"description\": \"<string>\",\n  \"downloads\": 123,\n  \"github_stars\": 123,\n  \"icon\": \"<string>\",\n  \"id\": \"<string>\",\n  \"latest_version\": {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  },\n  \"license\": \"<string>\",\n  \"name\": \"<string>\",\n  \"preempted_comfy_node_names\": [\n    \"<string>\"\n  ],\n  \"publisher\": {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  },\n  \"rating\": 123,\n  \"repository\": \"<string>\",\n  \"search_ranking\": 123,\n  \"status\": \"NodeStatusActive\",\n  \"status_detail\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"tags\": [\n    \"<string>\"\n  ],\n  \"translations\": {}\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/BasicScheduler",
  "markdown": "# BasicScheduler - ComfyUI Built-in Node Documentation\n\nThe `BasicScheduler` node is designed to compute a sequence of sigma values for diffusion models based on the provided scheduler, model, and denoising parameters. It dynamically adjusts the total number of steps based on the denoise factor to fine-tune the diffusion process, providing precise “recipes” for different stages in advanced sampling processes that require fine control (such as multi-stage sampling).\n\n## Inputs\n\n| Parameter | Data Type | Input Type | Default | Range | Metaphor Description | Technical Purpose |\n| --- | --- | --- | --- | --- | --- | --- |\n| `model` | MODEL | Input | \\-  | \\-  | **Canvas Type**: Different canvas materials need different paint formulas | Diffusion model object, determines sigma calculation basis |\n| `scheduler` | COMBO\\[STRING\\] | Widget | \\-  | 9 options | **Mixing Technique**: Choose how paint concentration changes | Scheduling algorithm, controls noise decay mode |\n| `steps` | INT | Widget | 20  | 1-10000 | **Mixing Count**: 20 mixes vs 50 mixes precision difference | Sampling steps, affects generation quality and speed |\n| `denoise` | FLOAT | Widget | 1.0 | 0.0-1.0 | **Creation Intensity**: Control level from fine-tuning to repainting | Denoising strength, supports partial repainting scenarios |\n\n### Scheduler Types\n\nBased on source code `comfy.samplers.SCHEDULER_NAMES`, supports the following 9 schedulers:\n\n| Scheduler Name | Characteristics | Use Cases | Noise Decay Pattern |\n| --- | --- | --- | --- |\n| **normal** | Standard linear | General scenarios, balanced | Uniform decay |\n| **karras** | Smooth transition | High quality, detail-rich | Smooth non-linear decay |\n| **exponential** | Exponential decay | Fast generation, efficiency | Exponential rapid decay |\n| **sgm\\_uniform** | SGM uniform | Specific model optimization | SGM optimized decay |\n| **simple** | Simple scheduling | Quick testing, basic use | Simplified decay |\n| **ddim\\_uniform** | DDIM uniform | DDIM sampling optimization | DDIM specific decay |\n| **beta** | Beta distribution | Special distribution needs | Beta function decay |\n| **linear\\_quadratic** | Linear quadratic | Complex scenario optimization | Quadratic function decay |\n| **kl\\_optimal** | KL optimal | Theoretical optimization | KL divergence optimized decay |\n\n## Outputs\n\n| Parameter | Data Type | Output Type | Metaphor Description | Technical Meaning |\n| --- | --- | --- | --- | --- |\n| `sigmas` | SIGMAS | Output | **Paint Recipe Chart**: Detailed paint concentration list for step-by-step use | Noise level sequence, guides diffusion model denoising process |\n\n## Node Role: Artist’s Color Mixing Assistant\n\nImagine you are an artist creating a clear image from a chaotic mixture of paint (noise). `BasicScheduler` acts like your **professional color mixing assistant**, whose job is to prepare a series of precise paint concentration recipes:\n\n### Workflow\n\n*   **Step 1**: Use 90% concentration paint (high noise level)\n*   **Step 2**: Use 80% concentration paint\n*   **Step 3**: Use 70% concentration paint\n*   **…**\n*   **Final Step**: Use 0% concentration (clean canvas, no noise)\n\n### Color Assistant’s Special Skills\n\n**Different mixing methods (scheduler)**:\n\n*   **“karras” mixing method**: Paint concentration changes very smoothly, like professional artist’s gradient technique\n*   **“exponential” mixing method**: Paint concentration decreases rapidly, suitable for quick creation\n*   **“linear” mixing method**: Paint concentration decreases uniformly, stable and controllable\n\n**Fine control (steps)**:\n\n*   **20 mixes**: Quick painting, efficiency priority\n*   **50 mixes**: Fine painting, quality priority\n\n**Creation intensity (denoise)**:\n\n*   **1.0 = Complete new creation**: Start completely from blank canvas\n*   **0.5 = Half transformation**: Keep half of original painting, transform half\n*   **0.2 = Fine adjustment**: Only make subtle adjustments to original painting\n\n### Collaboration with Other Nodes\n\n`BasicScheduler` (Color Assistant) → Prepare Recipe → `SamplerCustom` (Artist) → Actual Painting → Completed Work"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/returns-a-node-version-to-be-installed",
  "markdown": "# Returns a node version to be installed.\n\n```\n{\n  \"changelog\": \"<string>\",\n  \"comfy_node_extract_status\": \"<string>\",\n  \"createdAt\": \"2023-11-07T05:31:56Z\",\n  \"dependencies\": [\n    \"<string>\"\n  ],\n  \"deprecated\": true,\n  \"downloadUrl\": \"<string>\",\n  \"id\": \"<string>\",\n  \"node_id\": \"<string>\",\n  \"status\": \"NodeVersionStatusActive\",\n  \"status_reason\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"version\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/add-review-to-a-specific-version-of-a-node",
  "markdown": "# Add review to a specific version of a node\n\n```\n{\n  \"author\": \"<string>\",\n  \"banner_url\": \"<string>\",\n  \"category\": \"<string>\",\n  \"created_at\": \"2023-11-07T05:31:56Z\",\n  \"description\": \"<string>\",\n  \"downloads\": 123,\n  \"github_stars\": 123,\n  \"icon\": \"<string>\",\n  \"id\": \"<string>\",\n  \"latest_version\": {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  },\n  \"license\": \"<string>\",\n  \"name\": \"<string>\",\n  \"preempted_comfy_node_names\": [\n    \"<string>\"\n  ],\n  \"publisher\": {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  },\n  \"rating\": 123,\n  \"repository\": \"<string>\",\n  \"search_ranking\": 123,\n  \"status\": \"NodeStatusActive\",\n  \"status_detail\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"tags\": [\n    \"<string>\"\n  ],\n  \"translations\": {}\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/list-all-versions-of-a-node",
  "markdown": "# List all versions of a node\n\n```\n[\n  {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  }\n]\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-node-translations",
  "markdown": "# Create Node Translations - ComfyUI\n\n#### Path Parameters\n\nThe unique identifier of the node.\n\n#### Body\n\n#### Response\n\nDetailed information about a specific node\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/list-comfy-nodes-for-certain-node",
  "markdown": "# list comfy-nodes for certain node\n\n```\n{\n  \"comfy_nodes\": [\n    {\n      \"category\": \"<string>\",\n      \"comfy_node_name\": \"<string>\",\n      \"deprecated\": true,\n      \"description\": \"<string>\",\n      \"experimental\": true,\n      \"function\": \"<string>\",\n      \"input_types\": \"<string>\",\n      \"output_is_list\": [\n        true\n      ],\n      \"return_names\": \"<string>\",\n      \"return_types\": \"<string>\"\n    }\n  ],\n  \"totalNumberOfPages\": 123\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-a-specific-version-of-a-node",
  "markdown": "# Retrieve a specific version of a node\n\n```\n{\n  \"changelog\": \"<string>\",\n  \"comfy_node_extract_status\": \"<string>\",\n  \"createdAt\": \"2023-11-07T05:31:56Z\",\n  \"dependencies\": [\n    \"<string>\"\n  ],\n  \"deprecated\": true,\n  \"downloadUrl\": \"<string>\",\n  \"id\": \"<string>\",\n  \"node_id\": \"<string>\",\n  \"status\": \"NodeVersionStatusActive\",\n  \"status_reason\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"version\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-comfy-nodes-for-certain-node",
  "markdown": "# create comfy-nodes for certain node\n\ncreate comfy-nodes for certain node\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/nodes/{nodeId}/versions/{version}/comfy-nodes \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n  \"cloud_build_info\": {\n    \"build_id\": \"<string>\",\n    \"location\": \"<string>\",\n    \"project_id\": \"<string>\",\n    \"project_number\": \"<string>\"\n  },\n  \"nodes\": {},\n  \"reason\": \"<string>\",\n  \"status\": \"<string>\",\n  \"success\": true\n}'\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncodeFlux",
  "markdown": "# ClipTextEncodeFlux - ComfyUI Built-in Node Documentation\n\n`CLIPTextEncodeFlux` is an advanced text encoding node in ComfyUI, specifically designed for the Flux architecture. It uses a dual-encoder mechanism (CLIP-L and T5XXL) to process both structured keywords and detailed natural language descriptions, providing the Flux model with more accurate and comprehensive text understanding for improved text-to-image generation quality. This node is based on a dual-encoder collaboration mechanism:\n\n1.  The `clip_l` input is processed by the CLIP-L encoder, extracting style, theme, and other keyword features—ideal for concise descriptions.\n2.  The `t5xxl` input is processed by the T5XXL encoder, which excels at understanding complex and detailed natural language scene descriptions.\n3.  The outputs from both encoders are fused, and combined with the `guidance` parameter to generate unified conditioning embeddings (`CONDITIONING`) for downstream Flux sampler nodes, controlling how closely the generated content matches the text description.\n\n## Inputs\n\n| Parameter | Data Type | Input Method | Default | Range | Description |\n| --- | --- | --- | --- | --- | --- |\n| `clip` | CLIP | Node input | None | \\-  | Must be a CLIP model supporting the Flux architecture, including both CLIP-L and T5XXL encoders |\n| `clip_l` | STRING | Text box | None | Up to 77 tokens | Suitable for concise keyword descriptions, such as style or theme |\n| `t5xxl` | STRING | Text box | None | Nearly unlimited | Suitable for detailed natural language descriptions, expressing complex scenes and details |\n| `guidance` | FLOAT | Slider | 3.5 | 0.0 - 100.0 | Controls the influence of text conditions on the generation process; higher values mean stricter adherence to the text |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| `CONDITIONING` | CONDITIONING | Contains the fused embeddings from both encoders and the guidance parameter, used for conditional image generation |\n\n## Usage Examples\n\n### Prompt Examples\n\n*   **clip\\_l input** (keyword style):\n    *   Use structured, concise keyword combinations\n    *   Example: `masterpiece, best quality, portrait, oil painting, dramatic lighting`\n    *   Focus on style, quality, and main subject\n*   **t5xxl input** (natural language description):\n    *   Use complete, fluent scene descriptions\n    *   Example: `A highly detailed portrait in oil painting style, featuring dramatic chiaroscuro lighting that creates deep shadows and bright highlights, emphasizing the subject's features with renaissance-inspired composition.`\n    *   Focus on scene details, spatial relationships, and lighting effects\n\n### Notes\n\n1.  Make sure to use a CLIP model compatible with the Flux architecture\n2.  It is recommended to fill in both `clip_l` and `t5xxl` to leverage the dual-encoder advantage\n3.  Note the 77-token limit for `clip_l`\n4.  Adjust the `guidance` parameter based on the generated results"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncodeHunyuanDit",
  "markdown": "# ClipTextEncodeHunyuanDit - ComfyUI Built-in Node Documentation\n\nThe `CLIPTextEncodeHunyuanDiT` node’s main function is to convert input text into a form that the model can understand. It is an advanced conditioning node specifically designed for the dual text encoder architecture of the HunyuanDiT model. Its primary role is like a translator, converting our text descriptions into “machine language” that the AI model can understand. The `bert` and `mt5xl` inputs prefer different types of prompt inputs.\n\n## Inputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip` | CLIP | A CLIP model instance used for text tokenization and encoding, which is core to generating conditions. |\n| `bert` | STRING | Text input for encoding, prefers phrases and keywords, supports multiline and dynamic prompts. |\n| `mt5xl` | STRING | Another text input for encoding, supports multiline and dynamic prompts (multilingual), can use complete sentences and complex descriptions. |\n\n## Outputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `CONDITIONING` | CONDITIONING | The encoded conditional output used for further processing in generation tasks. |\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncodeSdxl",
  "markdown": "# ClipTextEncodeSdxl - ComfyUI Built-in Node Documentation\n\nThis node is designed to encode text input using a CLIP model specifically customized for the SDXL architecture. It uses a dual encoder system (CLIP-L and CLIP-G) to process text descriptions, resulting in more accurate image generation.\n\n## Inputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip` | CLIP | CLIP model instance used for text encoding. |\n| `width` | INT | Specifies the image width in pixels, default 1024. |\n| `height` | INT | Specifies the image height in pixels, default 1024. |\n| `crop_w` | INT | Width of the crop area in pixels, default 0. |\n| `crop_h` | INT | Height of the crop area in pixels, default 0. |\n| `target_width` | INT | Target width for the output image, default 1024. |\n| `target_height` | INT | Target height for the output image, default 1024. |\n| `text_g` | STRING | Global text description for overall scene description. |\n| `text_l` | STRING | Local text description for detail description. |\n\n## Outputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `CONDITIONING` | CONDITIONING | Contains encoded text and conditional information needed for image generation. |\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/get-specify-comfy-node-based-on-its-id",
  "markdown": "# get specify comfy-node based on its id\n\n#### Path Parameters\n\n#### Response\n\nComy Nodes created successfully\n\nThe response is of type `object`.\n\nget specify comfy-node based on its id"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-all-publishers",
  "markdown": "# Retrieve all publishers - ComfyUI\n\n```\n[\n  {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  }\n]\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-a-new-publisher",
  "markdown": "# Create a new publisher - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Body\n\n#### Response\n\nPublisher created successfully\n\nThe response is of type `object`."
},
{
  "url": "https://docs.comfy.org/api-reference/registry/validate-if-a-publisher-username-is-available",
  "markdown": "# Validate if a publisher username is available\n\n#### Query Parameters\n\nThe publisher username to validate.\n\n#### Response\n\nUsername validation result\n\nThe response is of type `object`.\n\nValidate if a publisher username is available"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-1-1-pro-ultra-image",
  "markdown": "# Flux 1.1 \\[pro\\] Ultra Image - ComfyUI Native Node Documentation\n\n![ComfyUI Native Flux 1.1 [pro] Ultra Image node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg) The Flux 1.1 \\[pro\\] Ultra Image node allows you to generate ultra-high-resolution images through text prompts, directly connecting to Black Forest Labs’ latest image generation API. This node supports two main usage modes:\n\n1.  **Text-to-Image**: Generate high-quality images from text prompts (when no image input is used)\n2.  **Image-to-Image**: Combine existing images with prompts to create new images that blend features from both (Remix mode)\n\nThis node supports Ultra mode through API calls, capable of generating images at 4 times the resolution of standard Flux 1.1 \\[pro\\] (up to 4MP), without sacrificing prompt adherence, and maintaining super-fast generation times of just 10 seconds. Compared to other high-resolution models, it’s more than 2.5 times faster.\n\n## Parameter Description\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | String | \"\"  | Text description for generating the image |\n| prompt\\_upsampling | Boolean | False | Whether to use prompt upsampling technique to enhance details. When enabled, automatically modifies prompts for more creative generation, but results become non-deterministic (same seed won’t produce exactly the same result) |\n| seed | Integer | 0   | Random seed value, controls generation randomness |\n| aspect\\_ratio | String | ”16:9” | Width-to-height ratio of the image, must be between 1:4 and 4:1 |\n| raw | Boolean | False | When set to True, generates less processed, more natural-looking images |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image\\_prompt | Image | None | Optional input, used for Image-to-Image (Remix) mode |\n| image\\_prompt\\_strength | Float | 0.1 | Active when `image_prompt` is input, adjusts the blend between prompt and image prompt. Higher values make output closer to input image, range is 0.0-1.0 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | Image | Generated high-resolution image result |\n\n## Usage Examples\n\nPlease visit the tutorial below to see corresponding usage examples\n\n*   [Flux 1.1 Pro Ultra Image API Node ComfyUI Official Example Workflow](https://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)\n\n## How It Works\n\nFlux 1.1 \\[pro\\] Ultra mode uses optimized deep learning architecture and efficient GPU acceleration technology to achieve high-resolution image generation without sacrificing speed. When a request is sent to the API, the system parses the prompt, applies appropriate parameters, then computes the image in parallel, finally generating and returning the high-resolution result. Compared to regular models, Ultra mode particularly focuses on detail preservation and consistency at large scales, ensuring impressive quality even at 4MP high resolution.\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\nclass FluxProUltraImageNode(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    MINIMUM_RATIO = 1 / 4\n    MAXIMUM_RATIO = 4 / 1\n    MINIMUM_RATIO_STR = \"1:4\"\n    MAXIMUM_RATIO_STR = \"4:1\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"prompt_upsampling\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result).\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"16:9\",\n                        \"tooltip\": \"Aspect ratio of image; must be between 1:4 and 4:1.\",\n                    },\n                ),\n                \"raw\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"When True, generate less processed, more natural-looking images.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image_prompt\": (IO.IMAGE,),\n                \"image_prompt_strength\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.1,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Blend between the prompt and the image prompt.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, aspect_ratio: str):\n        try:\n            validate_aspect_ratio(\n                aspect_ratio,\n                minimum_ratio=cls.MINIMUM_RATIO,\n                maximum_ratio=cls.MAXIMUM_RATIO,\n                minimum_ratio_str=cls.MINIMUM_RATIO_STR,\n                maximum_ratio_str=cls.MAXIMUM_RATIO_STR,\n            )\n        except Exception as e:\n            return str(e)\n        return True\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/bfl\"\n\n    def api_call(\n        self,\n        prompt: str,\n        aspect_ratio: str,\n        prompt_upsampling=False,\n        raw=False,\n        seed=0,\n        image_prompt=None,\n        image_prompt_strength=0.1,\n        auth_token=None,\n        **kwargs,\n    ):\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/bfl/flux-pro-1.1-ultra/generate\",\n                method=HttpMethod.POST,\n                request_model=BFLFluxProUltraGenerateRequest,\n                response_model=BFLFluxProGenerateResponse,\n            ),\n            request=BFLFluxProUltraGenerateRequest(\n                prompt=prompt,\n                prompt_upsampling=prompt_upsampling,\n                seed=seed,\n                aspect_ratio=validate_aspect_ratio(\n                    aspect_ratio,\n                    minimum_ratio=self.MINIMUM_RATIO,\n                    maximum_ratio=self.MAXIMUM_RATIO,\n                    minimum_ratio_str=self.MINIMUM_RATIO_STR,\n                    maximum_ratio_str=self.MAXIMUM_RATIO_STR,\n                ),\n                raw=raw,\n                image_prompt=(\n                    image_prompt\n                    if image_prompt is None\n                    else convert_image_to_base64(image_prompt)\n                ),\n                image_prompt_strength=(\n                    None if image_prompt is None else round(image_prompt_strength, 2)\n                ),\n            ),\n            auth_token=auth_token,\n        )\n        output_image = handle_bfl_synchronous_operation(operation)\n        return (output_image,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1",
  "markdown": "# Ideogram V1 - ComfyUI Native Node Documentation\n\n![ComfyUI Native Ideogram V1 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg) The Ideogram V1 node allows you to generate images with high-quality text rendering capabilities using Ideogram’s text-to-image API.\n\n## Parameter Description\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the content to generate |\n| turbo | boolean | False | Whether to use turbo mode (faster but possibly lower quality) |\n| aspect\\_ratio | select | ”1:1” | Image aspect ratio |\n| magic\\_prompt\\_option | select | ”AUTO” | Determines whether to use MagicPrompt in generation, options: AUTO, ON, OFF |\n| seed | integer | 0   | Random seed value (0-2147483647) |\n| negative\\_prompt | string | \"\"  | Specifies elements you don’t want in the image |\n| num\\_images | integer | 1   | Number of images to generate (1-8) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image result |\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\nclass IdeogramV1(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V1 model.\n\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"turbo\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to use turbo mode (faster generation, potentially lower quality)\",\n                    }\n                ),\n            },\n            \"optional\": {\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V2_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Description of what to exclude from the image\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v1\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        turbo=False,\n        aspect_ratio=\"1:1\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        negative_prompt=\"\",\n        num_images=1,\n        auth_token=None,\n    ):\n        # Determine the model based on turbo setting\n        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)\n        model = \"V_1_TURBO\" if turbo else \"V_1\"\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/ideogram/generate\",\n                method=HttpMethod.POST,\n                request_model=IdeogramGenerateRequest,\n                response_model=IdeogramGenerateResponse,\n            ),\n            request=IdeogramGenerateRequest(\n                image_request=ImageRequest(\n                    prompt=prompt,\n                    model=model,\n                    num_images=num_images,\n                    seed=seed,\n                    aspect_ratio=aspect_ratio if aspect_ratio != \"ASPECT_1_1\" else None,\n                    magic_prompt_option=(\n                        magic_prompt_option if magic_prompt_option != \"AUTO\" else None\n                    ),\n                    negative_prompt=negative_prompt if negative_prompt else None,\n                )\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2",
  "markdown": "# Ideogram V2 - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Ideogram V2 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg) The Ideogram V2 node allows you to generate more refined images using Ideogram’s second-generation AI model, with significant improvements in text rendering, image quality, and overall aesthetics.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the content to generate |\n| turbo | boolean | False | Whether to use turbo mode (faster generation, possibly lower quality) |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| aspect\\_ratio | dropdown | ”1:1” | Image aspect ratio, effective when resolution is set to “Auto” |\n| resolution | dropdown | ”Auto” | Output image resolution, if not set to “Auto”, it will override the aspect\\_ratio setting |\n| magic\\_prompt\\_option | dropdown | ”AUTO” | Determines whether to use MagicPrompt feature during generation, options are \\[“AUTO”, “ON”, “OFF”\\] |\n| seed | integer | 0   | Random seed value, range 0-2147483647 |\n| style\\_type | dropdown | ”NONE” | Generation style type (V2 only), options are \\[“AUTO”, “GENERAL”, “REALISTIC”, “DESIGN”, “RENDER\\_3D”, “ANIME”\\] |\n| negative\\_prompt | string | \"\"  | Specifies elements you don’t want to appear in the image |\n| num\\_images | integer | 1   | Number of images to generate, range 1-8 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image(s) |\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\n\nclass IdeogramV2(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V2 model.\n\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"turbo\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to use turbo mode (faster generation, potentially lower quality)\",\n                    }\n                ),\n            },\n            \"optional\": {\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V2_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation. Ignored if resolution is not set to AUTO.\",\n                    },\n                ),\n                \"resolution\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V1_RES_MAP.keys()),\n                        \"default\": \"Auto\",\n                        \"tooltip\": \"The resolution for image generation. If not set to AUTO, this overrides the aspect_ratio setting.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"style_type\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"GENERAL\", \"REALISTIC\", \"DESIGN\", \"RENDER_3D\", \"ANIME\"],\n                        \"default\": \"NONE\",\n                        \"tooltip\": \"Style type for generation (V2 only)\",\n                    },\n                ),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Description of what to exclude from the image\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n                #\"color_palette\": (\n                #    IO.STRING,\n                #    {\n                #        \"multiline\": False,\n                #        \"default\": \"\",\n                #        \"tooltip\": \"Color palette preset name or hex colors with weights\",\n                #    },\n                #),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v2\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        turbo=False,\n        aspect_ratio=\"1:1\",\n        resolution=\"Auto\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        style_type=\"NONE\",\n        negative_prompt=\"\",\n        num_images=1,\n        color_palette=\"\",\n        auth_token=None,\n    ):\n        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)\n        resolution = V1_V1_RES_MAP.get(resolution, None)\n        # Determine the model based on turbo setting\n        model = \"V_2_TURBO\" if turbo else \"V_2\"\n\n        # Handle resolution vs aspect_ratio logic\n        # If resolution is not AUTO, it overrides aspect_ratio\n        final_resolution = None\n        final_aspect_ratio = None\n\n        if resolution != \"AUTO\":\n            final_resolution = resolution\n        else:\n            final_aspect_ratio = aspect_ratio if aspect_ratio != \"ASPECT_1_1\" else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/ideogram/generate\",\n                method=HttpMethod.POST,\n                request_model=IdeogramGenerateRequest,\n                response_model=IdeogramGenerateResponse,\n            ),\n            request=IdeogramGenerateRequest(\n                image_request=ImageRequest(\n                    prompt=prompt,\n                    model=model,\n                    num_images=num_images,\n                    seed=seed,\n                    aspect_ratio=final_aspect_ratio,\n                    resolution=final_resolution,\n                    magic_prompt_option=(\n                        magic_prompt_option if magic_prompt_option != \"AUTO\" else None\n                    ),\n                    style_type=style_type if style_type != \"NONE\" else None,\n                    negative_prompt=negative_prompt if negative_prompt else None,\n                    color_palette=color_palette if color_palette else None,\n                )\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-a-publisher-by-id",
  "markdown": "# Retrieve a publisher by ID\n\n```\n{\n  \"createdAt\": \"2023-11-07T05:31:56Z\",\n  \"description\": \"<string>\",\n  \"id\": \"<string>\",\n  \"logo\": \"<string>\",\n  \"members\": [\n    {\n      \"id\": \"<string>\",\n      \"role\": \"<string>\",\n      \"user\": {\n        \"email\": \"<string>\",\n        \"id\": \"<string>\",\n        \"name\": \"<string>\"\n      }\n    }\n  ],\n  \"name\": \"<string>\",\n  \"source_code_repo\": \"<string>\",\n  \"status\": \"PublisherStatusActive\",\n  \"support\": \"<string>\",\n  \"website\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image",
  "markdown": "# Luma Image to Image - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Luma Image to Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg) The Luma Image to Image node allows you to modify existing images using Luma AI technology based on text prompts, while preserving certain features and structure of the original image.\n\nThis node connects to Luma AI’s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail, particularly excelling at generating photorealistic content and artistic style images.\n\nThe Luma Image to Image node analyzes the input image and combines it with text prompts to guide the modification process. It uses Luma AI’s generation models to make creative changes to images based on prompts. Node process:\n\nThe image\\_weight parameter controls the degree of influence from the original image - values closer to 0 will preserve more of the original image features, while values closer to 1 allow for more substantial modifications.\n\n```\n\nclass LumaImageModifyNode(ComfyNodeABC):\n    \"\"\"\n    Modifies images synchronously based on prompt and aspect ratio.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE,),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"image_weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.02,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of the image; the closer to 0.0, the less the image will be modified.\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaImageModel],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {},\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        image: torch.Tensor,\n        image_weight: float,\n        seed,\n        auth_token=None,\n        **kwargs,\n    ):\n        # first, upload image\n        download_urls = upload_images_to_comfyapi(\n            image, max_images=1, auth_token=auth_token\n        )\n        image_url = download_urls[0]\n        # next, make Luma call with download url provided\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations/image\",\n                method=HttpMethod.POST,\n                request_model=LumaImageGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaImageGenerationRequest(\n                prompt=prompt,\n                model=model,\n                modify_image_ref=LumaModifyImageRef(\n                    url=image_url, weight=round(image_weight, 2)\n                ),\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        img_response = requests.get(response_poll.assets.image)\n        img = process_image_response(img_response)\n        return (img,)\n\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/update-a-publisher",
  "markdown": "# Update a publisher - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n#### Response\n\nPublisher updated successfully\n\nThe response is of type `object`."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2",
  "markdown": "# OpenAI DALL·E 2 - ComfyUI Native Node Documentation\n\n![ComfyUI Native Stability AI Stable Image Ultra Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg) The OpenAI DALL·E 2 node allows you to use OpenAI’s DALL·E 2 API to generate creative images from text descriptions.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt for DALL·E to generate images, supports multi-line input |\n| seed | integer | 0   | The result is not actually related to the seed, this parameter only determines whether to re-execute |\n| size | select | ”1024x1024” | Output image size, options: 256x256, 512x512, 1024x1024 |\n| n   | integer | 1   | Number of images to generate, range 1-8 |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | None | Optional reference image for image editing |\n| mask | mask | None | Optional mask for inpainting (white areas will be replaced) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image(s) |\n\n## Features\n\n*   Basic function: Generate images from text prompts\n*   Image editing: When both image and mask parameters are provided, performs image editing (white masked areas will be replaced)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass OpenAIDalle2(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's DALL·E 2 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for DALL·E\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"256x256\", \"512x512\", \"1024x1024\"],\n                        \"default\": \"1024x1024\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"How many images to generate\",\n                    },\n                ),\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        image=None,\n        mask=None,\n        n=1,\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"dall-e-2\"\n        path = \"/proxy/openai/images/generations\"\n        content_type = \"application/json\"\n        request_class = OpenAIImageGenerationRequest\n        img_binary = None\n\n        if image is not None and mask is not None:\n            path = \"/proxy/openai/images/edits\"\n            content_type = \"multipart/form-data\"\n            request_class = OpenAIImageEditRequest\n\n            input_tensor = image.squeeze().cpu()\n            height, width, channels = input_tensor.shape\n            rgba_tensor = torch.ones(height, width, 4, device=\"cpu\")\n            rgba_tensor[:, :, :channels] = input_tensor\n\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n            rgba_tensor[:, :, 3] = 1 - mask.squeeze().cpu()\n\n            rgba_tensor = downscale_image_tensor(rgba_tensor.unsqueeze(0)).squeeze()\n\n            image_np = (rgba_tensor.numpy() * 255).astype(np.uint8)\n            img = Image.fromarray(image_np)\n            img_byte_arr = io.BytesIO()\n            img.save(img_byte_arr, format=\"PNG\")\n            img_byte_arr.seek(0)\n            img_binary = img_byte_arr  # .getvalue()\n            img_binary.name = \"image.png\"\n        elif image is not None or mask is not None:\n            raise Exception(\"Dall-E 2 image editing requires an image AND a mask\")\n\n        # Build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=path,\n                method=HttpMethod.POST,\n                request_model=request_class,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=request_class(\n                model=model,\n                prompt=prompt,\n                n=n,\n                size=size,\n                seed=seed,\n            ),\n            files=(\n                {\n                    \"image\": img_binary,\n                }\n                if img_binary\n                else None\n            ),\n            content_type=content_type,\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-all-nodes",
  "markdown": "# Retrieve all nodes - ComfyUI\n\n```\n[\n  {\n    \"author\": \"<string>\",\n    \"banner_url\": \"<string>\",\n    \"category\": \"<string>\",\n    \"created_at\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"downloads\": 123,\n    \"github_stars\": 123,\n    \"icon\": \"<string>\",\n    \"id\": \"<string>\",\n    \"latest_version\": {\n      \"changelog\": \"<string>\",\n      \"comfy_node_extract_status\": \"<string>\",\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"dependencies\": [\n        \"<string>\"\n      ],\n      \"deprecated\": true,\n      \"downloadUrl\": \"<string>\",\n      \"id\": \"<string>\",\n      \"node_id\": \"<string>\",\n      \"status\": \"NodeVersionStatusActive\",\n      \"status_reason\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"version\": \"<string>\"\n    },\n    \"license\": \"<string>\",\n    \"name\": \"<string>\",\n    \"preempted_comfy_node_names\": [\n      \"<string>\"\n    ],\n    \"publisher\": {\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"id\": \"<string>\",\n      \"logo\": \"<string>\",\n      \"members\": [\n        {\n          \"id\": \"<string>\",\n          \"role\": \"<string>\",\n          \"user\": {\n            \"email\": \"<string>\",\n            \"id\": \"<string>\",\n            \"name\": \"<string>\"\n          }\n        }\n      ],\n      \"name\": \"<string>\",\n      \"source_code_repo\": \"<string>\",\n      \"status\": \"PublisherStatusActive\",\n      \"support\": \"<string>\",\n      \"website\": \"<string>\"\n    },\n    \"rating\": 123,\n    \"repository\": \"<string>\",\n    \"search_ranking\": 123,\n    \"status\": \"NodeStatusActive\",\n    \"status_detail\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"tags\": [\n      \"<string>\"\n    ],\n    \"translations\": {}\n  }\n]\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-a-new-custom-node",
  "markdown": "# Create a new custom node\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n#### Response\n\nNode created successfully\n\nThe response is of type `object`."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb",
  "markdown": "# Recraft Color RGB - ComfyUI Native Node Documentation\n\n ![ComfyUI Native Recraft Color RGB Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg) The Recraft Color RGB node lets you define precise RGB color values to control colors in Recraft image generation.\n\n## Node Function\n\nThis node creates a color configuration object that connects to the Recraft Controls node to specify colors used in generated images.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| r   | integer | 0   | Red channel (0-255) |\n| g   | integer | 0   | Green channel (0-255) |\n| b   | integer | 0   | Blue channel (0-255) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_color | Recraft Color | Color config object to connect to Recraft Controls |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftColorRGBNode:\n    \"\"\"\n    Create Recraft Color by choosing specific RGB values.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.COLOR,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    RETURN_NAMES = (\"recraft_color\",)\n    FUNCTION = \"create_color\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"r\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Red value of color.\"\n                }),\n                \"g\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Green value of color.\"\n                }),\n                \"b\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Blue value of color.\"\n                }),\n            },\n            \"optional\": {\n                \"recraft_color\": (RecraftIO.COLOR,),\n            }\n        }\n\n    def create_color(self, r: int, g: int, b: int, recraft_color: RecraftColorChain=None):\n        recraft_color = recraft_color.clone() if recraft_color else RecraftColorChain()\n        recraft_color.add(RecraftColor(r, g, b))\n        return (recraft_color, )\n\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/delete-a-publisher",
  "markdown": "# Delete a publisher - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Response\n\nPublisher deleted successfully\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls",
  "markdown": "# Recraft Controls - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Controls Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg) The Recraft Controls node lets you define control parameters (like colors and background colors) to guide Recraft’s image generation process. This node combines multiple control inputs into a unified control object.\n\n## Parameters\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| colors | Recraft Color | Color controls for image generation |\n| background\\_color | Recraft Color | Background color control |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_controls | Recraft Controls | Control config object for Recraft generation nodes |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## How It Works\n\nNode process:\n\n1.  Collects input control parameters (colors and background\\_color)\n2.  Combines these parameters into a structured control object\n3.  Outputs this control object for connecting to Recraft generation nodes\n\nWhen connected to Recraft generation nodes, these control parameters influence the AI generation process. The AI considers multiple factors beyond just the text prompt’s semantic content. If color inputs are configured, the AI will try to use these colors appropriately in the generated image.\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftControlsNode:\n    \"\"\"\n    Create Recraft Controls for customizing Recraft generation.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.CONTROLS,)\n    RETURN_NAMES = (\"recraft_controls\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_controls\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n            },\n            \"optional\": {\n                \"colors\": (RecraftIO.COLOR,),\n                \"background_color\": (RecraftIO.COLOR,),\n            }\n        }\n\n    def create_controls(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None):\n        return (RecraftControls(colors=colors, background_color=background_color), )\n\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-all-nodes-1",
  "markdown": "# Retrieve all nodes - ComfyUI\n\n```\n{\n  \"limit\": 123,\n  \"nodes\": [\n    {\n      \"author\": \"<string>\",\n      \"banner_url\": \"<string>\",\n      \"category\": \"<string>\",\n      \"created_at\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"downloads\": 123,\n      \"github_stars\": 123,\n      \"icon\": \"<string>\",\n      \"id\": \"<string>\",\n      \"latest_version\": {\n        \"changelog\": \"<string>\",\n        \"comfy_node_extract_status\": \"<string>\",\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"dependencies\": [\n          \"<string>\"\n        ],\n        \"deprecated\": true,\n        \"downloadUrl\": \"<string>\",\n        \"id\": \"<string>\",\n        \"node_id\": \"<string>\",\n        \"status\": \"NodeVersionStatusActive\",\n        \"status_reason\": \"<string>\",\n        \"supported_accelerators\": [\n          \"<string>\"\n        ],\n        \"supported_comfyui_frontend_version\": \"<string>\",\n        \"supported_comfyui_version\": \"<string>\",\n        \"supported_os\": [\n          \"<string>\"\n        ],\n        \"version\": \"<string>\"\n      },\n      \"license\": \"<string>\",\n      \"name\": \"<string>\",\n      \"preempted_comfy_node_names\": [\n        \"<string>\"\n      ],\n      \"publisher\": {\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"description\": \"<string>\",\n        \"id\": \"<string>\",\n        \"logo\": \"<string>\",\n        \"members\": [\n          {\n            \"id\": \"<string>\",\n            \"role\": \"<string>\",\n            \"user\": {\n              \"email\": \"<string>\",\n              \"id\": \"<string>\",\n              \"name\": \"<string>\"\n            }\n          }\n        ],\n        \"name\": \"<string>\",\n        \"source_code_repo\": \"<string>\",\n        \"status\": \"PublisherStatusActive\",\n        \"support\": \"<string>\",\n        \"website\": \"<string>\"\n      },\n      \"rating\": 123,\n      \"repository\": \"<string>\",\n      \"search_ranking\": 123,\n      \"status\": \"NodeStatusActive\",\n      \"status_detail\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"tags\": [\n        \"<string>\"\n      ],\n      \"translations\": {}\n    }\n  ],\n  \"page\": 123,\n  \"total\": 123,\n  \"totalPages\": 123\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/update-a-specific-node",
  "markdown": "# Update a specific node - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n#### Response\n\nNode updated successfully\n\nThe response is of type `object`."
},
{
  "url": "https://docs.comfy.org/api-reference/registry/delete-a-specific-node",
  "markdown": "# Delete a specific node - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Response\n\nNode deleted successfully"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/claim-nodeid-into-publisherid-for-the-authenticated-publisher",
  "markdown": "# Claim nodeId into publisherId for the authenticated publisher\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n#### Response\n\nNode claimed successfully\n\nClaim nodeId into publisherId for the authenticated publisher"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fcreate-text-to-video-prompt",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/installation/desktop/windows",
  "markdown": "# Windows Desktop Version - ComfyUI\n\n**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files. You can quickly migrate from an existing [ComfyUI Portable version](https://docs.comfy.org/installation/comfyui_portable_windows) to the Desktop version. ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop) ComfyUI Desktop hardware requirements:\n\n*   NVIDIA GPU\n\nThis tutorial will guide you through the software installation process and explain related configuration details.\n\n## ComfyUI Desktop (Windows) Download\n\nPlease click the button below to download the installation package for Windows **ComfyUI Desktop**\n\n[\n\nDownload for Windows (NVIDIA)\n\n](https://download.comfy.org/windows/nsis/x64)\n\n## ComfyUI Desktop Installation Steps\n\nDouble-click the downloaded installation package file, which will first perform an automatic installation and create a **ComfyUI Desktop** shortcut on the desktop ![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-shortcut.jpg) Double-click the corresponding shortcut to enter ComfyUI initialization settings\n\n### ComfyUI Desktop Initialization Process\n\n## First Image Generation\n\nAfter successful installation, you can refer to the section below to start your ComfyUI journey~\n\n[\n\n## First Image Generation\n\nThis tutorial will guide you through your first model installation and text-to-image generation\n\n\n\n](https://docs.comfy.org/get_started/first_generation)\n\n## How to Update ComfyUI Desktop\n\nCurrently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings ![ComfyUI Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg) You can also choose to manually check for available updates in the `Menu` —> `Help` —> `Check for Updates` ![ComfyUI Desktop Check for Updates](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_check_for_updates.jpg)\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI’s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 lora/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## Desktop Python Environment\n\nThe desktop installation will create a Python virtual environment in your chosen installation directory, typically a hidden `.venv` folder. If you need to handle dependencies for ComfyUI plugins, you’ll need to do so within this environment. Using the system command line directly risks installing dependencies to the system environment, so please follow the instructions below to activate the appropriate environment.\n\n### How to use the Desktop Python environment?\n\nYou can use the built-in terminal in the desktop app to access the Python environment. ![ComfyUI Desktop Terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_terminal.jpg) \n\n1.  Click the icon in the menu bar to open the bottom panel\n2.  Click `Terminal` to open the terminal\n3.  If you want to check the Python installation location for the corresponding environment, you can use the following command\n\n```\n  python -c \"import sys; print(sys.executable)\"\n```\n\n## How to Uninstall ComfyUI Desktop\n\nFor **ComfyUI Desktop** you can use the system uninstall function in Windows Settings to complete software uninstallation ![ComfyUI Desktop Uninstallation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-uninstall-comfyui.jpg) If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:\n\n*   C:\\\\Users<YOUR\\_USERNAME>\\\\AppData\\\\Local@comfyorgcomfyui-electron-updater\n*   C:\\\\Users<YOUR\\_USERNAME>\\\\AppData\\\\Local\\\\Programs@comfyorgcomfyui-electron\n*   C:\\\\Users<YOUR\\_USERNAME>\\\\AppData\\\\Roaming\\\\ComfyUI\n\nThe above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:\n\n*   models files\n*   custom nodes\n*   input/output directories\n\n## Troubleshooting\n\n### Display unsupported devices\n\n![ComfyUI Installation Steps - Unsupported Device](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-0.jpg) Since ComfyUI Desktop (Windows) only supports **NVIDIA GPUs with CUDA**, you may see this screen if your device is not supported\n\n*   Please switch to a supported device\n*   Or consider using [ComfyUI Portable](https://docs.comfy.org/installation/comfyui_portable_windows) or through [manual installation](https://docs.comfy.org/installation/manual_install) to use ComfyUI\n\n### ​Error identification​\n\nIf installation fails, you should see the following screen ![ComfyUI Installation Failed](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg) It is recommended to take these steps to find the error cause:\n\n1.  Click `Show Terminal` to view error output\n2.  Click `Open Logs` to view installation logs\n3.  Visit official forum to search for error reports\n4.  Click `Reinstall` to try reinstalling\n\nBefore submitting feedback, it’s recommended to provide the **error output** and **log files** to tools like **GPT** ![ComfyUI Installation Failed - Error Log](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg) ![ComfyUI Installation Failed - GPT Feedback](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg) As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.\n\n### Feedback Installation Failure\n\nIf you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:\n\n*   Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)\n*   Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)\n\nWhen submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:\n\n1.  Log Files\n\n| Filename | Description | Location |\n| --- | --- | --- |\n| main.log | Contains logs related to desktop application and server startup from the Electron process |     |\n| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output |     |\n\n![ComfyUI Log Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)\n\n2.  Configuration Files\n\n| Filename | Description | Location |\n| --- | --- | --- |\n| extra\\_model\\_paths.yaml | Contains additional paths where ComfyUI will search for models and custom nodes |     |\n| config.json | Contains application configuration. This file should not be edited directly |     |\n\n![ComfyUI Config Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fcreate-image-to-video-prompt",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/installation/comfyui_portable_windows",
  "markdown": "# ComfyUI(portable) Windows - ComfyUI\n\n**ComfyUI Portable** is a standalone packaged complete ComfyUI Windows version that has integrated an independent **Python (python\\_embeded)** required for ComfyUI to run. You only need to extract it to use it. Currently, the portable version supports running through **Nvidia GPU** or **CPU**. This guide section will walk you through installing ComfyUI Portable.\n\n## Download ComfyUI Portable\n\nYou can get the latest ComfyUI Portable download link by clicking the link below\n\n[\n\nDownload ComfyUI Portable\n\n](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)\n\nAfter downloading, you can use decompression software like [7-ZIP](https://7-zip.org/) to extract the compressed package The file structure and description after extracting the portable version are as follows:\n\n```\nComfyUI_windows_portable\n├── 📂ComfyUI                   // ComfyUI main program\n├── 📂python_embeded            // Independent Python environment\n├── 📂update                    // Batch scripts for upgrading portable version\n├── README_VERY_IMPORTANT.txt   // ComfyUI Portable usage instructions in English\n├── run_cpu.bat                 // Double click to start ComfyUI (CPU only)\n└── run_nvidia_gpu.bat          // Double click to start ComfyUI (Nvidia GPU)\n```\n\n## How to Launch ComfyUI\n\nDouble click either `run_nvidia_gpu.bat` or `run_cpu.bat` depending on your computer’s configuration to launch ComfyUI. You will see the command running as shown in the image below ![ComfyUI Portable Command Prompt](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui-portable-cmd.png) When you see something similar to the image\n\n```\nTo see the GUI go to: http://127.0.0.1:8188\n```\n\nAt this point, your ComfyUI service has started. Normally, ComfyUI will automatically open your default browser and navigate to `http://127.0.0.1:8188`. If it doesn’t open automatically, please manually open your browser and visit this address.\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI’s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 lora/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## First Image Generation\n\nAfter successful installation, you can refer to the section below to start your ComfyUI journey~\n\n[\n\n## First Image Generation\n\nThis tutorial will guide you through your first model installation and text-to-image generation\n\n\n\n](https://docs.comfy.org/get_started/first_generation)\n\n## Additional ComfyUI Portable Instructions\n\n### 1\\. Upgrading ComfyUI Portable\n\nYou can use the batch commands in the update folder to upgrade your ComfyUI Portable version\n\n```\nComfyUI_windows_portable\n└─ 📂update\n   ├── update.py\n   ├── update_comfyui.bat                          // Update ComfyUI to the latest commit version\n   ├── update_comfyui_and_python_dependencies.bat  // Only use when you have issues with your runtime environment\n   └── update_comfyui_stable.bat                   // Update ComfyUI to the latest stable version\n```\n\n### 2\\. Setting Up LAN Access for ComfyUI Portable\n\nIf your ComfyUI is running on a local network and you want other devices to access ComfyUI, you can modify the `run_nvidia_gpu.bat` or `run_cpu.bat` file using Notepad to complete the configuration. This is mainly done by adding `--listen` to specify the listening address. Below is an example of the `run_nvidia_gpu.bat` file command with the `--listen` parameter added\n\n```\n.\\python_embeded\\python.exe -s ComfyUI\\main.py --listen --windows-standalone-build\npause\n```\n\nAfter enabling ComfyUI, you will notice the final running address will become\n\n```\nStarting server\n\nTo see the GUI go to: http://0.0.0.0:8188\nTo see the GUI go to: http://[::]:8188\n```\n\nYou can press `WIN + R` and type `cmd` to open the command prompt, then enter `ipconfig` to view your local IP address. Other devices can then access ComfyUI by entering `http://your-local-IP:8188` in their browser."
},
{
  "url": "https://docs.comfy.org/installation/manual_install",
  "markdown": "# How to install ComfyUI manually in different systems\n\nFor the installation of ComfyUI, it is mainly divided into several steps:\n\n1.  Create a virtual environment(avoid polluting the system-level Python environment)\n2.  Clone the ComfyUI code repository\n3.  Install dependencies\n4.  Start ComfyUI\n\nYou can also refer to [ComfyUI CLI](https://docs.comfy.org/comfy-cli/getting-started) to install ComfyUI, it is a command line tool that can easily install ComfyUI and manage its dependencies.\n\n## Create a virtual environment\n\n[Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI. Create an environment with Conda.\n\n```\nconda create -n comfyenv\nconda activate comfyenv\n```\n\n## Clone the ComfyUI code repository\n\nYou need to ensure that you have installed [Git](https://git-scm.com/downloads) on your system. First, you need to open the terminal (command line), then clone the code repository.\n\n```\ngit clone git@github.com:comfyanonymous/ComfyUI.git\n```\n\n## Install GPU and ComfyUI dependencies\n\n## How to update ComfyUI\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI’s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 lora/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```"
},
{
  "url": "https://docs.comfy.org/installation/install_custom_node",
  "markdown": "# How to Install Custom Nodes in ComfyUI\n\n## What are Custom Nodes ?\n\nCustom nodes are extensions for ComfyUI that add new functionality like advanced image processing, machine learning fine-tuning, color adjustments, and more. These community-developed nodes can significantly expand ComfyUI’s core capabilities.\n\nAll custom node installations require completing these two steps:\n\n1.  Clone the node code to the `ComfyUI/custom_nodes` directory\n2.  Install the required Python dependencies\n\nThis guide covers three installation methods. Here’s a comparison of their pros and cons. While [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) isn’t yet part of the core dependencies, it will be in the future. We still provide other installation guides to meet different needs.\n\n| Method | Advantages | Disadvantages |\n| --- | --- | --- |\n| **ComfyUI Manager** (Recommended) | 1\\. Automated installation  <br>2\\. Dependency handling  <br>3\\. GUI interface | Cannot directly search for nodes not registered in the registry |\n| **Git Clone** | Can install nodes not registered in the registry | 1\\. Requires Git knowledge  <br>2\\. Manual dependency handling  <br>3\\. Installation risks |\n| **Repository ZIP Download** | 1\\. No Git required  <br>2\\. Manual control | 1\\. Manual dependency handling  <br>2\\. No version control  <br>3\\. Installation risks |\n\nTip: Before installing custom nodes, check the plugin’s README file to understand installation methods, usage, and requirements like specific models, dependency versions, and common issue solutions.\n\n## Method 1: ComfyUI Manager (Recommended)\n\n## Method 2: Manual Installation Using Git\n\nSuitable for new nodes not found in Manager or when specific versions are needed. Requires [Git](https://git-scm.com/) installed on your system.\n\n## Method 3: ZIP Download Installation\n\nSuitable for users who cannot use Git or Manager"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fchangelog",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image",
  "markdown": "# Stability AI Stable Diffusion 3.5 - ComfyUI Native Node Documentation\n\n![ComfyUI Native Stability AI Stable Diffusion 3.5 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg) The Stability AI Stable Diffusion 3.5 Image node uses Stability AI’s Stable Diffusion 3.5 API to generate high-quality images. It supports both text-to-image and image-to-image generation, capable of creating detailed visual content from text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | What you want to see in the output image. Strong, descriptive prompts that clearly define elements, colors and themes will yield better results |\n| model | select | \\-  | Choose which Stability SD 3.5 model to use |\n| aspect\\_ratio | select | ”1:1” | Width to height ratio of generated image |\n| style\\_preset | select | ”None” | Optional preset style for the desired image |\n| cfg\\_scale | float | 4.0 | How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt). Range: 1.0 - 10.0, Step: 0.1 |\n| seed | integer | 0   | Random seed for noise generation (0-4294967294) |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image. When provided, the node switches to image-to-image mode |\n| negative\\_prompt | string | \"\"  | Keywords of what you don’t want to see in the output image. This is an advanced feature |\n| image\\_denoise | float | 0.5 | Denoising strength for input image. 0.0 yields image identical to input, 1.0 is as if no image was provided at all. Range: 0.0 - 1.0, Step: 0.01. Only effective when image is provided |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image |\n\n## Usage Example\n\n[\n\n## Stability AI Stable Diffusion 3.5 Image Workflow Example\n\nStability AI Stable Diffusion 3.5 Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)\n\n## Notes\n\n*   When an input image is provided, the node switches from text-to-image mode to image-to-image mode\n*   In image-to-image mode, aspect ratio parameters are ignored\n*   Mode selection automatically switches based on whether an image is provided:\n    *   No image provided: text-to-image mode\n    *   Image provided: image-to-image mode\n*   If style\\_preset is set to “None”, no preset style will be applied\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-07)\\]\n\n```\nclass StabilityStableImageSD_3_5Node:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Stability AI\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results.\"\n                    },\n                ),\n                \"model\": ([x.value for x in Stability_SD3_5_Model],),\n                \"aspect_ratio\": ([x.value for x in StabilityAspectRatio],\n                    {\n                        \"default\": StabilityAspectRatio.ratio_1_1,\n                        \"tooltip\": \"Aspect ratio of generated image.\",\n                    },\n                ),\n                \"style_preset\": (get_stability_style_presets(),\n                    {\n                        \"tooltip\": \"Optional desired style of generated image.\",\n                    },\n                ),\n                \"cfg_scale\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 4.0,\n                        \"min\": 1.0,\n                        \"max\": 10.0,\n                        \"step\": 0.1,\n                        \"tooltip\": \"How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 4294967294,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (IO.IMAGE,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"Keywords of what you do not wish to see in the output image. This is an advanced feature.\"\n                    },\n                ),\n                \"image_denoise\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(self, model: str, prompt: str, aspect_ratio: str, style_preset: str, seed: int, cfg_scale: float,\n                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,\n                 auth_token=None):\n        validate_string(prompt, strip_whitespace=False)\n        # prepare image binary if image present\n        image_binary = None\n        mode = Stability_SD3_5_GenerationMode.text_to_image\n        if image is not None:\n            image_binary = tensor_to_bytesio(image, total_pixels=1504*1504).read()\n            mode = Stability_SD3_5_GenerationMode.image_to_image\n            aspect_ratio = None\n        else:\n            image_denoise = None\n\n        if not negative_prompt:\n            negative_prompt = None\n        if style_preset == \"None\":\n            style_preset = None\n\n        files = {\n            \"image\": image_binary\n        }\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/stability/v2beta/stable-image/generate/sd3\",\n                method=HttpMethod.POST,\n                request_model=StabilityStable3_5Request,\n                response_model=StabilityStableUltraResponse,\n            ),\n            request=StabilityStable3_5Request(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                aspect_ratio=aspect_ratio,\n                seed=seed,\n                strength=image_denoise,\n                style_preset=style_preset,\n                cfg_scale=cfg_scale,\n                model=model,\n                mode=mode,\n            ),\n            files=files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.finish_reason != \"SUCCESS\":\n            raise Exception(f\"Stable Diffusion 3.5 Image generation failed: {response_api.finish_reason}.\")\n\n        image_data = base64.b64decode(response_api.image)\n        returned_image = bytesio_to_image_tensor(BytesIO(image_data))\n\n        return (returned_image,)\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fnodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fworkflow",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/custom-nodes/overview",
  "markdown": "# Overview - ComfyUI\n\nCustom nodes allow you to implement new features and share them with the wider community. A custom node is like any Comfy node: it takes input, does something to it, and produces an output. While some custom nodes perform highly complex tasks, many just do one thing. Here’s an example of a simple node that takes an image and inverts it. ![Unique Images Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/invert_image_node.png)\n\n## Client-Server Model\n\nComfy runs in a client-server model. The server, written in Python, handles all the real work: data-processing, models, image diffusion etc. The client, written in Javascript, handles the user interface. Comfy can also be used in API mode, in which a workflow is sent to the server by a non-Comfy client (such as another UI, or a command line script). Custom nodes can be placed into one of four categories:\n\n### Server side only\n\nThe majority of Custom Nodes run purely on the server side, by defining a Python class that specifies the input and output types, and provides a function that can be called to process inputs and produce an output.\n\n### Client side only\n\nA few Custom Nodes provide a modification to the client UI, but do not add core functionality. Despite the name, they may not even add new nodes to the system.\n\n### Independent Client and Server\n\nCustom nodes may provide additional server features, and additional (related) UI features (such as a new widget to deal with a new data type). In most cases, communication between the client and server can be handled by the Comfy data flow control.\n\n### Connected Client and Server\n\nIn a small number of cases, the UI features and the server need to interact with each other directly."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video",
  "markdown": "# Google Veo2 Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native Google Veo2 Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg) The Google Veo2 Video node generates high-quality videos from text descriptions using Google’s Veo2 API technology, converting text prompts into dynamic video content.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text description of the video content to generate |\n| aspect\\_ratio | select | ”16:9” | Output video aspect ratio, “16:9” or “9:16” |\n| negative\\_prompt | string | \"\"  | Text describing what to avoid in the video |\n| duration\\_seconds | integer | 5   | Video duration, 5-8 seconds |\n| enhance\\_prompt | boolean | True | Whether to use AI to enhance the prompt |\n| person\\_generation | select | ”ALLOW” | Allow or block person generation, “ALLOW” or “BLOCK” |\n| seed | integer | 0   | Random seed, 0 means randomly generated |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | None | Optional reference image to guide video creation |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass VeoVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos from text prompts using Google's Veo API.\n\n    This node can create videos from text descriptions and optional image inputs,\n    with control over parameters like aspect ratio, duration, and more.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text description of the video\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"16:9\", \"9:16\"],\n                        \"default\": \"16:9\",\n                        \"tooltip\": \"Aspect ratio of the output video\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Negative text prompt to guide what to avoid in the video\",\n                    },\n                ),\n                \"duration_seconds\": (\n                    IO.INT,\n                    {\n                        \"default\": 5,\n                        \"min\": 5,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"Duration of the output video in seconds\",\n                    },\n                ),\n                \"enhance_prompt\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": True,\n                        \"tooltip\": \"Whether to enhance the prompt with AI assistance\",\n                    }\n                ),\n                \"person_generation\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"ALLOW\", \"BLOCK\"],\n                        \"default\": \"ALLOW\",\n                        \"tooltip\": \"Whether to allow generating people in the video\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFF,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation (0 for random)\",\n                    },\n                ),\n                \"image\": (IO.IMAGE, {\n                    \"default\": None,\n                    \"tooltip\": \"Optional reference image to guide video generation\",\n                }),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (IO.VIDEO,)\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Veo\"\n    DESCRIPTION = \"Generates videos from text prompts using Google's Veo API\"\n    API_NODE = True\n\n    def generate_video(\n        self,\n        prompt,\n        aspect_ratio=\"16:9\",\n        negative_prompt=\"\",\n        duration_seconds=5,\n        enhance_prompt=True,\n        person_generation=\"ALLOW\",\n        seed=0,\n        image=None,\n        auth_token=None,\n    ):\n        # Prepare the instances for the request\n        instances = []\n\n        instance = {\n            \"prompt\": prompt\n        }\n\n        # Add image if provided\n        if image is not None:\n            image_base64 = convert_image_to_base64(image)\n            if image_base64:\n                instance[\"image\"] = {\n                    \"bytesBase64Encoded\": image_base64,\n                    \"mimeType\": \"image/png\"\n                }\n\n        instances.append(instance)\n\n        # Create parameters dictionary\n        parameters = {\n            \"aspectRatio\": aspect_ratio,\n            \"personGeneration\": person_generation,\n            \"durationSeconds\": duration_seconds,\n            \"enhancePrompt\": enhance_prompt,\n        }\n\n        # Add optional parameters if provided\n        if negative_prompt:\n            parameters[\"negativePrompt\"] = negative_prompt\n        if seed > 0:\n            parameters[\"seed\"] = seed\n\n        # Initial request to start video generation\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/veo/generate\",\n                method=HttpMethod.POST,\n                request_model=Veo2GenVidRequest,\n                response_model=Veo2GenVidResponse\n            ),\n            request=Veo2GenVidRequest(\n                instances=instances,\n                parameters=parameters\n            ),\n            auth_token=auth_token\n        )\n\n        initial_response = initial_operation.execute()\n        operation_name = initial_response.name\n\n        logging.info(f\"Veo generation started with operation name: {operation_name}\")\n\n        # Define status extractor function\n        def status_extractor(response):\n            # Only return \"completed\" if the operation is done, regardless of success or failure\n            # We'll check for errors after polling completes\n            return \"completed\" if response.done else \"pending\"\n\n        # Define progress extractor function\n        def progress_extractor(response):\n            # Could be enhanced if the API provides progress information\n            return None\n\n        # Define the polling operation\n        poll_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=\"/proxy/veo/poll\",\n                method=HttpMethod.POST,\n                request_model=Veo2GenVidPollRequest,\n                response_model=Veo2GenVidPollResponse\n            ),\n            completed_statuses=[\"completed\"],\n            failed_statuses=[],  # No failed statuses, we'll handle errors after polling\n            status_extractor=status_extractor,\n            progress_extractor=progress_extractor,\n            request=Veo2GenVidPollRequest(\n                operationName=operation_name\n            ),\n            auth_token=auth_token,\n            poll_interval=5.0\n        )\n\n        # Execute the polling operation\n        poll_response = poll_operation.execute()\n\n        # Now check for errors in the final response\n        # Check for error in poll response\n        if hasattr(poll_response, 'error') and poll_response.error:\n            error_message = f\"Veo API error: {poll_response.error.message} (code: {poll_response.error.code})\"\n            logging.error(error_message)\n            raise Exception(error_message)\n\n        # Check for RAI filtered content\n        if (hasattr(poll_response.response, 'raiMediaFilteredCount') and\n            poll_response.response.raiMediaFilteredCount > 0):\n\n            # Extract reason message if available\n            if (hasattr(poll_response.response, 'raiMediaFilteredReasons') and\n                poll_response.response.raiMediaFilteredReasons):\n                reason = poll_response.response.raiMediaFilteredReasons[0]\n                error_message = f\"Content filtered by Google's Responsible AI practices: {reason} ({poll_response.response.raiMediaFilteredCount} videos filtered.)\"\n            else:\n                error_message = f\"Content filtered by Google's Responsible AI practices ({poll_response.response.raiMediaFilteredCount} videos filtered.)\"\n\n            logging.error(error_message)\n            raise Exception(error_message)\n\n        # Extract video data\n        video_data = None\n        if poll_response.response and hasattr(poll_response.response, 'videos') and poll_response.response.videos and len(poll_response.response.videos) > 0:\n            video = poll_response.response.videos[0]\n\n            # Check if video is provided as base64 or URL\n            if hasattr(video, 'bytesBase64Encoded') and video.bytesBase64Encoded:\n                # Decode base64 string to bytes\n                video_data = base64.b64decode(video.bytesBase64Encoded)\n            elif hasattr(video, 'gcsUri') and video.gcsUri:\n                # Download from URL\n                video_url = video.gcsUri\n                video_response = requests.get(video_url)\n                video_data = video_response.content\n            else:\n                raise Exception(\"Video returned but no data or URL was provided\")\n        else:\n            raise Exception(\"Video generation completed but no video was returned\")\n\n        if not video_data:\n            raise Exception(\"No video data was returned\")\n\n        logging.info(\"Video generation completed successfully\")\n\n        # Convert video data to BytesIO object\n        video_io = io.BytesIO(video_data)\n\n        # Return VideoFromFile object\n        return (VideoFromFile(video_io),)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v",
  "markdown": "# Kling Text to Video (Camera Control) - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Text to Video (Camera Control) Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg) The Kling Text to Video (Camera Control) node converts text into videos with professional camera movements. It extends the standard Kling Text to Video node by adding camera control capabilities.\n\n```\n\nclass KlingCameraControlT2VNode(KlingTextToVideoNode):\n    \"\"\"\n    Kling Text to Video Camera Control Node. This node is a text to video node, but it supports controlling the camera.\n    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingText2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingText2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"camera_control\": (\n                    \"CAMERA_CONTROL\",\n                    {\n                        \"tooltip\": \"Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Transform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text.\"\n\n    def api_call(\n        self,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        camera_control: Optional[CameraControl] = None,\n        auth_token: Optional[str] = None,\n    ):\n        return super().api_call(\n            model_name=\"kling-v1-5\",\n            cfg_scale=cfg_scale,\n            mode=\"pro\",\n            aspect_ratio=aspect_ratio,\n            duration=\"5\",\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            camera_control=camera_control,\n            auth_token=auth_token,\n        )\n\n\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls",
  "markdown": "# Kling Camera Controls - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Kling Camera Controls Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg) The Kling Camera Controls node defines virtual camera behavior parameters to control camera movement and view changes during Kling video generation.\n\n**Note**: At least one non-zero camera control parameter is required for the effect to work.\n\n**Note**: Not all model and mode combinations support camera control. Please check the Kling API documentation for details.\n\n```\n\nclass KlingCameraControls(KlingNodeBase):\n    \"\"\"Kling Camera Controls Node\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"camera_control_type\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\n                            camera_control_type.value\n                            for camera_control_type in CameraType\n                        ],\n                        \"default\": \"simple\",\n                        \"tooltip\": \"Predefined camera movements type. simple: Customizable camera movement. down_back: Camera descends and moves backward. forward_up: Camera moves forward and tilts up. right_turn_forward: Rotate right and move forward. left_turn_forward: Rotate left and move forward.\",\n                    },\n                ),\n                \"horizontal_movement\": get_camera_control_input_config(\n                    \"Controls camera's movement along horizontal axis (x-axis). Negative indicates left, positive indicates right\"\n                ),\n                \"vertical_movement\": get_camera_control_input_config(\n                    \"Controls camera's movement along vertical axis (y-axis). Negative indicates downward, positive indicates upward.\"\n                ),\n                \"pan\": get_camera_control_input_config(\n                    \"Controls camera's rotation in vertical plane (x-axis). Negative indicates downward rotation, positive indicates upward rotation.\",\n                    default=0.5,\n                ),\n                \"tilt\": get_camera_control_input_config(\n                    \"Controls camera's rotation in horizontal plane (y-axis). Negative indicates left rotation, positive indicates right rotation.\",\n                ),\n                \"roll\": get_camera_control_input_config(\n                    \"Controls camera's rolling amount (z-axis). Negative indicates counterclockwise, positive indicates clockwise.\",\n                ),\n                \"zoom\": get_camera_control_input_config(\n                    \"Controls change in camera's focal length. Negative indicates narrower field of view, positive indicates wider field of view.\",\n                ),\n            }\n        }\n\n    DESCRIPTION = \"Kling Camera Controls Node. Not all model and mode combinations support camera control. Please refer to the Kling API documentation for more information.\"\n    RETURN_TYPES = (\"CAMERA_CONTROL\",)\n    RETURN_NAMES = (\"camera_control\",)\n    FUNCTION = \"main\"\n\n    @classmethod\n    def VALIDATE_INPUTS(\n        cls,\n        horizontal_movement: float,\n        vertical_movement: float,\n        pan: float,\n        tilt: float,\n        roll: float,\n        zoom: float,\n    ) -> bool | str:\n        if not is_valid_camera_control_configs(\n            [\n                horizontal_movement,\n                vertical_movement,\n                pan,\n                tilt,\n                roll,\n                zoom,\n            ]\n        ):\n            return \"Invalid camera control configs: at least one of the values must be non-zero\"\n        return True\n\n    def main(\n        self,\n        camera_control_type: str,\n        horizontal_movement: float,\n        vertical_movement: float,\n        pan: float,\n        tilt: float,\n        roll: float,\n        zoom: float,\n    ) -> tuple[CameraControl]:\n        return (\n            CameraControl(\n                type=CameraType(camera_control_type),\n                config=CameraConfig(\n                    horizontal=horizontal_movement,\n                    vertical=vertical_movement,\n                    pan=pan,\n                    roll=roll,\n                    tilt=tilt,\n                    zoom=zoom,\n                ),\n            ),\n        )\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video",
  "markdown": "# Kling Image to Video - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg) The Kling Image to Video node converts static images into dynamic video content using Kling’s image-to-video API.\n\n## Parameters\n\n### Basic Parameters\n\nAll parameters below are required:\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| start\\_frame | Image | \\-  | Input source image |\n| prompt | String | \"\"  | Text prompt describing video action and content |\n| negative\\_prompt | String | \"\"  | Elements to avoid in the video |\n| cfg\\_scale | Float | 7.0 | Controls how closely to follow the prompt |\n| model\\_name | Select | ”kling-v1-5” | Model type to use |\n| aspect\\_ratio | Select | ”16:9” | Output video aspect ratio |\n| duration | Select | ”5s” | Generated video duration |\n| mode | Select | ”pro” | Video generation mode |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n| video\\_id | String | Unique video identifier |\n| duration | String | Actual video duration |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass KlingImage2VideoNode(KlingNodeBase):\n    \"\"\"Kling Image to Video Node\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"model_name\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"model_name\",\n                    enum_type=KlingVideoGenModelName,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"mode\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"mode\",\n                    enum_type=KlingVideoGenMode,\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=KlingVideoGenAspectRatio,\n                ),\n                \"duration\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"duration\",\n                    enum_type=KlingVideoGenDuration,\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (\"VIDEO\", \"STRING\", \"STRING\")\n    RETURN_NAMES = (\"VIDEO\", \"video_id\", \"duration\")\n    DESCRIPTION = \"Kling Image to Video Node\"\n\n    def get_response(self, task_id: str, auth_token: str) -> KlingImage2VideoResponse:\n        return poll_until_finished(\n            auth_token,\n            ApiEndpoint(\n                path=f\"{PATH_IMAGE_TO_VIDEO}/{task_id}\",\n                method=HttpMethod.GET,\n                request_model=KlingImage2VideoRequest,\n                response_model=KlingImage2VideoResponse,\n            ),\n        )\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        model_name: str,\n        cfg_scale: float,\n        mode: str,\n        aspect_ratio: str,\n        duration: str,\n        camera_control: Optional[KlingCameraControl] = None,\n        end_frame: Optional[torch.Tensor] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_I2V)\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_IMAGE_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=KlingImage2VideoRequest,\n                response_model=KlingImage2VideoResponse,\n            ),\n            request=KlingImage2VideoRequest(\n                model_name=KlingVideoGenModelName(model_name),\n                image=tensor_to_base64_string(start_frame),\n                image_tail=(\n                    tensor_to_base64_string(end_frame)\n                    if end_frame is not None\n                    else None\n                ),\n                prompt=prompt,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                cfg_scale=cfg_scale,\n                mode=KlingVideoGenMode(mode),\n                aspect_ratio=KlingVideoGenAspectRatio(aspect_ratio),\n                duration=KlingVideoGenDuration(duration),\n                camera_control=camera_control,\n            ),\n            auth_token=auth_token,\n        )\n\n        task_creation_response = initial_operation.execute()\n        validate_task_creation_response(task_creation_response)\n        task_id = task_creation_response.data.task_id\n\n        final_response = self.get_response(task_id, auth_token)\n        validate_video_result_response(final_response)\n\n        video = get_video_from_response(final_response)\n        return video_result_to_node_output(video)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts",
  "markdown": "# Luma Concepts - ComfyUI Native Node Documentation\n\n![ComfyUI Native Luma Concepts Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg) The Luma Concepts node allows you to apply predefined camera concepts to the Luma generation process, providing precise control over camera angles and perspectives without complex prompt descriptions.\n\n## Node Function\n\nThis node serves as a helper tool for Luma generation nodes, enabling users to select and apply predefined camera concepts. These concepts include different shooting angles (like overhead or low angle), camera distances (like close-up or long shot), and movement styles (like push-in or follow). It simplifies the creative workflow by providing an intuitive way to control camera effects in the generated output.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| concept1 | select | First camera concept choice, includes various presets and “none” |\n| concept2 | select | Second camera concept choice, includes various presets and “none” |\n| concept3 | select | Third camera concept choice, includes various presets and “none” |\n| concept4 | select | Fourth camera concept choice, includes various presets and “none” |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPTS | Optional Camera Concepts to merge with selected concepts |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPT | Combined object containing all selected concepts |\n\n## Usage Examples\n\n[\n\n## Luma Text to Video Workflow Example\n\nLuma Text to Video Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)[\n\n## Luma Image to Video Workflow Example\n\nLuma Image to Video Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)\n\n## How It Works\n\nThe Luma Concepts node offers a variety of predefined camera concepts including:\n\n*   Camera distances (close-up, medium shot, long shot)\n*   View angles (eye level, overhead, low angle)\n*   Movement types (push-in, follow, orbit)\n*   Special effects (handheld, stabilized, floating)\n\nUsers can select up to 4 concepts to use together. The node creates an object containing the selected camera concepts, which is then passed to Luma generation nodes. During generation, Luma AI uses these camera concepts to influence the viewpoint and composition of the output, ensuring the results reflect the chosen photographic effects. By combining multiple camera concepts, users can create complex camera guidance without writing detailed prompt descriptions. This is particularly useful when specific camera angles or compositions are needed.\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass LumaConceptsNode(ComfyNodeABC):\n    \"\"\"\n    Holds one or more Camera Concepts for use with Luma Text to Video and Luma Image to Video nodes.\n    \"\"\"\n\n    RETURN_TYPES = (LumaIO.LUMA_CONCEPTS,)\n    RETURN_NAMES = (\"luma_concepts\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_concepts\"\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"concept1\": (get_luma_concepts(include_none=True),),\n                \"concept2\": (get_luma_concepts(include_none=True),),\n                \"concept3\": (get_luma_concepts(include_none=True),),\n                \"concept4\": (get_luma_concepts(include_none=True),),\n            },\n            \"optional\": {\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to add to the ones chosen here.\"\n                    },\n                ),\n            },\n        }\n\n    def create_concepts(\n        self,\n        concept1: str,\n        concept2: str,\n        concept3: str,\n        concept4: str,\n        luma_concepts: LumaConceptChain = None,\n    ):\n        chain = LumaConceptChain(str_list=[concept1, concept2, concept3, concept4])\n        if luma_concepts is not None:\n            chain = luma_concepts.clone_and_merge(chain)\n        return (chain,)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fcustom-nodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fproperties",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video",
  "markdown": "# Luma Image to Video - ComfyUI Native API Node Documentation\n\n![ComfyUI Native Luma Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg) The Luma Image to Video node uses Luma AI’s technology to transform static images into smooth, dynamic videos, bringing your images to life.\n\n## Node Function\n\nThis node connects to Luma AI’s image-to-video API, allowing users to create dynamic videos from input images. It understands the image content and generates natural, coherent motion while maintaining the original visual style. Combined with text prompts, users can precisely control the video’s dynamic effects.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing video motion and content |\n| model | select | \\-  | Video generation model to use |\n| resolution | select | ”540p” | Output video resolution |\n| duration | select | \\-  | Video length options |\n| loop | boolean | False | Whether to loop the video |\n| seed | integer | 0   | Seed value for node rerun, results are nondeterministic |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| first\\_image | image | First frame of video (required if no last\\_image) |\n| last\\_image | image | Last frame of video (required if no first\\_image) |\n| luma\\_concepts | LUMA\\_CONCEPTS | Concepts for controlling camera motion and shot style |\n\n### Requirements\n\n*   Either **first\\_image** or **last\\_image** must be provided\n*   Each image input (first\\_image and last\\_image) accepts only 1 image\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Usage Example\n\n[\n\n## Luma Image to Video Workflow Example\n\nLuma Image to Video Workflow Tutorial\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass LumaImageToVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt, input images, and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaVideoModel],),\n                # \"aspect_ratio\": ([ratio.value for ratio in LumaAspectRatio], {\n                #     \"default\": LumaAspectRatio.ratio_16_9,\n                # }),\n                \"resolution\": (\n                    [resolution.value for resolution in LumaVideoOutputResolution],\n                    {\n                        \"default\": LumaVideoOutputResolution.res_540p,\n                    },\n                ),\n                \"duration\": ([dur.value for dur in LumaVideoModelOutputDuration],),\n                \"loop\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"first_image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"First frame of generated video.\"},\n                ),\n                \"last_image\": (IO.IMAGE, {\"tooltip\": \"Last frame of generated video.\"}),\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        resolution: str,\n        duration: str,\n        loop: bool,\n        seed,\n        first_image: torch.Tensor = None,\n        last_image: torch.Tensor = None,\n        luma_concepts: LumaConceptChain = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        if first_image is None and last_image is None:\n            raise Exception(\n                \"At least one of first_image and last_image requires an input.\"\n            )\n        keyframes = self._convert_to_keyframes(first_image, last_image, auth_token)\n        duration = duration if model != LumaVideoModel.ray_1_6 else None\n        resolution = resolution if model != LumaVideoModel.ray_1_6 else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations\",\n                method=HttpMethod.POST,\n                request_model=LumaGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaGenerationRequest(\n                prompt=prompt,\n                model=model,\n                aspect_ratio=LumaAspectRatio.ratio_16_9,  # ignored, but still needed by the API for some reason\n                resolution=resolution,\n                duration=duration,\n                loop=loop,\n                keyframes=keyframes,\n                concepts=luma_concepts.create_api_model() if luma_concepts else None,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.assets.video)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n    def _convert_to_keyframes(\n        self,\n        first_image: torch.Tensor = None,\n        last_image: torch.Tensor = None,\n        auth_token=None,\n    ):\n        if first_image is None and last_image is None:\n            return None\n        frame0 = None\n        frame1 = None\n        if first_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                first_image, max_images=1, auth_token=auth_token\n            )\n            frame0 = LumaImageReference(type=\"image\", url=download_urls[0])\n        if last_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                last_image, max_images=1, auth_token=auth_token\n            )\n            frame1 = LumaImageReference(type=\"image\", url=download_urls[0])\n        return LumaKeyframes(frame0=frame0, frame1=frame1)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v",
  "markdown": "# Kling Image to Video (Camera Control) - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Image to Video (Camera Control) Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg) The Kling Image to Video (Camera Control) node converts static images into videos with professional camera movements. It supports camera controls like zoom, rotation, pan, tilt and first-person view while maintaining focus on the original image content.\n\n```\n\nclass KlingCameraControlI2VNode(KlingImage2VideoNode):\n    \"\"\"\n    Kling Image to Video Camera Control Node. This node is a image to video node, but it supports controlling the camera.\n    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"camera_control\": (\n                    \"CAMERA_CONTROL\",\n                    {\n                        \"tooltip\": \"Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Transform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image.\"\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        camera_control: CameraControl,\n        auth_token: Optional[str] = None,\n    ):\n        return super().api_call(\n            model_name=\"kling-v1-5\",\n            start_frame=start_frame,\n            cfg_scale=cfg_scale,\n            mode=\"pro\",\n            aspect_ratio=aspect_ratio,\n            duration=\"5\",\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            camera_control=camera_control,\n            auth_token=auth_token,\n        )\n\n\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video",
  "markdown": "# MiniMax Text to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native MiniMax Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg) The MiniMax Text to Video node connects to MiniMax’s API to generate high-quality, smooth videos from text prompts. It supports different video generation models to create short video clips in various styles.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt\\_text | String | \"\"  | Text prompt that guides the video generation |\n| model | Select | ”T2V-01” | Video model to use, options are “T2V-01” and “T2V-01-Director” |\n| seed | Integer | 0   | Random seed for noise generation, defaults to 0 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass MinimaxTextToVideoNode:\n    \"\"\"\n    Generates videos synchronously based on a prompt, and optional parameters using Minimax's API.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt_text\": (\n                    \"STRING\",\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt to guide the video generation\",\n                    },\n                ),\n                \"model\": (\n                    [\n                        \"T2V-01\",\n                        \"T2V-01-Director\",\n                    ],\n                    {\n                        \"default\": \"T2V-01\",\n                        \"tooltip\": \"Model to use for video generation\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from prompts using Minimax's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Minimax\"\n    API_NODE = True\n    OUTPUT_NODE = True\n\n    def generate_video(\n        self,\n        prompt_text,\n        seed=0,\n        model=\"T2V-01\",\n        image: torch.Tensor=None, # used for ImageToVideo\n        subject: torch.Tensor=None, # used for SubjectToVideo\n        auth_token=None,\n    ):\n        '''\n        Function used between Minimax nodes - supports T2V, I2V, and S2V, based on provided arguments.\n        '''\n        # upload image, if passed in\n        image_url = None\n        if image is not None:\n            image_url = upload_images_to_comfyapi(image, max_images=1, auth_token=auth_token)[0]\n\n        # TODO: figure out how to deal with subject properly, API returns invalid params when using S2V-01 model\n        subject_reference = None\n        if subject is not None:\n            subject_url = upload_images_to_comfyapi(subject, max_images=1, auth_token=auth_token)[0]\n            subject_reference = [SubjectReferenceItem(image=subject_url)]\n\n\n        video_generate_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/video_generation\",\n                method=HttpMethod.POST,\n                request_model=MinimaxVideoGenerationRequest,\n                response_model=MinimaxVideoGenerationResponse,\n            ),\n            request=MinimaxVideoGenerationRequest(\n                model=Model(model),\n                prompt=prompt_text,\n                callback_url=None,\n                first_frame_image=image_url,\n                subject_reference=subject_reference,\n                prompt_optimizer=None,\n            ),\n            auth_token=auth_token,\n        )\n        response = video_generate_operation.execute()\n\n        task_id = response.task_id\n        if not task_id:\n            raise Exception(f\"Minimax generation failed: {response.base_resp}\")\n\n        video_generate_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/query/video_generation\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=MinimaxTaskResultResponse,\n                query_params={\"task_id\": task_id},\n            ),\n            completed_statuses=[\"Success\"],\n            failed_statuses=[\"Fail\"],\n            status_extractor=lambda x: x.status.value,\n            auth_token=auth_token,\n        )\n        task_result = video_generate_operation.execute()\n\n        file_id = task_result.file_id\n        if file_id is None:\n            raise Exception(\"Request was not successful. Missing file ID.\")\n        file_retrieve_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/files/retrieve\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=MinimaxFileRetrieveResponse,\n                query_params={\"file_id\": int(file_id)},\n            ),\n            request=EmptyRequest(),\n            auth_token=auth_token,\n        )\n        file_result = file_retrieve_operation.execute()\n\n        file_url = file_result.file.download_url\n        if file_url is None:\n            raise Exception(\n                f\"No video was found in the response. Full response: {file_result.model_dump()}\"\n            )\n        logging.info(f\"Generated video URL: {file_url}\")\n\n        video_io = download_url_to_bytesio(file_url)\n        if video_io is None:\n            error_msg = f\"Failed to download video from {file_url}\"\n            logging.error(error_msg)\n            raise Exception(error_msg)\n        return (VideoFromFile(video_io),)\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fmodels",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video",
  "markdown": "# Pika 2.2 Image to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native Pika 2.2 Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg) The Pika 2.2 Image to Video node connects to Pika’s latest 2.2 API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | Image | \\-  | Input image to convert to video |\n| prompt\\_text | String | \"\"  | Text prompt describing video motion and content |\n| negative\\_prompt | String | \"\"  | Elements to avoid in the video |\n| seed | Integer | 0   | Random seed for generation |\n| resolution | Select | ”1080p” | Output video resolution |\n| duration | Select | ”5s” | Length of generated video |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## How It Works\n\nThe node sends the input image and parameters (prompts, resolution, duration, etc.) to Pika’s API server as multipart form data. The API processes this and returns the generated video. Users can control the output by adjusting the prompts, negative prompts, random seed and other parameters.\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\n\nclass PikaImageToVideoV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Image to Video Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"The image to convert to video\"},\n                ),\n                **cls.get_base_inputs_types(PikaBodyGenerate22I2vGenerate22I2vPost),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    DESCRIPTION = \"Sends an image and prompt to the Pika API v2.2 to generate a video.\"\n    RETURN_TYPES = (\"VIDEO\",)\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika 2.2 Image to Video.\"\"\"\n        # Convert image to BytesIO\n        image_bytes_io = tensor_to_bytesio(image)\n        image_bytes_io.seek(0)  # Reset stream position\n\n        # Prepare file data for multipart upload\n        pika_files = {\"image\": (\"image.png\", image_bytes_io, \"image/png\")}\n\n        # Prepare non-file data using the Pydantic model\n        pika_request_data = PikaBodyGenerate22I2vGenerate22I2vPost(\n            promptText=prompt_text,\n            negativePrompt=negative_prompt,\n            seed=seed,\n            resolution=resolution,\n            duration=duration,\n        )\n\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_IMAGE_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22I2vGenerate22I2vPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=pika_request_data,\n            files=pika_files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Flinks",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes",
  "markdown": "# Pika 2.2 Scenes - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Pika 2.2 Scenes Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg) The Pika 2.2 Scenes node allows you to upload multiple images and generate a high-quality video incorporating these elements. It uses Pika’s 2.2 API to create smooth scene transitions between the images.\n\nThe Pika 2.2 Scenes node analyzes all input images and creates a video containing these image elements. The node sends the images and parameters to Pika’s API server, which processes them and returns the generated video. Users can guide the video style and content through prompts, and exclude unwanted elements using negative prompts. The node supports up to 5 input images as ingredients and generates the final video based on the specified combination mode, resolution, duration, and aspect ratio.\n\n```\n\nclass PikaScenesV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Scenes Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        image_ingredient_input = (\n            IO.IMAGE,\n            {\"tooltip\": \"Image that will be used as ingredient to create a video.\"},\n        )\n        return {\n            \"required\": {\n                **cls.get_base_inputs_types(\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                ),\n                \"ingredients_mode\": model_field_to_node_input(\n                    IO.COMBO,\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                    \"ingredientsMode\",\n                    enum_type=IngredientsMode,\n                    default=\"creative\",\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.FLOAT,\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                    \"aspectRatio\",\n                    step=0.001,\n                    min=0.4,\n                    max=2.5,\n                    default=1.7777777777777777,\n                ),\n            },\n            \"optional\": {\n                \"image_ingredient_1\": image_ingredient_input,\n                \"image_ingredient_2\": image_ingredient_input,\n                \"image_ingredient_3\": image_ingredient_input,\n                \"image_ingredient_4\": image_ingredient_input,\n                \"image_ingredient_5\": image_ingredient_input,\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    DESCRIPTION = \"Combine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them.\"\n    RETURN_TYPES = (\"VIDEO\",)\n\n    def api_call(\n        self,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        ingredients_mode: str,\n        aspect_ratio: float,\n        image_ingredient_1: Optional[torch.Tensor] = None,\n        image_ingredient_2: Optional[torch.Tensor] = None,\n        image_ingredient_3: Optional[torch.Tensor] = None,\n        image_ingredient_4: Optional[torch.Tensor] = None,\n        image_ingredient_5: Optional[torch.Tensor] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika Scenes 2.2.\"\"\"\n        all_image_bytes_io = []\n        for image in [\n            image_ingredient_1,\n            image_ingredient_2,\n            image_ingredient_3,\n            image_ingredient_4,\n            image_ingredient_5,\n        ]:\n            if image is not None:\n                image_bytes_io = tensor_to_bytesio(image)\n                image_bytes_io.seek(0)\n                all_image_bytes_io.append(image_bytes_io)\n\n        # Prepare files data for multipart upload\n        pika_files = [\n            (\"images\", (f\"image_{i}.png\", image_bytes_io, \"image/png\"))\n            for i, image_bytes_io in enumerate(all_image_bytes_io)\n        ]\n\n        # Prepare non-file data using the Pydantic model\n        pika_request_data = PikaBodyGenerate22C2vGenerate22PikascenesPost(\n            ingredientsMode=ingredients_mode,\n            promptText=prompt_text,\n            negativePrompt=negative_prompt,\n            seed=seed,\n            resolution=resolution,\n            duration=duration,\n            aspectRatio=aspect_ratio,\n        )\n\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_PIKASCENES,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=pika_request_data,\n            files=pika_files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/comms_overview",
  "markdown": "# Server Overview - ComfyUI\n\nThe Comfy server runs on top of the [aiohttp framework](https://docs.aiohttp.org/), which in turn uses [asyncio](https://pypi.org/project/asyncio/). Messages from the server to the client are sent by socket messages through the `send_sync` method of the server, which is an instance of `PromptServer` (defined in `server.py`). They are processed by a socket event listener registered in `api.js`. See [messages](https://docs.comfy.org/development/comfyui-server/comms_messages). Messages from the client to the server are sent by the `api.fetchApi()` method defined in `api.js`, and are handled by http routes defined by the server. See [routes](https://docs.comfy.org/development/comfyui-server/comms_routes)."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video",
  "markdown": "# PixVerse Image to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native PixVerse Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg) The PixVerse Image to Video node uses PixVerse’s API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | Image | \\-  | Input image to convert to video |\n| prompt | String | \"\"  | Text prompt describing video motion/content |\n| negative\\_prompt | String | \"\"  | Elements to avoid in the video |\n| seed | Integer | \\-1 | Random seed (-1 for random) |\n| quality | Select | ”high” | Output video quality level |\n| aspect\\_ratio | Select | ”r16\\_9” | Output video aspect ratio |\n| duration | Select | ”seconds\\_4” | Length of generated video |\n| motion\\_mode | Select | ”standard” | Video motion style |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | Optional PixVerse template |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\nclass PixverseImageToVideoNode(ComfyNodeABC):\n    \"\"\"\n    Pixverse Image to Video\n\n    Generates videos from an image and prompts.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"prompt\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n                \"negative_prompt\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n                \"seed\": (\"INT\", {\"default\": -1, \"min\": -1, \"max\": 0xffffffffffffffff}),\n                \"quality\": (list(PixverseQuality.__members__.keys()), {\"default\": \"high\"}),\n                \"aspect_ratio\": (list(PixverseAspectRatio.__members__.keys()), {\"default\": \"r16_9\"}),\n                \"duration\": (list(PixverseDuration.__members__.keys()), {\"default\": \"seconds_4\"}),\n                \"motion_mode\": (list(PixverseMotionMode.__members__.keys()), {\"default\": \"standard\"}),\n            },\n            \"optional\": {\n                \"pixverse_template\": (\"PIXVERSE_TEMPLATE\",),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from an image and prompts using Pixverse's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Pixverse\"\n    API_NODE = True\n    OUTPUT_NODE = True\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video",
  "markdown": "# PixVerse Transition Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native PixVerse Transition Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg) The Pixverse Transition Video node connects to PixVerse’s API to generate smooth video transitions between two images. It automatically creates all intermediate frames to produce fluid transformations, perfect for morphing effects, scene transitions, and object evolution.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| first\\_frame | Image | \\-  | Starting frame image |\n| last\\_frame | Image | \\-  | Ending frame image |\n| prompt | String | \"\"  | Text prompt describing video and transition |\n| quality | Select | ”PixverseQuality.res\\_540p” | Output video quality |\n| duration\\_seconds | Select | \\-  | Length of generated video |\n| motion\\_mode | Select | \\-  | Video motion style |\n| seed | Integer | 0   | Random seed (range: 0-2147483647) |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| negative\\_prompt | String | \"\"  | Elements to avoid in video |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | Optional style preset |\n\n### Parameter Constraints\n\n*   When quality is set to 1080p, motion\\_mode is forced to normal and duration\\_seconds to 5 seconds\n*   When duration\\_seconds is not 5 seconds, motion\\_mode is forced to normal\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## Source Code\n\n```\n\nclass PixverseTransitionVideoNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"first_frame\": (\n                    IO.IMAGE,\n                ),\n                \"last_frame\": (\n                    IO.IMAGE,\n                ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"quality\": (\n                    [resolution.value for resolution in PixverseQuality],\n                    {\n                        \"default\": PixverseQuality.res_540p,\n                    },\n                ),\n                \"duration_seconds\": ([dur.value for dur in PixverseDuration],),\n                \"motion_mode\": ([mode.value for mode in PixverseMotionMode],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"pixverse_template\": (\n                    PixverseIO.TEMPLATE,\n                    {\n                        \"tooltip\": \"An optional template to influence style of generation, created by the Pixverse Template node.\"\n                    }\n                )\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        first_frame: torch.Tensor,\n        last_frame: torch.Tensor,\n        prompt: str,\n        quality: str,\n        duration_seconds: int,\n        motion_mode: str,\n        seed,\n        negative_prompt: str=None,\n        pixverse_template: int=None,\n        auth_token=None,\n        **kwargs,\n    ):\n        first_frame_id = upload_image_to_pixverse(first_frame, auth_token=auth_token)\n        last_frame_id = upload_image_to_pixverse(last_frame, auth_token=auth_token)\n\n        # 1080p is limited to 5 seconds duration\n        # only normal motion_mode supported for 1080p or for non-5 second duration\n        if quality == PixverseQuality.res_1080p:\n            motion_mode = PixverseMotionMode.normal\n            duration_seconds = PixverseDuration.dur_5\n        elif duration_seconds != PixverseDuration.dur_5:\n            motion_mode = PixverseMotionMode.normal\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/pixverse/video/transition/generate\",\n                method=HttpMethod.POST,\n                request_model=PixverseTransitionVideoRequest,\n                response_model=PixverseVideoResponse,\n            ),\n            request=PixverseTransitionVideoRequest(\n                first_frame_img=first_frame_id,\n                last_frame_img=last_frame_id,\n                prompt=prompt,\n                quality=quality,\n                duration=duration_seconds,\n                motion_mode=motion_mode,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                template_id=pixverse_template,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.Resp is None:\n            raise Exception(f\"Pixverse request failed: '{response_api.ErrMsg}'\")\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/pixverse/video/result/{response_api.Resp.video_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=PixverseGenerationStatusResponse,\n            ),\n            completed_statuses=[PixverseStatus.successful],\n            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],\n            status_extractor=lambda x: x.Resp.status,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.Resp.url)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video",
  "markdown": "# PixVerse Text to Video - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in PixVerse Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg) The PixVerse Text to Video node connects to PixVerse’s text-to-video API, allowing users to generate high-quality videos from text descriptions. Users can customize their creations by adjusting various parameters like video quality, duration, and motion mode.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the video content |\n| aspect\\_ratio | select | \\-  | Output video aspect ratio |\n| quality | select | PixverseQuality.res\\_540p | Video quality level |\n| duration\\_seconds | select | \\-  | Video duration |\n| motion\\_mode | select | \\-  | Video motion mode |\n| seed | integer | 0   | Random seed for consistent generation results |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| negative\\_prompt | string | \"\"  | Elements to exclude from the video |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | Optional template for style settings |\n\n### Limitations\n\n*   1080p quality only supports normal motion mode with 5-second duration\n*   Non 5-second durations only support normal motion mode\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\n\nclass PixverseTextToVideoNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in PixverseAspectRatio],\n                ),\n                \"quality\": (\n                    [resolution.value for resolution in PixverseQuality],\n                    {\n                        \"default\": PixverseQuality.res_540p,\n                    },\n                ),\n                \"duration_seconds\": ([dur.value for dur in PixverseDuration],),\n                \"motion_mode\": ([mode.value for mode in PixverseMotionMode],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"pixverse_template\": (\n                    PixverseIO.TEMPLATE,\n                    {\n                        \"tooltip\": \"An optional template to influence style of generation, created by the Pixverse Template node.\"\n                    }\n                )\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        aspect_ratio: str,\n        quality: str,\n        duration_seconds: int,\n        motion_mode: str,\n        seed,\n        negative_prompt: str=None,\n        pixverse_template: int=None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # 1080p is limited to 5 seconds duration\n        # only normal motion_mode supported for 1080p or for non-5 second duration\n        if quality == PixverseQuality.res_1080p:\n            motion_mode = PixverseMotionMode.normal\n            duration_seconds = PixverseDuration.dur_5\n        elif duration_seconds != PixverseDuration.dur_5:\n            motion_mode = PixverseMotionMode.normal\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/pixverse/video/text/generate\",\n                method=HttpMethod.POST,\n                request_model=PixverseTextVideoRequest,\n                response_model=PixverseVideoResponse,\n            ),\n            request=PixverseTextVideoRequest(\n                prompt=prompt,\n                aspect_ratio=aspect_ratio,\n                quality=quality,\n                duration=duration_seconds,\n                motion_mode=motion_mode,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                template_id=pixverse_template,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.Resp is None:\n            raise Exception(f\"Pixverse request failed: '{response_api.ErrMsg}'\")\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/pixverse/video/result/{response_api.Resp.video_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=PixverseGenerationStatusResponse,\n            ),\n            completed_statuses=[PixverseStatus.successful],\n            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],\n            status_extractor=lambda x: x.Resp.status,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.Resp.url)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/conditioning/video-models/wan-vace-to-video",
  "markdown": "# Wan Vace To Video - ComfyUI Built-in Node Documentation\n\n![Wan Vace To Video](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg) The Wan Vace To Video node allows you to generate videos through text prompts and supports multiple input methods, including text, images, videos, masks, and control signals. This node combines input conditions (prompts), control videos, and masks to generate high-quality videos. It first preprocesses and encodes the inputs, then applies the conditional information to generate the final video latent representation. When a reference image is provided, it serves as the initial reference for the video. Control videos and masks can be used to guide the generation process, making the generated video more aligned with expectations.\n\n## Parameter Description\n\n### Required Parameters\n\n| Parameter | Type | Default | Range | Description |\n| --- | --- | --- | --- | --- |\n| positive | CONDITIONING | \\-  | \\-  | Positive prompt condition |\n| negative | CONDITIONING | \\-  | \\-  | Negative prompt condition |\n| vae | VAE | \\-  | \\-  | VAE model for encoding/decoding |\n| width | INT | 832 | 16-MAX\\_RESOLUTION | Video width, step size 16 |\n| height | INT | 480 | 16-MAX\\_RESOLUTION | Video height, step size 16 |\n| length | INT | 81  | 1-MAX\\_RESOLUTION | Number of video frames, step size 4 |\n| batch\\_size | INT | 1   | 1-4096 | Batch size |\n| strength | FLOAT | 1.0 | 0.0-1000.0 | Condition strength, step size 0.01 |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| control\\_video | IMAGE | Control video for guiding the generation process |\n| control\\_masks | MASK | Control masks defining which areas should be controlled |\n| reference\\_image | IMAGE | Reference image as starting point or reference (single image) |\n\n### Output Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| positive | CONDITIONING | Processed positive prompt condition |\n| negative | CONDITIONING | Processed negative prompt condition |\n| latent | LATENT | Generated video latent representation |\n| trim\\_latent | INT | Parameter for trimming latent representation, default value is 0. When a reference image is provided, this value is set to the shape size of the reference image in latent space. It indicates how much content from the reference image downstream nodes should trim from the generated latent representation to ensure proper control of the reference image’s influence in the final video output. |\n\n## Source Code\n\n\\[Source code update time: 2025-05-15\\]\n\n```\nclass WanVaceToVideo:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"vae\": (\"VAE\", ),\n                             \"width\": (\"INT\", {\"default\": 832, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                             \"height\": (\"INT\", {\"default\": 480, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                             \"length\": (\"INT\", {\"default\": 81, \"min\": 1, \"max\": nodes.MAX_RESOLUTION, \"step\": 4}),\n                             \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1000.0, \"step\": 0.01}),\n                },\n                \"optional\": {\"control_video\": (\"IMAGE\", ),\n                             \"control_masks\": (\"MASK\", ),\n                             \"reference_image\": (\"IMAGE\", ),\n                }}\n\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\", \"INT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\", \"trim_latent\")\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/video_models\"\n\n    EXPERIMENTAL = True\n\n    def encode(self, positive, negative, vae, width, height, length, batch_size, strength, control_video=None, control_masks=None, reference_image=None):\n        latent_length = ((length - 1) // 4) + 1\n        if control_video is not None:\n            control_video = comfy.utils.common_upscale(control_video[:length].movedim(-1, 1), width, height, \"bilinear\", \"center\").movedim(1, -1)\n            if control_video.shape[0] < length:\n                control_video = torch.nn.functional.pad(control_video, (0, 0, 0, 0, 0, 0, 0, length - control_video.shape[0]), value=0.5)\n        else:\n            control_video = torch.ones((length, height, width, 3)) * 0.5\n\n        if reference_image is not None:\n            reference_image = comfy.utils.common_upscale(reference_image[:1].movedim(-1, 1), width, height, \"bilinear\", \"center\").movedim(1, -1)\n            reference_image = vae.encode(reference_image[:, :, :, :3])\n            reference_image = torch.cat([reference_image, comfy.latent_formats.Wan21().process_out(torch.zeros_like(reference_image))], dim=1)\n\n        if control_masks is None:\n            mask = torch.ones((length, height, width, 1))\n        else:\n            mask = control_masks\n            if mask.ndim == 3:\n                mask = mask.unsqueeze(1)\n            mask = comfy.utils.common_upscale(mask[:length], width, height, \"bilinear\", \"center\").movedim(1, -1)\n            if mask.shape[0] < length:\n                mask = torch.nn.functional.pad(mask, (0, 0, 0, 0, 0, 0, 0, length - mask.shape[0]), value=1.0)\n\n        control_video = control_video - 0.5\n        inactive = (control_video * (1 - mask)) + 0.5\n        reactive = (control_video * mask) + 0.5\n\n        inactive = vae.encode(inactive[:, :, :, :3])\n        reactive = vae.encode(reactive[:, :, :, :3])\n        control_video_latent = torch.cat((inactive, reactive), dim=1)\n        if reference_image is not None:\n            control_video_latent = torch.cat((reference_image, control_video_latent), dim=2)\n\n        vae_stride = 8\n        height_mask = height // vae_stride\n        width_mask = width // vae_stride\n        mask = mask.view(length, height_mask, vae_stride, width_mask, vae_stride)\n        mask = mask.permute(2, 4, 0, 1, 3)\n        mask = mask.reshape(vae_stride * vae_stride, length, height_mask, width_mask)\n        mask = torch.nn.functional.interpolate(mask.unsqueeze(0), size=(latent_length, height_mask, width_mask), mode='nearest-exact').squeeze(0)\n\n        trim_latent = 0\n        if reference_image is not None:\n            mask_pad = torch.zeros_like(mask[:, :reference_image.shape[2], :, :])\n            mask = torch.cat((mask_pad, mask), dim=1)\n            latent_length += reference_image.shape[2]\n            trim_latent = reference_image.shape[2]\n\n        mask = mask.unsqueeze(0)\n        positive = node_helpers.conditioning_set_values(positive, {\"vace_frames\": control_video_latent, \"vace_mask\": mask, \"vace_strength\": strength})\n        negative = node_helpers.conditioning_set_values(negative, {\"vace_frames\": control_video_latent, \"vace_mask\": mask, \"vace_strength\": strength})\n\n        latent = torch.zeros([batch_size, 16, latent_length, height // 8, width // 8], device=comfy.model_management.intermediate_device())\n        out_latent = {}\n        out_latent[\"samples\"] = latent\n        return (positive, negative, out_latent, trim_latent)\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Finterface%2Foverview",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/interface/settings/overview",
  "markdown": "# ComfyUI Settings Overview - ComfyUI\n\nDetailed description of ComfyUI settings overview\n\nThis section covers detailed setting descriptions in the ComfyUI frontend settings menu. All user settings are automatically saved to the `ComfyUI/user/default/comfy.settings.json` file. You can use the `Ctrl + ,` keyboard shortcut to open the settings panel, then click on the corresponding setting options to configure them. Since custom nodes can also register corresponding setting categories in the menu, our official documentation currently only includes native setting content. Additionally, some setting options are **only effective for ComfyUI Desktop**, which we have noted on the corresponding pages.\n\n## ComfyUI Settings Menu\n\n[\n\n## User\n\nUser settings related to ComfyUI account, mainly used for logging into ComfyUI account to use API nodes\n\n\n\n](https://docs.comfy.org/interface/user)[\n\n## Credits\n\nEntry for purchasing credits and credit balance history, only visible after logging into ComfyUI account\n\n\n\n](https://docs.comfy.org/interface/credits)[\n\n## Comfy\n\nDetailed description of ComfyUI core setting options\n\n\n\n](https://docs.comfy.org/interface/settings/comfy)[\n\n## Lite Graph\n\nDetailed description of Canvas (Lite Graph) setting options in ComfyUI\n\n\n\n](https://docs.comfy.org/interface/settings/lite-graph)[\n\n## Appearance\n\nModify ComfyUI appearance options such as themes, background colors, sidebar position, etc.\n\n\n\n](https://docs.comfy.org/interface/appearance)[\n\n## Extension\n\nManage the enable/disable status of frontend extension plugins in ComfyUI\n\n\n\n](https://docs.comfy.org/interface/settings/extension)[\n\n## 3D\n\nSome setting options for 3D node initialization\n\n\n\n](https://docs.comfy.org/interface/settings/3d)[\n\n## Comfy Desktop\n\nDesktop update settings, mirror settings, etc. (only effective for ComfyUI Desktop)\n\n\n\n](https://docs.comfy.org/interface/settings/comfy-desktop)[\n\n## Mask Editor\n\nAdjust mask editor usage preferences\n\n\n\n](https://docs.comfy.org/interface/settings/mask-editor)[\n\n## Keybinding\n\nModify ComfyUI keyboard shortcut settings\n\n\n\n](https://docs.comfy.org/interface/shortcuts)[\n\n## About\n\nLearn about current ComfyUI version information, device runtime information, etc., which is very useful for daily feedback\n\n\n\n](https://docs.comfy.org/interface/settings/about)[\n\n## Server Config\n\nModify ComfyUI configuration file, this setting is only effective for ComfyUI Desktop\n\n\n\n](https://docs.comfy.org/interface/settings/server-config)"
},
{
  "url": "https://docs.comfy.org/changelog/index",
  "markdown": "# Changelog - ComfyUI\n\n**Advanced Sampling & Training Infrastructure Improvements**This release introduces significant enhancements to sampling algorithms, training capabilities, and node functionality for AI researchers and workflow creators:\n\n## New Sampling & Generation Features\n\n*   **SA-Solver Sampler**: New reconstructed SA-Solver sampling algorithm providing enhanced numerical stability and quality for complex generation workflows\n*   **Experimental CFGNorm Node**: Advanced classifier-free guidance normalization for improved control over generation quality and style consistency\n*   **Nested Dual CFG Support**: Added nested style configuration to DualCFGGuider node, offering more sophisticated guidance control patterns\n*   **SamplingPercentToSigma Node**: New utility node for precise sigma calculation from sampling percentages, improving workflow flexibility\n\n## Enhanced Training Capabilities\n\n*   **Multi Image-Caption Dataset Support**: LoRA training node now handles multiple image-caption datasets simultaneously, streamlining training workflows\n*   **Better Training Loop Implementation**: Optimized training algorithms for improved convergence and stability during model fine-tuning\n*   **Enhanced Error Detection**: Added model detection error hints for LoRA operations, providing clearer feedback when issues occur\n\n## Platform & Performance Improvements\n\n*   **Async Node Support**: Full support for asynchronous node functions with earlier execution optimization, improving workflow performance for I/O intensive operations\n*   **Chroma Flexibility**: Un-hardcoded patch\\_size parameter in Chroma, allowing better adaptation to different model configurations\n*   **LTXV VAE Decoder**: Switched to improved default padding mode for better image quality with LTXV models\n*   **Safetensors Memory Management**: Added workaround for mmap issues, improving reliability when loading large model files\n\n## API & Integration Enhancements\n\n*   **Custom Prompt IDs**: API now allows specifying prompt IDs for better workflow tracking and management\n*   **Kling API Optimization**: Increased polling timeout to prevent user timeouts during video generation workflows\n*   **History Token Cleanup**: Removed sensitive tokens from history items for improved security\n*   **Python 3.9 Compatibility**: Fixed compatibility issues ensuring broader platform support\n\n## Bug Fixes & Stability\n\n*   **MaskComposite Fixes**: Resolved errors when destination masks have 2 dimensions, improving mask workflow reliability\n*   **Fresca Input/Output**: Corrected input and output handling for Fresca model workflows\n*   **Reference Bug Fixes**: Resolved incorrect reference bugs in Gemini node implementations\n*   **Line Ending Standardization**: Automated detection and removal of Windows line endings for cross-platform consistency\n\n## Developer Experience\n\n*   **Warning Systems**: Added torch import mistake warnings to catch common configuration issues\n*   **Template Updates**: Multiple template version updates (0.1.36, 0.1.37, 0.1.39) for improved custom node development\n*   **Documentation**: Enhanced fast\\_fp16\\_accumulation documentation in portable configurations\n\nThese improvements make ComfyUI more robust for production workflows while introducing powerful new sampling techniques and training capabilities essential for advanced AI research and creative applications.\n\n**Advanced Sampling & Model Control Enhancements**This release delivers significant improvements to sampling algorithms and model control systems, particularly benefiting advanced AI researchers and workflow creators:\n\n## New Sampling Capabilities\n\n*   **TCFG Node**: Enhanced classifier-free guidance control for more nuanced generation control in your workflows\n*   **ER-SDE Sampler**: Migrated from VE to VP algorithm with new sampler node, providing better numerical stability for complex generation tasks\n*   **Skip Layer Guidance (SLG)**: Alternative implementation for precise layer-level control during inference, perfect for advanced model steering workflows\n\n## Enhanced Development Tools\n\n*   **Custom Node Management**: New `--whitelist-custom-nodes` argument pairs with `--disable-all-custom-nodes` for precise development control\n*   **Performance Optimizations**: Dual CFG node now optimizes automatically when CFG is 1.0, reducing computational overhead\n*   **GitHub Actions Integration**: Automated release webhook notifications keep developers informed of new updates\n\n## Image Processing Improvements\n\n*   **New Transform Nodes**: Added ImageRotate and ImageFlip nodes for enhanced image manipulation workflows\n*   **ImageColorToMask Fix**: Corrected mask value returns for more accurate color-based masking operations\n*   **3D Model Support**: Upload 3D models to custom subfolders for better organization in complex projects\n\n## Guidance & Conditioning Enhancements\n\n*   **PerpNeg Guider**: Updated with improved pre and post-CFG handling plus performance optimizations\n*   **Latent Conditioning Fix**: Resolved issues with conditioning at index > 0 for multi-step workflows\n*   **Denoising Steps**: Added denoising step support to several samplers for cleaner outputs\n\n## Platform Stability\n\n*   **PyTorch Compatibility**: Fixed contiguous memory issues with PyTorch nightly builds\n*   **FP8 Fallback**: Automatic fallback to regular operations when FP8 operations encounter exceptions\n*   **Audio Processing**: Removed deprecated torchaudio.save function dependencies with warning fixes\n\n## Model Integration\n\n*   **Moonvalley Nodes**: Added native support for Moonvalley model workflows\n*   **Scheduler Reordering**: Simple scheduler now defaults first for better user experience\n*   **Template Updates**: Multiple template version updates (0.1.31-0.1.35) for improved custom node development\n\n## Security & Safety\n\n*   **Safe Loading**: Added warnings when loading files unsafely, with documentation noting that checkpoint files are loaded safely by default\n*   **File Validation**: Enhanced checkpoint loading safety measures for secure workflow execution\n\nThese improvements make ComfyUI more robust for production workflows while expanding creative possibilities for AI artists working with advanced sampling techniques and model control systems.\n\n**Enhanced Model Support & Workflow Reliability**This release brings significant improvements to model compatibility and workflow stability:\n\n*   **Expanded Model Documentation**: Added comprehensive support documentation for Flux Kontext and Omnigen 2 models, making it easier for creators to integrate these powerful models into their workflows\n*   **VAE Encoding Improvements**: Removed unnecessary random noise injection during VAE encoding, resulting in more consistent and predictable outputs across workflow runs\n*   **Memory Management Fix**: Resolved a critical memory estimation bug specifically affecting Kontext model usage, preventing out-of-memory errors and improving workflow stability\n\nThese changes enhance the reliability of advanced model workflows while maintaining ComfyUI’s flexibility for AI artists and researchers working with cutting-edge generative models.\n\n**Major Model Support Additions**\n\n*   **Cosmos Predict2 Support**: Full implementation for both text-to-image (2B and 14B models) and image-to-video generation workflows, expanding video creation capabilities\n*   **Enhanced Flux Compatibility**: Chroma Text Encoder now works seamlessly with regular Flux models, improving text conditioning quality\n*   **LoRA Training Integration**: New native LoRA training node using weight adapter scheme, enabling direct model fine-tuning within ComfyUI workflows\n\n**Performance & Hardware Optimizations**\n\n*   **AMD GPU Enhancements**: Enabled FP8 operations and PyTorch attention on GFX1201 and other compatible AMD GPUs for faster inference\n*   **Apple Silicon Fixes**: Addressed long-standing FP16 attention issues on Apple devices, improving stability for Mac users\n*   **Flux Model Stability**: Resolved black image generation issues with certain Flux models in FP16 precision\n\n**Advanced Sampling Improvements**\n\n*   **Rectified Flow (RF) Samplers**: Added SEEDS and multistep DPM++ SDE samplers with RF support, providing more sampling options for cutting-edge models\n*   **ModelSamplingContinuousEDM**: New cosmos\\_rflow option for enhanced sampling control with Cosmos models\n*   **Memory Optimization**: Improved memory estimation for Cosmos models with uncapped resolution support\n\n**Developer & Integration Features**\n\n*   **SQLite Database Support**: Enhanced data management capabilities for custom nodes and workflow storage\n*   **PyProject.toml Integration**: Automatic web folder registration and settings configuration from pyproject files\n*   **Frontend Flexibility**: Support for semver suffixes and prerelease frontend versions for custom deployments\n*   **Tokenizer Enhancements**: Configurable min\\_length settings with tokenizer\\_data for better text processing\n\n**Quality of Life Improvements**\n\n*   **Kontext Aspect Ratio Fix**: Resolved widget-only limitation, now works properly in all connection modes\n*   **SaveLora Consistency**: Standardized filename format across all save nodes for better file organization\n*   **Python Version Warnings**: Added alerts for outdated Python installations to prevent compatibility issues\n*   **WebcamCapture Fixes**: Corrected IS\\_CHANGED signature for reliable live input workflows\n\nThis release significantly expands ComfyUI’s model ecosystem support while delivering crucial stability improvements and enhanced hardware compatibility across different platforms.\n\nThis release brings powerful new workflow utilities and performance optimizations for ComfyUI creators:\n\n## New Workflow Tools\n\n*   **ImageStitch Node**: Concatenate multiple images seamlessly in your workflows - perfect for creating comparison grids or composite outputs\n*   **GetImageSize Node**: Extract image dimensions with batch processing support, essential for dynamic sizing workflows\n*   **Regex Replace Node**: Advanced text manipulation capabilities for prompt engineering and string processing workflows\n\n## Enhanced Model Compatibility\n\n*   **Improved Tensor Handling**: Streamlined list processing makes complex multi-model workflows more reliable\n*   **BFL API Optimization**: Refined support for Kontext \\[pro\\] and \\[max\\] models with cleaner node interfaces\n*   **Performance Boost**: Fused multiply-add operations in chroma processing for faster generation times\n\n## Developer Experience Improvements\n\n*   **Custom Node Support**: Added pyproject.toml support for better custom node dependency management\n*   **Help Menu Integration**: New help system in the Node Library sidebar for faster node discovery\n*   **API Documentation**: Enhanced API nodes documentation for workflow automation\n\n## Frontend & UI Enhancements\n\n*   **Frontend Updated to v1.21.7**: Multiple stability fixes and performance improvements\n*   **Custom API Base Support**: Better subpath handling for custom deployment configurations\n*   **Security Hardening**: XSS vulnerability fixes for safer workflow sharing\n\n## Bug Fixes & Stability\n\n*   **Pillow Compatibility**: Updated deprecated API calls to maintain compatibility with latest image processing libraries\n*   **ROCm Support**: Improved version detection for AMD GPU users\n*   **Template Updates**: Enhanced project templates for custom node development\n\nThese updates strengthen ComfyUI’s foundation for complex AI workflows while making the platform more accessible to new users through improved documentation and helper tools."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Finterface%2Fmaskeditor",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/interface/user",
  "markdown": "# Account Management - ComfyUI\n\nThe account system was added to support `API Nodes`, which enable calls to closed-source model APIs, greatly expanding the possibilities of ComfyUI. Since these API calls consume tokens, we have added a corresponding user system. Currently, we support the following login methods:\n\n*   Email login\n*   Google login\n*   Github login\n*   API Key login (for non-whitelisted site authorization)\n\nWe will provide relevant login requirements and explanations in this document.\n\n## ComfyUI Version Requirements\n\nYou may need to use at least [ComfyUI v0.3.0](https://github.com/comfyanonymous/ComfyUI/releases/tag/v0.3.30) to use the account system. Ensure that the corresponding frontend version is at least `1.17.11`. Sometimes the frontend may fail to install and revert to an older version, so please check if the frontend version is greater than `1.17.11` in `Settings` -> `About`. In some regions, network restrictions may prevent normal access to the login API, causing timeouts or failures. Before logging in, please **ensure that your network environment does not restrict access to the corresponding API**, and make sure you can access sites like Google or Github.\n\n## Network Requirements\n\nTo login to ComfyUI account, you must be in a secure network environment:\n\n*   Only allow access from `127.0.0.1` or `localhost`.\n*   Do not support using the `--listen` parameter to access the API node through a local network.\n*   If you are using a non-SSL certificate or a site that does not start with `https`, you may not be able to successfully log in.\n*   You may not be able to log in on a site that is not in our whitelist (but you can log in using an API Key now).\n*   Ensure you can connect to our service normally (some regions may require a proxy).\n\n## How to Log In\n\nLog in via `Settings` -> `User`: ![ComfyUI User Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user.jpg)\n\n## Login Methods\n\n![user-login](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user-login.jpg) If this is your first login, please create an account first.\n\n## Logging in with an API Key\n\nSince not all ComfyUI deployments are on our domain authorization whitelist, we have provided API Key login in a recent update (2025-05-10) for logging in through non-whitelisted sites. Below are the steps for logging in with an API Key:\n\n## Post-Login Status\n\nAfter logging in, a login button is displayed in the top menu bar of the ComfyUI interface. You can open the corresponding login interface through this button and log out of the corresponding account in the settings menu. ![user-logged](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user-logged.jpg) ![menu-user-logged](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/menu-user-logged.jpg)\n\n## Frequently Asked Questions"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fdependencies",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/interface/credits",
  "markdown": "# Credits Management - ComfyUI\n\nThe credit system was added to support the `API Nodes`, as calling closed-source AI models requires token consumption, so proper credit management is necessary. By default, the credits interface is not displayed. Please first log in to your ComfyUI account in `Settings` -> `User`, and then you can view your associated account’s credit information in `Settings` -> `Credits`. ![ComfyUI Credits Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/menu-credits.jpg)\n\n## How to Purchase Credits?\n\nBelow is a demonstration video for purchasing credits:\n\nDetailed steps are as follows:\n\n## Frequently Asked Questions"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/tripo/model-generation",
  "markdown": "# Tripo API Node Model Generation ComfyUI Official Example\n\nTripo AI is a company focused on generative AI 3D modeling. It provides user-friendly platforms and API services that can quickly convert text prompts or 2D images (single or multiple) into high-quality 3D models. ComfyUI has now natively integrated the corresponding Tripo API, allowing you to conveniently use the related nodes in ComfyUI for model generation. Currently, ComfyUI’s API nodes support the following Tripo model generation capabilities:\n\n*   Text-to-model\n*   Image-to-model\n*   Multi-view model generation\n*   Rig model\n*   Retarget rigged model\n\n## Text-to-Model Workflow\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/api_tripo_text_to_model.json)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic text-to-model workflow execution:\n\n1.  In the `Tripo: Text to Model` node, input your prompt in the `prompt` field\n    *   model: You can select different models, currently only v1.4 model supports subsequent optimization with `Tripo: Refine Draft model`\n    *   style: You can set different styles\n    *   texture\\_quality: You can set different texture qualities\n2.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/` directory\n3.  In the `Preview 3D` node, click to expand the menu\n4.  Select `Export` to directly export the corresponding model\n\n## Image-to-Model Workflow\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/api_tripo_image_to_model.json)\n\nDownload the image below as input image ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/panda.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic image-to-model workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  In the `Tripo: Image to Model` node, modify the corresponding parameter settings\n    *   model: You can select different models, currently only v1.4 model supports subsequent optimization with `Tripo: Refine Draft model`\n    *   style: You can set different styles\n    *   texture\\_quality: You can set different texture qualities\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/` directory\n4.  For model download, please refer to the instructions in the text-to-model section\n\n## Multi-view Model Generation Workflow\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/api_tripo_multiview_to_model.json)\n\nDownload the images below as input images ![Front View](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/front.jpg) ![Back View](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/back.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic multi-view to model workflow execution:\n\n1.  In the `Load Image` nodes, load the provided input images respectively\n2.  In the `Tripo: Image to Model` node, modify the corresponding parameter settings\n    *   model: You can select different models, currently only v1.4 model supports subsequent optimization with `Tripo: Refine Draft model`\n    *   style: You can set different styles\n    *   texture\\_quality: You can set different texture qualities\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/` directory\n4.  For other view inputs, you can refer to the step diagram and set the corresponding node mode to `Always` to enable it\n5.  For model download, please refer to the instructions in the text-to-model section\n\n## Subsequent Task Processing for the Same Task\n\nTripo’s corresponding nodes provide subsequent processing for the same task, you only need to input the corresponding `model_task_id` in the relevant nodes, and we have also provided the corresponding nodes in the relevant templates, you can also modify the corresponding node mode as needed to enable it ![Tripo Task Processing](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/other_nodes.jpg)"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/datatypes",
  "markdown": "# Datatypes - ComfyUI\n\nThese are the most important built in datatypes. You can also [define your own](https://docs.comfy.org/custom-nodes/backend/more_on_inputs#custom-datatypes). Datatypes are used on the client side to prevent a workflow from passing the wrong form of data into a node - a bit like strong typing. The JavaScript client side code will generally not allow a node output to be connected to an input of a different datatype, although a few exceptions are noted below.\n\n## Comfy datatypes\n\n### COMBO\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype: defined as `list[str]`, output value is `str`\n\nRepresents a dropdown menu widget. Unlike other datatypes, `COMBO` it is not specified in `INPUT_TYPES` by a `str`, but by a `list[str]` corresponding to the options in the dropdown list, with the first option selected by default. `COMBO` inputs are often dynamically generated at run time. For instance, in the built-in `CheckpointLoaderSimple` node, you find\n\n```\n\"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), )\n```\n\nor they might just be a fixed list of options,\n\n```\n\"play_sound\": ([\"no\",\"yes\"], {}),\n```\n\n### Primitive and reroute\n\nPrimitive and reroute nodes only exist on the client side. They do not have an intrinsic datatype, but when connected they take on the datatype of the input or output to which they have been connected (which is why they can’t connect to a `*` input…)\n\n## Python datatypes\n\n### INT\n\n*   Additional parameters in `INPUT_TYPES`:\n    *   `default` is required\n    *   `min` and `max` are optional\n*   Python datatype `int`\n\n### FLOAT\n\n*   Additional parameters in `INPUT_TYPES`:\n    *   `default` is required\n    *   `min`, `max`, `step` are optional\n*   Python datatype `float`\n\n### STRING\n\n*   Additional parameters in `INPUT_TYPES`:\n    *   `default` is required\n*   Python datatype `str`\n\n### BOOLEAN\n\n*   Additional parameters in `INPUT_TYPES`:\n    *   `default` is required\n*   Python datatype `bool`\n\n## Tensor datatypes\n\n### IMAGE\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype `torch.Tensor` with _shape_ \\[B,H,W,C\\]\n\nA batch of `B` images, height `H`, width `W`, with `C` channels (generally `C=3` for `RGB`).\n\n### LATENT\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype `dict`, containing a `torch.Tensor` with _shape_ \\[B,C,H,W\\]\n\nThe `dict` passed contains the key `samples`, which is a `torch.Tensor` with _shape_ \\[B,C,H,W\\] representing a batch of `B` latents, with `C` channels (generally `C=4` for existing stable diffusion models), height `H`, width `W`. The height and width are 1/8 of the corresponding image size (which is the value you set in the Empty Latent Image node). Other entries in the dictionary contain things like latent masks.\n\n### MASK\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype `torch.Tensor` with _shape_ \\[H,W\\] or \\[B,C,H,W\\]\n\n### AUDIO\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype `dict`, containing a `torch.Tensor` with _shape_ \\[B, C, T\\] and a sample rate.\n\nThe `dict` passed contains the key `waveform`, which is a `torch.Tensor` with _shape_ \\[B, C, T\\] representing a batch of `B` audio samples, with `C` channels (`C=2` for stereo and `C=1` for mono), and `T` time steps (i.e., the number of audio samples). The `dict` contains another key `sample_rate`, which indicates the sampling rate of the audio.\n\n## Custom Sampling datatypes\n\n### Noise\n\nThe `NOISE` datatype represents a _source_ of noise (not the actual noise itself). It can be represented by any Python object that provides a method to generate noise, with the signature `generate_noise(self, input_latent:Tensor) -> Tensor`, and a property, `seed:Optional[int]`.\n\nWhen noise is to be added, the latent is passed into this method, which should return a `Tensor` of the same shape containing the noise. See the [noise mixing example](https://docs.comfy.org/custom-nodes/backend/snippets#creating-noise-variations)\n\n### Sampler\n\nThe `SAMPLER` datatype represents a sampler, which is represented as a Python object providing a `sample` method. Stable diffusion sampling is beyond the scope of this guide; see `comfy/samplers.py` if you want to dig into this part of the code.\n\n### Sigmas\n\nThe `SIGMAS` datatypes represents the values of sigma before and after each step in the sampling process, as produced by a scheduler. This is represented as a one-dimensional tensor, of length `steps+1`, where each element represents the noise expected to be present before the corresponding step, with the final value representing the noise present after the final step. A `normal` scheduler, with 20 steps and denoise of 1, for an SDXL model, produces:\n\n```\ntensor([14.6146, 10.7468,  8.0815,  6.2049,  4.8557,  \n         3.8654,  3.1238,  2.5572,  2.1157,  1.7648,  \n         1.4806,  1.2458,  1.0481,  0.8784,  0.7297,  \n         0.5964,  0.4736,  0.3555,  0.2322,  0.0292,  0.0000])\n```\n\n### Guider\n\nA `GUIDER` is a generalisation of the denoising process, as ‘guided’ by a prompt or any other form of conditioning. In Comfy the guider is represented by a `callable` Python object providing a `__call__(*args, **kwargs)` method which is called by the sample. The `__call__` method takes (in `args[0]`) a batch of noisy latents (tensor `[B,C,H,W]`), and returns a prediction of the noise (a tensor of the same shape).\n\n## Model datatypes\n\nThere are a number of more technical datatypes for stable diffusion models. The most significant ones are `MODEL`, `CLIP`, `VAE` and `CONDITIONING`. Working with these is (for the time being) beyond the scope of this guide!\n\n## Additional Parameters\n\nBelow is a list of officially supported keys that can be used in the ‘extra options’ portion of an input definition.\n\n| Key | Description |\n| --- | --- |\n| `default` | The default value of the widget |\n| `min` | The minimum value of a number (`FLOAT` or `INT`) |\n| `max` | The maximum value of a number (`FLOAT` or `INT`) |\n| `step` | The amount to increment or decrement a widget |\n| `label_on` | The label to use in the UI when the bool is `True` (`BOOL`) |\n| `label_off` | The label to use in the UI when the bool is `False` (`BOOL`) |\n| `defaultInput` | Defaults to an input socket rather than a supported widget |\n| `forceInput` | `defaultInput` and also don’t allow converting to a widget |\n| `multiline` | Use a multiline text box (`STRING`) |\n| `placeholder` | Placeholder text to display in the UI when empty (`STRING`) |\n| `dynamicPrompts` | Causes the front-end to evaluate dynamic prompts |\n| `lazy` | Declares that this input uses [Lazy Evaluation](https://docs.comfy.org/custom-nodes/backend/lazy_evaluation) |\n| `rawLink` | When a link exists, rather than receiving the evaluated value, you will receive the link (i.e. `[\"nodeId\", <outputIndex>]`). Primarily useful when your node uses [Node Expansion](https://docs.comfy.org/custom-nodes/backend/expansion). |"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/expansion",
  "markdown": "# Node Expansion - ComfyUI\n\nNormally, when a node is executed, that execution function immediately returns the output results of that node. “Node Expansion” is a relatively advanced technique that allows nodes to return a new subgraph of nodes that should take its place in the graph. This technique is what allows custom nodes to implement loops.\n\n### A Simple Example\n\nFirst, here’s a simple example of what node expansion looks like:\n\n```\ndef load_and_merge_checkpoints(self, checkpoint_path1, checkpoint_path2, ratio):\n    from comfy_execution.graph_utils import GraphBuilder # Usually at the top of the file\n    graph = GraphBuilder()\n    checkpoint_node1 = graph.node(\"CheckpointLoaderSimple\", checkpoint_path=checkpoint_path1)\n    checkpoint_node2 = graph.node(\"CheckpointLoaderSimple\", checkpoint_path=checkpoint_path2)\n    merge_model_node = graph.node(\"ModelMergeSimple\", model1=checkpoint_node1.out(0), model2=checkpoint_node2.out(0), ratio=ratio)\n    merge_clip_node = graph.node(\"ClipMergeSimple\", clip1=checkpoint_node1.out(1), clip2=checkpoint_node2.out(1), ratio=ratio)\n    return {\n        # Returning (MODEL, CLIP, VAE) outputs\n        \"result\": (merge_model_node.out(0), merge_clip_node.out(0), checkpoint_node1.out(2)),\n        \"expand\": graph.finalize(),\n    }\n```\n\nWhile this same node could previously be implemented by manually calling into ComfyUI internals, using expansion means that each subnode will be cached separately (so if you change `model2`, you don’t have to reload `model1`).\n\n### Requirements\n\nIn order to perform node expansion, a node must return a dictionary with the following keys:\n\n1.  `result`: A tuple of the outputs of the node. This may be a mix of finalized values (like you would return from a normal node) and node outputs.\n2.  `expand`: The finalized graph to perform expansion on. See below if you are not using the `GraphBuilder`.\n\n#### Additional Requirements if not using GraphBuilder\n\nThe format expected from the `expand` key is the same as the ComfyUI API format. The following requirements are handled by the `GraphBuilder`, but must be handled manually if you choose to forego it:\n\n1.  Node IDs must be unique across the entire graph. (This includes between multiple executions of the same node due to the use of lists.)\n2.  Node IDs must be deterministic and consistent between multiple executions of the graph (including partial executions due to caching).\n\nEven if you don’t want to use the `GraphBuilder` for actually building the graph (e.g. because you’re loading the raw json of the graph from a file), you can use the `GraphBuilder.alloc_prefix()` function to generate a prefix and `comfy.graph_utils.add_graph_prefix` to fix existing graphs to meet these requirements.\n\n### Efficient Subgraph Caching\n\nWhile you can pass non-literal inputs to nodes within the subgraph (like torch tensors), this can inhibit caching _within_ the subgraph. When possible, you should pass links to subgraph objects rather than the node itself. (You can declare an input as a `rawLink` within the input’s [Additional Parameters](https://docs.comfy.org/custom-nodes/backend/datatypes#additional-parameters) to do this easily.)"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/images_and_masks",
  "markdown": "# Images, Latents, and Masks - ComfyUI\n\nWhen working with these datatypes, you will need to know about the `torch.Tensor` class. Complete documentation is [here](https://pytorch.org/docs/stable/tensors.html), or an introduction to the key concepts required for Comfy [here](https://docs.comfy.org/custom-nodes/backend/tensors).\n\nMost of the concepts below are illustrated in the [example code snippets](https://docs.comfy.org/custom-nodes/backend/snippets).\n\nAn IMAGE is a `torch.Tensor` with shape `[B,H,W,C]`, `C=3`. If you are going to save or load images, you will need to convert to and from `PIL.Image` format - see the code snippets below! Note that some `pytorch` operations offer (or expect) `[B,C,H,W]`, known as ‘channel first’, for reasons of computational efficiency. Just be careful.\n\n### Working with PIL.Image\n\nIf you want to load and save images, you’ll want to use PIL:\n\n```\nfrom PIL import Image, ImageOps\n```\n\n## Masks\n\nA MASK is a `torch.Tensor` with shape `[B,H,W]`. In many contexts, masks have binary values (0 or 1), which are used to indicate which pixels should undergo specific operations. In some cases values between 0 and 1 are used indicate an extent of masking, (for instance, to alter transparency, adjust filters, or composite layers).\n\n### Masks from the Load Image Node\n\nThe `LoadImage` node uses an image’s alpha channel (the “A” in “RGBA”) to create MASKs. The values from the alpha channel are normalized to the range \\[0,1\\] (torch.float32) and then inverted. The `LoadImage` node always produces a MASK output when loading an image. Many images (like JPEGs) don’t have an alpha channel. In these cases, `LoadImage` creates a default mask with the shape `[1, 64, 64]`.\n\n### Understanding Mask Shapes\n\nIn libraries like `numpy`, `PIL`, and many others, single-channel images (like masks) are typically represented as 2D arrays, shape `[H,W]`. This means the `C` (channel) dimension is implicit, and thus unlike IMAGE types, batches of MASKs have only three dimensions: `[B, H, W]`. It is not uncommon to encounter a mask which has had the `B` dimension implicitly squeezed, giving a tensor `[H,W]`. To use a MASK, you will often have to match shapes by unsqueezing to produce a shape `[B,H,W,C]` with `C=1` To unsqueezing the `C` dimension, so you should `unsqueeze(-1)`, to unsqueeze `B`, you `unsqueeze(0)`. If your node receives a MASK as input, you would be wise to always check `len(mask.shape)`.\n\n## Latents\n\nA LATENT is a `dict`; the latent sample is referenced by the key `samples` and has shape `[B,C,H,W]`, with `C=4`."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftroubleshooting%2Foverview",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/lazy_evaluation",
  "markdown": "# Lazy Evaluation - ComfyUI\n\nBy default, all `required` and `optional` inputs are evaluated before a node can be run. Sometimes, however, an input won’t necessarily be used and evaluating it would result in unnecessary processing. Here are some examples of nodes where lazy evaluation may be beneficial:\n\n1.  A `ModelMergeSimple` node where the ratio is either `0.0` (in which case the first model doesn’t need to be loaded) or `1.0` (in which case the second model doesn’t need to be loaded).\n2.  Interpolation between two images where the ratio (or mask) is either entirely `0.0` or entirely `1.0`.\n3.  A Switch node where one input determines which of the other inputs will be passed through.\n\n### Creating Lazy Inputs\n\nThere are two steps to making an input a “lazy” input. They are:\n\n1.  Mark the input as lazy in the dictionary returned by `INPUT_TYPES`\n2.  Define a method named `check_lazy_status` (note: _not_ a class method) that will be called prior to evaluation to determine if any more inputs are necessary.\n\nTo demonstrate these, we’ll make a “MixImages” node that interpolates between two images according to a mask. If the entire mask is `0.0`, we don’t need to evaluate any part of the tree leading up to the second image. If the entire mask is `1.0`, we can skip evaluating the first image.\n\n#### Defining `INPUT_TYPES`\n\nDeclaring that an input is lazy is as simple as adding a `lazy: True` key-value pair to the input’s options dictionary.\n\n```\n@classmethod\ndef INPUT_TYPES(cls):\n    return {\n        \"required\": {\n            \"image1\": (\"IMAGE\",{\"lazy\": True}),\n            \"image2\": (\"IMAGE\",{\"lazy\": True}),\n            \"mask\": (\"MASK\",),\n        },\n    }\n```\n\nIn this example, `image1` and `image2` are both marked as lazy inputs, but `mask` will always be evaluated.\n\n#### Defining `check_lazy_status`\n\nA `check_lazy_status` method is called if there are one or more lazy inputs that are not yet available. This method receives the same arguments as the standard execution function. All available inputs are passed in with their final values while unavailable lazy inputs have a value of `None`. The responsibility of the `check_lazy_status` function is to return a list of the names of any lazy inputs that are needed to proceed. If all lazy inputs are available, the function should return an empty list. Note that `check_lazy_status` may be called multiple times. (For example, you might find after evaluating one lazy input that you need to evaluate another.)\n\n```\ndef check_lazy_status(self, mask, image1, image2):\n    mask_min = mask.min()\n    mask_max = mask.max()\n    needed = []\n    if image1 is None and (mask_min != 1.0 or mask_max != 1.0):\n        needed.append(\"image1\")\n    if image2 is None and (mask_min != 0.0 or mask_max != 0.0):\n        needed.append(\"image2\")\n    return needed\n```\n\n### Full Example\n\n```\nclass LazyMixImages:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image1\": (\"IMAGE\",{\"lazy\": True}),\n                \"image2\": (\"IMAGE\",{\"lazy\": True}),\n                \"mask\": (\"MASK\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"mix\"\n\n    CATEGORY = \"Examples\"\n\n    def check_lazy_status(self, mask, image1, image2):\n        mask_min = mask.min()\n        mask_max = mask.max()\n        needed = []\n        if image1 is None and (mask_min != 1.0 or mask_max != 1.0):\n            needed.append(\"image1\")\n        if image2 is None and (mask_min != 0.0 or mask_max != 0.0):\n            needed.append(\"image2\")\n        return needed\n\n    # Not trying to handle different batch sizes here just to keep the demo simple\n    def mix(self, mask, image1, image2):\n        mask_min = mask.min()\n        mask_max = mask.max()\n        if mask_min == 0.0 and mask_max == 0.0:\n            return (image1,)\n        elif mask_min == 1.0 and mask_max == 1.0:\n            return (image2,)\n\n        result = image1 * (1. - mask) + image2 * mask,\n        return (result[0],)\n```\n\n## Execution Blocking\n\nWhile Lazy Evaluation is the recommended way to “disable” part of a graph, there are times when you want to disable an `OUTPUT` node that doesn’t implement lazy evaluation itself. If it’s an output node that you developed yourself, you should just add lazy evaluation as follows:\n\n1.  Add a required (if this is a new node) or optional (if you care about backward compatibility) input for `enabled` that defaults to `True`\n2.  Make all other inputs `lazy` inputs\n3.  Only evaluate the other inputs if `enabled` is `True`\n\nIf it’s not a node you control, you can make use of a `comfy_execution.graph.ExecutionBlocker`. This special object can be returned as an output from any socket. Any nodes which receive an `ExecutionBlocker` as input will skip execution and return that `ExecutionBlocker` for any outputs.\n\n### Usage\n\nThere are two ways to construct and use an `ExecutionBlocker`\n\n1.  Pass `None` into the constructor to silently block execution. This is useful for cases where blocking execution is part of a successful run — like disabling an output.\n\n```\ndef silent_passthrough(self, passthrough, blocked):\n    if blocked:\n        return (ExecutionBlocker(None),)\n    else:\n        return (passthrough,)\n```\n\n2.  Pass a string into the constructor to display an error message when a node is blocked due to receiving the object. This can be useful if you want to display a meaningful error message if someone uses a meaningless output — for example, the `VAE` output when loading a model that doesn’t contain VAEs.\n\n```\ndef load_checkpoint(self, ckpt_name):\n    ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n    model, clip, vae = load_checkpoint(ckpt_path)\n    if vae is None:\n        # This error is more useful than a \"'NoneType' has no attribute\" error\n        # in a later node\n        vae = ExecutionBlocker(f\"No VAE contained in the loaded model {ckpt_name}\")\n    return (model, clip, vae)\n```"
},
{
  "url": "https://docs.comfy.org/comfy-cli/getting-started",
  "markdown": "# Getting Started - ComfyUI\n\n### Overview\n\n`comfy-cli` is a [command line tool](https://github.com/Comfy-Org/comfy-cli) that makes it easier to install and manage Comfy.\n\n### Install CLI\n\nTo get shell completion hints:\n\n```\ncomfy --install-completion\n```\n\n### Install ComfyUI\n\nCreate a virtual environment with any Python version greater than 3.9.\n\nInstall ComfyUI\n\n### Run ComfyUI\n\n### Manage Custom Nodes\n\n```\ncomfy node install <NODE_NAME>\n```\n\nWe use `cm-cli` for installing custom nodes. See the [docs](https://github.com/ltdrdata/ComfyUI-Manager/blob/main/docs/en/cm-cli.md) for more information.\n\n### Manage Models\n\nDownloading models with `comfy-cli` is easy. Just run:\n\n```\ncomfy model download <url> models/checkpoints\n```\n\n### Contributing\n\nWe encourage contributions to comfy-cli! If you have suggestions, ideas, or bug reports, please open an issue on our [GitHub repository](https://github.com/Comfy-Org/comfy-cli/issues). If you want to contribute code, fork the repository and submit a pull request. Refer to the [Dev Guide](https://github.com/Comfy-Org/comfy-cli/blob/main/DEV_README.md) for further details.\n\n### Analytics\n\nWe track usage of the CLI to improve the user experience. You can disable this by running:\n\nTo re-enable tracking, run:"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipSetLastLayer",
  "markdown": "# ClipSetLastLayer - ComfyUI Built-in Node Documentation\n\n`CLIP Set Last Layer` is a core node in ComfyUI for controlling the processing depth of CLIP models. It allows users to precisely control where the CLIP text encoder stops processing, affecting both the depth of text understanding and the style of generated images. Imagine the CLIP model as a 24-layer intelligent brain:\n\n*   Shallow layers (1-8): Recognize basic letters and words\n*   Middle layers (9-16): Understand grammar and sentence structure\n*   Deep layers (17-24): Grasp abstract concepts and complex semantics\n\n`CLIP Set Last Layer` works like a **“thinking depth controller”**: \\-1: Use all 24 layers (complete understanding) -2: Stop at layer 23 (slightly simplified) -12: Stop at layer 13 (medium understanding) -24: Use only layer 1 (basic understanding)\n\n## Inputs\n\n| Parameter | Data Type | Default | Range | Description |\n| --- | --- | --- | --- | --- |\n| `clip` | CLIP | \\-  | \\-  | The CLIP model to be modified |\n| `stop_at_clip_layer` | INT | \\-1 | \\-24 to -1 | Specifies which layer to stop at, -1 uses all layers, -24 uses only the first layer |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| clip | CLIP | The modified CLIP model with the specified layer set as the last one |\n\n## Why Set the Last Layer\n\n*   **Performance Optimization**: Like not needing a PhD to understand simple sentences, sometimes shallow understanding is enough and faster\n*   **Style Control**: Different levels of understanding produce different artistic styles\n*   **Compatibility**: Some models might perform better at specific layers"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncode",
  "markdown": "# ClipTextEncode - ComfyUI Built-in Node Documentation\n\n`CLIP Text Encode (CLIPTextEncode)` acts like a translator, converting your creative text prompts into a special “language” that AI can understand, helping the AI accurately interpret what kind of image you want to create. Imagine communicating with a foreign artist - you need a translator to help accurately convey the artwork you want. This node acts as that translator, using the CLIP model (an AI model trained on vast amounts of image-text pairs) to understand your text descriptions and convert them into “instructions” that the AI art model can understand.\n\n## Inputs\n\n| Parameter | Data Type | Input Method | Default | Range | Description |\n| --- | --- | --- | --- | --- | --- |\n| text | STRING | Text Input | Empty | Any text | Like detailed instructions to an artist, enter your image description here. Supports multi-line text for detailed descriptions. |\n| clip | CLIP | Model Selection | None | Loaded CLIP models | Like choosing a specific translator, different CLIP models are like different translators with slightly different understandings of artistic styles. |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| CONDITIONING | CONDITIONING | These are the translated “painting instructions” containing detailed creative guidance that the AI model can understand. These instructions tell the AI model how to create an image matching your description. |\n\n## Usage Tips\n\n1.  **Basic Text Prompt Usage**\n    *   Write detailed descriptions like you’re writing a short essay\n    *   More specific descriptions lead to more accurate results\n    *   Use English commas to separate different descriptive elements\n2.  **Special Feature: Using Embedding Models**\n    *   Embedding models are like preset art style packages that can quickly apply specific artistic effects\n    *   Currently supports .safetensors, .pt, and .bin file formats, and you don’t necessarily need to use the complete model name\n    *   How to use:\n        \n        1.  Place the embedding model file (in .pt format) in the `ComfyUI/models/embeddings` folder\n        2.  Use `embedding:model_name` in your text Example: If you have a model called `EasyNegative.pt`, you can use it like this:\n        \n        ```\n        a beautiful landscape, embedding:EasyNegative, high quality\n        ```\n        \n3.  **Prompt Weight Adjustment**\n    *   Use parentheses to adjust the importance of certain descriptions\n    *   For example: `(beautiful:1.2)` will make the “beautiful” feature more prominent\n    *   Regular parentheses `()` have a default weight of 1.1\n    *   Use keyboard shortcuts `ctrl + up/down arrow` to quickly adjust weights\n    *   The weight adjustment step size can be modified in settings\n4.  **Important Notes**\n    *   Ensure the CLIP model is properly loaded\n    *   Use positive and clear text descriptions\n    *   When using embedding models, make sure the file name is correct and compatible with your current main model’s architecture\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)\n*   [Usage Tips](#usage-tips)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipVisionEncode",
  "markdown": "# ClipVisionEncode - ComfyUI Built-in Node Documentation\n\nThe `CLIP Vision Encode` node is an image encoding node in ComfyUI, used to convert input images into visual feature vectors through the CLIP Vision model. This node is an important bridge connecting image and text understanding, and is widely used in various AI image generation and processing workflows. **Node Functionality**\n\n*   **Image feature extraction**: Converts input images into high-dimensional feature vectors\n*   **Multimodal bridging**: Provides a foundation for joint processing of images and text\n*   **Conditional generation**: Provides visual conditions for image-based conditional generation\n\n## Inputs\n\n| Parameter Name | Data Type | Description |\n| --- | --- | --- |\n| `clip_vision` | CLIP\\_VISION | CLIP vision model, usually loaded via the CLIPVisionLoader node |\n| `image` | IMAGE | The input image to be encoded |\n| `crop` | Dropdown | Image cropping method, options: center (center crop), none (no crop) |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| CLIP\\_VISION\\_OUTPUT | CLIP\\_VISION\\_OUTPUT | Encoded visual features |\n\nThis output object contains:\n\n*   `last_hidden_state`: The last hidden state\n*   `image_embeds`: Image embedding vector\n*   `penultimate_hidden_states`: The penultimate hidden state\n*   `mm_projected`: Multimodal projection result (if available)\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/Canny",
  "markdown": "# Canny - ComfyUI Built-in Node Documentation\n\nExtract all edge lines from photos, like using a pen to outline a photo, drawing out the contours and detail boundaries of objects.\n\n## Working Principle\n\nImagine you are an artist who needs to use a pen to outline a photo. The Canny node acts like an intelligent assistant, helping you decide where to draw lines (edges) and where not to. This process is like a screening job:\n\n*   **High threshold** is the “must draw line standard”: only very obvious and clear contour lines will be drawn, such as facial contours of people and building frames\n*   **Low threshold** is the “definitely don’t draw line standard”: edges that are too weak will be ignored to avoid drawing noise and meaningless lines\n*   **Middle area**: edges between the two standards will be drawn together if they connect to “must draw lines”, but won’t be drawn if they are isolated\n\nThe final output is a black and white image, where white parts are detected edge lines and black parts are areas without edges.\n\n## Inputs\n\n| Parameter Name | Data Type | Input Type | Default | Range | Function Description |\n| --- | --- | --- | --- | --- | --- |\n| `image` | IMAGE | Input | \\-  | \\-  | Original photo that needs edge extraction |\n| `low_threshold` | FLOAT | Widget | 0.4 | 0.01-0.99 | Low threshold, determines how weak edges to ignore. Lower values preserve more details but may produce noise |\n| `high_threshold` | FLOAT | Widget | 0.8 | 0.01-0.99 | High threshold, determines how strong edges to preserve. Higher values only keep the most obvious contour lines |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| `image` | IMAGE | Black and white edge image, white lines are detected edges, black areas are parts without edges |\n\n## Parameter Comparison\n\n![Original Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/canny/input.webp) ![Parameter Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/canny/compare.webp) **Common Issues:**\n\n*   Broken edges: Try lowering high threshold\n*   Too much noise: Raise low threshold\n*   Missing important details: Lower low threshold\n*   Edges too rough: Check input image quality and resolution"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/CheckpointLoaderSimple",
  "markdown": "# CheckpointLoaderSimple - ComfyUI Built-in Node Documentation\n\nThis is a model loader node that loads model files from specified locations and decomposes them into three core components: the main model, text encoder, and image encoder/decoder. This node automatically detects all model files in the `ComfyUI/models/checkpoints` folder, as well as additional paths configured in your `extra_model_paths.yaml` file.\n\n1.  **Model Compatibility**: Ensure the selected model is compatible with your workflow. Different model types (such as SD1.5, SDXL, Flux, etc.) need to be paired with corresponding samplers and other nodes\n2.  **File Management**: Place model files in the `ComfyUI/models/checkpoints` folder, or configure other paths through extra\\_model\\_paths.yaml\n3.  **Interface Refresh**: If new model files are added while ComfyUI is running, you need to refresh the browser (Ctrl+R) to see the new files in the dropdown list\n\n## Inputs\n\n| Parameter | Data Type | Input Type | Default | Range | Description |\n| --- | --- | --- | --- | --- | --- |\n| `ckpt_name` | STRING | Widget | null | All model files in checkpoints folder | Select the checkpoint model file name to load, which determines the AI model used for subsequent image generation |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| `MODEL` | MODEL | The main diffusion model used for image denoising generation, the core component of AI image creation |\n| `CLIP` | CLIP | The model used for encoding text prompts, converting text descriptions into information that AI can understand |\n| `VAE` | VAE | The model used for image encoding and decoding, responsible for converting between pixel space and latent space |\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipLoader",
  "markdown": "# ClipLoader - ComfyUI Built-in Node Documentation\n\nThis node is primarily used for loading CLIP text encoder models independently. The model files can be detected in the following paths:\n\n*   “ComfyUI/models/text\\_encoders/”\n*   “ComfyUI/models/clip/”\n\n> If you save a model after ComfyUI has started, you’ll need to refresh the ComfyUI frontend to get the latest model file path list\n\nSupported model formats:\n\n*   `.ckpt`\n*   `.pt`\n*   `.pt2`\n*   `.bin`\n*   `.pth`\n*   `.safetensors`\n*   `.pkl`\n*   `.sft`\n\nFor more details on the latest model file loading, please refer to [folder\\_paths](https://github.com/comfyanonymous/ComfyUI/blob/master/folder_paths.py)\n\n## Inputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip_name` | COMBO\\[STRING\\] | Specifies the name of the CLIP model to be loaded. This name is used to locate the model file within a predefined directory structure. |\n| `type` | COMBO\\[STRING\\] | Determines the type of CLIP model to load. As ComfyUI supports more models, new types will be added here. Please check the `CLIPLoader` class definition in [node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py) for details. |\n| `device` | COMBO\\[STRING\\] | Choose the device for loading the CLIP model. `default` will run the model on GPU, while selecting `CPU` will force loading on CPU. |\n\n### Device Options Explained\n\n**When to choose “default”:**\n\n*   Have sufficient GPU memory\n*   Want the best performance\n*   Let the system optimize memory usage automatically\n\n**When to choose “cpu”:**\n\n*   Insufficient GPU memory\n*   Need to reserve GPU memory for other models (like UNet)\n*   Running in a low VRAM environment\n*   Debugging or special purpose needs\n\n**Performance Impact** Running on CPU will be much slower than GPU, but it can save valuable GPU memory for other more important model components. In memory-constrained environments, putting the CLIP model on CPU is a common optimization strategy.\n\n### Supported Combinations\n\n| Model Type | Corresponding Encoder |\n| --- | --- |\n| stable\\_diffusion | clip-l |\n| stable\\_cascade | clip-g |\n| sd3 | t5 xxl/ clip-g / clip-l |\n| stable\\_audio | t5 base |\n| mochi | t5 xxl |\n| cosmos | old t5 xxl |\n| lumina2 | gemma 2 2B |\n| wan | umt5 xxl |\n\nAs ComfyUI updates, these combinations may expand. For details, please refer to the `CLIPLoader` class definition in [node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py)\n\n## Outputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip` | CLIP | The loaded CLIP model, ready for use in downstream tasks or further processing. |\n\n## Additional Notes\n\nCLIP models play a core role as text encoders in ComfyUI, responsible for converting text prompts into numerical representations that diffusion models can understand. You can think of them as translators, responsible for translating your text into a language that large models can understand. Of course, different models have their own “dialects,” so different CLIP encoders are needed between different architectures to complete the text encoding process."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipVisionLoader",
  "markdown": "# Load CLIP Vision - ComfyUI Built-in Node Documentation\n\nThis node automatically detects models located in the `ComfyUI/models/clip_vision` folder, as well as any additional model paths configured in the `extra_model_paths.yaml` file. If you add models after starting ComfyUI, please **refresh the ComfyUI interface** to ensure the latest model files are listed.\n\n## Inputs\n\n| Field | Data Type | Description |\n| --- | --- | --- |\n| `clip_name` | COMBO\\[STRING\\] | Lists all supported model files in the `ComfyUI/models/clip_vision` folder. |\n\n## Outputs\n\n| Field | Data Type | Description |\n| --- | --- | --- |\n| `clip_vision` | CLIP\\_VISION | Loaded CLIP Vision model, ready for encoding images or other vision-related tasks. |\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_about_panel_badges",
  "markdown": "# About Panel Badges - ComfyUI\n\nThe About Panel Badges API allows extensions to add custom badges to the ComfyUI about page. These badges can display information about your extension and contain links to documentation, source code, or other resources.\n\n## Basic Usage\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  aboutPageBadges: [\n    {\n      label: \"Documentation\",\n      url: \"https://example.com/docs\",\n      icon: \"pi pi-file\"\n    },\n    {\n      label: \"GitHub\",\n      url: \"https://github.com/username/repo\",\n      icon: \"pi pi-github\"\n    }\n  ]\n});\n```\n\n## Badge Configuration\n\nEach badge requires all of these properties:\n\n```\n{\n  label: string,           // Text to display on the badge\n  url: string,             // URL to open when badge is clicked\n  icon: string             // Icon class (e.g., PrimeVue icon)\n}\n```\n\n## Icon Options\n\nBadge icons use PrimeVue’s icon set. Here are some commonly used icons:\n\n*   Documentation: `pi pi-file` or `pi pi-book`\n*   GitHub: `pi pi-github`\n*   External link: `pi pi-external-link`\n*   Information: `pi pi-info-circle`\n*   Download: `pi pi-download`\n*   Website: `pi pi-globe`\n*   Discord: `pi pi-discord`\n\nFor a complete list of available icons, refer to the [PrimeVue Icons documentation](https://primevue.org/icons/).\n\n## Example\n\n```\napp.registerExtension({\n  name: \"BadgeExample\",\n  aboutPageBadges: [\n    {\n      label: \"Website\",\n      url: \"https://example.com\",\n      icon: \"pi pi-home\"\n    },\n    {\n      label: \"Donate\",\n      url: \"https://example.com/donate\",\n      icon: \"pi pi-heart\"\n    },\n    {\n      label: \"Documentation\",\n      url: \"https://example.com/docs\",\n      icon: \"pi pi-book\"\n    }\n  ]\n});\n```\n\nBadges appear in the About panel of the Settings dialog, which can be accessed via the gear icon in the top-right corner of the ComfyUI interface."
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_commands_keybindings",
  "markdown": "# Commands and Keybindings - ComfyUI\n\nThe Commands and Keybindings API allows extensions to register custom commands and associate them with keyboard shortcuts. This enables users to quickly trigger actions without using the mouse.\n\n## Basic Usage\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  // Register commands\n  commands: [\n    {\n      id: \"myCommand\",\n      label: \"My Command\",\n      function: () => {\n        console.log(\"Command executed!\");\n      }\n    }\n  ],\n  // Associate keybindings with commands\n  keybindings: [\n    {\n      combo: { key: \"k\", ctrl: true },\n      commandId: \"myCommand\"\n    }\n  ]\n});\n```\n\n## Command Configuration\n\nEach command requires an `id`, `label`, and `function`:\n\n```\n{\n  id: string,              // Unique identifier for the command\n  label: string,           // Display name for the command\n  function: () => void     // Function to execute when command is triggered\n}\n```\n\n## Keybinding Configuration\n\nEach keybinding requires a `combo` and `commandId`:\n\n```\n{\n  combo: {                 // Key combination\n    key: string,           // The main key (single character or special key)\n    ctrl?: boolean,        // Require Ctrl key (optional)\n    shift?: boolean,       // Require Shift key (optional)\n    alt?: boolean,         // Require Alt key (optional)\n    meta?: boolean         // Require Meta/Command key (optional)\n  },\n  commandId: string        // ID of the command to trigger\n}\n```\n\n### Special Keys\n\nFor non-character keys, use one of these values:\n\n*   Arrow keys: `\"ArrowUp\"`, `\"ArrowDown\"`, `\"ArrowLeft\"`, `\"ArrowRight\"`\n*   Function keys: `\"F1\"` through `\"F12\"`\n*   Other special keys: `\"Escape\"`, `\"Tab\"`, `\"Enter\"`, `\"Backspace\"`, `\"Delete\"`, `\"Home\"`, `\"End\"`, `\"PageUp\"`, `\"PageDown\"`\n\n## Command Examples\n\n```\napp.registerExtension({\n  name: \"CommandExamples\",\n  commands: [\n    {\n      id: \"runWorkflow\",\n      label: \"Run Workflow\",\n      function: () => {\n        app.queuePrompt();\n      }\n    },\n    {\n      id: \"clearWorkflow\",\n      label: \"Clear Workflow\",\n      function: () => {\n        if (confirm(\"Clear the workflow?\")) {\n          app.graph.clear();\n        }\n      }\n    },\n    {\n      id: \"saveWorkflow\",\n      label: \"Save Workflow\",\n      function: () => {\n        app.graphToPrompt().then(workflow => {\n          const blob = new Blob([JSON.stringify(workflow)], {type: \"application/json\"});\n          const url = URL.createObjectURL(blob);\n          const a = document.createElement(\"a\");\n          a.href = url;\n          a.download = \"workflow.json\";\n          a.click();\n          URL.revokeObjectURL(url);\n        });\n      }\n    }\n  ]\n});\n```\n\n## Keybinding Examples\n\n```\napp.registerExtension({\n  name: \"KeybindingExamples\",\n  commands: [\n    /* Commands defined above */\n  ],\n  keybindings: [\n    // Ctrl+R to run workflow\n    {\n      combo: { key: \"r\", ctrl: true },\n      commandId: \"runWorkflow\"\n    },\n    // Ctrl+Shift+C to clear workflow\n    {\n      combo: { key: \"c\", ctrl: true, shift: true },\n      commandId: \"clearWorkflow\"\n    },\n    // Ctrl+S to save workflow\n    {\n      combo: { key: \"s\", ctrl: true },\n      commandId: \"saveWorkflow\"\n    },\n    // F5 to run workflow (alternative)\n    {\n      combo: { key: \"F5\" },\n      commandId: \"runWorkflow\"\n    }\n  ]\n});\n```\n\n## Notes and Limitations\n\n*   Keybindings defined in the ComfyUI core cannot be overwritten by extensions. Check the core keybindings in these source files:\n    *   [Core Commands](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/composables/useCoreCommands.ts)\n    *   [Core Menu Commands](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreMenuCommands.ts)\n    *   [Core Keybindings](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreKeybindings.ts)\n    *   [Reserved Key Combos](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/reservedKeyCombos.ts)\n*   Some key combinations are reserved by the browser (like Ctrl+F for search) and cannot be overridden\n*   If multiple extensions register the same keybinding, the behavior is undefined"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_dialog",
  "markdown": "# Dialog API - ComfyUI\n\nThe Dialog API provides standardized dialogs that work consistently across desktop and web environments. Extension authors will find the prompt and confirm methods most useful.\n\n## Basic Usage\n\n### Prompt Dialog\n\n```\n// Show a prompt dialog\napp.extensionManager.dialog.prompt({\n  title: \"User Input\",\n  message: \"Please enter your name:\",\n  defaultValue: \"User\"\n}).then(result => {\n  if (result !== null) {\n    console.log(`Input: ${result}`);\n  }\n});\n```\n\n### Confirm Dialog\n\n```\n// Show a confirmation dialog\napp.extensionManager.dialog.confirm({\n  title: \"Confirm Action\",\n  message: \"Are you sure you want to continue?\",\n  type: \"default\"\n}).then(result => {\n  console.log(result ? \"User confirmed\" : \"User cancelled\");\n});\n```\n\n## API Reference\n\n### Prompt\n\n```\napp.extensionManager.dialog.prompt({\n  title: string,             // Dialog title\n  message: string,           // Message/question to display\n  defaultValue?: string      // Initial value in the input field (optional)\n}).then((result: string | null) => {\n  // result is the entered text, or null if cancelled\n});\n```\n\n### Confirm\n\n```\napp.extensionManager.dialog.confirm({\n  title: string,             // Dialog title\n  message: string,           // Message to display\n  type?: \"default\" | \"overwrite\" | \"delete\" | \"dirtyClose\" | \"reinstall\", // Dialog type (optional)\n  itemList?: string[],       // List of items to display (optional)\n  hint?: string              // Hint text to display (optional)\n}).then((result: boolean | null) => {\n  // result is true if confirmed, false if denied, null if cancelled\n});\n```\n\nFor other specialized dialogs available in ComfyUI, extension authors can refer to the `dialogService.ts` file in the source code."
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_examples",
  "markdown": "# Annotated Examples - ComfyUI\n\nA growing collection of fragments of example code…\n\nThe main background menu (right-click on the canvas) is generated by a call to  \n`LGraph.getCanvasMenuOptions`. One way to add your own menu options is to hijack this call:\n\n```\n/* in setup() */\n    const original_getCanvasMenuOptions = LGraphCanvas.prototype.getCanvasMenuOptions;\n    LGraphCanvas.prototype.getCanvasMenuOptions = function () {\n        // get the basic options \n        const options = original_getCanvasMenuOptions.apply(this, arguments);\n        options.push(null); // inserts a divider\n        options.push({\n            content: \"The text for the menu\",\n            callback: async () => {\n                // do whatever\n            }\n        })\n        return options;\n    }\n```\n\nWhen you right click on a node, the menu is similarly generated by `node.getExtraMenuOptions`. But instead of returning an options object, this one gets it passed in…\n\n```\n/* in beforeRegisterNodeDef() */\nif (nodeType?.comfyClass==\"MyNodeClass\") { \n    const original_getExtraMenuOptions = nodeType.prototype.getExtraMenuOptions;\n    nodeType.prototype.getExtraMenuOptions = function(_, options) {\n        original_getExtraMenuOptions?.apply(this, arguments);\n        options.push({\n            content: \"Do something fun\",\n            callback: async () => {\n                // fun thing\n            }\n        })\n    }   \n}\n```\n\nIf you want a submenu, provide a callback which uses `LiteGraph.ContextMenu` to create it:\n\n```\nfunction make_submenu(value, options, e, menu, node) {\n    const submenu = new LiteGraph.ContextMenu(\n        [\"option 1\", \"option 2\", \"option 3\"],\n        { \n            event: e, \n            callback: function (v) { \n                // do something with v (==\"option x\")\n            }, \n            parentMenu: menu, \n            node:node\n        }\n    )\n}\n\n/* ... */\n    options.push(\n        {\n            content: \"Menu with options\",\n            has_submenu: true,\n            callback: make_submenu,\n        }\n    )\n```\n\n## Capture UI events\n\nThis works just like you’d expect - find the UI element in the DOM and add an eventListener. `setup()` is a good place to do this, since the page has fully loaded. For instance, to detect a click on the ‘Queue’ button:\n\n```\nfunction queue_button_pressed() { console.log(\"Queue button was pressed!\") }\ndocument.getElementById(\"queue-button\").addEventListener(\"click\", queue_button_pressed);\n```\n\n## Detect when a workflow starts\n\nThis is one of many `api` events:\n\n```\nimport { api } from \"../../scripts/api.js\";\n/* in setup() */\n    function on_execution_start() { \n        /* do whatever */\n    }\n    api.addEventListener(\"execution_start\", on_execution_start);\n```\n\n## Detect an interrupted workflow\n\nA simple example of hijacking the api:\n\n```\nimport { api } from \"../../scripts/api.js\";\n/* in setup() */\n    const original_api_interrupt = api.interrupt;\n    api.interrupt = function () {\n        /* Do something before the original method is called */\n        original_api_interrupt.apply(this, arguments);\n        /* Or after */\n    }\n```\n\n## Catch clicks on your node\n\n`node` has a mouseDown method you can hijack. This time we’re careful to pass on any return value.\n\n```\nasync nodeCreated(node) {\n    if (node?.comfyClass === \"My Node Name\") {\n        const original_onMouseDown = node.onMouseDown;\n        node.onMouseDown = function( e, pos, canvas ) {\n            alert(\"ouch!\");\n            return original_onMouseDown?.apply(this, arguments);\n        }        \n    }\n}\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_hooks",
  "markdown": "# Comfy Hooks - ComfyUI\n\n## Extension hooks\n\nAt various points during Comfy execution, the application calls `#invokeExtensionsAsync` or `#invokeExtensions` with the name of a hook. These invoke, on all registered extensions, the appropriately named method (if present), such as `setup` in the example above. Comfy provides a variety of hooks for custom extension code to use to modify client behavior.\n\nA few of the most significant hooks are described below. As Comfy is being actively developed, from time to time additional hooks are added, so search for `#invokeExtensions` in `app.js` to find all available hooks. See also the [sequence](#call-sequences) in which hooks are invoked.\n\n### Commonly used hooks\n\nStart with `beforeRegisterNodeDef`, which is used by the majority of extensions, and is often the only one needed.\n\n#### beforeRegisterNodeDef()\n\nCalled once for each node type (the list of nodes available in the `AddNode` menu), and is used to modify the behaviour of the node.\n\n```\nasync beforeRegisterNodeDef(nodeType, nodeData, app) \n```\n\nThe object passed in the `nodeType` parameter essentially serves as a template for all nodes that will be created of this type, so modifications made to `nodeType.prototype` will apply to all nodes of this type. `nodeData` is an encapsulation of aspects of the node defined in the Python code, such as its category, inputs, and outputs. `app` is a reference to the main Comfy app object (which you have already imported anyway!)\n\nThe usual idiom is to check `nodeType.ComfyClass`, which holds the Python class name corresponding to this node, to see if you need to modify the node. Often this means modifying the custom nodes that you have added, although you may sometimes need to modify the behavior of other nodes (or other custom nodes might modify yours!), in which case care should be taken to ensure interoperability.\n\nA very common idiom in `beforeRegisterNodeDef` is to ‘hijack’ an existing method:\n\n```\nasync beforeRegisterNodeDef(nodeType, nodeData, app) {\n\tif (nodeType.comfyClass==\"MyNodeClass\") { \n\t\tconst onConnectionsChange = nodeType.prototype.onConnectionsChange;\n\t\tnodeType.prototype.onConnectionsChange = function (side,slot,connect,link_info,output) {     \n\t\t\tconst r = onConnectionsChange?.apply(this, arguments);   \n\t\t\tconsole.log(\"Someone changed my connection!\");\n\t\t\treturn r;\n\t\t}\n\t}\n}\n```\n\nIn this idiom the existing prototype method is stored, and then replaced. The replacement calls the original method (the `?.apply` ensures that if there wasn’t one this is still safe) and then performs additional operations. Depending on your code logic, you may need to place the `apply` elsewhere in your replacement code, or even make calling it conditional. When hijacking a method in this way, you will want to look at the core comfy code (breakpoints are your friend) to check and conform with the method signature.\n\n#### nodeCreated()\n\nCalled when a specific instance of a node gets created (right at the end of the `ComfyNode()` function on `nodeType` which serves as a constructor). In this hook you can make modifications to individual instances of your node.\n\n#### init()\n\nCalled when the Comfy webpage is loaded (or reloaded). The call is made after the graph object has been created, but before any nodes are registered or created. It can be used to modify core Comfy behavior by hijacking methods of the app, or of the graph (a `LiteGraph` object). This is discussed further in [Comfy Objects](https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking).\n\n#### setup()\n\nCalled at the end of the startup process. A good place to add event listeners (either for Comfy events, or DOM events), or adding to the global menus, both of which are discussed elsewhere.\n\n### Call sequences\n\nThese sequences were obtained by insert logging code into the Comfy `app.js` file. You may find similar code helpful in understanding the execution flow.\n\n```\n/* approx line 220 at time of writing: */\n\t#invokeExtensions(method, ...args) {\n\t\tconsole.log(`invokeExtensions      ${method}`) // this line added\n\t\t// ...\n\t}\n/* approx line 250 at time of writing: */\n\tasync #invokeExtensionsAsync(method, ...args) {\n\t\tconsole.log(`invokeExtensionsAsync ${method}`) // this line added\n\t\t// ...\n\t}\n```\n\n#### Web page load\n\n```\ninvokeExtensionsAsync init\ninvokeExtensionsAsync addCustomNodeDefs\ninvokeExtensionsAsync getCustomWidgets\ninvokeExtensionsAsync beforeRegisterNodeDef    [repeated multiple times]\ninvokeExtensionsAsync registerCustomNodes\ninvokeExtensionsAsync beforeConfigureGraph\ninvokeExtensionsAsync nodeCreated\ninvokeExtensions      loadedGraphNode\ninvokeExtensionsAsync afterConfigureGraph\ninvokeExtensionsAsync setup\n```\n\n#### Loading workflow\n\n```\ninvokeExtensionsAsync beforeConfigureGraph\ninvokeExtensionsAsync beforeRegisterNodeDef   [zero, one, or multiple times]\ninvokeExtensionsAsync nodeCreated             [repeated multiple times]\ninvokeExtensions      loadedGraphNode         [repeated multiple times]\ninvokeExtensionsAsync afterConfigureGraph\n```\n\n#### Adding new node\n\n```\ninvokeExtensionsAsync nodeCreated\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/latent/video/trim-video-latent",
  "markdown": "# TrimVideoLatent Node - ComfyUI\n\n![ComfyUI TrimVideoLatent Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/latent/video/trim-video-latent.jpg) The TrimVideoLatent node is used to trim video frames in latent space (LATENT). It is commonly used when processing video latent sequences to remove unwanted frames from the beginning, achieving “forward trimming” of the video. Basic usage: Input the video latent data to be trimmed into samples, and set trim\\_amount to the number of frames to trim. The node will trim the specified number of frames from the beginning of the video and output the remaining latent sequence. Typical scenarios: Used in video generation, video editing and other scenarios to remove unwanted leading frames, or to work with other nodes to achieve video segment splicing and processing.\n\n## Parameters\n\n### Input Parameters\n\n| Parameter | Type | Required | Default | Description |\n| --- | --- | --- | --- | --- |\n| samples | LATENT | Yes | None | Input latent video data |\n| trim\\_amount | INT | Yes | 0   | Number of frames to trim (from start) |\n\n### Output Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| samples | LATENT | Trimmed video latent data |\n\n## Usage Example\n\n[\n\n## Wan2.1 VACE Video Generation Workflow Example\n\nWan2.1 VACE Video Generation Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/video/wan/vace)\n\n### Source Code\n\n```\nclass TrimVideoLatent:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"trim_amount\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 99999}),\n                             }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/video\"\n\n    EXPERIMENTAL = True\n\n    def op(self, samples, trim_amount):\n        samples_out = samples.copy()\n\n        s1 = samples[\"samples\"]\n        samples_out[\"samples\"] = s1[:, :, trim_amount:]\n        return (samples_out,)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipMergeSimple",
  "markdown": "# ClipMergeSimple - ComfyUI Built-in Node Documentation\n\n`CLIPMergeSimple` is an advanced model merging node used to combine two CLIP text encoder models based on a specified ratio. This node specializes in merging two CLIP models based on a specified ratio, effectively blending their characteristics. It selectively applies patches from one model to another, excluding specific components like position IDs and logit scale, to create a hybrid model that combines features from both source models.\n\n## Inputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip1` | CLIP | The first CLIP model to be merged. It serves as the base model for the merging process. |\n| `clip2` | CLIP | The second CLIP model to be merged. Its key patches, except for position IDs and logit scale, are applied to the first model based on the specified ratio. |\n| `ratio` | FLOAT | Range `0.0 - 1.0`, determines the proportion of features from the second model to blend into the first model. A ratio of 1.0 means fully adopting the second model’s features, while 0.0 retains only the first model’s features. |\n\n## Outputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip` | CLIP | The resulting merged CLIP model, incorporating features from both input models according to the specified ratio. |\n\n## Merging Mechanism Explained\n\n### Merging Algorithm\n\nThe node uses weighted averaging to merge the two models:\n\n1.  **Clone Base Model**: First clones `clip1` as the base model\n2.  **Get Patches**: Obtains all key patches from `clip2`\n3.  **Filter Special Keys**: Skips keys ending with `.position_ids` and `.logit_scale`\n4.  **Apply Weighted Merge**: Uses the formula `(1.0 - ratio) * clip1 + ratio * clip2`\n\n### Ratio Parameter Explained\n\n*   **ratio = 0.0**: Fully uses clip1, ignores clip2\n*   **ratio = 0.5**: 50% contribution from each model\n*   **ratio = 1.0**: Fully uses clip2, ignores clip1\n\n## Use Cases\n\n1.  **Model Style Fusion**: Combine characteristics of CLIP models trained on different data\n2.  **Performance Optimization**: Balance strengths and weaknesses of different models\n3.  **Experimental Research**: Explore combinations of different CLIP encoders"
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/comms_routes",
  "markdown": "# Routes - ComfyUI\n\nThe server defines a series of `get` and `post` methods which can be found by searching for `@routes` in `server.py`. When you submit a workflow in the web client, it is posted to `/prompt` which validates the prompt and adds it to an execution queue, returning either a `prompt_id` and `number` (the position in the queue), or `error` and `node_errors` if validation fails. The prompt queue is defined in `execution.py`, which also defines the `PromptExecutor` class.\n\n### Built in routes\n\n`server.py` defines the following routes:\n\n| path | get/post | purpose |\n| --- | --- | --- |\n| `/` | get | load the comfy webpage |\n| `/embeddings` | get | retrieve a list of the names of embeddings available |\n| `/extensions` | get | retrieve a list of the extensions registering a `WEB_DIRECTORY` |\n| `/workflow_templates` | get | retrieve a map of custom node modules and associated template workflows |\n| `/upload/image` | post | upload an image |\n| `/upload/mask` | post | upload a mask |\n| `/view` | get | view an image. Lots of options, see `@routes.get(\"/view\")` in `server.py` |\n| `/view_metadata`/ | get | retrieve metadata for a model |\n| `/system_stats` | get | retrieve information about the system (python version, devices, vram etc) |\n| `/prompt` | get | retrieve current status |\n| `/prompt` | post | submit a prompt to the queue |\n| `/object_info` | get | retrieve details of all node types |\n| `/object_info/{node_class}` | get | retrieve details of one node type |\n| `/history` | get | retrieve the queue history |\n| `/history/{prompt_id}` | get | retrieve the queue history for a specific prompt |\n| `/history` | post | clear history or delete history item |\n| `/queue` | get | retrieve the state of the queue |\n| `/interrupt` | post | stop the current workflow |\n| `/free` | post | free memory by unloading specified models |\n\n### Custom routes\n\nIf you want to send a message from the client to the server during execution, you will need to add a custom route to the server. For anything complicated, you will need to dive into the [aiohttp framework docs](https://docs.aiohttp.org/), but most cases can be handled as follows:\n\n```\nfrom server import PromptServer\nfrom aiohttp import web\nroutes = PromptServer.instance.routes\n@routes.post('/my_new_path')\nasync def my_function(request):\n    the_data = await request.post()\n    # the_data now holds a dictionary of the values sent\n    MyClass.handle_my_message(the_data)\n    return web.json_response({})\n```\n\nThe client can use this new route by sending a `FormData` object with code something like this, which would result in `the_data`, in the above code, containing `message` and `node_id` keys:\n\n```\nimport { api } from \"../../scripts/api.js\";\nfunction send_message(node_id, message) {\n    const body = new FormData();\n    body.append('message',message);\n    body.append('node_id', node_id);\n    api.fetchApi(\"/my_new_path\", { method: \"POST\", body, });\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipSave",
  "markdown": "# ClipSave - ComfyUI Built-in Node Documentation\n\nThe `CLIPSave` node is designed for saving CLIP text encoder models in SafeTensors format. This node is part of advanced model merging workflows and is typically used in conjunction with nodes like `CLIPMergeSimple` and `CLIPMergeAdd`. The saved files use the SafeTensors format to ensure security and compatibility.\n\n## Inputs\n\n| Parameter | Data Type | Required | Default Value | Description |\n| --- | --- | --- | --- | --- |\n| clip | CLIP | Yes | \\-  | The CLIP model to be saved |\n| filename\\_prefix | STRING | Yes | ”clip/ComfyUI” | The prefix path for the saved file |\n| prompt | PROMPT | Hidden | \\-  | Workflow prompt information (for metadata) |\n| extra\\_pnginfo | EXTRA\\_PNGINFO | Hidden | \\-  | Additional PNG information (for metadata) |\n\n## Outputs\n\nThis node has no defined output types. It saves the processed files to the `ComfyUI/output/` folder.\n\n### Multi-file Saving Strategy\n\nThe node saves different components based on the CLIP model type:\n\n| Prefix Type | File Suffix | Description |\n| --- | --- | --- |\n| `clip_l.` | `_clip_l` | CLIP-L text encoder |\n| `clip_g.` | `_clip_g` | CLIP-G text encoder |\n| Empty prefix | No suffix | Other CLIP components |\n\n## Usage Notes\n\n1.  **File Location**: All files are saved in the `ComfyUI/output/` directory\n2.  **File Format**: Models are saved in SafeTensors format for security\n3.  **Metadata**: Includes workflow information and PNG metadata if available\n4.  **Naming Convention**: Uses the specified prefix plus appropriate suffixes based on model type"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/Load3D",
  "markdown": "# Load3D - ComfyUI Built-in Node Documentation\n\nThe Load3D node is a core node for loading and processing 3D model files. When loading the node, it automatically retrieves available 3D resources from `ComfyUI/input/3d/`. You can also upload supported 3D files for preview using the upload function. **Supported Formats** Currently, this node supports multiple 3D file formats, including `.gltf`, `.glb`, `.obj`, `.fbx`, and `.stl`. **3D Node Preferences** Some related preferences for 3D nodes can be configured in ComfyUI’s settings menu. Please refer to the following documentation for corresponding settings: [Settings Menu - 3D](https://docs.comfy.org/interface/settings/3d) Besides regular node outputs, Load3D has lots of 3D view-related settings in the canvas menu.\n\n## Inputs\n\n| Parameter Name | Type | Description | Default | Range |\n| --- | --- | --- | --- | --- |\n| model\\_file | File Selection | 3D model file path, supports upload, defaults to reading model files from `ComfyUI/input/3d/` | \\-  | Supported formats |\n| width | INT | Canvas rendering width | 1024 | 1-4096 |\n| height | INT | Canvas rendering height | 1024 | 1-4096 |\n\n## Outputs\n\n| Parameter Name | Data Type | Description |\n| --- | --- | --- |\n| image | IMAGE | Canvas rendered image |\n| mask | MASK | Mask containing current model position |\n| mesh\\_path | STRING | Model file path |\n| normal | IMAGE | Normal map |\n| lineart | IMAGE | Line art image output, corresponding `edge_threshold` can be adjusted in the canvas model menu |\n| camera\\_info | LOAD3D\\_CAMERA | Camera information |\n| recording\\_video | VIDEO | Recorded video (only when recording exists) |\n\nAll corresponding outputs preview ![View Operation Demo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/load3d_outputs.jpg) \n\n## Canvas Area Description\n\nThe Load3D node’s Canvas area contains numerous view operations, including:\n\n*   Preview view settings (grid, background color, preview view)\n*   Camera control: Control FOV, camera type\n*   Global illumination intensity: Adjust lighting intensity\n*   Video recording: Record and export videos\n*   Model export: Supports `GLB`, `OBJ`, `STL` formats\n*   And more\n\n![Load 3D Node UI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/load3d_ui.jpg)\n\n1.  Contains multiple menus and hidden menus of the Load 3D node\n2.  Menu for `resizing preview window` and `canvas video recording`\n3.  3D view operation axis\n4.  Preview thumbnail\n5.  Preview size settings, scale preview view display by setting dimensions and then resizing window\n\n### 1\\. View Operations\n\nView control operations:\n\n*   Left-click + drag: Rotate the view\n*   Right-click + drag: Pan the view\n*   Middle wheel scroll or middle-click + drag: Zoom in/out\n*   Coordinate axis: Switch views\n\n![Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu.jpg) In the canvas, some settings are hidden in the menu. Click the menu button to expand different menus\n\n*   1.  Scene: Contains preview window grid, background color, preview settings\n*   2.  Model: Model rendering mode, texture materials, up direction settings\n*   3.  Camera: Switch between orthographic and perspective views, and set the perspective angle size\n*   4.  Light: Scene global illumination intensity\n*   5.  Export: Export model to other formats (GLB, OBJ, STL)\n\n#### Scene\n\n![scene menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_scene.jpg) The Scene menu provides some basic scene setting functions\n\n1.  Show/Hide grid\n2.  Set background color\n3.  Click to upload a background image\n4.  Hide the preview\n\n#### Model\n\n![Menu_Scene](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_model.jpg) The Model menu provides some model-related functions\n\n1.  **Up direction**: Determine which axis is the up direction for the model\n2.  **Material mode**: Switch model rendering modes - Original, Normal, Wireframe, Lineart\n\n#### Camera\n\n![menu_modelmenu_camera](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_camera.jpg) This menu provides switching between orthographic and perspective views, and perspective angle size settings\n\n1.  **Camera**: Quickly switch between orthographic and orthographic views\n2.  **FOV**: Adjust FOV angle\n\n#### Light\n\n![menu_modelmenu_camera](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_light.jpg) Through this menu, you can quickly adjust the scene’s global illumination intensity\n\n#### Export\n\n![menu_export](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_export.jpg) This menu provides the ability to quickly convert and export model formats\n\nThe right menu has two main functions:\n\n1.  **Reset view ratio**: After clicking the button, the view will adjust the canvas rendering area ratio according to the set width and height\n2.  **Video recording**: Allows you to record current 3D view operations as video, allows import, and can be output as `recording_video` to subsequent nodes"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/sampling/ksampler",
  "markdown": "# Ksampler - ComfyUI Built-in Node Documentation\n\n![Ksampler](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/sampling/ksampler.jpg) The KSampler node performs multi-step denoising sampling on latent images. It combines positive and negative conditions (prompts) and uses specified sampling algorithms and schedulers to generate high-quality latent images. It is commonly used in AI image generation workflows like text-to-image and image-to-image.\n\n## Parameter Description\n\n### Input Parameters\n\n| Parameter | Type | Required | Default | Description |\n| --- | --- | --- | --- | --- |\n| model | MODEL | Yes | None | Model used for denoising (e.g. Stable Diffusion model) |\n| seed | INT | Yes | 0   | Random seed to ensure reproducible results |\n| steps | INT | Yes | 20  | Number of denoising steps - more steps mean finer details but slower generation |\n| cfg | FLOAT | Yes | 8.0 | Classifier-Free Guidance scale - higher values better match prompts but too high impacts quality |\n| sampler\\_name | Enum | Yes | None | Name of sampling algorithm, affects generation speed, style and quality |\n| scheduler | Enum | Yes | None | Scheduler that controls the noise removal process |\n| positive | CONDITIONING | Yes | None | Positive conditions describing desired image content |\n| negative | CONDITIONING | Yes | None | Negative conditions describing content to exclude |\n| latent\\_image | LATENT | Yes | None | Latent image to denoise, usually noise or output from previous step |\n| denoise | FLOAT | Yes | 1.0 | Denoising strength - 1.0 for full denoising, lower values preserve original structure, suitable for image-to-image |\n\n### Output Parameters\n\n| Output | Type | Description |\n| --- | --- | --- |\n| samples | LATENT | Denoised latent image that can be decoded to final image |\n\n## Usage Examples\n\n[\n\n## Stable diffusion 1.5 Text-to-Image Workflow Example\n\nStable diffusion 1.5 Text-to-Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/basic/text-to-image)[\n\n## Stable diffusion 1.5 Image-to-Image Workflow Example\n\nStable diffusion 1.5 Image-to-Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/basic/image-to-image)\n\n## Source Code\n\n\\[Updated on May 15, 2025\\]\n\n```\n\ndef common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n    latent_image = latent[\"samples\"]\n    latent_image = comfy.sample.fix_empty_latent_channels(model, latent_image)\n\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = comfy.sample.prepare_noise(latent_image, seed, batch_inds)\n\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]\n\n    callback = latent_preview.prepare_callback(model, steps)\n    disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,\n                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    out = latent.copy()\n    out[\"samples\"] = samples\n    return (out, )\n\n\nclass KSampler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"model\": (\"MODEL\", {\"tooltip\": \"The model used for denoising the input latent.\"}),\n                \"seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff, \"control_after_generate\": True, \"tooltip\": \"The random seed used for creating the noise.\"}),\n                \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000, \"tooltip\": \"The number of steps used in the denoising process.\"}),\n                \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01, \"tooltip\": \"The Classifier-Free Guidance scale balances creativity and adherence to the prompt. Higher values result in images more closely matching the prompt however too high values will negatively impact quality.\"}),\n                \"sampler_name\": (comfy.samplers.KSampler.SAMPLERS, {\"tooltip\": \"The algorithm used when sampling, this can affect the quality, speed, and style of the generated output.\"}),\n                \"scheduler\": (comfy.samplers.KSampler.SCHEDULERS, {\"tooltip\": \"The scheduler controls how noise is gradually removed to form the image.\"}),\n                \"positive\": (\"CONDITIONING\", {\"tooltip\": \"The conditioning describing the attributes you want to include in the image.\"}),\n                \"negative\": (\"CONDITIONING\", {\"tooltip\": \"The conditioning describing the attributes you want to exclude from the image.\"}),\n                \"latent_image\": (\"LATENT\", {\"tooltip\": \"The latent image to denoise.\"}),\n                \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01, \"tooltip\": \"The amount of denoising applied, lower values will maintain the structure of the initial image allowing for image to image sampling.\"}),\n            }\n        }\n\n    RETURN_TYPES = (\"LATENT\",)\n    OUTPUT_TOOLTIPS = (\"The denoised latent.\",)\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling\"\n    DESCRIPTION = \"Uses the provided model, positive and negative conditioning to denoise the latent image.\"\n\n    def sample(self, model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0):\n        return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_bottom_panel_tabs",
  "markdown": "# Bottom Panel Tabs - ComfyUI\n\nThe Bottom Panel Tabs API allows extensions to add custom tabs to the bottom panel of the ComfyUI interface. This is useful for adding features like logs, debugging tools, or custom panels.\n\n## Basic Usage\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  bottomPanelTabs: [\n    {\n      id: \"customTab\",\n      title: \"Custom Tab\",\n      type: \"custom\",\n      render: (el) => {\n        el.innerHTML = '<div>This is my custom tab content</div>';\n      }\n    }\n  ]\n});\n```\n\n## Tab Configuration\n\nEach tab requires an `id`, `title`, and `type`, along with a render function:\n\n```\n{\n  id: string,              // Unique identifier for the tab\n  title: string,           // Display title shown on the tab\n  type: string,            // Tab type (usually \"custom\")\n  icon?: string,           // Icon class (optional)\n  render: (element) => void // Function that populates the tab content\n}\n```\n\nThe `render` function receives a DOM element where you should insert your tab’s content.\n\n## Interactive Elements\n\nYou can add interactive elements like buttons:\n\n```\napp.registerExtension({\n  name: \"InteractiveTabExample\",\n  bottomPanelTabs: [\n    {\n      id: \"controlsTab\",\n      title: \"Controls\",\n      type: \"custom\",\n      render: (el) => {\n        el.innerHTML = `\n          <div style=\"padding: 10px;\">\n            <button id=\"runBtn\">Run Workflow</button>\n          </div>\n        `;\n        \n        // Add event listeners\n        el.querySelector('#runBtn').addEventListener('click', () => {\n          app.queuePrompt();\n        });\n      }\n    }\n  ]\n});\n```\n\n## Using React Components\n\nYou can mount React components in bottom panel tabs:\n\n```\n// Import React dependencies in your extension\nimport React from \"react\";\nimport ReactDOM from \"react-dom/client\";\n\n// Simple React component\nfunction TabContent() {\n  const [count, setCount] = React.useState(0);\n  \n  return (\n    <div style={{ padding: \"10px\" }}>\n      <h3>React Component</h3>\n      <p>Count: {count}</p>\n      <button onClick={() => setCount(count + 1)}>Increment</button>\n    </div>\n  );\n}\n\n// Register the extension with React content\napp.registerExtension({\n  name: \"ReactTabExample\",\n  bottomPanelTabs: [\n    {\n      id: \"reactTab\",\n      title: \"React Tab\",\n      type: \"custom\",\n      render: (el) => {\n        const container = document.createElement(\"div\");\n        container.id = \"react-tab-container\";\n        el.appendChild(container);\n        \n        // Mount React component\n        ReactDOM.createRoot(container).render(\n          <React.StrictMode>\n            <TabContent />\n          </React.StrictMode>\n        );\n      }\n    }\n  ]\n});\n```\n\n## Standalone Registration\n\nYou can also register tabs outside of `registerExtension`:\n\n```\napp.extensionManager.registerBottomPanelTab({\n  id: \"standAloneTab\",\n  title: \"Stand-Alone Tab\",\n  type: \"custom\",\n  render: (el) => {\n    el.innerHTML = '<div>This tab was registered independently</div>';\n  }\n});\n```\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/execution_model_inversion_guide",
  "markdown": "# Execution Model Inversion Guide - ComfyUI\n\n[PR #2666](https://github.com/comfyanonymous/ComfyUI/pull/2666) inverts the execution model from a back-to-front recursive model to a front-to-back topological sort. While most custom nodes should continue to “just work”, this page is intended to serve as a guide for custom node creators to the things that _could_ break.\n\n## Breaking Changes\n\n### Monkey Patching\n\nAny code that monkey patched the execution model is likely to stop working. Note that the performance of execution with this PR exceeds that with the most popular monkey patches, so many of them will be unnecessary.\n\n### Optional Input Validation\n\nPrior to this PR, only nodes that were connected to outputs exclusively through a string of `\"required\"` inputs were actually validated. If you had custom nodes that were only ever connected to `\"optional\"` inputs, you previously wouldn’t have been seeing that they failed validation.\n\nHere are some of the things that could cause you to fail validation along with recommended solutions:\n\n*   Use of reserved [Additional Parameters](https://docs.comfy.org/custom-nodes/backend/datatypes#additional-parameters) like `min` and `max` on types that aren’t comparable (e.g. dictionaries) in order to configure custom widgets.\n    *   Change the additional parameters used to non-reserved keys like `uiMin` and `uiMax`. _(Recommended Solution)_\n        \n        ```\n        @classmethod\n        def INPUT_TYPES(cls):\n            return {\n                \"required\": {\n                    \"my_size\": (\"VEC2\", {\"uiMin\": 0.0, \"uiMax\": 1.0}),\n                }\n            }\n        ```\n        \n    *   Define a custom [VALIDATE\\_INPUTS](https://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) function with this input so validation of it is skipped. _(Quick Solution)_\n        \n        ```\n        @classmethod\n        def VALIDATE_INPUTS(cls, my_size):\n            return True\n        ```\n        \n*   Use of composite types (e.g. `CUSTOM_A,CUSTOM_B`)\n    *   (When used as output) Define and use a wrapper like `MakeSmartType` [seen here in the PR’s unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R2)\n        \n        ```\n        class MyCustomNode:\n        \n            @classmethod\n            def INPUT_TYPES(cls):\n                return {\n                    \"required\": {\n                        \"input\": (MakeSmartType(\"FOO,BAR\"), {}),\n                    }\n                }\n        \n            RETURN_TYPES = (MakeSmartType(\"FOO,BAR\"),)\n        \n            # ...\n        ```\n        \n    *   (When used as input) Define a custom[VALIDATE\\_INPUTS](https://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) function that takes a `input_types` argument so type validation is skipped.\n        \n        ```\n        @classmethod\n        def VALIDATE_INPUTS(cls, input_types):\n            return True\n        ```\n        \n    *   (Supports both, convenient) Define and use the `@VariantSupport` decorator [seen here in the PR’s unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R15)\n        \n        ```\n        @VariantSupport\n        class MyCustomNode:\n        \n            @classmethod\n            def INPUT_TYPES(cls):\n                return {\n                    \"required\": {\n                        \"input\": (\"FOO,BAR\", {}),\n                    }\n                }\n            \n            RETURN_TYPES = (MakeSmartType(\"FOO,BAR\"),)\n        \n            # ...\n        ```\n        \n*   The use of lists (e.g. `[1, 2, 3]`) as constants in the graph definition (e.g. to represent a const `VEC3` input). This would have required a front-end extension before. Previously, lists of size exactly `2` would have failed anyway — they would have been treated as broken links.\n    *   Wrap the lists in a dictionary like `{ \"value\": [1, 2, 3] }`\n\n### Execution Order\n\nExecution order has always changed depending on which nodes happen to have which IDs, but it may now change depending on which values are cached as well. In general, the execution order should be considered non-deterministic and subject to change (beyond what is enforced by the graph’s structure). Don’t rely on the execution order. _HIC SUNT DRACONES_\n\n## New Functionality\n\n### Validation Changes\n\nA number of features were added to the `VALIDATE_INPUTS` function in order to lessen the impact of the [Optional Input Validation](#optional-input-validation) mentioned above.\n\n*   Default validation will now be skipped for inputs which are received by the `VALIDATE_INPUTS` function.\n*   The `VALIDATE_INPUTS` function can now take `**kwargs` which causes all inputs to be treated as validated by the node creator.\n*   The `VALIDATE_INPUTS` function can take an input named `input_types`. This input will be a dict mapping each input (connected via a link) to the type of the connected output. When this argument exists, type validation for the node’s inputs is skipped.\n\nYou can read more at [VALIDATE\\_INPUTS](https://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs).\n\n### Lazy Evaluation\n\nInputs can be evaluated lazily (i.e. you can wait to see if they are needed before evaluating the attached node and all its ancestors). See [Lazy Evaluation](https://docs.comfy.org/custom-nodes/backend/lazy_evaluation) for more information.\n\n### Node Expansion\n\nAt runtime, nodes can expand into a subgraph of nodes. This is what allows loops to be implemented (via tail-recursion). See [Node Expansion](https://docs.comfy.org/custom-nodes/backend/expansion) for more information."
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/api-key-integration",
  "markdown": "# Integration of API Key to use ComfyUI API nodes\n\nStarting from [PR #8041](https://github.com/comfyanonymous/ComfyUI/pull/8041), ComfyUI supports directly using built-in API nodes through API Keys, without requiring a specific frontend interface (you can even run without a frontend). This means you can create workflows that combine:\n\n*   local OS models\n*   tools from the custom node community\n*   popular paid models\n\nThen run everything together by simply sending the prompt to the Comfy webserver API, letting it handle all the orchestration. This is helpful for users who want to use Comfy as a backend service, via the command line, with their own frontend, etc.\n\n## Prerequisites\n\nUsing API Key to call ComfyUI’s built-in API nodes requires:\n\n*   API Key for the corresponding account\n*   Sufficient account credits\n\nTo use API Key to call ComfyUI’s built-in API nodes, you need to first register an account on [ComfyUI Platform](https://platform.comfy.org/login) and create an API key\n\n[\n\n## Login with API Key\n\nPlease refer to the User Interface section to learn how to login with API Key\n\n\n\n](https://docs.comfy.org/interface/user#logging-in-with-an-api-key)\n\nYou need to ensure your ComfyUI account has sufficient credits to test the corresponding features.\n\n[\n\n## Credits\n\nPlease refer to the Credits section to learn how to purchase credits for your account\n\n\n\n](https://docs.comfy.org/interface/credits)\n\n## Python Example\n\nHere is an example of how to send a workflow containing API nodes to the ComfyUI API using Python code:\n\n```\n\"\"\"Using API nodes when running ComfyUI headless or with alternative frontend\n\nYou can execute a ComfyUI workflow that contains API nodes by including an API key in the prompt.\nThe API key should be added to the `extra_data` field of the payload.\nBelow we show an example of how to do this.\n\nSee more:\n\n- API nodes overview: https://docs.comfy.org/tutorials/api-nodes/overview\n- To generate an API key, login here: https://platform.comfy.org/login\n\"\"\"\n\nimport json\nfrom urllib import request\n\nSERVER_URL = \"http://127.0.0.1:8188\"\n\n# We have a prompt/job (workflow in \"API format\") that contains API nodes.\nworkflow_with_api_nodes = \"\"\"{\n  \"11\": {\n    \"inputs\": {\n      \"prompt\": \"A dreamy, surreal half-body portrait of a young woman meditating. She has a short, straight bob haircut dyed in pastel pink, with soft bangs covering her forehead. Her eyes are gently closed, and her hands are raised in a calm, open-palmed meditative pose, fingers slightly curved, as if levitating or in deep concentration. She wears a colorful dress made of patchwork-like pastel tiles, featuring clouds, stars, and rainbows. Around her float translucent, iridescent soap bubbles reflecting the rainbow hues. The background is a fantastical sky filled with cotton-candy clouds and vivid rainbow waves, giving the entire scene a magical, dreamlike atmosphere. Emphasis on youthful serenity, whimsical ambiance, and vibrant soft lighting.\",\n      \"prompt_upsampling\": false,\n      \"seed\": 589991183902375,\n      \"aspect_ratio\": \"1:1\",\n      \"raw\": false,\n      \"image_prompt_strength\": 0.4000000000000001,\n      \"image_prompt\": [\n        \"14\",\n        0\n      ]\n    },\n    \"class_type\": \"FluxProUltraImageNode\",\n    \"_meta\": {\n      \"title\": \"Flux 1.1 [pro] Ultra Image\"\n    }\n  },\n  \"12\": {\n    \"inputs\": {\n      \"filename_prefix\": \"ComfyUI\",\n      \"images\": [\n        \"11\",\n        0\n      ]\n    },\n    \"class_type\": \"SaveImage\",\n    \"_meta\": {\n      \"title\": \"Save Image\"\n    }\n  },\n  \"14\": {\n    \"inputs\": {\n      \"image\": \"example.png\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"Load Image\"\n    }\n  }\n}\"\"\"\n\n\nprompt = json.loads(workflow_with_api_nodes)\npayload = {\n    \"prompt\": prompt,\n    # Add the `api_key_comfy_org` to the payload.\n    # You can first get the key from the associated user if handling multiple clients.\n    \"extra_data\": {\n        \"api_key_comfy_org\": \"comfyui-87d01e28d*******************************************************\"  # replace with actual key\n    },\n}\ndata = json.dumps(payload).encode(\"utf-8\")\nreq = request.Request(f\"{SERVER_URL}/prompt\", data=data)\nrequest.urlopen(req)\n\n```\n\n*   [API nodes overview](https://docs.comfy.org/tutorials/api-nodes/overview)\n*   [Account management](https://docs.comfy.org/interface/user)\n*   [Credits](https://docs.comfy.org/interface/credits)"
},
{
  "url": "https://docs.comfy.org/comfy-cli/troubleshooting",
  "markdown": "# Getting Started - ComfyUI\n\n### Prerequisites\n\nYou need to have git installed on your system. Install it [here](https://git-scm.com/downloads).\n\nOn this page\n\n*   [Prerequisites](#prerequisites)"
},
{
  "url": "https://docs.comfy.org/comfy-cli/reference",
  "markdown": "# Reference - ComfyUI\n\n## CLI\n\n## Nodes\n\n**Usage**:\n\n```\n$ comfy node [OPTIONS] COMMAND [ARGS]...\n```\n\n**Options**:\n\n*   `--install-completion`: Install completion for the current shell.\n*   `--show-completion`: Show completion for the current shell, to copy it or customize the installation.\n*   `--help`: Show this message and exit.\n\n**Commands**:\n\n*   `deps-in-workflow`\n*   `disable`\n*   `enable`\n*   `fix`\n*   `install`\n*   `install-deps`\n*   `reinstall`\n*   `restore-dependencies`\n*   `restore-snapshot`\n*   `save-snapshot`: Save a snapshot of the current ComfyUI…\n*   `show`\n*   `simple-show`\n*   `uninstall`\n*   `update`\n\n### `deps-in-workflow`\n\n**Usage**:\n\n```\n$ deps-in-workflow [OPTIONS]\n```\n\n**Options**:\n\n*   `--workflow TEXT`: Workflow file (.json/.png) \\[required\\]\n*   `--output TEXT`: Workflow file (.json/.png) \\[required\\]\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `disable`\n\n**Usage**:\n\n```\n$ disable [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: disable custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `enable`\n\n**Usage**:\n\n```\n$ enable [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: enable custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `fix`\n\n**Usage**:\n\n**Arguments**:\n\n*   `ARGS...`: fix dependencies for specified custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `install`\n\n**Usage**:\n\n```\n$ install [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: install custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `install-deps`\n\n**Usage**:\n\n**Options**:\n\n*   `--deps TEXT`: Dependency spec file (.json)\n*   `--workflow TEXT`: Workflow file (.json/.png)\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `reinstall`\n\n**Usage**:\n\n```\n$ reinstall [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: reinstall custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `restore-dependencies`\n\n**Usage**:\n\n```\n$ restore-dependencies [OPTIONS]\n```\n\n**Options**:\n\n*   `--help`: Show this message and exit.\n\n### `restore-snapshot`\n\n**Usage**:\n\n```\n$ restore-snapshot [OPTIONS] PATH\n```\n\n**Arguments**:\n\n*   `PATH`: \\[required\\]\n\n**Options**:\n\n*   `--help`: Show this message and exit.\n\n### `save-snapshot`\n\nSave a snapshot of the current ComfyUI environment **Usage**:\n\n```\n$ save-snapshot [OPTIONS]\n```\n\n**Options**:\n\n*   `--output TEXT`: Specify the output file path. (.json/.yaml)\n*   `--help`: Show this message and exit.\n\n### `show`\n\n**Usage**:\n\n**Arguments**:\n\n*   `ARGS...`: \\[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list\\] \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `simple-show`\n\n**Usage**:\n\n```\n$ simple-show [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: \\[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list\\] \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `uninstall`\n\n**Usage**:\n\n```\n$ uninstall [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: uninstall custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `update`\n\n**Usage**:\n\n```\n$ update [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: update custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n## Models\n\n**Usage**:\n\n```\n$ comfy model [OPTIONS] COMMAND [ARGS]...\n```\n\n**Options**:\n\n*   `--install-completion`: Install completion for the current shell.\n*   `--show-completion`: Show completion for the current shell, to copy it or customize the installation.\n*   `--help`: Show this message and exit.\n\n**Commands**:\n\n*   `download`: Download a model to a specified relative…\n*   `list`: Display a list of all models currently…\n*   `remove`: Remove one or more downloaded models,…\n\n### `download`\n\nDownload a model to a specified relative path if it is not already downloaded. **Usage**:\n\n**Options**:\n\n*   `--url TEXT`: The URL from which to download the model \\[required\\]\n*   `--relative-path TEXT`: The relative path from the current workspace to install the model. \\[default: models/checkpoints\\]\n*   `--help`: Show this message and exit.\n\n### `list`\n\nDisplay a list of all models currently downloaded in a table format. **Usage**:\n\n**Options**:\n\n*   `--relative-path TEXT`: The relative path from the current workspace where the models are stored. \\[default: models/checkpoints\\]\n*   `--help`: Show this message and exit.\n\n### `remove`\n\nRemove one or more downloaded models, either by specifying them directly or through an interactive selection. **Usage**:\n\n**Options**:\n\n*   `--relative-path TEXT`: The relative path from the current workspace where the models are stored. \\[default: models/checkpoints\\]\n*   `--model-names TEXT`: List of model filenames to delete, separated by spaces\n*   `--help`: Show this message and exit."
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/comms_messages",
  "markdown": "# Messages - ComfyUI\n\nDuring execution (or when the state of the queue changes), the `PromptExecutor` sends messages back to the client through the `send_sync` method of `PromptServer`. These messages are received by a socket event listener defined in `api.js` (at time of writing around line 90, or search for `this.socket.addEventListener`), which creates a `CustomEvent` object for any known message type, and dispatches it to any registered listeners. An extension can register to receive events (normally done in the `setup()` function) following the standard Javascript idiom:\n\n```\napi.addEventListener(message_type, messageHandler);\n```\n\nIf the `message_type` is not one of the built in ones, it will be added to the list of known message types automatically. The message `messageHandler` will be called with a `CustomEvent` object, which extends the event raised by the socket to add a `.detail` property, which is a dictionary of the data sent by the server. So usage is generally along the lines of:\n\n```\nfunction messageHandler(event) {\n    if (event.detail.node == aNodeIdThatIsInteresting) {\n        // do something with event.detail.other_things\n    }\n}\n```\n\n### Built in message types\n\nDuring execution (or when the state of the queue changes), the `PromptExecutor` sends the following messages back to the client through the `send_sync` method of `PromptServer`. An extension can register as a listener for any of these.\n\n| event | when | data |\n| --- | --- | --- |\n| `execution_start` | When a prompt is about to run | `prompt_id` |\n| `execution_error` | When an error occurs during execution | `prompt_id`, plus additional information |\n| `execution_interrupted` | When execution is stopped by a node raising `InterruptProcessingException` | `prompt_id`, `node_id`, `node_type` and `executed` (a list of executed nodes) |\n| `execution_cached` | At the start of execution | `prompt_id`, `nodes` (a list of nodes which are being skipped because their cached outputs can be used) |\n| `execution_success` | When all nodes from the prompt have been successfully executed | `prompt_id`, `timestamp` |\n| `executing` | When a new node is about to be executed | `node` (node id or `None` to indicate completion), `prompt_id` |\n| `executed` | When a node returns a ui element | `node` (node id), `prompt_id`, `output` |\n| `progress` | During execution of a node that implements the required hook | `node` (node id), `prompt_id`, `value`, `max` |\n| `status` | When the state of the queue changes | `exec_info`, a dictionary holding `queue_remaining`, the number of entries in the queue |\n\n### Using executed\n\nDespite the name, an `executed` message is not sent whenever a node completes execution (unlike `executing`), but only when the node returns a ui update. To do this, the main function needs to return a dictionary instead of a tuple:\n\n```\n# at the end of my main method\n        return { \"ui\":a_new_dictionary, \"result\": the_tuple_of_output_values }\n```\n\n`a_new_dictionary` will then be sent as the value of `output` in an `executed` message. The `result` key can be omitted if the node has no outputs (see, for instance, the code for `SaveImage` in `nodes.py`)\n\n### Custom message types\n\nAs indicated above, on the client side, a custom message type can be added simply by registering as a listener for a unique message type name.\n\n```\napi.addEventListener(\"my.custom.message\", messageHandler);\n```\n\nOn the server, the code is equally simple:\n\n```\nfrom server import PromptServer\n# then, in your main execution function (normally)\n        PromptServer.instance.send_sync(\"my.custom.message\", a_dictionary)\n```\n\n#### Getting node\\_id\n\nMost of the built-in messages include the current node id in the value of `node`. It’s likely that you will want to do the same. The node\\_id is available on the server side through a hidden input, which is obtained with the `hidden` key in the `INPUT_TYPES` dictionary:\n\n```\n    @classmethod    \n    def INPUT_TYPES(s):\n        return {\"required\" : { }, # whatever your required inputs are \n                \"hidden\": { \"node_id\": \"UNIQUE_ID\" } } # Add the hidden key\n\n    def my_main_function(self, required_inputs, node_id):\n        # do some things\n        PromptServer.instance.send_sync(\"my.custom.message\", {\"node\": node_id, \"other_things\": etc})\n```\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/custom-nodes/tips",
  "markdown": "# Tips - ComfyUI\n\n### Recommended Development Lifecycle"
},
{
  "url": "https://docs.comfy.org/registry/overview",
  "markdown": "# Overview - ComfyUI\n\n## Introduction\n\nThe Registry is a public collection of custom nodes. Developers can publish, version, deprecate, and track metrics related to their custom nodes. ComfyUI users can discover, install, and rate custom nodes from the registry.\n\n## Why use the Registry?\n\nThe Comfy Registry helps the community by standardizing the development of custom nodes:   **Node Versioning:** Developers frequently publish new versions of their custom nodes which often break workflows that rely on them. With registry nodes being [semantically versioned](https://semver.org/), users can now choose to safely upgrade, deprecate, or lock their node versions in place, knowing in advance how their actions will impact their workflows. The workflow JSON will store the version of the node used, so you can always reliably reproduce your workflows.   **Node Security:** The registry will serve as a backend for the [ComfyUI-manager](https://github.com/ltdrdata/ComfyUI-Manager). All nodes will be scanned for malicious behaviour such as custom pip wheels, arbitrary system calls, etc. Nodes that pass these checks will have a verification flag () beside their name on the UI-manager. For a list of security standards, see the [standards](https://docs.comfy.org/registry/standards).   **Search:** Search across all nodes on the Registry to find existing nodes for your workflow.x\n\n## Publishing Nodes\n\nGet started publishing your first node by following the [tutorial](https://docs.comfy.org/registry/publishing).\n\n## Frequently Asked Questions"
},
{
  "url": "https://docs.comfy.org/custom-nodes/workflow_templates",
  "markdown": "# Workflow templates - ComfyUI\n\nIf you have example workflow files associated with your custom nodes then ComfyUI can show these to the user in the template browser (`Workflow`/`Browse Templates` menu). Workflow templates are a great way to support people getting started with your nodes. All you have to do as a node developer is to create an `example_workflows` folder and place the `json` files there. Optionally you can place `jpg` files with the same name to be shown as the template thumbnail. Under the hood ComfyUI statically serves these files along with an endpoint (`/api/workflow_templates`) that returns the collection of workflow templates.\n\n## Example\n\nUnder `ComfyUI-MyCustomNodeModule/example_workflows/` directory:\n\n*   `My_example_workflow_1.json`\n*   `My_example_workflow_1.jpg`\n*   `My_example_workflow_2.json`\n\nIn this example ComfyUI’s template browser shows a category called `ComfyUI-MyCustomNodeModule` with two items, one of which has a thumbnail."
},
{
  "url": "https://docs.comfy.org/custom-nodes/help_page",
  "markdown": "# Help Page - ComfyUI\n\n## Node Documentation with Markdown\n\nCustom nodes can include rich markdown documentation that will be displayed in the UI instead of the generic node description. This provides users with detailed information about your node’s functionality, parameters, and usage examples.\n\n## Setup\n\nTo add documentation for your nodes:\n\n1.  Create a `docs` folder inside your `WEB_DIRECTORY`\n2.  Add markdown files named after your nodes (the names of your nodes are the dictionary keys in the `NODE_CLASS_MAPPINGS` dictionary used to register the nodes):\n    *   `WEB_DIRECTORY/docs/NodeName.md` - Default documentation\n    *   `WEB_DIRECTORY/docs/NodeName/en.md` - English documentation\n    *   `WEB_DIRECTORY/docs/NodeName/zh.md` - Chinese documentation\n    *   Add other locales as needed (e.g., `fr.md`, `de.md`, etc.)\n\nThe system will automatically load the appropriate documentation based on the user’s locale, falling back to `NodeName.md` if a localized version is not available.\n\n## Supported Markdown Features\n\n*   Standard markdown syntax (headings, lists, code blocks, etc.)\n*   Images using markdown syntax: `![alt text](image.png)`\n*   HTML media elements with specific attributes:\n    *   `<video>` and `<source>` tags\n    *   Allowed attributes: `controls`, `autoplay`, `loop`, `muted`, `preload`, `poster`\n\n## Example Structure\n\n```\nmy-custom-node/\n├── __init__.py\n├── web/              # WEB_DIRECTORY\n│   ├── js/\n│   │   └── my-node.js\n│   └── docs/\n│       ├── MyNode.md           # Fallback documentation\n│       └── MyNode/\n│           ├── en.md           # English version\n│           └── zh.md           # Chinese version\n```\n\n## Example Markdown File\n\n```\n# My Custom Node\n\nThis node processes images using advanced algorithms.\n\n## Parameters\n\n- **image**: Input image to process\n- **strength**: Processing strength (0.0 - 1.0)\n\n## Usage\n\n![example usage](example.png)\n\n<video controls loop muted>\n  <source src=\"demo.mp4\" type=\"video/mp4\">\n</video>\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/walkthrough",
  "markdown": "# Getting Started - ComfyUI\n\nThis page will take you step-by-step through the process of creating a custom node. Our example will take a batch of images, and return one of the images. Initially, the node will return the image which is, on average, the lightest in color; we’ll then extend it to have a range of selection criteria, and then finally add some client side code. This page assumes very little knowledge of Python or Javascript. After this walkthrough, dive into the details of [backend code](https://docs.comfy.org/custom-nodes/backend/server_overview), and [frontend code](https://docs.comfy.org/custom-nodes/backend/server_overview).\n\n## Write a basic node\n\n### Prerequisites\n\n*   A working ComfyUI [installation](https://docs.comfy.org/installation/manual_install). For development, we recommend installing ComfyUI manually.\n*   A working comfy-cli [installation](https://docs.comfy.org/comfy-cli/getting-started).\n\n### Setting up\n\n```\ncd ComfyUI/custom_nodes\ncomfy node scaffold\n```\n\nAfter answering a few questions, you’ll have a new directory set up.\n\n```\n ~  % comfy node scaffold\nYou've downloaded .cookiecutters/cookiecutter-comfy-extension before. Is it okay to delete and re-download it? [y/n] (y): y\n  [1/9] full_name (): Comfy\n  [2/9] email (you@gmail.com): me@comfy.org\n  [3/9] github_username (your_github_username): comfy\n  [4/9] project_name (My Custom Nodepack): FirstComfyNode\n  [5/9] project_slug (firstcomfynode): \n  [6/9] project_short_description (A collection of custom nodes for ComfyUI): \n  [7/9] version (0.0.1): \n  [8/9] Select open_source_license\n    1 - GNU General Public License v3\n    2 - MIT license\n    3 - BSD license\n    4 - ISC license\n    5 - Apache Software License 2.0\n    6 - Not open source\n    Choose from [1/2/3/4/5/6] (1): 1\n  [9/9] include_web_directory_for_custom_javascript [y/n] (n): y\nInitialized empty Git repository in firstcomfynode/.git/\n✓ Custom node project created successfully!\n```\n\n### Defining the node\n\nAdd the following code to the end of `src/nodes.py`:\n\n```\nclass ImageSelector:\n    CATEGORY = \"example\"\n    @classmethod    \n    def INPUT_TYPES(s):\n        return { \"required\":  { \"images\": (\"IMAGE\",), } }\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"choose_image\"\n```\n\nA custom node is defined using a Python class, which must include these four things: `CATEGORY`, which specifies where in the add new node menu the custom node will be located, `INPUT_TYPES`, which is a class method defining what inputs the node will take (see [later](https://docs.comfy.org/custom-nodes/backend/server_overview#input-types) for details of the dictionary returned), `RETURN_TYPES`, which defines what outputs the node will produce, and `FUNCTION`, the name of the function that will be called when the node is executed.\n\n### The main function\n\nThe main function, `choose_image`, receives named arguments as defined in `INPUT_TYPES`, and returns a `tuple` as defined in `RETURN_TYPES`. Since we’re dealing with images, which are internally stored as `torch.Tensor`,\n\nThen add the function to your class. The datatype for image is `torch.Tensor` with shape `[B,H,W,C]`, where `B` is the batch size and `C` is the number of channels - 3, for RGB. If we iterate over such a tensor, we will get a series of `B` tensors of shape `[H,W,C]`. The `.flatten()` method turns this into a one dimensional tensor, of length `H*W*C`, `torch.mean()` takes the mean, and `.item()` turns a single value tensor into a Python float.\n\n```\ndef choose_image(self, images):\n    brightness = list(torch.mean(image.flatten()).item() for image in images)\n    brightest = brightness.index(max(brightness))\n    result = images[brightest].unsqueeze(0)\n    return (result,)\n```\n\nNotes on those last two lines:\n\n*   `images[brightest]` will return a Tensor of shape `[H,W,C]`. `unsqueeze` is used to insert a (length 1) dimension at, in this case, dimension zero, to give us `[B,H,W,C]` with `B=1`: a single image.\n*   in `return (result,)`, the trailing comma is essential to ensure you return a tuple.\n\n### Register the node\n\nTo make Comfy recognize the new node, it must be available at the package level. Modify the `NODE_CLASS_MAPPINGS` variable at the end of `src/nodes.py`. You must restart ComfyUI to see any changes.\n\n```\n\nNODE_CLASS_MAPPINGS = {\n    \"Example\" : Example,\n    \"Image Selector\" : ImageSelector,\n}\n\n# Optionally, you can rename the node in the `NODE_DISPLAY_NAME_MAPPINGS` dictionary.\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"Example\": \"Example Node\",\n    \"Image Selector\": \"Image Selector\",\n}\n```\n\n## Add some options\n\nThat node is maybe a bit boring, so we might add some options; a widget that allows you to choose the brightest image, or the reddest, bluest, or greenest. Edit your `INPUT_TYPES` to look like:\n\n```\n@classmethod    \ndef INPUT_TYPES(s):\n    return { \"required\":  { \"images\": (\"IMAGE\",), \n                            \"mode\": ([\"brightest\", \"reddest\", \"greenest\", \"bluest\"],)} }\n```\n\nThen update the main function. We’ll use a fairly naive definition of ‘reddest’ as being the average `R` value of the pixels divided by the average of all three colors. So:\n\n```\ndef choose_image(self, images, mode):\n    batch_size = images.shape[0]\n    brightness = list(torch.mean(image.flatten()).item() for image in images)\n    if (mode==\"brightest\"):\n        scores = brightness\n    else:\n        channel = 0 if mode==\"reddest\" else (1 if mode==\"greenest\" else 2)\n        absolute = list(torch.mean(image[:,:,channel].flatten()).item() for image in images)\n        scores = list( absolute[i]/(brightness[i]+1e-8) for i in range(batch_size) )\n    best = scores.index(max(scores))\n    result = images[best].unsqueeze(0)\n    return (result,)\n```\n\n## Tweak the UI\n\nMaybe we’d like a bit of visual feedback, so let’s send a little text message to be displayed.\n\n### Send a message from server\n\nThis requires two lines to be added to the Python code:\n\n```\nfrom server import PromptServer\n```\n\nand, at the end of the `choose_image` method, add a line to send a message to the front end (`send_sync` takes a message type, which should be unique, and a dictionary)\n\n```\nPromptServer.instance.send_sync(\"example.imageselector.textmessage\", {\"message\":f\"Picked image {best+1}\"})\nreturn (result,)\n```\n\n### Write a client extension\n\nTo add some Javascript to the client, create a subdirectory, `web/js` in your custom node directory, and modify the end of `__init__.py` to tell Comfy about it by exporting `WEB_DIRECTORY`:\n\n```\nWEB_DIRECTORY = \"./web/js\"\n__all__ = ['NODE_CLASS_MAPPINGS', 'WEB_DIRECTORY']\n```\n\nThe client extension is saved as a `.js` file in the `web/js` subdirectory, so create `image_selector/web/js/imageSelector.js` with the code below. (For more, see [client side coding](https://docs.comfy.org/custom-nodes/js/javascript_overview)).\n\n```\napp.registerExtension({\n\tname: \"example.imageselector\",\n    async setup() {\n        function messageHandler(event) { alert(event.detail.message); }\n        app.api.addEventListener(\"example.imageselector.textmessage\", messageHandler);\n    },\n})\n```\n\nAll we’ve done is register an extension and add a listener for the message type we are sending in the `setup()` method. This reads the dictionary we sent (which is stored in `event.detail`). Stop the Comfy server, start it again, reload the webpage, and run your workflow.\n\n### The complete example\n\nThe complete example is available [here](https://gist.github.com/robinjhuang/fbf54b7715091c7b478724fc4dffbd03). You can download the example workflow [JSON file](https://github.com/Comfy-Org/docs/blob/main/public/workflow.json) or view it below:\n\n![Image Selector Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/firstnodeworkflow.png)"
},
{
  "url": "https://docs.comfy.org/registry/cicd",
  "markdown": "# Custom Node CI/CD - ComfyUI\n\n## Introduction\n\nWhen making changes to custom nodes, it’s not uncommon to break things in Comfy or other custom nodes. It is often unrealistic to test on every operating system and different configurations of Pytorch.\n\n### Run Comfy Workflows using Github Actions\n\n[Comfy-Action](https://github.com/Comfy-Org/comfy-action) allows you to run a Comfy workflow.json file on Github Actions. It supports downloading models, custom nodes, and runs on Linux/Mac/Windows.\n\n### Results\n\nOutput files are uploaded to the [CI/CD Dashboard](https://comfyci.org/) and can be viewed as a last step before committing new changes or publishing new versions of the custom node. ![ComfyCI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyci.png)"
},
{
  "url": "https://docs.comfy.org/registry/publishing",
  "markdown": "# Publishing Nodes - ComfyUI\n\n## Set up a Registry Account\n\nFollow the steps below to set up a registry account and publish your first node.\n\n### Watch a Tutorial\n\n### Create a Publisher\n\nA publisher is an identity that can publish custom nodes to the registry. Every custom node needs to include a publisher identifier in the pyproject.toml file. Go to [Comfy Registry](https://registry.comfy.org/), and create a publisher account. Your publisher id is globally unique, and cannot be changed later because it is used in the URL of your custom node. Your publisher id is found after the `@` symbol on your profile page. ![Hero Dark](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/publisherid.png)\n\n### Create an API Key for publishing\n\nGo [here](https://registry.comfy.org/nodes) and click on the publisher you want to create an API key for. This will be used to publish a custom node via the CLI. ![Create key for Specific Publisher](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-1.png) Name the API key and save it somewhere safe. If you lose it, you’ll have to create a new key. ![Create API Key](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-2.png)\n\n### Add Metadata\n\nThis command will generate the following metadata:\n\n```\n# pyproject.toml\n[project]\nname = \"\" # Unique identifier for your node. Immutable after creation.\ndescription = \"\"\nversion = \"1.0.0\" # Custom Node version. Must be semantically versioned.\nlicense = { file = \"LICENSE.txt\" }\ndependencies  = [] # Filled in from requirements.txt\n\n[project.urls]\nRepository = \"https://github.com/...\"\n\n[tool.comfy]\nPublisherId = \"\" # TODO (fill in Publisher ID from Comfy Registry Website).\nDisplayName = \"\" # Display name for the Custom Node. Can be changed later.\nIcon = \"https://example.com/icon.png\" # SVG, PNG, JPG or GIF (MAX. 800x400px)\n```\n\nAdd this file to your repository. Check the [specifications](https://docs.comfy.org/registry/specifications) for more information on the pyproject.toml file.\n\n## Publish to the Registry\n\n### Option 1: Comfy CLI\n\nRun the command below to manually publish your node to the registry.\n\nYou’ll be prompted for the API key.\n\n```\nAPI Key for publisher '<publisher id>': ****************************************************\n\n...Version 1.0.0 Published. \nSee it here: https://registry.comfy.org/publisherId/your-node\n```\n\n### Option 2: Github Actions\n\nAutomatically publish your node through github actions."
},
{
  "url": "https://docs.comfy.org/registry/standards",
  "markdown": "# Standards - ComfyUI\n\n## Base Standards\n\nCustom nodes must provide valuable functionality to the ComfyUI community Avoid:\n\n*   Excessive self-promotion\n*   Impersonation or misleading behavior\n*   Malicious behavior\n*   Self-promotion is permitted only within your designated settings menu section\n*   Top and side menus should contain only useful functionality\n\n### 2\\. Node Compatibility\n\nDo not interfere with other custom nodes’ operations (installation, updates, removal)\n\n*   For dependencies on other custom nodes:\n    *   Display clear warnings when dependent functionality is used\n    *   Provide example workflows demonstrating required nodes\n\n### 3\\. Legal Compliance\n\nMust comply with all applicable laws and regulations\n\n### 5\\. Quality Requirements\n\nNodes must be fully functional, well documented, and actively maintained.\n\n### 6\\. Fork Guidelines\n\nForked nodes must:\n\n*   Have clearly distinct names from original\n*   Provide significant differences in functionality or code\n\nBelow are standards that must be met to publish custom nodes to the registry.\n\n## Security Standards\n\nCustom nodes should be secure. We will start working with custom nodes that violate these standards to be rewritten. If there is some major functionality that should be exposed by core, please request it in the [rfcs repo](https://github.com/comfy-org/rfcs).\n\n### eval/exec Calls\n\n#### Policy\n\nThe use of `eval` and `exec` functions is prohibited in custom nodes due to security concerns.\n\n#### Reasoning\n\nThese functions can enable arbitrary code execution, creating potential Remote Code Execution (RCE) vulnerabilities when processing user inputs. Workflows containing nodes that pass user inputs into `eval` or `exec` could be exploited for various cyberattacks, including:\n\n*   Keylogging\n*   Ransomware\n*   Other malicious code execution\n\n### subprocess for pip install\n\n#### Policy\n\nRuntime package installation through subprocess calls is not permitted.\n\n#### Reasoning\n\n*   First item ComfyUI manager will ship with ComfyUI and lets the user install dependencies\n*   Centralized dependency management improves security and user experience\n*   Helps prevent potential supply chain attacks\n*   Eliminates need for multiple ComfyUI reloads\n\n### Code Obfuscation\n\n#### Policy\n\nCode obfuscation is prohibited in custom nodes.\n\n#### Reasoning\n\nObfuscated code:\n\n*   Impossible to review and likely to be malicious"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/image-to-image",
  "markdown": "# ComfyUI Image to Image Workflow\n\n## What is Image to Image\n\nImage to Image is a workflow in ComfyUI that allows users to input an image and generate a new image based on it. Image to Image can be used in scenarios such as:\n\n*   Converting original image styles, like transforming realistic photos into artistic styles\n*   Converting line art into realistic images\n*   Image restoration\n*   Colorizing old photos\n*   … and other scenarios\n\nTo explain it with an analogy: It’s like asking an artist to create a specific piece based on your reference image. If you carefully compare this tutorial with the [Text to Image](https://docs.comfy.org/tutorials/basic/text-to-image) tutorial, you’ll notice that the Image to Image process is very similar to Text to Image, just with an additional input reference image as a condition. In Text to Image, we let the artist (image model) create freely based on our prompts, while in Image to Image, we let the artist create based on both our reference image and prompts.\n\n## ComfyUI Image to Image Workflow Example Guide\n\n### Model Installation\n\nDownload the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder.\n\n### Image to Image Workflow and Input Image\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![Image to Image Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image_to_image.png) \n\nDownload the image below and we will use it as the input image: ![Example Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/input.jpeg) \n\n### Complete the Workflow Step by Step\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![ComfyUI Image to Image Workflow - Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image-to-image-02-guide.jpg)\n\n1.  Ensure `Load Checkpoint` loads **v1-5-pruned-emaonly-fp16.safetensors**\n2.  Upload the input image to the `Load Image` node\n3.  Click `Queue` or press `Ctrl/Cmd + Enter` to generate\n\n## Key Points of Image to Image Workflow\n\nThe key to the Image to Image workflow lies in the `denoise` parameter in the `KSampler` node, which should be **less than 1** If you’ve adjusted the `denoise` parameter and generated images, you’ll notice:\n\n*   The smaller the `denoise` value, the smaller the difference between the generated image and the reference image\n*   The larger the `denoise` value, the larger the difference between the generated image and the reference image\n\nThis is because `denoise` determines the strength of noise added to the latent space image after converting the reference image. If `denoise` is 1, the latent space image will become completely random noise, making it the same as the latent space generated by the `empty latent image` node, losing all characteristics of the reference image. For the corresponding principles, please refer to the principle explanation in the [Text to Image](https://docs.comfy.org/tutorials/basic/text-to-image) tutorial.\n\n## Try It Yourself\n\n1.  Try modifying the `denoise` parameter in the **KSampler** node, gradually changing it from 1 to 0, and observe the changes in the generated images\n2.  Replace with your own prompts and reference images to generate your own image effects"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/inpaint",
  "markdown": "# ComfyUI Inpainting Workflow - ComfyUI\n\nThis article will introduce the concept of inpainting in AI image generation and guide you through creating an inpainting workflow in ComfyUI. We’ll cover:\n\n*   Using inpainting workflows to modify images\n*   Using the ComfyUI mask editor to draw masks\n*   `VAE Encoder (for Inpainting)` node\n\n## About Inpainting\n\nIn AI image generation, we often encounter situations where we’re satisfied with the overall image but there are elements we don’t want or that contain errors. Simply regenerating might produce a completely different image, so using inpainting to fix specific parts becomes very useful. It’s like having an **artist (AI model)** paint a picture, but we’re still not satisfied with the specific details. We need to tell the artist **which areas to adjust (mask)**, and then let them **repaint (inpaint)** according to our requirements. Common inpainting scenarios include:\n\n*   **Defect Repair:** Removing unwanted objects, fixing incorrect AI-generated body parts, etc.\n*   **Detail Optimization:** Precisely adjusting local elements (like modifying clothing textures, adjusting facial expressions)\n*   And other scenarios\n\n### Model and Resource Preparation\n\n#### 1\\. Model Installation\n\nDownload the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder:\n\n#### 2\\. Inpainting Asset\n\nPlease download the following image which we’ll use as input: ![ComfyUI Inpainting Input Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/input.png)\n\n#### 3\\. Inpainting Workflow\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![ComfyUI Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)\n\n### ComfyUI Inpainting Workflow Example Explanation\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![ComfyUI Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_workflow.png)\n\n1.  Ensure `Load Checkpoint` loads `512-inpainting-ema.safetensors`\n2.  Upload the input image to the `Load Image` node\n3.  Click `Queue` or use `Ctrl + Enter` to generate\n\n![Inpainting Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png) For comparison, here’s the result using the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model: ![SD1.5 Inpainting Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png) You will find that the results generated by the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) model have better inpainting effects and more natural transitions. This is because this model is specifically designed for inpainting, which helps us better control the generation area, resulting in improved inpainting effects. Do you remember the analogy we’ve been using? Different models are like artists with varying abilities, but each artist has their own limits. Choosing the right model can help you achieve better generation results. You can try these approaches to achieve better results:\n\n1.  Modify positive and negative prompts with more specific descriptions\n2.  Try multiple runs using different seeds in the `KSampler` for different generation results\n3.  After learning about the mask editor in this tutorial, you can re-inpaint the generated results to achieve satisfactory outcomes.\n\nNext, we’ll learn about using the **Mask Editor**. While our input image already includes an `alpha` transparency channel (the area we want to edit), so manual mask drawing isn’t necessary, you’ll often use the Mask Editor to create masks in practical applications.\n\n### Using the Mask Editor\n\nFirst right-click the `Save Image` node and select `Copy(Clipspace)`: ![Copy Image to Clipboard](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png) Then right-click the **Load Image** node and select `Paste(Clipspace)`: ![Paste Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png) Right-click the **Load Image** node again and select `Open in MaskEditor`: ![Open Mask Editor](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg) ![Mask Editor Demo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint-maskeditor.gif)\n\n1.  Adjust brush parameters on the right panel\n2.  Use eraser to correct mistakes\n3.  Click `Save` when finished\n\nThe drawn content will be used as a Mask input to the VAE Encoder (for Inpainting) node for encoding Then try adjusting your prompts and generating again until you achieve satisfactory results.\n\n## VAE Encoder (for Inpainting) Node\n\nComparing this workflow with [Text-to-Image](https://docs.comfy.org/tutorials/basic/text-to-image) and [Image-to-Image](https://docs.comfy.org/tutorials/basic/image-to-image), you’ll notice the main differences are in the VAE section’s conditional inputs. In this workflow, we use the **VAE Encoder (for Inpainting)** node, specifically designed for inpainting to help us better control the generation area and achieve better results. ![VAE Encoder (for Inpainting) Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg) **Input Types**\n\n| Parameter Name | Function |\n| --- | --- |\n| `pixels` | Input image to be encoded into latent space. |\n| `vae` | VAE model used to encode the image from pixel space to latent space. |\n| `mask` | Image mask specifying which areas need modification. |\n| `grow_mask_by` | Pixel value to expand the original mask outward, ensuring a transition area around the mask to avoid hard edges between inpainted and original areas. |\n\n**Output Types**\n\n| Parameter Name | Function |\n| --- | --- |\n| `latent` | Image encoded into latent space by the VAE. |"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/outpaint",
  "markdown": "# ComfyUI Outpainting Workflow Example - ComfyUI\n\nThis guide will introduce you to the concept of outpainting in AI image generation and how to create an outpainting workflow in ComfyUI. We will cover:\n\n*   Using outpainting workflow to extend an image\n*   Understanding and using outpainting-related nodes in ComfyUI\n*   Mastering the basic outpainting process\n\n## About Outpainting\n\nIn AI image generation, we often encounter situations where an existing image has good composition but the canvas area is too small, requiring us to extend the canvas to get a larger scene. This is where outpainting comes in. Basically, it requires similar content to [Inpainting](https://docs.comfy.org/tutorials/basic/inpaint), but we use different nodes to **build the mask**. Outpainting applications include:\n\n*   **Scene Extension:** Expand the scene range of the original image to show a more complete environment\n*   **Composition Adjustment:** Optimize the overall composition by extending the canvas\n*   **Content Addition:** Add more related scene elements to the original image\n\n### Preparation\n\n#### 1\\. Model Installation\n\nDownload the following model file and save it to `ComfyUI/models/checkpoints` directory:\n\n*   [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)\n\n#### 2\\. Input Image\n\nPrepare an image you want to extend. In this example, we will use the following image: ![ComfyUI Outpainting Input Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/input.png)\n\n#### 3\\. Outpainting Workflow\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![ComfyUI Outpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpaint.png)\n\n### Outpainting Workflow Usage Explanation\n\n![ComfyUI Outpainting Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpainting_workflow.jpg) The key steps of the outpainting workflow are as follows:\n\n1.  Load the locally installed model file in the `Load Checkpoint` node\n2.  Click the `Upload` button in the `Load Image` node to upload your image\n3.  Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute the image generation\n\nIn this workflow, we mainly use the `Pad Image for outpainting` node to control the direction and range of image extension. This is actually an [Inpaint](https://docs.comfy.org/tutorials/basic/inpaint) workflow, but we use different nodes to build the mask.\n\n### Pad Image for outpainting Node\n\n![Pad Image for outpainting Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/pad_image_for_outpainting.jpg) This node accepts an input image and outputs an extended image with a corresponding mask, where the mask is built based on the node parameters.\n\n#### Input Parameters\n\n| Parameter Name | Function |\n| --- | --- |\n| `image` | Input image |\n| `left` | Left padding amount |\n| `top` | Top padding amount |\n| `right` | Right padding amount |\n| `bottom` | Bottom padding amount |\n| `feathering` | Controls the smoothness of the transition between the original image and the added padding, higher values create smoother transitions |\n\n#### Output Parameters\n\n| Parameter Name | Function |\n| --- | --- |\n| `image` | Output `image` represents the padded image |\n| `mask` | Output `mask` indicates the original image area and the added padding area |\n\n#### Node Output Content\n\nAfter processing by the `Pad Image for outpainting` node, the output image and mask preview are as follows: ![Pad Image for outpainting Node Results](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg) You can see the corresponding output results:\n\n*   The `Image` output is the extended image\n*   The `Mask` output is the mask marking the extension areas"
},
{
  "url": "https://docs.comfy.org/index",
  "markdown": "# ComfyUI Official Documentation - ComfyUI\n\nGet Started\n\nComfyUI Official Documentation"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/lora",
  "markdown": "# ComfyUI LoRA Example - ComfyUI\n\n**LoRA (Low-Rank Adaptation)** is an efficient technique for fine-tuning large generative models like Stable Diffusion. It introduces trainable low-rank matrices to the pre-trained model, adjusting only a portion of parameters rather than retraining the entire model, thus achieving optimization for specific tasks at a lower computational cost. Compared to base models like SD1.5, LoRA models are smaller and easier to train. ![LoRA Model vs Base Model Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/compare.png) The image above compares generation with the same parameters using [dreamshaper\\_8](https://civitai.com/models/4384?modelVersionId=128713) directly versus using the [blindbox\\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA model. As you can see, by using a LoRA model, we can generate images in different styles without adjusting the base model. We will demonstrate how to use a LoRA model. All LoRA variants: Lycoris, loha, lokr, locon, etc… are used in the same way. In this example, we will learn how to load and use a LoRA model in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), covering the following topics:\n\n1.  Installing a LoRA model\n2.  Generating images using a LoRA model\n3.  A simple introduction to the `Load LoRA` node\n\n## Required Model Installation\n\nDownload the [dreamshaper\\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder. Download the [blindbox\\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.\n\n## LoRA Workflow File\n\nDownload the image below and **drag it into ComfyUI** to load the workflow. ![ComfyUI Workflow - LoRA](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/lora.png) \n\n## Complete the Workflow Step by Step\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![ComfyUI Workflow - LoRA Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/flow_diagram.png)\n\n1.  Ensure `Load Checkpoint` loads `dreamshaper_8.safetensors`\n2.  Ensure `Load LoRA` loads `blindbox_V1Mix.safetensors`\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n\n## Load LoRA Node Introduction\n\n![Load LoRA Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_lora.jpg) Models in the `ComfyUI\\models\\loras` folder will be detected by ComfyUI and can be loaded using this node.\n\n### Input Types\n\n| Parameter Name | Function |\n| --- | --- |\n| `model` | Connect to the base model |\n| `clip` | Connect to the CLIP model |\n| `lora_name` | Select the LoRA model to load and use |\n| `strength_model` | Affects how strongly the LoRA influences the model weights; higher values make the LoRA style stronger |\n| `strength_clip` | Affects how strongly the LoRA influences the CLIP text embeddings |\n\n### Output Types\n\n| Parameter Name | Function |\n| --- | --- |\n| `model` | Outputs the model with LoRA adjustments applied |\n| `clip` | Outputs the CLIP model with LoRA adjustments applied |\n\nThis node supports chain connections, allowing multiple `Load LoRA` nodes to be linked in series to apply multiple LoRA models. For more details, please refer to [ComfyUI Multiple LoRAs Example](https://docs.comfy.org/tutorials/basic/multiple-loras) ![LoRA Node Chain Connection](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)\n\n## Try It Yourself\n\n1.  Try modifying the prompt or adjusting different parameters of the `Load LoRA` node, such as `strength_model`, to observe changes in the generated images and become familiar with the `Load LoRA` node.\n2.  Visit [CivitAI](https://civitai.com/models) to download other kinds of LoRA models and try using them."
},
{
  "url": "https://docs.comfy.org/tutorials/basic/multiple-loras",
  "markdown": "# ComfyUI Multiple LoRAs Example - ComfyUI\n\nIn our [ComfyUI LoRA Example](https://docs.comfy.org/tutorials/basic/lora), we introduced how to load and use a single LoRA model, mentioning the node’s chain connection capability. ![LoRA Node Chaining](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png) This tutorial demonstrates chaining multiple `Load LoRA` nodes to apply two LoRA models simultaneously: [blindbox\\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) and [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856). The comparison below shows individual effects of these LoRAs using identical parameters: ![Single LoRA Effects Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/compare.png) By chaining multiple LoRA models, we achieve a blended style in the final output: ![Multi-LoRA Application Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)\n\n## Model Installation\n\nDownload the [dreamshaper\\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder. Download the [blindbox\\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder. Download the [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.\n\n## Multi-LoRA Workflow\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![ComfyUI Workflow - Multiple LoRAs](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png) \n\n## Complete the Workflow Step by Step\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![Multi-LoRA Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/flow_diagram.png)\n\n1.  Ensure `Load Checkpoint` loads **dreamshaper\\_8.safetensors**\n2.  Ensure first `Load LoRA` loads **blindbox\\_V1Mix.safetensors**\n3.  Ensure second `Load LoRA` loads **MoXinV1.safetensors**\n4.  Click `Queue` or press `Ctrl/Cmd + Enter` to generate\n\n## Try It Yourself\n\n1.  Adjust `strength_model` values in both `Load LoRA` nodes to control each LoRA’s influence\n2.  Explore [CivitAI](https://civitai.com/models) for additional LoRAs and create custom combinations"
},
{
  "url": "https://docs.comfy.org/interface/settings/server-config",
  "markdown": "# Server Config - ComfyUI\n\n## Network\n\n### Host: The IP address to listen on\n\n*   **Function**: Sets the IP address the server binds to. Default `127.0.0.1` means only local access is allowed. If you need LAN access, you can set it to `0.0.0.0`\n\n### Port: The port to listen on\n\n**Function**: The port number the server listens on. Desktop version defaults to port 8000, Web version typically uses port 8188\n\n### TLS Key File: Path to TLS key file for HTTPS\n\n**Function**: The private key file path required for HTTPS encryption, used to establish secure connections\n\n### TLS Certificate File: Path to TLS certificate file for HTTPS\n\n**Function**: The certificate file path required for HTTPS encryption, used in conjunction with the private key\n\n### Enable CORS header: Use ”\\*” for all origins or specify domain\n\n**Function**: Cross-Origin Resource Sharing settings, allowing web browsers to access the server from different domains\n\n### Maximum upload size (MB)\n\n**Function**: Limits the maximum size of single file uploads, in MB, default 100MB. Affects upload limits for images, models and other files\n\n## CUDA\n\n### CUDA device index to use\n\n**Function**: Specifies which NVIDIA graphics card to use. 0 represents the first graphics card, 1 represents the second, and so on. Important for multi-GPU systems\n\n### Use CUDA malloc for memory allocation\n\n**Function**: Controls whether to use CUDA’s memory allocator. Can improve memory management efficiency in certain situations\n\n## Inference\n\n### Global floating point precision\n\n**Function**: Sets the numerical precision for model calculations. FP16 saves VRAM but may affect quality, FP32 is more precise but uses more VRAM\n\n### UNET precision\n\n**Options**:\n\n*   `auto`: Automatically selects the most suitable precision\n*   `fp64`: 64-bit floating point precision, highest precision but largest VRAM usage\n*   `fp32`: 32-bit floating point precision, standard precision\n*   `fp16`: 16-bit floating point precision, can save VRAM\n*   `bf16`: 16-bit brain floating point precision, between fp16 and fp32\n*   `fp8_e4m3fn`: 8-bit floating point precision (e4m3), minimal VRAM usage\n*   `fp8_e5m2`: 8-bit floating point precision (e5m2), minimal VRAM usage\n\n**Function**: Specifically controls the computational precision of the UNET core component of diffusion models. Higher precision can provide better image generation quality but uses more VRAM. Lower precision can significantly save VRAM but may affect the quality of generated results.\n\n### VAE precision\n\n**Options and Recommendations**:\n\n*   `auto`: Automatically selects the most suitable precision, recommended for users with 8-12GB VRAM\n*   `fp16`: 16-bit floating point precision, recommended for users with 6GB or less VRAM, can save VRAM but may affect quality\n*   `fp32`: 32-bit floating point precision, recommended for users with 16GB or more VRAM who pursue the best quality\n*   `bf16`: 16-bit brain floating point precision, recommended for newer graphics cards that support this format, can achieve better performance balance\n\n**Function**: Controls the computational precision of the Variational Autoencoder (VAE), affecting the quality and speed of image encoding/decoding. Higher precision can provide better image reconstruction quality but uses more VRAM. Lower precision can save VRAM but may affect image detail restoration.\n\n### Run VAE on CPU\n\n**Function**: Forces VAE to run on CPU, can save VRAM but will reduce processing speed\n\n### Text Encoder precision\n\n**Options**:\n\n*   `auto`: Automatically selects the most suitable precision\n*   `fp8_e4m3fn`: 8-bit floating point precision (e4m3), minimal VRAM usage\n*   `fp8_e5m2`: 8-bit floating point precision (e5m2), minimal VRAM usage\n*   `fp16`: 16-bit floating point precision, can save VRAM\n*   `fp32`: 32-bit floating point precision, standard precision\n\n**Function**: Controls the computational precision of the text prompt encoder, affecting the accuracy of text understanding and VRAM usage. Higher precision can provide more accurate text understanding but uses more VRAM. Lower precision can save VRAM but may affect prompt parsing effectiveness.\n\n## Memory\n\n### Force channels-last memory format\n\n**Function**: Changes the data arrangement in memory, may improve performance on certain hardware\n\n### DirectML device index\n\n**Function**: Specifies the device when using DirectML acceleration on Windows, mainly for AMD graphics cards\n\n### Disable IPEX optimization\n\n**Function**: Disables Intel CPU optimization, mainly affects Intel processor performance\n\n### VRAM management mode\n\n**Options**:\n\n*   `auto`: Automatically manages VRAM, allocating VRAM based on model size and requirements\n*   `lowvram`: Low VRAM mode, uses minimal VRAM, may affect generation quality\n*   `normalvram`: Standard VRAM mode, balances VRAM usage and performance\n*   `highvram`: High VRAM mode, uses more VRAM for better performance\n*   `novram`: No VRAM usage, runs entirely on system memory\n*   `cpu`: CPU-only mode, doesn’t use graphics card\n\n**Function**: Controls VRAM usage strategy, such as automatic management, low VRAM mode, etc.\n\n### Reserved VRAM (GB)\n\n**Function**: Amount of VRAM reserved for the operating system and other programs, prevents system freezing\n\n### Disable smart memory management\n\n**Function**: Disables automatic memory optimization, forces models to move to system memory to free VRAM\n\n## Preview\n\n### Method used for latent previews\n\n**Options**:\n\n*   `none`: No preview images displayed, only shows progress bar during generation\n*   `auto`: Automatically selects the most suitable preview method, dynamically adjusts based on system performance and VRAM\n*   `latent2rgb`: Directly converts latent space data to RGB images for preview, faster but average quality\n*   `taesd`: Uses lightweight TAESD model for preview, balances speed and quality\n\n**Function**: Controls how to preview intermediate results during generation. Different preview methods affect preview quality and performance consumption. Choosing the right preview method can find a balance between preview effects and system resource usage.\n\n### Size of preview images\n\n**Function**: Sets the resolution of preview images, affects preview clarity and performance. Larger sizes provide higher preview quality but also consume more VRAM\n\n## Cache\n\n### Use classic cache system\n\n**Function**: Uses traditional caching strategy, more conservative but stable\n\n### Use LRU caching with a maximum of N node results cached\n\n**Function**: Uses Least Recently Used (LRU) algorithm caching system, can cache a specified number of node computation results **Description**:\n\n*   Set a specific number to control maximum cache count, such as 10, 50, 100, etc.\n*   Caching can avoid repeated computation of the same node operations, improving workflow execution speed\n*   When cache reaches the limit, automatically clears the least recently used results\n*   Cached results occupy system memory (RAM/VRAM), larger values use more memory\n\n**Usage Recommendations**:\n\n*   Default value is null, meaning LRU caching is not enabled\n*   Set appropriate cache count based on system memory capacity and usage requirements\n*   Recommended for workflows that frequently reuse the same node configurations\n*   If system memory is sufficient, larger values can be set for better performance improvement\n\n## Attention\n\n### Cross attention method\n\n**Options**:\n\n*   `auto`: Automatically selects the most suitable attention computation method\n*   `split`: Block-wise attention computation, can save VRAM but slower speed\n*   `quad`: Uses quad attention algorithm, balances speed and VRAM usage\n*   `pytorch`: Uses PyTorch native attention computation, faster but higher VRAM usage\n\n**Function**: Controls the specific algorithm used when the model computes attention. Different algorithms make different trade-offs between generation quality, speed, and VRAM usage. Usually recommended to use auto for automatic selection.\n\n### Force attention upcast\n\n**Function**: Forces high-precision attention computation, improves quality but increases VRAM usage\n\n### Prevent attention upcast\n\n**Function**: Disables high-precision attention computation, saves VRAM\n\n## General\n\n### Disable xFormers optimization\n\n**Function**: Disables the optimization features of the xFormers library. xFormers is a library specifically designed to optimize the attention mechanisms of Transformer models, typically improving computational efficiency, reducing memory usage, and accelerating inference speed. Disabling this optimization will:\n\n*   Fall back to standard attention computation methods\n*   May increase memory usage and computation time\n*   Provide a more stable runtime environment in certain situations\n\n**Use Cases**:\n\n*   When encountering compatibility issues related to xFormers\n*   When more precise computation results are needed (some optimizations may affect numerical precision)\n*   When debugging or troubleshooting requires using standard implementations\n\n### Default hashing function for model files\n\n**Options**:\n\n*   `sha256`: Uses SHA-256 algorithm for hash verification, high security but slower computation\n*   `sha1`: Uses SHA-1 algorithm, faster but slightly lower security\n*   `sha512`: Uses SHA-512 algorithm, provides highest security but slowest computation\n*   `md5`: Uses MD5 algorithm, fastest but lowest security\n\n**Function**: Sets the hash algorithm for model file verification, used to verify file integrity. Different hash algorithms have different trade-offs between computation speed and security. Usually recommended to use sha256 as the default option, which achieves a good balance between security and performance.\n\n### Make pytorch use slower deterministic algorithms when it can\n\n**Function**: Forces PyTorch to use deterministic algorithms when possible to improve result reproducibility. **Description**:\n\n*   When enabled, PyTorch will prioritize deterministic algorithms over faster non-deterministic algorithms\n*   Same inputs will produce same outputs, helpful for debugging and result verification\n*   Deterministic algorithms typically run slower than non-deterministic algorithms\n*   Even with this setting enabled, completely identical image results cannot be guaranteed in all situations\n\n**Use Cases**:\n\n*   Scientific research requiring strict result reproducibility\n*   Debugging processes requiring stable output results\n*   Production environments requiring result consistency\n\n### Enable some untested and potentially quality deteriorating optimizations\n\n**Function**: Enables experimental optimizations that may improve speed but could potentially affect generation quality\n\n### Don’t print server output to console\n\n**Function**: Prevents displaying server runtime information in the console, keeping the interface clean. **Description**:\n\n*   When enabled, ComfyUI server logs and runtime information will not be displayed\n*   Can reduce console information interference, making the interface cleaner\n*   May slightly improve system performance when there’s heavy log output\n*   Default is disabled (false), meaning server output is displayed by default\n\n**Use Cases**:\n\n*   Production environments where debugging information is not needed\n*   When wanting to keep the console interface clean\n*   When the system runs stably and log monitoring is not required\n\n**Note**: It’s recommended to keep this option disabled during development and debugging to promptly view server runtime status and error information.\n\n### Disable saving prompt metadata in files\n\n**Function**: Does not save workflow information in generated images, reducing file size, but also means the loss of corresponding workflow information, preventing you from using workflow output files to reproduce the corresponding generation results\n\n### Disable loading all custom nodes\n\n**Function**: Prevents loading all third-party extension nodes, typically used when troubleshooting issues to locate whether errors are caused by third-party extension nodes\n\n### Logging verbosity level\n\n**Function**: Controls the verbosity level of log output, used for debugging and monitoring system runtime status. **Options**:\n\n*   `CRITICAL`: Only outputs critical error information that may cause the program to stop running\n*   `ERROR`: Outputs error information indicating some functions cannot work properly\n*   `WARNING`: Outputs warning information indicating possible issues that don’t affect main functionality\n*   `INFO`: Outputs general information including system runtime status and important operation records\n*   `DEBUG`: Outputs the most detailed debugging information including system internal runtime details\n\n**Description**:\n\n*   Log levels increase in verbosity from top to bottom\n*   Each level includes all log information from higher levels\n*   Recommended to set to INFO level for normal use\n*   Can be set to DEBUG level when troubleshooting for more information\n*   Can be set to WARNING or ERROR level in production environments to reduce log volume\n\n## Directories\n\n### Input directory\n\n**Function**: Sets the default storage path for input files (such as images, models)\n\n### Output directory\n\n**Function**: Sets the save path for generation results"
},
{
  "url": "https://docs.comfy.org/registry/specifications",
  "markdown": "# pyproject.toml - ComfyUI\n\n## Specifications\n\nThe `pyproject.toml` file contains two main sections for ComfyUI custom nodes: `[project]` and `[tool.comfy]`. Below are the specifications for each section.\n\n## \\[project\\] Section\n\n### name (required)\n\nThe node id uniquely identifies the custom node and will be used in URLs from the registry. Users can install the node by referencing this name:\n\n```\ncomfy node install <node-id>\n```\n\n**Requirements:**\n\n*   Must be less than 100 characters\n*   Can only contain alphanumeric characters, hyphens, underscores, and periods\n*   Cannot have consecutive special characters\n*   Cannot start with a number or special character\n*   Case-insensitive comparison\n\n**Best Practices:**\n\n*   Use a short, descriptive name\n*   Don’t include “ComfyUI” in the name\n*   Make it memorable and easy to type\n\n**Examples:**\n\n```\nname = \"image-processor\"      # ✅ Good: Simple and clear\nname = \"super-resolution\"     # ✅ Good: Describes functionality\nname = \"ComfyUI-enhancer\"    # ❌ Bad: Includes ComfyUI\nname = \"123-tool\"            # ❌ Bad: Starts with number\n```\n\nSee the official [python documentation](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#name) for more details.\n\n### version (required)\n\nUses [semantic versioning](https://semver.org/) with a three-digit version number X.Y.Z:\n\n*   X (**MAJOR**): Breaking changes\n*   Y (**MINOR**): New features (backwards compatible)\n*   Z (**PATCH**): Bug fixes\n\n**Examples:**\n\n```\nversion = \"1.0.0\"    # Initial release\nversion = \"1.1.0\"    # Added new features\nversion = \"1.1.1\"    # Bug fix\nversion = \"2.0.0\"    # Breaking changes\n```\n\n### license (optional)\n\nSpecifies the license for your custom node. Can be specified in two ways:\n\n1.  **File Reference:**\n\n```\nlicense = { file = \"LICENSE\" }     # ✅ Points to LICENSE file\nlicense = { file = \"LICENSE.txt\" } # ✅ Points to LICENSE.txt\nlicense = \"LICENSE\"                # ❌ Incorrect format\n```\n\n2.  **License Name:**\n\n```\nlicense = { text = \"MIT License\" }  # ✅ Correct format\nlicense = { text = \"Apache-2.0\" }   # ✅ Correct format\nlicense = \"MIT LICENSE\"             # ❌ Incorrect format\n```\n\nCommon licenses: [MIT](https://opensource.org/license/mit), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html), [Apache](https://www.apache.org/licenses/LICENSE-2.0)\n\n### description (recommended)\n\nA brief description of what your custom node does.\n\n```\ndescription = \"A super resolution node for enhancing image quality\"\n```\n\n### repository (required)\n\nLinks to related resources:\n\n```\n[project.urls]\nRepository = \"https://github.com/username/repository\"\n```\n\n### urls (recommended)\n\nLinks to related resources:\n\n```\n[project.urls]\nDocumentation = \"https://github.com/username/repository/wiki\"\n\"Bug Tracker\" = \"https://github.com/username/repository/issues\"\n```\n\n### requires-python (recommended)\n\nSpecifies the Python versions that your node supports:\n\n```\nrequires-python = \">=3.8\"        # Python 3.8 or higher\nrequires-python = \">=3.8,<3.11\"  # Python 3.8 up to (but not including) 3.11\n```\n\n### Frontend Version Compatibility (optional)\n\nIf your node has specific requirements for which ComfyUI frontend versions it supports, you can specify this using the `comfyui-frontend-package` dependency. This package is published on [PyPI](https://pypi.org/project/comfyui-frontend-package/). For example, use this field when:\n\n*   Your custom node uses frontend APIs that were introduced in a specific version\n*   You’ve identified incompatibilities between your node and certain frontend versions\n*   Your node requires specific UI features only available in newer frontend versions\n\n```\n[project]\ndependencies = [\n    \"comfyui-frontend-package>=1.20.0\"       # Requires frontend 1.20.0 or newer\n    \"comfyui-frontend-package<=1.21.6\"       # Restricts to frontend versions up to 1.21.6\n    \"comfyui-frontend-package>=1.19,<1.22\"   # Works with frontend 1.19 to 1.21.x\n    \"comfyui-frontend-package~=1.20.0\"       # Compatible with 1.20.x but not 1.21.0\n    \"comfyui-frontend-package!=1.21.3\"       # Works with any version except 1.21.3\n]\n```\n\n### classifiers (recommended)\n\nUse classifiers to specify operating system compatibility and GPU accelerators. This information is used to help users find the right node for their system.\n\n```\n[project]\nclassifiers = [\n    # For OS-independent nodes (works on all operating systems)\n    \"Operating System :: OS Independent\",\n\n    # OR for OS-specific nodes, specify the supported systems:\n    \"Operating System :: Microsoft :: Windows\",  # Windows specific\n    \"Operating System :: POSIX :: Linux\",  # Linux specific\n    \"Operating System :: MacOS\",  # macOS specific\n    \n    # GPU Accelerator support\n    \"Environment :: GPU :: NVIDIA CUDA\",    # NVIDIA CUDA support\n    \"Environment :: GPU :: AMD ROCm\",       # AMD ROCm support\n    \"Environment :: GPU :: Intel Arc\",      # Intel Arc support\n    \"Environment :: NPU :: Huawei Ascend\",  # Huawei Ascend support\n    \"Environment :: GPU :: Apple Metal\",    # Apple Metal support\n]\n```\n\n### PublisherId (required)\n\nYour unique publisher identifier, typically matching your GitHub username. **Examples:**\n\n```\nPublisherId = \"john-doe\"        # ✅ Matches GitHub username\nPublisherId = \"image-wizard\"    # ✅ Unique identifier\n```\n\n### DisplayName (optional)\n\nA user-friendly name for your custom node.\n\n```\nDisplayName = \"Super Resolution Node\"\n```\n\n### Icon (optional)\n\nURL to your custom node’s icon that will be displayed on the ComfyUI Registry and ComfyUI-Manager. **Requirements:**\n\n*   File types: SVG, PNG, JPG, or GIF\n*   Maximum resolution: 400px × 400px\n*   Aspect ratio should be square\n\n```\nIcon = \"https://raw.githubusercontent.com/username/repo/main/icon.png\"\n```\n\nURL to a larger banner image that will be displayed on the ComfyUI Registry and ComfyUI-Manager. **Requirements:**\n\n*   File types: SVG, PNG, JPG, or GIF\n*   Aspect ratio: 21:9\n\n```\nBanner = \"https://raw.githubusercontent.com/username/repo/main/banner.png\"\n```\n\n### requires-comfyui (optional)\n\nSpecifies which version of ComfyUI your node is compatible with. This helps users ensure they have the correct version of ComfyUI installed. **Supported operators:** `<`, `>`, `<=`, `>=`, `~=`, `<>`, `!=` and ranges\n\n```\nrequires-comfyui = \">=1.0.0\"        # ComfyUI 1.0.0 or higher\nrequires-comfyui = \">=1.0.0,<2.0.0\"  # ComfyUI 1.0.0 up to (but not including) 2.0.0\nrequires-comfyui = \"~=1.0.0\"         # Compatible release: version 1.0.0 or newer, but not version 2.0.0\nrequires-comfyui = \"!=1.2.3\"         # Any version except 1.2.3\nrequires-comfyui = \">0.1.3,<1.0.0\"   # Greater than 0.1.3 and less than 1.0.0\n```\n\n### includes (optional)\n\nSpecifies whether to force include certain specific folders. For some situations, such as custom nodes in frontend projects, the final packaged output folder might be included in .gitignore. In such cases, we need to force include it for registry use.\n\n## Complete Example\n\n```\n[project]\nname = \"super-resolution-node\"\nversion = \"1.0.0\"\ndescription = \"Enhance image quality using advanced super resolution techniques\"\nlicense = { file = \"LICENSE\" }\nrequires-python = \">=3.8\"\ndependencies = [\n    \"comfyui-frontend-package<=1.21.6\"  # Frontend version compatibility\n]\nclassifiers = [\n    \"Operating System :: OS Independent\"  # Works on all operating systems\n]\ndynamic = [\"dependencies\"]\n\n[tool.setuptools.dynamic]\ndependencies = {file = [\"requirements.txt\"]}\n\n[project.urls]\nRepository = \"https://github.com/username/super-resolution-node\"\nDocumentation = \"https://github.com/username/super-resolution-node/wiki\"\n\"Bug Tracker\" = \"https://github.com/username/super-resolution-node/issues\"\n\n[tool.comfy]\nPublisherId = \"image-wizard\"\nDisplayName = \"Super Resolution Node\"\nIcon = \"https://raw.githubusercontent.com/username/super-resolution-node/main/icon.png\"\nBanner = \"https://raw.githubusercontent.com/username/super-resolution-node/main/banner.png\"\nrequires-comfyui = \">=1.0.0\"  # ComfyUI version compatibility\n```\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/installation/update_comfyui",
  "markdown": "# How to Update ComfyUI - ComfyUI\n\nWhile we’ve covered ComfyUI updates across different installation methods in various sections, this comprehensive guide consolidates all update procedures to help users clearly understand how to update ComfyUI.\n\nComfyUI Portable provides convenient batch scripts for easy updates.\n\n### Update Script Location\n\nIn the `update` folder within your portable installation directory, you’ll find the following update scripts:\n\n```\nComfyUI_windows_portable\n└─ 📂update\n   ├── update.py\n   ├── update_comfyui.bat                           // Update to latest development version\n   ├── update_comfyui_stable.bat                    // Update to latest stable version\n   └── update_comfyui_and_python_dependencies.bat   // Update dependencies (for troubleshooting)\n```\n\n## ComfyUI Version Types\n\nDepending on your installation method, ComfyUI offers several installation versions. The links below contain update instructions for each version.\n\n## What Needs to Be Updated When Updating ComfyUI?\n\nComfyUI updates primarily consist of two components:\n\n1.  Update ComfyUI’s core code\n2.  Update ComfyUI’s core dependencies, including necessary Python dependencies and ComfyUI functional dependency packages\n\n**Core Code**: New nodes, new model support, new features, etc. **Core Dependencies**: Mainly includes ComfyUI’s frontend functionality, workflow templates, node help documentation, etc.\n\n```\ncomfyui-frontend-package   # ComfyUI frontend functionality\ncomfyui-workflow-templates # ComfyUI workflow templates  \ncomfyui-embedded-docs      # ComfyUI node help documentation\n```\n\nThese three core dependencies are maintained in separate repositories:\n\n*   [ComfyUI\\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend/) - Frontend interface and interactive features\n*   [workflow\\_templates](https://github.com/Comfy-Org/workflow_templates) - Pre-built workflow templates\n*   [comfyui-embedded-docs](https://github.com/Comfy-Org/embedded-docs) - Node help documentation\n\nIt’s important to understand the difference between development (nightly) and stable (release) versions:\n\n*   **Development version (nightly)**: Latest commit code, giving you access to the newest features, but may contain potential issues\n*   **Stable version (release)**: Built on stable releases, usually lags behind development versions but offers higher stability. We support stable versions after features are tested and stabilized\n\nMany users often find themselves on release versions or desktop versions during updates, but discover that needed features are only available in development versions. In such cases, check if your local `ComfyUI/requirements.txt` matches the [nightly version dependencies](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) to determine if all dependencies support the latest features.\n\n## Common Update Issues\n\n### Missing or Outdated Frontend, Workflow Templates, Node After Updates\n\nUsers often only use the `git pull` command to update ComfyUI code but **neglect core dependency updates**, leading to the following issues:\n\n*   Missing or abnormal frontend functionality\n*   Cannot find newly added workflow templates\n*   Outdated or missing node help documentation\n*   New features lack corresponding frontend support\n\nAfter using the `git pull` command, use the corresponding ComfyUI environment to use `pip install -r requirements.txt` to update dependencies.\n\n### How to Properly Update Core Dependencies\n\n**Recommended Method**: Use the `ComfyUI_windows_portable\\update\\update_comfyui.bat` batch script, which will update both ComfyUI code and all Python dependencies.**Manual Dependency Update**: If you need to manually update dependencies, use the following command:\n\n```\n# Open command line in portable version directory\n.\\python_embeded\\python.exe -m pip install -r ComfyUI\\requirements.txt\n```\n\n### Core Dependency Update Troubleshooting\n\nIf core dependency updates fail, follow these troubleshooting steps:\n\n### Why Can’t I Find New Features After Updating?\n\nThis is one of the most common issues:\n\n*   If you’re using the **Desktop version**, features may lag behind because the desktop version is built on stable releases\n*   Ensure you’re using the **development version (nightly)**, not the **stable version (release)**\n\nAdditionally, ensure that corresponding dependencies have been successfully updated during the update process. If issues persist after updating, refer to the [Dependency Update Troubleshooting](#dependency-update-troubleshooting) section to diagnose problems.\n\n### How to Switch Between Development (Nightly) and Stable (Release) Versions?\n\nDifferences between versions:\n\n*   **Characteristics**: Contains the latest commit code\n*   **Advantages**: Experience the latest features and improvements first\n*   **Risks**: May contain undiscovered bugs or unstable factors\n*   **Suitable for**: Developers, testers, users wanting to experience the latest features\n\nUse `update_comfyui.bat` instead of `update_comfyui_stable.bat`:\n\n```\n# Development version (latest features)\ndouble-click: update_comfyui.bat\n\n# Stable version\ndouble-click: update_comfyui_stable.bat\n```\n\n### What to Do When Errors Occur After Updates?\n\n1.  **Check Dependencies**: Run `pip install -r requirements.txt` to ensure all dependencies are updated\n2.  **Check Custom Nodes**: Some custom nodes may be incompatible with new versions\n3.  **Roll Back Version**: If issues are severe, you can roll back to a previous stable version\n\nIf problems occur, refer to our troubleshooting page for solutions.\n\n[\n\n## Troubleshooting\n\nLearn how to troubleshoot ComfyUI issues\n\n\n\n](https://docs.comfy.org/troubleshooting/overview)\n\n### How to Stay Updated on New Features?\n\n*   **GitHub Releases**: Check [ComfyUI Releases](https://github.com/comfyanonymous/ComfyUI/releases) for stable version updates\n*   **GitHub Commits**: View [latest commits](https://github.com/comfyanonymous/ComfyUI/commits/master) to understand development progress\n*   **Community Discussion**: Follow our [blog](https://blog.comfy.org/) and [Twitter](https://x.com/comfyui) for the latest updates"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Funpublish-delete-a-specific-version-of-a-node",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fretrieve-permissions-the-user-has-for-a-given-publisher-1",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fcreate-a-new-personal-access-token",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncodeSdxlRefiner",
  "markdown": "# ClipTextEncodeSdxlRefiner - ComfyUI Built-in Node Documentation\n\nThis node is specifically designed for the SDXL Refiner model to convert text prompts into conditioning information by incorporating aesthetic scores and dimensional information to enhance the conditions for generation tasks, thereby improving the final refinement effect. It acts like a professional art director, not only conveying your creative intent but also injecting precise aesthetic standards and specification requirements into the work.\n\n## About SDXL Refiner\n\nSDXL Refiner is a specialized refinement model that focuses on enhancing image details and quality based on the SDXL base model. This process is like having an art retoucher:\n\n1.  First, it receives preliminary images or text descriptions generated by the base model\n2.  Then, it guides the refinement process through precise aesthetic scoring and dimensional parameters\n3.  Finally, it focuses on processing high-frequency image details to improve overall quality\n\nRefiner can be used in two ways:\n\n*   As a standalone refinement step for post-processing images generated by the base model\n*   As part of an expert integration system, taking over processing during the low-noise phase of generation\n\n## Inputs\n\n| Parameter Name | Data Type | Input Type | Default Value | Value Range | Description |\n| --- | --- | --- | --- | --- | --- |\n| `clip` | CLIP | Required | \\-  | \\-  | CLIP model instance used for text tokenization and encoding, the core component for converting text into model-understandable format |\n| `ascore` | FLOAT | Optional | 6.0 | 0.0-1000.0 | Controls the visual quality and aesthetics of generated images, similar to setting quality standards for artwork:  <br>\\- High scores(7.5-8.5): Pursues more refined, detail-rich effects  <br>\\- Medium scores(6.0-7.0): Balanced quality control  <br>\\- Low scores(2.0-3.0): Suitable for negative prompts |\n| `width` | INT | Required | 1024 | 64-16384 | Specifies output image width (pixels), must be multiple of 8. SDXL performs best when total pixel count is close to 1024×1024 (about 1M pixels) |\n| `height` | INT | Required | 1024 | 64-16384 | Specifies output image height (pixels), must be multiple of 8. SDXL performs best when total pixel count is close to 1024×1024 (about 1M pixels) |\n| `text` | STRING | Required | \\-  | \\-  | Text prompt description, supports multi-line input and dynamic prompt syntax. In Refiner, text prompts should focus more on describing desired visual quality and detail characteristics |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| `CONDITIONING` | CONDITIONING | Refined conditional output containing integrated encoding of text semantics, aesthetic standards, and dimensional information, specifically for guiding SDXL Refiner model in precise image refinement |\n\n## Notes\n\n1.  This node is specifically optimized for the SDXL Refiner model and differs from regular CLIPTextEncode nodes\n2.  An aesthetic score of 7.5 is recommended as the baseline, which is the standard setting used in SDXL training\n3.  All dimensional parameters must be multiples of 8, and total pixel count close to 1024×1024 (about 1M pixels) is recommended\n4.  The Refiner model focuses on enhancing image details and quality, so text prompts should emphasize desired visual effects rather than scene content\n5.  In practical use, Refiner is typically used in the later stages of generation (approximately the last 20% of steps), focusing on detail optimization"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fupload-files",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fretrieves-a-list-of-nodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/basic/upscale",
  "markdown": "# ComfyUI Image Upscale Workflow - ComfyUI\n\n## What is Image Upscaling?\n\nImage Upscaling is the process of converting low-resolution images to high-resolution using algorithms. Unlike traditional interpolation methods, AI upscaling models (like ESRGAN) can intelligently reconstruct details while maintaining image quality. For instance, the default SD1.5 model often struggles with large-size image generation. To achieve high-resolution results,we typically generate smaller images first and then use upscaling techniques. This article covers one of many upscaling methods in ComfyUI. In this tutorial, we’ll guide you through:\n\n1.  Downloading and installing upscaling models\n2.  Performing basic image upscaling\n3.  Combining text-to-image workflows with upscaling\n\n## Upscaling Workflow\n\n### Model Installation\n\nRequired ESRGAN models download:\n\n### Workflow and Assets\n\nDownload and drag the following image into ComfyUI to load the basic upscaling workflow: ![Upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_workflow.png) \n\nUse this image in smaller size as input: ![Upscale-input](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale-input.jpg) \n\n### Complete the Workflow Step by Step\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![Upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_simple_workflow.jpg)\n\n1.  Ensure `Load Upscale Model` loads `4x-ESRGAN.pth`\n2.  Upload the input image to the `Load Image` node\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n\nThe core components are the `Load Upscale Model` and `Upscale Image (Using Model)` nodes, which receive an image input and upscale it using the selected model.\n\n## Text-to-Image Combined Workflow\n\nAfter mastering basic upscaling, we can combine it with the [text-to-image](https://docs.comfy.org/tutorials/basic/text-to-image) workflow. For text-to-image basics, refer to the [text-to-image tutorial](https://docs.comfy.org/tutorials/basic/text-to-image). Download and drag this image into ComfyUI to load the combined workflow: ![Text-to-image upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/esrgan_example.png) This workflow connects the text-to-image output image directly to the upscaling nodes for final processing.\n\n## Additional Tips\n\n1.  **Chained Upscaling**: Combine multiple upscale nodes (e.g., 2x → 4x) for ultra-high magnification\n2.  **Hybrid Workflow**: Connect upscale nodes after generation for “generate+enhance” pipelines\n3.  **Comparative Testing**: Different models perform better on specific image types - test multiple options"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fcreate-node-translations",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fretrieve-a-specific-node-by-id",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image",
  "markdown": "# Flux 1.1 Pro Ultra Image API Node ComfyUI Official Workflow Examples\n\nFLUX 1.1 Pro Ultra is a high-performance AI image generation tool by BlackForestLabs, featuring ultra-high resolution and efficient generation capabilities. It supports up to 4MP resolution (4x the standard version) while keeping single image generation time under 10 seconds - 2.5x faster than similar high-resolution models. The tool offers two core modes:\n\n*   **Ultra Mode**: Designed for high-resolution needs, perfect for advertising and e-commerce where detail magnification is important. It accurately reflects prompts while maintaining generation speed.\n*   **Raw Mode**: Focuses on natural realism, optimizing skin tones, lighting, and landscape details. Reduces the “AI look” and is ideal for photography and realistic style creation.\n\nWe now support the Flux 1.1 Pro Ultra Image node in ComfyUI. This guide will cover:\n\n*   Flux 1.1 Pro Text-to-Image\n*   Flux 1.1 Pro Image-to-Image (Remix)\n\n## Flux 1.1 Pro Ultra Image Node Documentation\n\nCheck the following documentation for detailed node parameter settings:\n\n*   [Flux 1.1 Pro Ultra Image](https://docs.comfy.org/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg)\n\n## Flux 1.1 \\[pro\\] Text-to-Image Tutorial\n\n### 1\\. Download Workflow File\n\nDownload and drag the following file into ComfyUI to load the workflow: ![Flux 1.1 pro Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_t2i.png)\n\n### 2\\. Complete the Workflow Steps\n\n![Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg) Follow the numbered steps to complete the basic workflow:\n\n1.  (Optional) Modify the prompt in the `Flux 1.1 [pro] Ultra Image` node\n2.  (Optional) Set `raw` parameter to `false` for more realistic output\n3.  Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image\n4.  After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\n## Flux 1.1\\[pro\\] Image-to-Image Tutorial\n\nWhen adding an `image_prompt` to the node input, the output will blend features from the input image (Remix). The `image_prompt_strength` value affects the blend ratio: higher values make the output more similar to the input image.\n\n### 1\\. Download Workflow File\n\nDownload and drag the following file into ComfyUI, or right-click the purple node in the Text-to-Image workflow and set `mode` to `always` to enable `image_prompt` input: ![Flux 1.1 pro Image-to-Image Remix](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_i2i.png) We’ll use this image as input: ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png) \n\n### 2\\. Complete the Workflow Steps\n\n![Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg) Follow these numbered steps:\n\n1.  Click **Upload** on the `Load Image` node to upload your input image\n2.  (Optional) Adjust `image_prompt_strength` in `Flux 1.1 [pro] Ultra Image` to change the blend ratio\n3.  Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image\n4.  After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\nHere’s a comparison of outputs with different `image_prompt_strength` values: ![Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference",
  "markdown": "# Luma Reference - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Luma Reference Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-reference.jpg) The Luma Reference node allows you to set reference images and weights to guide the creation process of Luma image generation nodes, making the generated images closer to specific features of the reference images.\n\n## Node Function\n\nThis node works as a helper tool for Luma generation nodes, allowing users to provide reference images to influence generation results. It enables users to set the weight of reference images to control how much they affect the final result. Multiple Luma Reference nodes can be chained together, with a maximum of 4 working simultaneously according to API requirements.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | Image | \\-  | Input image used as reference |\n| weight | Float | 1.0 | Controls the strength of the reference image’s influence (0-1) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| luma\\_ref | LUMA\\_REF | Reference object containing image and weight |\n\n## Usage Example\n\n[\n\n## Luma Text to Image Workflow Example\n\nLuma Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)\n\n## How It Works\n\nThe Luma Reference node receives image input and allows setting a weight value. The node doesn’t directly generate or modify images but creates a reference object containing image data and weight information, which is then passed to Luma generation nodes. During the generation process, Luma AI analyzes the features of the reference image and incorporates these features into the generation results based on the set weight. Higher weight values mean the generated image will be closer to the reference image’s features, while lower weight values indicate the reference image will only slightly influence the final result.\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\n\nclass LumaReferenceNode(ComfyNodeABC):\n    \"\"\"\n    Holds an image and weight for use with Luma Generate Image node.\n    \"\"\"\n\n    RETURN_TYPES = (LumaIO.LUMA_REF,)\n    RETURN_NAMES = (\"luma_ref\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_luma_reference\"\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Image to use as reference.\",\n                    },\n                ),\n                \"weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of image reference.\",\n                    },\n                ),\n            },\n            \"optional\": {\"luma_ref\": (LumaIO.LUMA_REF,)},\n        }\n\n    def create_luma_reference(\n        self, image: torch.Tensor, weight: float, luma_ref: LumaReferenceChain = None\n    ):\n        if luma_ref is not None:\n            luma_ref = luma_ref.clone()\n        else:\n            luma_ref = LumaReferenceChain()\n        luma_ref.add(LumaReference(image=image, weight=round(weight, 2)))\n        return (luma_ref,)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3",
  "markdown": "# Ideogram V3 - ComfyUI Native Node Documentation\n\n![ComfyUI Native Ideogram V3 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg) This node connects to the Ideogram V3 API to perform image generation tasks. Currently, this node supports two image generation modes:\n\n*   **Text-to-Image Mode** - Generate new images from text prompts\n*   **Inpainting Mode** - Regenerate specific areas by providing an original image and mask\n\n### Text-to-Image Mode\n\nThis is the default mode, activated when no image or mask inputs are provided. Simply provide a prompt and the desired parameters:\n\n1.  Describe the image you want in the prompt field\n2.  Select an appropriate aspect ratio or resolution\n3.  Adjust other parameters like magic prompt, seed, and rendering quality\n4.  Run the node to generate the image\n\n### Inpainting Mode\n\n**Important Note**: This mode requires both image and mask inputs. If only one is provided, the node will throw an error.\n\n1.  Connect the original image to the `image` input port\n2.  Create a mask with the same dimensions as the original image, where white areas represent parts to be regenerated\n3.  Connect the mask to the `mask` input port\n4.  Describe what you want to generate in the masked area in the prompt\n5.  Run the node to perform local editing\n\n## Parameter Descriptions\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the content to generate |\n| aspect\\_ratio | combo | ”1:1” | Image aspect ratio (text-to-image mode only) |\n| resolution | combo | ”Auto” | Image resolution, overrides aspect ratio when set |\n| magic\\_prompt\\_option | combo | ”AUTO” | Magic prompt enhancement: AUTO, ON, or OFF |\n| seed | int | 0   | Random seed value, 0 for random generation |\n| num\\_images | int | 1   | Number of images to generate (1-8) |\n| rendering\\_speed | combo | ”BALANCED” | Rendering speed: BALANCED, TURBO, or QUALITY |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| image | image | Input image for inpainting mode (**must be provided with mask**) |\n| mask | mask | Mask for inpainting, white areas will be replaced (**must be provided with image**) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image |\n\n## How It Works\n\nThe Ideogram V3 node uses state-of-the-art AI models to process user input, capable of understanding complex design intentions and text layout requirements. It supports two main modes:\n\n1.  **Generation Mode**: Creates new images from text prompts\n2.  **Edit Mode**: Uses original image + mask combination, replacing only the areas specified by the mask\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass IdeogramV3(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V3 model.\n\n    Supports both regular image generation from text prompts and image editing with mask.\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation or editing\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V3_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation. Ignored if resolution is not set to Auto.\",\n                    },\n                ),\n                \"resolution\": (\n                    IO.COMBO,\n                    {\n                        \"options\": V3_RESOLUTIONS,\n                        \"default\": \"Auto\",\n                        \"tooltip\": \"The resolution for image generation. If not set to Auto, this overrides the aspect_ratio setting.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n                \"rendering_speed\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"BALANCED\", \"TURBO\", \"QUALITY\"],\n                        \"default\": \"BALANCED\",\n                        \"tooltip\": \"Controls the trade-off between generation speed and quality\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v3\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        image=None,\n        mask=None,\n        resolution=\"Auto\",\n        aspect_ratio=\"1:1\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        num_images=1,\n        rendering_speed=\"BALANCED\",\n        auth_token=None,\n    ):\n        # Check if both image and mask are provided for editing mode\n        if image is not None and mask is not None:\n            # Edit mode\n            path = \"/proxy/ideogram/ideogram-v3/edit\"\n\n            # Process image and mask\n            input_tensor = image.squeeze().cpu()\n\n            # Validate mask dimensions match image\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n\n            # Process image\n            img_np = (input_tensor.numpy() * 255).astype(np.uint8)\n            img = Image.fromarray(img_np)\n            img_byte_arr = io.BytesIO()\n            img.save(img_byte_arr, format=\"PNG\")\n            img_byte_arr.seek(0)\n            img_binary = img_byte_arr\n            img_binary.name = \"image.png\"\n\n            # Process mask - white areas will be replaced\n            mask_np = (mask.squeeze().cpu().numpy() * 255).astype(np.uint8)\n            mask_img = Image.fromarray(mask_np)\n            mask_byte_arr = io.BytesIO()\n            mask_img.save(mask_byte_arr, format=\"PNG\")\n            mask_byte_arr.seek(0)\n            mask_binary = mask_byte_arr\n            mask_binary.name = \"mask.png\"\n\n            # Create edit request\n            edit_request = IdeogramV3EditRequest(\n                prompt=prompt,\n                rendering_speed=rendering_speed,\n            )\n\n            # Add optional parameters\n            if magic_prompt_option != \"AUTO\":\n                edit_request.magic_prompt = magic_prompt_option\n            if seed != 0:\n                edit_request.seed = seed\n            if num_images > 1:\n                edit_request.num_images = num_images\n\n            # Execute the operation for edit mode\n            operation = SynchronousOperation(\n                endpoint=ApiEndpoint(\n                    path=path,\n                    method=HttpMethod.POST,\n                    request_model=IdeogramV3EditRequest,\n                    response_model=IdeogramGenerateResponse,\n                ),\n                request=edit_request,\n                files={\n                    \"image\": img_binary,\n                    \"mask\": mask_binary,\n                },\n                content_type=\"multipart/form-data\",\n                auth_token=auth_token,\n            )\n\n        elif image is not None or mask is not None:\n            # If only one of image or mask is provided, raise an error\n            raise Exception(\"Ideogram V3 image editing requires both an image AND a mask\")\n        else:\n            # Generation mode\n            path = \"/proxy/ideogram/ideogram-v3/generate\"\n\n            # Create generation request\n            gen_request = IdeogramV3Request(\n                prompt=prompt,\n                rendering_speed=rendering_speed,\n            )\n\n            # Handle resolution vs aspect ratio\n            if resolution != \"Auto\":\n                gen_request.resolution = resolution\n            elif aspect_ratio != \"1:1\":\n                v3_aspect = V3_RATIO_MAP.get(aspect_ratio)\n                if v3_aspect:\n                    gen_request.aspect_ratio = v3_aspect\n\n            # Add optional parameters\n            if magic_prompt_option != \"AUTO\":\n                gen_request.magic_prompt = magic_prompt_option\n            if seed != 0:\n                gen_request.seed = seed\n            if num_images > 1:\n                gen_request.num_images = num_images\n\n            # Execute the operation for generation mode\n            operation = SynchronousOperation(\n                endpoint=ApiEndpoint(\n                    path=path,\n                    method=HttpMethod.POST,\n                    request_model=IdeogramV3Request,\n                    response_model=IdeogramGenerateResponse,\n                ),\n                request=gen_request,\n                auth_token=auth_token,\n            )\n\n        # Execute the operation and process response\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra",
  "markdown": "# Stability Stable Image Ultra - ComfyUI Native Node Documentation\n\n![ComfyUI Native Stability Stable Image Ultra Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg) The Stability Stable Image Ultra node uses Stability AI’s Stable Diffusion Ultra API to generate high-quality images. It supports both text-to-image and image-to-image generation, creating detailed and artistic visuals from text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text description of what you want to generate. Better results come from clear, descriptive prompts that specify elements, colors and themes. You can control word importance using `(word:weight)` format, where weight is 0-1. For example: `The sky was (blue:0.3) and (green:0.8)` makes the sky more green than blue. |\n| aspect\\_ratio | select | ”1:1” | Width to height ratio of output image |\n| style\\_preset | select | ”None” | Optional preset style for generated image |\n| seed | integer | 0   | Random seed for noise generation (0-4294967294) |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image for image-to-image generation |\n| negative\\_prompt | string | \"\"  | Describes what you don’t want in the output image. This is an advanced feature |\n| image\\_denoise | float | 0.5 | Denoising strength for input image (0.0-1.0). 0.0 keeps input image unchanged, 1.0 is like having no input image |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image |\n\n## Notes\n\n*   image\\_denoise has no effect when no input image is provided\n*   No preset style is applied when style\\_preset is “None”\n*   For image-to-image, input images are converted to the proper format before API submission\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass StabilityStableImageUltraNode:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/stability\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines\" +\n                                    \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines\" +\n                                    \"elements, colors, and subjects will lead to better results. \" +\n                                    \"To control the weight of a given word use the format `(word:weight)`,\" +\n                                    \"where `word` is the word you'd like to control the weight of and `weight`\" +\n                                    \"is a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\" +\n                                    \"would convey a sky that was blue and green, but more green than blue.\"\n                    },\n                ),\n                \"aspect_ratio\": ([x.value for x in StabilityAspectRatio],\n                    {\n                        \"default\": StabilityAspectRatio.ratio_1_1,\n                        \"tooltip\": \"Aspect ratio of generated image.\",\n                    },\n                ),\n                \"style_preset\": (get_stability_style_presets(),\n                    {\n                        \"tooltip\": \"Optional desired style of generated image.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 4294967294,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (IO.IMAGE,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"A blurb of text describing what you do not wish to see in the output image. This is an advanced feature.\"\n                    },\n                ),\n                \"image_denoise\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(self, prompt: str, aspect_ratio: str, style_preset: str, seed: int,\n                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,\n                 auth_token=None):\n        # prepare image binary if image present\n        image_binary = None\n        if image is not None:\n            image_binary = tensor_to_bytesio(image, 1504 * 1504).read()\n        else:\n            image_denoise = None\n\n        if not negative_prompt:\n            negative_prompt = None\n        if style_preset == \"None\":\n            style_preset = None\n\n        files = {\n            \"image\": image_binary\n        }\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/stability/v2beta/stable-image/generate/ultra\",\n                method=HttpMethod.POST,\n                request_model=StabilityStableUltraRequest,\n                response_model=StabilityStableUltraResponse,\n            ),\n            request=StabilityStableUltraRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                aspect_ratio=aspect_ratio,\n                seed=seed,\n                strength=image_denoise,\n                style_preset=style_preset,\n            ),\n            files=files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.finish_reason != \"SUCCESS\":\n            raise Exception(f\"Stable Image Ultra generation failed: {response_api.finish_reason}.\")\n\n        image_data = base64.b64decode(response_api.image)\n        returned_image = bytesio_to_image_tensor(BytesIO(image_data))\n\n        return (returned_image,)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background",
  "markdown": "# Recraft Replace Background - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Replace Background Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg) The Recraft Replace Background node uses Recraft’s API to intelligently detect subjects in images and generate new backgrounds based on text prompts.\n\n```\n\nclass RecraftReplaceBackgroundNode:\n    \"\"\"\n    Replace background on image, based on provided prompt.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt: str,\n        n: int,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n        )\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/replaceBackground\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg",
  "markdown": "# Save SVG - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Save SVG Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/save-svg.jpg) The Save SVG node allows you to save SVG data from Recraft vector generation nodes as usable files in the filesystem. This is an essential component for handling and exporting vector graphics.\n\n## Node Function\n\nThis node receives SVG vector data and saves it as standard SVG files in the filesystem. It supports automatic file naming and save path specification, allowing vector graphics to be opened and edited in other software.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| svg | SVG | \\-  | SVG vector data to save |\n| filename\\_prefix | string | ”recraft” | Prefix for the filename |\n| output\\_dir | string | \\-  | Output directory, defaults to ComfyUI output folder at `ComfyUI/output/svg/` |\n| index | integer | \\-1 | Save index, -1 means save all generated SVGs |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| SVG | SVG | Passes through the input SVG data |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\nclass SaveSVGNode:\n    \"\"\"\n    Save SVG files on disk.\n    \"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n\n    RETURN_TYPES = ()\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"save_svg\"\n    CATEGORY = \"api node/image/Recraft\"\n    OUTPUT_NODE = True\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"svg\": (RecraftIO.SVG,),\n                \"filename_prefix\": (\"STRING\", {\"default\": \"svg/ComfyUI\", \"tooltip\": \"The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes.\"})\n            },\n            \"hidden\": {\n                \"prompt\": \"PROMPT\",\n                \"extra_pnginfo\": \"EXTRA_PNGINFO\"\n            }\n        }\n\n    def save_svg(self, svg: SVG, filename_prefix=\"svg/ComfyUI\", prompt=None, extra_pnginfo=None):\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n        results = list()\n\n        # Prepare metadata JSON\n        metadata_dict = {}\n        if prompt is not None:\n            metadata_dict[\"prompt\"] = prompt\n        if extra_pnginfo is not None:\n            metadata_dict.update(extra_pnginfo)\n\n        # Convert metadata to JSON string\n        metadata_json = json.dumps(metadata_dict, indent=2) if metadata_dict else None\n\n        for batch_number, svg_bytes in enumerate(svg.data):\n            filename_with_batch_num = filename.replace(\"%batch_num%\", str(batch_number))\n            file = f\"{filename_with_batch_num}_{counter:05}_.svg\"\n\n            # Read SVG content\n            svg_bytes.seek(0)\n            svg_content = svg_bytes.read().decode('utf-8')\n\n            # Inject metadata if available\n            if metadata_json:\n                # Create metadata element with CDATA section\n                metadata_element = f\"\"\"  <metadata>\n    <![CDATA[\n{metadata_json}\n    ]]>\n  </metadata>\n\"\"\"\n                # Insert metadata after opening svg tag using regex\n                import re\n                svg_content = re.sub(r'(<svg[^>]*>)', r'\\1\\n' + metadata_element, svg_content)\n\n            # Write the modified SVG to file\n            with open(os.path.join(full_output_folder, file), 'wb') as svg_file:\n                svg_file.write(svg_content.encode('utf-8'))\n\n            results.append({\n                \"filename\": file,\n                \"subfolder\": subfolder,\n                \"type\": self.type\n            })\n            counter += 1\n        return { \"ui\": { \"images\": results } }\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image",
  "markdown": "# Luma Text to Image - ComfyUI Native Node Documentation\n\n![ComfyUI Native Luma Text to Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg) The Luma Text to Image node allows you to create highly realistic and artistic images from text descriptions using Luma AI’s advanced image generation capabilities.\n\n## Node Function\n\nThis node connects to Luma AI’s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail representation, particularly excelling at producing photorealistic content and artistic style images.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | String | \"\"  | Text prompt describing the content to generate |\n| model | Select | \\-  | Choose which generation model to use |\n| aspect\\_ratio | Select | 16:9 | Set the output image’s aspect ratio |\n| seed | Integer | 0   | Seed value to determine if the node should re-run, but actual results are independent of the seed |\n| style\\_image\\_weight | Float | 1.0 | Style image weight, range 0.0-1.0, only applies when style\\_image is provided, higher means stronger style reference |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| image\\_luma\\_ref | LUMA\\_REF | Luma reference node connection to influence generation with input images; up to 4 images |\n| style\\_image | Image | Style reference image; only 1 image will be used |\n| character\\_image | Image | Character reference images; can be a batch of multiple, up to 4 images |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | Image | Generated image result |\n\n## Usage Example\n\n[\n\n## Luma Text to Image Usage Example\n\nDetailed guide for Luma Text to Image workflow\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)\n\n## How It Works\n\nThe Luma Text to Image node analyzes the text prompt provided by the user and creates corresponding images through Luma AI’s generation models. This process uses deep learning technology to understand text descriptions and convert them into visual representations. Users can fine-tune the generation process by adjusting various parameters, including resolution, guidance scale, and negative prompts. Additionally, the node supports using reference images and concept guidance to further influence the generation results, allowing creators to more precisely achieve their creative vision.\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\n\nclass LumaImageGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously based on prompt and aspect ratio.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaImageModel],),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in LumaAspectRatio],\n                    {\n                        \"default\": LumaAspectRatio.ratio_16_9,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n                \"style_image_weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of style image. Ignored if no style_image provided.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image_luma_ref\": (\n                    LumaIO.LUMA_REF,\n                    {\n                        \"tooltip\": \"Luma Reference node connection to influence generation with input images; up to 4 images can be considered.\"\n                    },\n                ),\n                \"style_image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"Style reference image; only 1 image will be used.\"},\n                ),\n                \"character_image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Character reference images; can be a batch of multiple, up to 4 images can be considered.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        aspect_ratio: str,\n        seed,\n        style_image_weight: float,\n        image_luma_ref: LumaReferenceChain = None,\n        style_image: torch.Tensor = None,\n        character_image: torch.Tensor = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # handle image_luma_ref\n        api_image_ref = None\n        if image_luma_ref is not None:\n            api_image_ref = self._convert_luma_refs(\n                image_luma_ref, max_refs=4, auth_token=auth_token\n            )\n        # handle style_luma_ref\n        api_style_ref = None\n        if style_image is not None:\n            api_style_ref = self._convert_style_image(\n                style_image, weight=style_image_weight, auth_token=auth_token\n            )\n        # handle character_ref images\n        character_ref = None\n        if character_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                character_image, max_images=4, auth_token=auth_token\n            )\n            character_ref = LumaCharacterRef(\n                identity0=LumaImageIdentity(images=download_urls)\n            )\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations/image\",\n                method=HttpMethod.POST,\n                request_model=LumaImageGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaImageGenerationRequest(\n                prompt=prompt,\n                model=model,\n                aspect_ratio=aspect_ratio,\n                image_ref=api_image_ref,\n                style_ref=api_style_ref,\n                character_ref=character_ref,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        img_response = requests.get(response_poll.assets.image)\n        img = process_image_response(img_response)\n        return (img,)\n\n    def _convert_luma_refs(\n        self, luma_ref: LumaReferenceChain, max_refs: int, auth_token=None\n    ):\n        luma_urls = []\n        ref_count = 0\n        for ref in luma_ref.refs:\n            download_urls = upload_images_to_comfyapi(\n                ref.image, max_images=1, auth_token=auth_token\n            )\n            luma_urls.append(download_urls[0])\n            ref_count += 1\n            if ref_count >= max_refs:\n                break\n        return luma_ref.create_api_model(download_urls=luma_urls, max_refs=max_refs)\n\n    def _convert_style_image(\n        self, style_image: torch.Tensor, weight: float, auth_token=None\n    ):\n        chain = LumaReferenceChain(\n            first_ref=LumaReference(image=style_image, weight=weight)\n        )\n        return self._convert_luma_refs(chain, max_refs=1, auth_token=auth_token)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1",
  "markdown": "# OpenAI GPT Image 1 - ComfyUI Native Node Documentation\n\n![ComfyUI Native OpenAI GPT Image 1 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg) This node connects to OpenAI’s GPT Image 1 API, allowing users to generate images through detailed text prompts. Unlike traditional DALL·E models, GPT Image 1 leverages GPT-4’s language understanding capabilities to handle more complex prompts and generate images that better match user intent.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing what to generate |\n| quality | selection | ”low” | Image quality level, options: “low”, “medium”, “high” |\n| size | selection | ”auto” | Output image size, options: “auto”, “1024x1024”, “1024x1536”, “1536x1024” |\n\n### Image Editing Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| image | image | Input image for editing, supports multiple images |\n| mask | mask | Optional mask to specify areas to modify (single image only) |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| background | selection | Background options: “opaque” or “transparent” |\n| seed | integer | Random seed (not yet implemented in backend) |\n| n   | integer | Number of images to generate, range 1-8 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image result |\n\n## How It Works\n\nThe OpenAI GPT Image 1 node combines GPT-4’s language understanding with image generation. It analyzes the text prompt to understand its meaning and intent, then generates matching images. In image editing mode, the node can modify existing images. Using a mask allows precise control over which areas to change. Note that mask input only works with single image input. Users can control the output by adjusting parameters like quality level, size, background handling, and number of generations.\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass OpenAIGPTImage1(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's GPT Image 1 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for GPT Image 1\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"quality\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"low\", \"medium\", \"high\"],\n                        \"default\": \"low\",\n                        \"tooltip\": \"Image quality, affects cost and generation time.\",\n                    },\n                ),\n                \"background\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"opaque\", \"transparent\"],\n                        \"default\": \"opaque\",\n                        \"tooltip\": \"Return image with or without background\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"auto\", \"1024x1024\", \"1024x1536\", \"1536x1024\"],\n                        \"default\": \"auto\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"How many images to generate\",\n                    },\n                ),\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        quality=\"low\",\n        background=\"opaque\",\n        image=None,\n        mask=None,\n        n=1,\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"gpt-image-1\"\n        path = \"/proxy/openai/images/generations\"\n        content_type=\"application/json\"\n        request_class = OpenAIImageGenerationRequest\n        img_binaries = []\n        mask_binary = None\n        files = []\n\n        if image is not None:\n            path = \"/proxy/openai/images/edits\"\n            request_class = OpenAIImageEditRequest\n            content_type =\"multipart/form-data\"\n\n            batch_size = image.shape[0]\n\n            for i in range(batch_size):\n                single_image = image[i : i + 1]\n                scaled_image = downscale_image_tensor(single_image).squeeze()\n\n                image_np = (scaled_image.numpy() * 255).astype(np.uint8)\n                img = Image.fromarray(image_np)\n                img_byte_arr = io.BytesIO()\n                img.save(img_byte_arr, format=\"PNG\")\n                img_byte_arr.seek(0)\n                img_binary = img_byte_arr\n                img_binary.name = f\"image_{i}.png\"\n\n                img_binaries.append(img_binary)\n                if batch_size == 1:\n                    files.append((\"image\", img_binary))\n                else:\n                    files.append((\"image[]\", img_binary))\n\n        if mask is not None:\n            if image.shape[0] != 1:\n                raise Exception(\"Cannot use a mask with multiple image\")\n            if image is None:\n                raise Exception(\"Cannot use a mask without an input image\")\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n            batch, height, width = mask.shape\n            rgba_mask = torch.zeros(height, width, 4, device=\"cpu\")\n            rgba_mask[:, :, 3] = 1 - mask.squeeze().cpu()\n\n            scaled_mask = downscale_image_tensor(rgba_mask.unsqueeze(0)).squeeze()\n\n            mask_np = (scaled_mask.numpy() * 255).astype(np.uint8)\n            mask_img = Image.fromarray(mask_np)\n            mask_img_byte_arr = io.BytesIO()\n            mask_img.save(mask_img_byte_arr, format=\"PNG\")\n            mask_img_byte_arr.seek(0)\n            mask_binary = mask_img_byte_arr\n            mask_binary.name = \"mask.png\"\n            files.append((\"mask\", mask_binary))\n\n        # Build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=path,\n                method=HttpMethod.POST,\n                request_model=request_class,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=request_class(\n                model=model,\n                prompt=prompt,\n                quality=quality,\n                background=background,\n                n=n,\n                seed=seed,\n                size=size,\n            ),\n            files=files if files else None,\n            content_type=content_type,\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3",
  "markdown": "# OpenAI DALL·E 3 - ComfyUI Native Node Documentation\n\n![ComfyUI Native OpenAI DALL·E 3 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg) This node connects to OpenAI’s DALL·E 3 API, allowing users to generate high-quality images through detailed text prompts. DALL·E 3 is OpenAI’s image generation model that offers significantly improved image quality, more accurate prompt understanding, and better detail rendering compared to previous versions.\n\n```\n\nclass OpenAIDalle3(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's DALL·E 3 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for DALL·E\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"quality\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"standard\", \"hd\"],\n                        \"default\": \"standard\",\n                        \"tooltip\": \"Image quality\",\n                    },\n                ),\n                \"style\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"natural\", \"vivid\"],\n                        \"default\": \"natural\",\n                        \"tooltip\": \"Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"1024x1024\", \"1024x1792\", \"1792x1024\"],\n                        \"default\": \"1024x1024\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        style=\"natural\",\n        quality=\"standard\",\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"dall-e-3\"\n\n        # build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/openai/images/generations\",\n                method=HttpMethod.POST,\n                request_model=OpenAIImageGenerationRequest,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=OpenAIImageGenerationRequest(\n                model=model,\n                prompt=prompt,\n                quality=quality,\n                size=size,\n                style=style,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale",
  "markdown": "# Recraft Creative Upscale - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Creative Upscale Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg) The Recraft Creative Upscale node uses Recraft’s API to increase image resolution while creatively enhancing and enriching image details.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image to be creatively upscaled |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | High-resolution image after creative upscaling |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftCreativeUpscaleNode(RecraftCrispUpscaleNode):\n    \"\"\"\n    Upscale image synchronously.\n    Enhances a given raster image using ‘creative upscale’ tool, boosting resolution with a focus on refining small details and faces.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_PATH = \"/proxy/recraft/images/creativeUpscale\"\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector",
  "markdown": "# Recraft Text to Vector - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Text to Vector Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg) The Recraft Text to Vector node lets you create high-quality vector graphics (SVG format) from text descriptions using Recraft’s API. It’s perfect for making logos, icons, and infinitely scalable illustrations.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text description of the vector graphic to generate |\n| substyle | select | \\-  | Vector style subtype |\n| size | select | 1024x1024 | Canvas size for the vector output |\n| n   | integer | 1   | Number of results to generate (1-6) |\n| seed | integer | 0   | Random seed value |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| negative\\_prompt | string | Elements to exclude from generation |\n| recraft\\_controls | Recraft Controls | Additional control parameters (colors, etc.) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| SVG | vector | Generated SVG vector graphic, connect to SaveSVG node to save |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass RecraftTextToVectorNode:\n    \"\"\"\n    Generates SVG synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.SVG,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"substyle\": (get_v3_substyles(RecraftStyleV3.vector_illustration),),\n                \"size\": (\n                    [res.value for res in RecraftImageSize],\n                    {\n                        \"default\": RecraftImageSize.res_1024x1024,\n                        \"tooltip\": \"The size of the generated image.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        substyle: str,\n        size: str,\n        n: int,\n        seed,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # create RecraftStyle so strings will be formatted properly (i.e. \"None\" will become None)\n        recraft_style = RecraftStyle(RecraftStyleV3.vector_illustration, substyle=substyle)\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/recraft/image_generation\",\n                method=HttpMethod.POST,\n                request_model=RecraftImageGenerationRequest,\n                response_model=RecraftImageGenerationResponse,\n            ),\n            request=RecraftImageGenerationRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                model=RecraftModel.recraftv3,\n                size=size,\n                n=n,\n                style=recraft_style.style,\n                substyle=recraft_style.substyle,\n                controls=controls_api,\n            ),\n            auth_token=auth_token,\n        )\n        response: RecraftImageGenerationResponse = operation.execute()\n        svg_data = []\n        for data in response.data:\n            svg_data.append(download_url_to_bytesio(data.url, timeout=1024))\n\n        return (SVG(svg_data),)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image",
  "markdown": "# Recraft Style - Realistic Image - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Style - Realistic Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg) The Recraft Style - Realistic Image node helps set up realistic photo styles for Recraft image generation, with various substyle options to control the visual characteristics of generated images.\n\n## Node Function\n\nThis node creates a style configuration object that guides Recraft’s image generation process towards realistic photo effects.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| substyle | select | None | Specific substyle of realistic photo (Required) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style config object to connect to Recraft generation nodes |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass RecraftStyleV3RealisticImageNode:\n    \"\"\"\n    Select realistic_image style and optional substyle.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.STYLEV3,)\n    RETURN_NAMES = (\"recraft_style\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_style\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_STYLE = RecraftStyleV3.realistic_image\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"substyle\": (get_v3_substyles(s.RECRAFT_STYLE),),\n            }\n        }\n\n    def create_style(self, substyle: str):\n        if substyle == \"None\":\n            substyle = None\n        return (RecraftStyle(self.RECRAFT_STYLE, substyle),)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image",
  "markdown": "# Recraft Image to Image - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Image to Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg) The Recraft Image to Image node uses Recraft’s API to generate new images based on a reference image and text prompts.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Reference image input |\n| prompt | string | \"\"  | Text description for the generated image |\n| n   | integer | 1   | Number of images to generate (1-6) |\n| seed | integer | 0   | Random seed value |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style settings for generated images |\n| negative\\_prompt | string | Elements to avoid in generated images |\n| recraft\\_controls | Recraft Controls | Additional controls like colors |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image result |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass RecraftImageToImageNode:\n    \"\"\"\n    Modify image based on prompt and strength.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"strength\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity.\"\n                    }\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt: str,\n        n: int,\n        strength: float,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            strength=round(strength, 2),\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n            controls=controls_api,\n            random_seed=seed,\n        )\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/imageToImage\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale",
  "markdown": "# Recraft Crisp Upscale - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Crisp Upscale Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg) The Recraft Crisp Upscale node uses Recraft’s API to improve image resolution and clarity.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image to be upscaled |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Upscaled and enhanced output image |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftCrispUpscaleNode:\n    \"\"\"\n    Upscale image synchronously.\n    Enhances a given raster image using ‘crisp upscale’ tool, increasing image resolution, making the image sharper and cleaner.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_PATH = \"/proxy/recraft/images/crispUpscale\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=self.RECRAFT_PATH,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image",
  "markdown": "# Recraft Text to Image - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Recraft Text to Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg) The Recraft Text to Image node lets you generate high-quality images from text prompts by directly connecting to Recraft AI’s image generation API to create images in various styles.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text description for the image |\n| size | select | 1024x1024 | Output image size |\n| n   | int | 1   | Number of images (1-6) |\n| seed | int | 0   | Random seed value |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Image style setting, default is “realistic photo” |\n| negative\\_prompt | string | Elements to exclude from generation |\n| recraft\\_controls | Recraft Controls | Additional control parameters (colors, etc.) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image(s) |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftTextToImageNode:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"size\": (\n                    [res.value for res in RecraftImageSize],\n                    {\n                        \"default\": RecraftImageSize.res_1024x1024,\n                        \"tooltip\": \"The size of the generated image.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        size: str,\n        n: int,\n        seed,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/recraft/image_generation\",\n                method=HttpMethod.POST,\n                request_model=RecraftImageGenerationRequest,\n                response_model=RecraftImageGenerationResponse,\n            ),\n            request=RecraftImageGenerationRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                model=RecraftModel.recraftv3,\n                size=size,\n                n=n,\n                style=recraft_style.style,\n                substyle=recraft_style.substyle,\n                style_id=recraft_style.style_id,\n                controls=controls_api,\n            ),\n            auth_token=auth_token,\n        )\n        response: RecraftImageGenerationResponse = operation.execute()\n        images = []\n        for data in response.data:\n            image = bytesio_to_image_tensor(\n                download_url_to_bytesio(data.url, timeout=1024)\n            )\n            if len(image.shape) < 4:\n                image = image.unsqueeze(0)\n            images.append(image)\n        output_image = torch.cat(images, dim=0)\n\n        return (output_image,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting",
  "markdown": "# Recraft Image Inpainting - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Image Inpainting Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg) The Recraft Image Inpainting node lets you modify specific areas of an image while keeping the rest unchanged. By providing an image, mask and text prompt, you can generate new content to fill the selected areas.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image to modify |\n| mask | mask | \\-  | Black and white mask defining areas to change |\n| prompt | string | \"\"  | Text describing what to generate in masked area |\n| n   | integer | 1   | Number of results to generate (1-6) |\n| seed | integer | 0   | Random seed value |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style settings for generated content |\n| negative\\_prompt | string | Elements to avoid in generated content |\n| recraft\\_controls | Recraft Controls | Additional controls like colors |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Modified image result |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftImageInpaintingNode:\n    \"\"\"\n    Modify image based on prompt and mask.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"mask\": (IO.MASK, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        mask: torch.Tensor,\n        prompt: str,\n        n: int,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n            random_seed=seed,\n        )\n\n        # prepare mask tensor\n        _, H, W, _ = image.shape\n        mask = mask.unsqueeze(-1)\n        mask = mask.movedim(-1,1)\n        mask = common_upscale(mask, width=W, height=H, upscale_method=\"nearest-exact\", crop=\"disabled\")\n        mask = mask.movedim(1,-1)\n        mask = (mask > 0.5).float()\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                mask=mask[i:i+1],\n                path=\"/proxy/recraft/images/inpaint\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n```"
},
{
  "url": "https://docs.comfy.org/specs/nodedef_json",
  "markdown": "# Node Definition JSON - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyNodeDefV2\",\n  \"definitions\": {\n    \"ComfyNodeDefV2\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"inputs\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"anyOf\": [\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": \"number\"\n                        }\n                      }\n                    ]\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"min\": {\n                    \"type\": \"number\"\n                  },\n                  \"max\": {\n                    \"type\": \"number\"\n                  },\n                  \"step\": {\n                    \"type\": \"number\"\n                  },\n                  \"display\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"slider\",\n                      \"number\",\n                      \"knob\"\n                    ]\n                  },\n                  \"control_after_generate\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"INT\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": \"number\"\n                        }\n                      }\n                    ]\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"min\": {\n                    \"type\": \"number\"\n                  },\n                  \"max\": {\n                    \"type\": \"number\"\n                  },\n                  \"step\": {\n                    \"type\": \"number\"\n                  },\n                  \"display\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"slider\",\n                      \"number\",\n                      \"knob\"\n                    ]\n                  },\n                  \"round\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"boolean\",\n                        \"const\": false\n                      }\n                    ]\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"FLOAT\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"label_on\": {\n                    \"type\": \"string\"\n                  },\n                  \"label_off\": {\n                    \"type\": \"string\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"BOOLEAN\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"type\": \"string\"\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"multiline\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"dynamicPrompts\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"defaultVal\": {\n                    \"type\": \"string\"\n                  },\n                  \"placeholder\": {\n                    \"type\": \"string\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"STRING\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {},\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"control_after_generate\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"image_upload\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"image_folder\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"input\",\n                      \"output\",\n                      \"temp\"\n                    ]\n                  },\n                  \"allow_batch\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"video_upload\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"remote\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"route\": {\n                        \"anyOf\": [\n                          {\n                            \"type\": \"string\",\n                            \"format\": \"uri\"\n                          },\n                          {\n                            \"type\": \"string\",\n                            \"pattern\": \"^\\\\/\"\n                          }\n                        ]\n                      },\n                      \"refresh\": {\n                        \"anyOf\": [\n                          {\n                            \"type\": \"number\",\n                            \"minimum\": -9007199254740991,\n                            \"maximum\": 9007199254740991\n                          },\n                          {\n                            \"type\": \"number\",\n                            \"maximum\": 9007199254740991,\n                            \"minimum\": -9007199254740991\n                          }\n                        ]\n                      },\n                      \"response_key\": {\n                        \"type\": \"string\"\n                      },\n                      \"query_params\": {\n                        \"type\": \"object\",\n                        \"additionalProperties\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"refresh_button\": {\n                        \"type\": \"boolean\"\n                      },\n                      \"control_after_refresh\": {\n                        \"type\": \"string\",\n                        \"enum\": [\n                          \"first\",\n                          \"last\"\n                        ]\n                      },\n                      \"timeout\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0\n                      },\n                      \"max_retries\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0\n                      }\n                    },\n                    \"required\": [\n                      \"route\"\n                    ],\n                    \"additionalProperties\": false\n                  },\n                  \"options\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": [\n                        \"string\",\n                        \"number\"\n                      ]\n                    }\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"COMBO\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {},\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              }\n            ]\n          }\n        },\n        \"outputs\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"index\": {\n                \"type\": \"number\"\n              },\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"is_list\": {\n                \"type\": \"boolean\"\n              },\n              \"options\": {\n                \"type\": \"array\"\n              },\n              \"tooltip\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"index\",\n              \"name\",\n              \"type\",\n              \"is_list\"\n            ],\n            \"additionalProperties\": false\n          }\n        },\n        \"hidden\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {}\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"display_name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"category\": {\n          \"type\": \"string\"\n        },\n        \"output_node\": {\n          \"type\": \"boolean\"\n        },\n        \"python_module\": {\n          \"type\": \"string\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\"\n        },\n        \"experimental\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"inputs\",\n        \"outputs\",\n        \"name\",\n        \"display_name\",\n        \"description\",\n        \"category\",\n        \"output_node\",\n        \"python_module\"\n      ],\n      \"additionalProperties\": false\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image",
  "markdown": "# Recraft Vectorize Image - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Recraft Vectorize Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg) The Recraft Vectorize Image node uses Recraft’s API to convert raster images (like photos, PNGs or JPEGs) into vector SVG format.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | Image | \\-  | Input image to be converted to vector |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| SVG | Vector | Converted SVG vector graphic, needs to be connected to SaveSVG node to save |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass RecraftVectorizeImageNode:\n    \"\"\"\n    Generates SVG synchronously from an input image.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.SVG,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        svgs = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/vectorize\",\n                auth_token=auth_token,\n            )\n            svgs.append(SVG(sub_bytes))\n            pbar.update(1)\n\n        return (SVG.combine_all(svgs), )\n\n```"
},
{
  "url": "https://docs.comfy.org/specs/nodedef_json_1_0",
  "markdown": "# Node Definition JSON 1.0 - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyNodeDefV1\",\n  \"definitions\": {\n    \"ComfyNodeDefV1\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"input\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"required\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"INT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"FLOAT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"round\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"boolean\",\n                                    \"const\": false\n                                  }\n                                ]\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"BOOLEAN\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"label_on\": {\n                                \"type\": \"string\"\n                              },\n                              \"label_off\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"STRING\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"string\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"multiline\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"dynamicPrompts\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultVal\": {\n                                \"type\": \"string\"\n                              },\n                              \"placeholder\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": [\n                            \"string\",\n                            \"number\"\n                          ]\n                        }\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"COMBO\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            \"optional\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"INT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"FLOAT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"round\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"boolean\",\n                                    \"const\": false\n                                  }\n                                ]\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"BOOLEAN\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"label_on\": {\n                                \"type\": \"string\"\n                              },\n                              \"label_off\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"STRING\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"string\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"multiline\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"dynamicPrompts\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultVal\": {\n                                \"type\": \"string\"\n                              },\n                              \"placeholder\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": [\n                            \"string\",\n                            \"number\"\n                          ]\n                        }\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"COMBO\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            \"hidden\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {}\n            }\n          },\n          \"additionalProperties\": false\n        },\n        \"output\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"anyOf\": [\n              {\n                \"type\": \"string\"\n              },\n              {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": [\n                    \"string\",\n                    \"number\"\n                  ]\n                }\n              }\n            ]\n          }\n        },\n        \"output_is_list\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"output_name\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"output_tooltips\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"display_name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"category\": {\n          \"type\": \"string\"\n        },\n        \"output_node\": {\n          \"type\": \"boolean\"\n        },\n        \"python_module\": {\n          \"type\": \"string\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\"\n        },\n        \"experimental\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"display_name\",\n        \"description\",\n        \"category\",\n        \"output_node\",\n        \"python_module\"\n      ],\n      \"additionalProperties\": false\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/specs/workflow_json_0.4",
  "markdown": "# Workflow JSON 0.4 - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyWorkflow0_4\",\n  \"definitions\": {\n    \"ComfyWorkflow0_4\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"last_node_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"last_link_id\": {\n          \"type\": \"number\"\n        },\n        \"nodes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"size\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"flags\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"collapsed\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"pinned\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"allow_interaction\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"horizontal\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"skip_repeated_outputs\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"order\": {\n                \"type\": \"number\"\n              },\n              \"mode\": {\n                \"type\": \"number\"\n              },\n              \"inputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"link\": {\n                      \"type\": [\n                        \"number\",\n                        \"null\"\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"outputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"links\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        {\n                          \"type\": \"null\"\n                        }\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"properties\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"Node name for S&R\": {\n                    \"type\": \"string\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"widgets_values\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\"\n                  },\n                  {\n                    \"type\": \"object\",\n                    \"additionalProperties\": {}\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"bgcolor\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"type\",\n              \"pos\",\n              \"size\",\n              \"flags\",\n              \"order\",\n              \"mode\",\n              \"properties\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"links\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"array\",\n            \"minItems\": 6,\n            \"maxItems\": 6,\n            \"items\": [\n              {\n                \"type\": \"number\"\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"string\"\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    }\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              }\n            ]\n          }\n        },\n        \"groups\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"title\": {\n                \"type\": \"string\"\n              },\n              \"bounding\": {\n                \"type\": \"array\",\n                \"minItems\": 4,\n                \"maxItems\": 4,\n                \"items\": [\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"font_size\": {\n                \"type\": \"number\"\n              },\n              \"locked\": {\n                \"type\": \"boolean\"\n              }\n            },\n            \"required\": [\n              \"title\",\n              \"bounding\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"config\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"links_ontop\": {\n                      \"type\": \"boolean\"\n                    },\n                    \"align_to_grid\": {\n                      \"type\": \"boolean\"\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"extra\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"ds\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"scale\": {\n                          \"type\": \"number\"\n                        },\n                        \"offset\": {\n                          \"anyOf\": [\n                            {\n                              \"type\": \"object\",\n                              \"properties\": {\n                                \"0\": {\n                                  \"type\": \"number\"\n                                },\n                                \"1\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              \"required\": [\n                                \"0\",\n                                \"1\"\n                              ],\n                              \"additionalProperties\": true\n                            },\n                            {\n                              \"type\": \"array\",\n                              \"minItems\": 2,\n                              \"maxItems\": 2,\n                              \"items\": [\n                                {\n                                  \"type\": \"number\"\n                                },\n                                {\n                                  \"type\": \"number\"\n                                }\n                              ]\n                            }\n                          ]\n                        }\n                      },\n                      \"required\": [\n                        \"scale\",\n                        \"offset\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"info\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"name\": {\n                          \"type\": \"string\"\n                        },\n                        \"author\": {\n                          \"type\": \"string\"\n                        },\n                        \"description\": {\n                          \"type\": \"string\"\n                        },\n                        \"version\": {\n                          \"type\": \"string\"\n                        },\n                        \"created\": {\n                          \"type\": \"string\"\n                        },\n                        \"modified\": {\n                          \"type\": \"string\"\n                        },\n                        \"software\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"required\": [\n                        \"name\",\n                        \"author\",\n                        \"description\",\n                        \"version\",\n                        \"created\",\n                        \"modified\",\n                        \"software\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"linkExtensions\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"parentId\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    },\n                    \"reroutes\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          },\n                          \"pos\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"0\": {\n                                    \"type\": \"number\"\n                                  },\n                                  \"1\": {\n                                    \"type\": \"number\"\n                                  }\n                                },\n                                \"required\": [\n                                  \"0\",\n                                  \"1\"\n                                ],\n                                \"additionalProperties\": true\n                              },\n                              {\n                                \"type\": \"array\",\n                                \"minItems\": 2,\n                                \"maxItems\": 2,\n                                \"items\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"number\"\n                                  }\n                                ]\n                              }\n                            ]\n                          },\n                          \"linkIds\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              {\n                                \"type\": \"null\"\n                              }\n                            ]\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"pos\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"version\": {\n          \"type\": \"number\"\n        },\n        \"models\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"url\": {\n                \"type\": \"string\",\n                \"format\": \"uri\"\n              },\n              \"hash\": {\n                \"type\": \"string\"\n              },\n              \"hash_type\": {\n                \"type\": \"string\"\n              },\n              \"directory\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"name\",\n              \"url\",\n              \"directory\"\n            ],\n            \"additionalProperties\": false\n          }\n        }\n      },\n      \"required\": [\n        \"last_node_id\",\n        \"last_link_id\",\n        \"nodes\",\n        \"links\",\n        \"version\"\n      ],\n      \"additionalProperties\": true\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration",
  "markdown": "# Recraft Style - Digital Illustration - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Style Digital Illustration Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg) This node creates a style configuration object that guides Recraft’s image generation process towards a digital illustration look.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| substyle | select | None | Specific substyle of digital illustration |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style config object to connect to Recraft generation nodes |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftStyleV3DigitalIllustrationNode(RecraftStyleV3RealisticImageNode):\n    \"\"\"\n    Select digital_illustration style and optional substyle.\n    \"\"\"\n\n    RECRAFT_STYLE = RecraftStyleV3.digital_illustration\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster",
  "markdown": "# Recraft Style - Logo Raster - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Recraft Style - Logo Raster Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg) This node creates a style configuration object that guides Recraft’s image generation process toward professional logo design effects. By selecting different substyles, you can define the design style, complexity and use cases of the generated logo.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| substyle | Selection | \\-  | Specific substyle for logo raster (Required) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style configuration object, connects to Recraft generation node |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\nclass RecraftStyleV3LogoRasterNode(RecraftStyleV3RealisticImageNode):\n    \"\"\"\n    Select vector_illustration style and optional substyle.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"substyle\": (get_v3_substyles(s.RECRAFT_STYLE, include_none=False),),\n            }\n        }\n\n    RECRAFT_STYLE = RecraftStyleV3.logo_raster\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background",
  "markdown": "# Recraft Remove Background - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Remove Background Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg) The Recraft Remove Background node uses Recraft’s API to intelligently detect and remove image backgrounds, creating images with transparent backgrounds and corresponding alpha masks.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image to remove background from |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Image with background removed (with alpha channel) |\n| MASK | mask | Mask of the main subject (white areas are preserved) |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftRemoveBackgroundNode:\n    \"\"\"\n    Remove background from image, and return processed image and mask.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE, IO.MASK)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/removeBackground\",\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        # use alpha channel as masks, in B,H,W format\n        masks_tensor = images_tensor[:,:,:,-1:].squeeze(-1)\n        return (images_tensor, masks_tensor)\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/3d/hunyuan3D-2",
  "markdown": "# ComfyUI Hunyuan3D-2 Examples - ComfyUI\n\n## Hunyuan3D 2.0 Introduction\n\n  ![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif) ![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif) [Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model released by Tencent, capable of generating high-fidelity 3D models with high-resolution texture maps through text or images. Hunyuan3D 2.0 adopts a two-stage generation approach, first generating a geometry model without textures, then synthesizing high-resolution texture maps. This effectively separates the complexity of shape and texture generation. Below are the two core components of Hunyuan3D 2.0:\n\n1.  **Geometry Generation Model (Hunyuan3D-DiT)**: Based on a flow diffusion Transformer architecture, it generates untextured geometric models that precisely match input conditions.\n2.  **Texture Generation Model (Hunyuan3D-Paint)**: Combines geometric conditions and multi-view diffusion techniques to add high-resolution textures to models, supporting PBR materials.\n\n**Key Advantages**\n\n*   **High-Precision Generation**: Sharp geometric structures, rich texture colors, support for PBR material generation, achieving near-realistic lighting effects.\n*   **Diverse Usage Methods**: Provides code calls, Blender plugins, Gradio applications, and online experience through the official website, suitable for different user needs.\n*   **Lightweight and Compatibility**: The Hunyuan3D-2mini model requires only 5GB VRAM, the standard version needs 6GB VRAM for shape generation, and the complete process (shape + texture) requires only 12GB VRAM.\n\nRecently (March 18, 2025), Hunyuan3D 2.0 also introduced a multi-view shape generation model (Hunyuan3D-2mv), which supports generating more detailed geometric structures from inputs at different angles. This example includes three workflows:\n\n*   Using Hunyuan3D-2mv with multiple view inputs to generate 3D models\n*   Using Hunyuan3D-2mv-turbo with multiple view inputs to generate 3D models\n*   Using Hunyuan3D-2 with a single view input to generate 3D models\n\n## ComfyUI Hunyuan3D-2mv Workflow Example\n\nIn the Hunyuan3D-2mv workflow, we’ll use multi-view images to generate a 3D model. Note that multiple view images are not mandatory in this workflow - you can use only the `front` view image to generate a 3D model.\n\n### 1\\. Workflow\n\nPlease download the images below and drag into ComfyUI to load the workflow. ![Hunyuan3D-2mv workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/hunyuan-3d-multiview-elf.webp) Download the images below we will use them as input images.\n\n![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/front.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/left.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/back.png)\n\n### 2\\. Manual Model Installation\n\nDownload the model below and save it to the corresponding ComfyUI folder\n\n*   hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv.safetensors`\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── hunyuan3d-dit-v2-mv.safetensors  // renamed file\n```\n\n### 3\\. Steps to Run the Workflow\n\n![ComfyUI hunyuan3d_2mv](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg)\n\n1.  Ensure that the Image Only Checkpoint Loader(img2vid model) has loaded our downloaded and renamed `hunyuan3d-dit-v2-mv.safetensors` model\n2.  Load the corresponding view images in each of the `Load Image` nodes\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\nIf you need to add more views, make sure to load other view images in the `Hunyuan3Dv2ConditioningMultiView` node, and ensure that you load the corresponding view images in the `Load Image` nodes.\n\n## Hunyuan3D-2mv-turbo Workflow\n\nIn the Hunyuan3D-2mv-turbo workflow, we’ll use the Hunyuan3D-2mv-turbo model to generate 3D models. This model is a step distillation version of Hunyuan3D-2mv, allowing for faster 3D model generation. In this version of the workflow, we set `cfg` to 1.0 and add a `flux guidance` node to control the `distilled cfg` generation.\n\n### 1\\. Workflow\n\nPlease download the images below and drag into ComfyUI to load the workflow. ![Hunyuan3D-2mv-turbo workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/hunyuan-3d-turbo.webp) Download the images below we will use them as input images.\n\n![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/front.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/right.png)\n\n### 2\\. Manual Model Installation\n\nDownload the model below and save it to the corresponding ComfyUI folder\n\n*   hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv-turbo.safetensors`\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── hunyuan3d-dit-v2-mv-turbo.safetensors  // renamed file\n```\n\n### 3\\. Steps to Run the Workflow\n\n![ComfyUI hunyuan3d_2mv_turbo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg)\n\n1.  Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2-mv-turbo.safetensors` model\n2.  Load the corresponding view images in each of the `Load Image` nodes\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Hunyuan3D-2 Single View Workflow\n\nIn the Hunyuan3D-2 workflow, we’ll use the Hunyuan3D-2 model to generate 3D models. This model is not a multi-view model. In this workflow, we use the `Hunyuan3Dv2Conditioning` node instead of the `Hunyuan3Dv2ConditioningMultiView` node.\n\n### 1\\. Workflow\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Hunyuan3D-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d-non-multiview-train.webp) Download the image below we will use it as input image. ![ComfyUI Hunyuan 3D 2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan_3d_v2_non_multiview_train.png) \n\n### 2\\. Manual Model Installation\n\nDownload the model below and save it to the corresponding ComfyUI folder\n\n*   hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2.safetensors`\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── hunyuan3d-dit-v2.safetensors  // renamed file\n```\n\n### 3\\. Steps to Run the Workflow\n\n![ComfyUI hunyuan3d_2](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg)\n\n1.  Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2.safetensors` model\n2.  Load the image in the `Load Image` node\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\nBelow are ComfyUI community resources related to Hunyuan3D-2\n\n*   [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)\n*   [Kijai/Hunyuan3D-2\\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)\n*   [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)\n\n## Hunyuan3D 2.0 Open-Source Model Series\n\nCurrently, Hunyuan3D 2.0 has open-sourced multiple models covering the complete 3D generation process. You can visit [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) for more information. **Hunyuan3D-2mini Series**\n\n| Model | Description | Date | Parameters | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-mini | Mini Image to Shape Model | 2025-03-18 | 0.6B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini) |\n\n**Hunyuan3D-2mv Series**\n\n| Model | Description | Date | Parameters | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-mv-Fast | Guidance Distillation Version, can halve DIT inference time | 2025-03-18 | 1.1B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast) |\n| Hunyuan3D-DiT-v2-mv | Multi-view Image to Shape Model, suitable for 3D creation requiring multiple angles to understand the scene | 2025-03-18 | 1.1B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv) |\n\n**Hunyuan3D-2 Series**\n\n| Model | Description | Date | Parameters | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-0-Fast | Guidance Distillation Model | 2025-02-03 | 1.1B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast) |\n| Hunyuan3D-DiT-v2-0 | Image to Shape Model | 2025-01-21 | 1.1B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0) |\n| Hunyuan3D-Paint-v2-0 | Texture Generation Model | 2025-01-21 | 1.3B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0) |\n| Hunyuan3D-Delight-v2-0 | Image Delight Model | 2025-01-21 | 1.3B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0) |"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image",
  "markdown": "# Recraft Text to Image API Node ComfyUI Official Example\n\nThe [Recraft Text to Image](https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image) node allows you to create high-quality images in various styles using Recraft AI’s image generation technology based on text descriptions. In this guide, we’ll show you how to set up a text-to-image workflow using this node.\n\n### 1\\. Download the Workflow File\n\nThe workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow. ![Recraft Text to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/recraft/t2i/recraft_t2i.png)\n\n### 2\\. Follow the Steps to Run the Workflow\n\n![Recraft Text to Image Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg) Follow these numbered steps to run the basic workflow:\n\n1.  (Optional) Change the `Recraft Color RGB` in the `Color` node to your desired color\n2.  (Optional) Modify the `Recraft Style` node to control the visual style, such as digital art, realistic photo, or logo design. This group includes other style nodes you can enable as needed\n3.  (Optional) Edit the `prompt` parameter in the `Recraft Text to Image` node. You can also change the `size` parameter\n4.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n5.  After the API returns the result, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory\n\n> (Optional) We’ve included a **Convert to SVG** group in the workflow. Since the `Recraft Vectorize Image` node in this group consumes additional credits, enable it only when you need to convert the generated image to SVG format\n\n### 3\\. Additional Notes\n\n*   **Recraft Style**: Offers various preset styles like realistic photos, digital art, and logo designs\n*   **Seed Parameter**: Only used to determine if the node should run again, the actual generation result is not affected by the seed value\n\nCheck the following documentation for detailed parameter settings of the nodes\n\n[\n\n## Recraft Text to Image Node Documentation\n\nDocumentation for the Recraft Text to Image API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)[\n\n## Recraft Style Node Documentation\n\nDocumentation for the Recraft Style - Realistic Image API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)[\n\n## Recraft Controls Node Documentation\n\nDocumentation for the Recraft Controls API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/google/gemini",
  "markdown": "# Google Gemini API Node ComfyUI Official Example\n\nGoogle Gemini is a powerful AI model developed by Google, supporting conversational and text generation functions. Currently, ComfyUI has integrated the Google Gemini API, allowing you to directly use the related nodes in ComfyUI to complete conversational functions. In this guide, we will walk you through completing the corresponding conversational functionality.\n\n## Google Gemini Chat Workflow\n\n### 1\\. Workflow File Download\n\nPlease download the Json file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/google/api_google_gemini.json)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![OpenAI Chat Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg)\n\nYou can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Load Image` node, load the image you need AI to interpret\n2.  (Optional) If needed, you can modify the prompt in `Google Gemini` to have AI execute specific tasks\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the conversation.\n4.  After waiting for the API to return results, you can view the corresponding AI returned content in the `Preview Any` node.\n\n### 3\\. Additional Notes\n\n*   Currently, the file input node `Gemini Input Files` requires files to be uploaded to the `ComfyUI/input/` directory first. This node is being improved, and we will modify the template after updates\n*   The workflow provides an example using `Batch Images` for input. If you have multiple images that need AI interpretation, you can refer to the step diagram and use right-click to set the corresponding node mode to `Always` to enable it"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/ideogram/ideogram-v3",
  "markdown": "# ComfyUI Ideogram 3.0 API Node Official Examples\n\nIdeogram 3.0 is a powerful text-to-image model by Ideogram, known for its photorealistic quality, accurate text rendering, and consistent style control. The [Ideogram V3](https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3) node currently supports two modes:\n\n*   Text-to-Image mode\n*   Image Editing mode (when both image and mask inputs are provided)\n\n## Ideogram 3.0 Node Documentation\n\nCheck the following documentation for detailed node parameter settings:\n\n*   [Ideogram V3](https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)\n\n## Ideogram 3.0 API Node Text-to-Image Mode\n\nWhen using [Ideogram V3](https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3) without image and mask inputs, the node operates in Text-to-Image mode.\n\n### 1\\. Download Workflow File\n\nDownload and drag the following file into ComfyUI to load the workflow: ![Ideogram 3.0 ComfyUI Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/ideogram/v3/ideogram_v3_t2i.png)\n\n### 2\\. Complete the Workflow Steps\n\n![Ideogram 3.0 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg) Follow the numbered steps to complete the basic workflow:\n\n1.  Enter your image description in the `prompt` field of the `Ideogram V3` node\n2.  Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image\n3.  After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\n## Ideogram 3.0 API Node Image Editing Mode\n\n\\[To be updated\\]"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image",
  "markdown": "# Luma Image to Image API Node ComfyUI Official Example\n\nThe [Luma Image to Image](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image) node allows you to modify existing images based on text prompts using Luma AI technology, while preserving certain features and structures from the original image. In this guide, we’ll show you how to set up an image-to-image workflow using this node.\n\n## Luma Image to Image Node Documentation\n\nCheck the following documentation for detailed node parameter settings:\n\n[\n\n## Luma Image to Image Node Documentation\n\nLuma Image to Image API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)\n\n### 1\\. Download Workflow File\n\nDownload and drag the following image into ComfyUI to load the workflow (workflow information is included in the image metadata): ![Luma Image to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/luma_i2i.png) Download this image to use as input: ![Luma Image to Image Workflow Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/input.png)\n\n### 2\\. Complete the Workflow Steps\n\n![Luma Image to Image Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg) Follow these numbered steps:\n\n1.  Click **Upload** on the `Load Image` node to upload your input image\n2.  (Optional) Modify the workflow prompts\n3.  (Optional) Adjust `image_weight` to change input image influence (lower values stay closer to original)\n4.  Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image\n5.  After API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\n### 3\\. Results with Different `image_weight` Values\n\n![Weight Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/i2i_image_weight.jpg)"
},
{
  "url": "https://docs.comfy.org/installation/desktop/macos",
  "markdown": "# MacOS Desktop Version - ComfyUI\n\n**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files. ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop).\n\nThis tutorial will guide you through the software installation process and explain related configuration details.\n\n## ComfyUI Desktop (MacOS) Download\n\nPlease click the button below to download the installation package for MacOS **ComfyUI Desktop**\n\n[\n\nDownload for MacOS\n\n](https://download.comfy.org/mac/dmg/arm64)\n\n## Install via Homebrew\n\nComfyUI Desktop can also be installed via [Homebrew](https://brew.sh/):\n\n## ComfyUI Desktop Installation Steps\n\nDouble-click the downloaded installation package file. As shown in the image, drag the **ComfyUI** application into the **Applications** folder following the arrow ![ComfyUI Installation Package](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0.png) If your folder shows as below with a prohibition sign on the icon after opening the installation package, it means your current system version is not compatible with **ComfyUI Desktop** ![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0-1.png) Then find the **ComfyUI icon** in **Launchpad** and click it to enter ComfyUI initialization settings ![ComfyUI Lanchpad](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-1.jpg) \n\n## ComfyUI Desktop Initialization Process\n\n## First Image Generation\n\nAfter successful installation, you can refer to the section below to start your ComfyUI journey~\n\n[\n\n## First Image Generation\n\nThis tutorial will guide you through your first model installation and text-to-image generation\n\n\n\n](https://docs.comfy.org/get_started/first_generation)\n\n## How to Update ComfyUI Desktop\n\nCurrently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings ![ComfyUI Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg) You can also choose to manually check for available updates in the `Menu` —> `Help` —> `Check for Updates` ![ComfyUI Desktop Check for Updates](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_check_for_updates.jpg)\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI’s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 lora/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## Desktop Python Environment\n\nThe desktop installation will create a Python virtual environment in your chosen installation directory, typically a hidden `.venv` folder. If you need to handle dependencies for ComfyUI plugins, you’ll need to do so within this environment. Using the system command line directly risks installing dependencies to the system environment, so please follow the instructions below to activate the appropriate environment.\n\n### How to use the Desktop Python environment?\n\nYou can use the built-in terminal in the desktop app to access the Python environment. ![ComfyUI Desktop Terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_terminal.jpg) \n\n1.  Click the icon in the menu bar to open the bottom panel\n2.  Click `Terminal` to open the terminal\n3.  If you want to check the Python installation location for the corresponding environment, you can use the following command\n\n```\n  python -c \"import sys; print(sys.executable)\"\n```\n\n## How to Uninstall ComfyUI Desktop\n\nFor **ComfyUI Desktop**, you can directly delete **ComfyUI** from the **Applications** folder If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:\n\n```\n~/Library/Application Support/ComfyUI\n```\n\nThe above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:\n\n*   models files\n*   custom nodes\n*   input/output directories\n\n## Troubleshooting\n\n### ​Error identification​\n\nIf installation fails, you should see the following screen ![ComfyUI Installation Failed](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg) It is recommended to take these steps to find the error cause:\n\n1.  Click `Show Terminal` to view error output\n2.  Click `Open Logs` to view installation logs\n3.  Visit official forum to search for error reports\n4.  Click `Reinstall` to try reinstalling\n\nBefore submitting feedback, it’s recommended to provide the **error output** and **log files** to tools like **GPT** ![ComfyUI Installation Failed - Error Log](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg) ![ComfyUI Installation Failed - GPT Feedback](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg) As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.\n\n### Feedback Installation Failure\n\nIf you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:\n\n*   Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)\n*   Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)\n\nWhen submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:\n\n1.  Log Files\n\n| Filename | Description | Location |\n| --- | --- | --- |\n| main.log | Contains logs related to desktop application and server startup from the Electron process |     |\n| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output |     |\n\n![ComfyUI Log Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)\n\n2.  Configuration Files\n\n| Filename | Description | Location |\n| --- | --- | --- |\n| extra\\_model\\_paths.yaml | Contains additional paths where ComfyUI will search for models and custom nodes |     |\n| config.json | Contains application configuration. This file should not be edited directly |     |\n\n![ComfyUI Config Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)"
},
{
  "url": "https://docs.comfy.org/installation/desktop/linux",
  "markdown": "# Linux Desktop Version - ComfyUI\n\nWhen Linux desktop packages become available, you can configure external model paths:\n\n## Adding External Model Paths\n\nIf you have models stored in other locations on your computer outside the ComfyUI installation directory, you can add them to ComfyUI by configuring the `extra_models_config.yaml` file. For ComfyUI Desktop, this file is located at:\n\n*   On Windows: `C:\\Users\\<YOUR_USERNAME>\\AppData\\Roaming\\ComfyUI\\extra_models_config.yaml`\n*   On macOS: `~/Library/Application Support/ComfyUI/extra_models_config.yaml`\n*   On Linux: `~/.config/ComfyUI/extra_models_config.yaml`\n\nFor detailed instructions, see [Models documentation](https://docs.comfy.org/development/core-concepts/models#adding-external-model-paths)"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image",
  "markdown": "# Stability AI Stable Diffusion 3.5 API Node ComfyUI Official Example\n\nThe [Stability AI Stable Diffusion 3.5 Image](https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image) node allows you to use Stability AI’s Stable Diffusion 3.5 model to create high-quality, detail-rich image content through text prompts or reference images. In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.\n\n## Stability AI Stable Diffusion 3.5 Text-to-Image Workflow\n\n### 1\\. Workflow File Download\n\nThe image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow. ![Stability AI Stable Diffusion 3.5 Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_diffusion_3-5-t2i.png)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Stability AI Stable Diffusion 3.5 Text-to-Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg) You can follow the numbered steps in the image to complete the basic text-to-image workflow:\n\n1.  (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to input your desired image description. More detailed prompts often result in better image quality.\n2.  (Optional) Select the `model` parameter to choose which SD 3.5 model version to use.\n3.  (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics, such as “cinematic” or “anime”. Select “None” to not apply any specific style.\n4.  (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you don’t want to appear in the generated image.\n5.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.\n6.  After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.\n\n### 3\\. Additional Notes\n\n*   **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions lead to better results. Can include elements like scene, subject, colors, lighting, and style.\n*   **CFG Scale**: Controls how closely the generator follows the prompt. Higher values make the image more closely match the prompt description, but too high may result in oversaturated or unnatural results.\n*   **Style Preset**: Offers various preset styles for quickly defining the overall style of the image.\n*   **Negative Prompt**: Used to specify elements you don’t want to appear in the generated image.\n*   **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during creation.\n*   Currently the `Load Image` node is in “Bypass” mode. To enable it, refer to the step guide and right-click the node to set “Mode” to “Always” to enable input, switching to image-to-image mode.\n*   `image_denoise` has no effect when there is no input image.\n\n## Stability AI Stable Diffusion 3.5 Image-to-Image Workflow\n\n### 1\\. Workflow File Download\n\nThe image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow. ![Stability AI Stable Diffusion 3.5 Image-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/stable_diffusion_3_5-i2i.png) Download the image below to use as input !\\[Stability AI Stable Diffusion 3.5 Image-to-Image Workflow Input Image\\](![Stability AI Stable Diffusion 3.5 图生图工作流输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/input.jpg)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Stability AI Stable Diffusion 3.5 Image-to-Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg) You can follow the numbered steps in the image to complete the image-to-image workflow:\n\n1.  Load a reference image through the `Load Image` node, which will serve as the basis for generation.\n2.  (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to describe elements you want to change or enhance in the reference image.\n3.  (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics.\n4.  (Optional|Important) Adjust the `image_denoise` parameter (range 0.0-1.0) to control how much the original image is modified:\n    *   Values closer to 0.0 make the generated image more similar to the input reference image (at 0.0, it’s basically identical to the original)\n    *   Values closer to 1.0 make the generated image more like pure text-to-image generation (at 1.0, it’s as if no reference image was provided)\n5.  (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you don’t want to appear in the generated image.\n6.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.\n7.  After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.\n\n### 3\\. Additional Notes\n\nThe image below shows a comparison of results with and without input image using the same parameter settings: ![Stability AI Stable Diffusion 3.5 With/Without Image Input Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg) **Image Denoise**: This parameter determines how much of the original image’s features are preserved during generation. It’s the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths: ![Stability AI Stable Diffusion 3.5 Image-to-Image Denoise Strength Explanation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg)\n\n*   **Reference Image Selection**: Choosing images with clear subjects and good composition usually yields better results.\n*   **Prompt Tips**: In image-to-image mode, prompts should focus more on elements you want to change or enhance, rather than describing everything already present in the image.\n*   **Mode Switching**: When an input image is provided, the node automatically switches from text-to-image mode to image-to-image mode, and aspect ratio parameters are ignored.\n\nYou can refer to the documentation below to understand detailed parameter settings for the corresponding node\n\n[\n\n## Stability Stable Diffusion 3.5 Image Node Documentation\n\nStability Stable Diffusion 3.5 Image API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fdelete-a-publisher",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video",
  "markdown": "# Kling Text to Video - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg) The Kling Text to Video node connects to Kling’s API service to generate videos from text descriptions. Users simply provide descriptive text to create corresponding video content.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | String | \"\"  | Text prompt describing desired video content |\n| negative\\_prompt | String | \"\"  | Elements to avoid in the video |\n| cfg\\_scale | Float | 7.0 | Controls how closely to follow the prompt |\n| model\\_name | Select | ”kling-v2-master” | Video generation model to use |\n| aspect\\_ratio | Select | AspectRatio enum | Output video aspect ratio |\n| duration | Select | Duration enum | Length of generated video |\n| mode | Select | Mode enum | Video generation mode |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n| Kling ID | String | Task identifier |\n| Duration (sec) | String | Video length in seconds |\n\n## How It Works\n\nThe node sends text prompts to Kling’s API server, which processes and returns the generated video. The process includes initial request and status polling. When complete, the node downloads and outputs the video. Users can control the generation by adjusting parameters like negative prompts, configuration scale, and video properties. The system validates prompt length to ensure API compliance.\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass KlingTextToVideoNode(KlingNodeBase):\n    \"\"\"Kling Text to Video Node\"\"\"\n\n    @staticmethod\n    def poll_for_task_status(task_id: str, auth_token: str) -> KlingText2VideoResponse:\n        \"\"\"Polls the Kling API endpoint until the task reaches a terminal state.\"\"\"\n        polling_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"{PATH_TEXT_TO_VIDEO}/{task_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=KlingText2VideoResponse,\n            ),\n            completed_statuses=[\n                TaskStatus.succeed.value,\n            ],\n            failed_statuses=[TaskStatus.failed.value],\n            status_extractor=lambda response: (\n                response.data.task_status.value\n                if response.data and response.data.task_status\n                else None\n            ),\n            auth_token=auth_token,\n        )\n        return polling_operation.execute()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"negative_prompt\", multiline=True\n                ),\n                \"model_name\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"model_name\",\n                    enum_type=ModelName,\n                    default=\"kling-v2-master\",\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingText2VideoRequest, \"cfg_scale\"\n                ),\n                \"mode\": model_field_to_node_input(\n                    IO.COMBO, KlingText2VideoRequest, \"mode\", enum_type=Mode\n                ),\n                \"duration\": model_field_to_node_input(\n                    IO.COMBO, KlingText2VideoRequest, \"duration\", enum_type=Duration\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (\"VIDEO\", \"STRING\", \"STRING\")\n    RETURN_NAMES = (\"VIDEO\", \"Kling ID\", \"Duration (sec)\")\n    DESCRIPTION = \"Kling Text to Video Node\"\n\n    def api_call(\n        self,\n        prompt: str,\n        negative_prompt: str,\n        model_name: str,\n        cfg_scale: float,\n        mode: str,\n        duration: int,\n        aspect_ratio: str,\n        camera_control: Optional[CameraControl] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile, str, str]:\n        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_T2V)\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_TEXT_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=KlingText2VideoRequest,\n                response_model=KlingText2VideoResponse,\n            ),\n            request=KlingText2VideoRequest(\n                prompt=prompt if prompt else None,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                duration=Duration(duration),\n                mode=Mode(mode),\n                model_name=ModelName(model_name),\n                cfg_scale=cfg_scale,\n                aspect_ratio=AspectRatio(aspect_ratio),\n                camera_control=camera_control,\n            ),\n            auth_token=auth_token,\n        )\n\n        initial_response = initial_operation.execute()\n        if not is_valid_initial_response(initial_response):\n            error_msg = f\"Kling initial request failed. Code: {initial_response.code}, Message: {initial_response.message}, Data: {initial_response.data}\"\n            logging.error(error_msg)\n            raise KlingApiError(error_msg)\n\n        task_id = initial_response.data.task_id\n        final_response = self.poll_for_task_status(task_id, auth_token)\n        if not is_valid_video_response(final_response):\n            error_msg = (\n                f\"Kling task {task_id} succeeded but no video data found in response.\"\n            )\n            logging.error(error_msg)\n            raise KlingApiError(error_msg)\n\n        video = final_response.data.task_result.videos[0]\n        logging.debug(\"Kling task %s succeeded. Video URL: %s\", task_id, video.url)\n        return (\n            download_url_to_video_output(video.url),\n            str(video.id),\n            str(video.duration),\n        )\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video",
  "markdown": "# Kling Start-End Frame to Video - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Start-End Frame to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg) The Kling Start-End Frame to Video node lets you create smooth video transitions between two images. It automatically generates all the intermediate frames to produce a fluid transformation.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| start\\_frame | Image | Starting image for the video |\n| end\\_frame | Image | Ending image for the video |\n| prompt | String | Text describing video content and transition |\n| negative\\_prompt | String | Elements to avoid in the video |\n| cfg\\_scale | Float | Controls how closely to follow the prompt |\n| aspect\\_ratio | Select | Output video aspect ratio |\n| mode | Select | Video generation settings (mode/duration/model) |\n\n### Mode Options\n\nAvailable mode combinations:\n\n*   standard mode / 5s duration / kling-v1\n*   standard mode / 5s duration / kling-v1-5\n*   pro mode / 5s duration / kling-v1\n*   pro mode / 5s duration / kling-v1-5\n*   pro mode / 5s duration / kling-v1-6\n*   pro mode / 10s duration / kling-v1-5\n*   pro mode / 10s duration / kling-v1-6\n\nDefault: “pro mode / 5s duration / kling-v1”\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## How It Works\n\nThe node analyzes the start and end frames to create a smooth transition sequence between them. It sends the images and parameters to Kling’s API server, which generates all necessary intermediate frames for a fluid transformation. The transition style and content can be guided using prompts, while negative prompts help avoid unwanted elements.\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\n\nclass KlingStartEndFrameNode(KlingImage2VideoNode):\n    \"\"\"\n    Kling First Last Frame Node. This node allows creation of a video from a first and last frame. It calls the normal image to video endpoint, but only allows the subset of input options that support the `image_tail` request field.\n    \"\"\"\n\n    @staticmethod\n    def get_mode_string_mapping() -> dict[str, tuple[str, str, str]]:\n        \"\"\"\n        Returns a mapping of mode strings to their corresponding (mode, duration, model_name) tuples.\n        Only includes config combos that support the `image_tail` request field.\n        \"\"\"\n        return {\n            \"standard mode / 5s duration / kling-v1\": (\"std\", \"5\", \"kling-v1\"),\n            \"standard mode / 5s duration / kling-v1-5\": (\"std\", \"5\", \"kling-v1-5\"),\n            \"pro mode / 5s duration / kling-v1\": (\"pro\", \"5\", \"kling-v1\"),\n            \"pro mode / 5s duration / kling-v1-5\": (\"pro\", \"5\", \"kling-v1-5\"),\n            \"pro mode / 5s duration / kling-v1-6\": (\"pro\", \"5\", \"kling-v1-6\"),\n            \"pro mode / 10s duration / kling-v1-5\": (\"pro\", \"10\", \"kling-v1-5\"),\n            \"pro mode / 10s duration / kling-v1-6\": (\"pro\", \"10\", \"kling-v1-6\"),\n        }\n\n    @classmethod\n    def INPUT_TYPES(s):\n        modes = list(KlingStartEndFrameNode.get_mode_string_mapping().keys())\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"end_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image_tail\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"mode\": (\n                    modes,\n                    {\n                        \"default\": modes[2],\n                        \"tooltip\": \"The configuration to use for the video generation following the format: mode / duration / model_name.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Generate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last.\"\n\n    def parse_inputs_from_mode(self, mode: str) -> tuple[str, str, str]:\n        \"\"\"Parses the mode input into a tuple of (model_name, duration, mode).\"\"\"\n        return KlingStartEndFrameNode.get_mode_string_mapping()[mode]\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        end_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        mode: str,\n        auth_token: Optional[str] = None,\n    ):\n        mode, duration, model_name = self.parse_inputs_from_mode(mode)\n        return super().api_call(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model_name=model_name,\n            start_frame=start_frame,\n            cfg_scale=cfg_scale,\n            mode=mode,\n            aspect_ratio=aspect_ratio,\n            duration=duration,\n            end_frame=end_frame,\n            auth_token=auth_token,\n        )\n\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2",
  "markdown": "# OpenAI DALL·E 2 Node - ComfyUI\n\n![OpenAI DALL·E 2 node screenshot](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-dall-e-2.jpg) OpenAI DALL·E 2 is part of the ComfyUI API Nodes series, allowing users to generate images through OpenAI’s **DALL·E 2** model. This node supports:\n\n*   Text-to-image generation\n*   Image editing functionality (inpainting through masks)\n\n## Node Overview\n\nThe **OpenAI DALL·E 2** node generates images synchronously through OpenAI’s image generation API. It receives text prompts and returns images that match the description.\n\n## Parameter Description\n\n### Required Parameters\n\n| Parameter | Description |\n| --- | --- |\n| `prompt` | Text prompt describing the image content you want to generate |\n\n### Widget Parameters\n\n| Parameter | Description | Options/Range | Default Value |\n| --- | --- | --- | --- |\n| `seed` | Seed value for image generation (currently not implemented in the backend) | 0 to 2^31-1 | 0   |\n| `size` | Output image dimensions | ”256x256”, “512x512”, “1024x1024\" | \"1024x1024” |\n| `n` | Number of images to generate | 1 to 8 | 1   |\n\n### Optional Parameters\n\n| Parameter | Description | Options/Range | Default Value |\n| --- | --- | --- | --- |\n| `image` | Optional reference image for image editing | Any image input | None |\n| `mask` | Optional mask for local inpainting | Mask input | None |\n\n## Usage Method\n\n## Workflow Examples\n\nThis API node currently supports two workflows:\n\n*   Text to Image\n*   Inpainting\n\n### Text to Image Example\n\nThe image below contains a simple text-to-image workflow. Please download the corresponding image and drag it into ComfyUI to load the workflow. ![ComfyUI openai-dall-e-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/text2image.png) The corresponding example is very simple ![ComfyUI openai-dall-e-2 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg) You only need to load the `OpenAI DALL·E 2` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.\n\n### Inpainting Workflow\n\nDALL·E 2 supports image editing functionality, allowing you to use a mask to specify the area to be replaced. Below is a simple inpainting workflow example:\n\n#### 1\\. Workflow File Download\n\nDownload the image below and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI openai-dall-e-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/inpainting.png) We will use the image below as input: ![ComfyUI openai-dall-e-2 workflow input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/input.jpg) \n\n#### 2\\. Workflow File Usage Instructions\n\n![ComfyUI openai-dall-e-2 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg) Since this workflow is relatively simple, if you want to manually implement the corresponding workflow yourself, you can follow the steps below:\n\n1.  Use the `Load Image` node to load the image\n2.  Right-click on the load image node and select `MaskEditor`\n3.  In the mask editor, use the brush to draw the area you want to redraw\n4.  Connect the loaded image to the `image` input of the **OpenAI DALL·E 2** node\n5.  Connect the mask to the `mask` input of the **OpenAI DALL·E 2** node\n6.  Edit the prompt in the `prompt` node\n7.  Run the workflow\n\n**Notes**\n\n*   If you want to use the image editing functionality, you must provide both an image and a mask (both are required)\n*   The mask and image must be the same size\n*   When inputting large images, the node will automatically resize the image to an appropriate size\n*   The URLs returned by the API are valid for a short period, please save the results promptly\n*   Each generation consumes credits, charged according to image size and quantity\n\n## FAQs"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video",
  "markdown": "# Luma Text to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native Luma Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg) The Luma Text to Video node lets you create high-quality, smooth videos from text descriptions using Luma AI’s video generation technology.\n\n## Node Function\n\nThis node connects to Luma AI’s text-to-video API, allowing users to generate dynamic video content from detailed text prompts.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the video content to generate |\n| model | select | \\-  | Video generation model to use |\n| aspect\\_ratio | select | ”ratio\\_16\\_9” | Video aspect ratio |\n| resolution | select | ”res\\_540p” | Video resolution |\n| duration | select | \\-  | Video length options |\n| loop | boolean | False | Whether to loop the video |\n| seed | integer | 0   | Seed value for node rerun, results are nondeterministic |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPTS | Camera concepts to control motion via Luma Concepts node |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Usage Example\n\n[\n\n## Luma Text to Video Workflow Example\n\nLuma Text to Video Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass LumaTextToVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaVideoModel],),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in LumaAspectRatio],\n                    {\n                        \"default\": LumaAspectRatio.ratio_16_9,\n                    },\n                ),\n                \"resolution\": (\n                    [resolution.value for resolution in LumaVideoOutputResolution],\n                    {\n                        \"default\": LumaVideoOutputResolution.res_540p,\n                    },\n                ),\n                \"duration\": ([dur.value for dur in LumaVideoModelOutputDuration],),\n                \"loop\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        aspect_ratio: str,\n        resolution: str,\n        duration: str,\n        loop: bool,\n        seed,\n        luma_concepts: LumaConceptChain = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        duration = duration if model != LumaVideoModel.ray_1_6 else None\n        resolution = resolution if model != LumaVideoModel.ray_1_6 else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations\",\n                method=HttpMethod.POST,\n                request_model=LumaGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaGenerationRequest(\n                prompt=prompt,\n                model=model,\n                resolution=resolution,\n                aspect_ratio=aspect_ratio,\n                duration=duration,\n                loop=loop,\n                concepts=luma_concepts.create_api_model() if luma_concepts else None,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.assets.video)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3",
  "markdown": "# OpenAI DALL·E 3 Node - ComfyUI\n\nOpenAI DALL·E 3 is part of the ComfyUI API Nodes series, allowing users to generate images through OpenAI’s **DALL·E 3** model. This node supports text-to-image generation functionality. ![OpenAI DALL·E 3 node screenshot](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg)\n\n## Node Overview\n\nDALL·E 3 is OpenAI’s latest image generation model, capable of creating detailed and high-quality images based on text prompts. Through this node in ComfyUI, you can directly access DALL·E 3’s generation capabilities without leaving the ComfyUI interface. The **OpenAI DALL·E 3** node generates images synchronously through OpenAI’s image generation API. It receives text prompts and returns images that match the description.\n\n## Parameter Details\n\n### Required Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| prompt | Text | Text prompt for generating images. Supports multi-line input, can describe in detail the image content you want to generate. |\n\n### Widget Parameters\n\n| Parameter | Type | Options | Default Value | Description |\n| --- | --- | --- | --- | --- |\n| seed | Integer | 0-2147483647 | 0   | Random seed used to control the generation result |\n| quality | Option | standard, hd | standard | Image quality setting. The “hd” option generates higher quality images but may require more computational resources |\n| style | Option | natural, vivid | natural | Image style. “Vivid” tends to generate hyperrealistic and dramatic images, while “natural” produces more natural, less exaggerated images |\n| size | Option | 1024x1024, 1024x1792, 1792x1024 | 1024x1024 | Size of the generated image. You can choose square or rectangular images in different orientations |\n\n## Usage Examples\n\nYou can download the image below and drag it into ComfyUI to load the corresponding workflow ![ComfyUI openai-dall-e-3 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png) Since the corresponding workflow is very simple, you can also directly add the **OpenAI DALL·E 3** node in ComfyUI, input the description of the image you want to generate, and then run the workflow ![ComfyUI openai-dall-e-3 workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg)\n\n1.  Add the **OpenAI DALL·E 3** node in ComfyUI\n2.  Enter the description of the image you want to generate in the prompt text box\n3.  Adjust optional parameters as needed (quality, style, size, etc.)\n4.  Run the workflow to generate the image\n\n## FAQs"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1",
  "markdown": "# OpenAI GPT-Image-1 Node - ComfyUI\n\n![OpenAI GPT-Image-1 Node Screenshot](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-gpt-image-1.jpg) OpenAI GPT-Image-1 is part of the ComfyUI API nodes series that allows users to generate images through OpenAI’s **GPT-Image-1** model. This is the same model used for image generation in ChatGPT 4o. This node supports:\n\n*   Text-to-image generation\n*   Image editing functionality (inpainting through masks)\n\n## Node Overview\n\nThe **OpenAI GPT-Image-1** node synchronously generates images through OpenAI’s image generation API. It receives text prompts and returns images matching the description. GPT-Image-1 is OpenAI’s most advanced image generation model currently available, capable of creating highly detailed and realistic images.\n\n## Parameter Description\n\n### Required Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| `prompt` | Text | Text prompt describing the image content you want to generate |\n\n### Widget Parameters\n\n| Parameter | Type | Options | Default | Description |\n| --- | --- | --- | --- | --- |\n| `seed` | Integer | 0-2147483647 | 0   | Random seed used to control generation results |\n| `quality` | Option | low, medium, high | low | Image quality setting, affects cost and generation time |\n| `background` | Option | opaque, transparent | opaque | Whether the returned image has a background |\n| `size` | Option | auto, 1024x1024, 1024x1536, 1536x1024 | auto | Size of the generated image |\n| `n` | Integer | 1-8 | 1   | Number of images to generate |\n\n### Optional Parameters\n\n| Parameter | Type | Options | Default | Description |\n| --- | --- | --- | --- | --- |\n| `image` | Image | Any image input | None | Optional reference image for image editing |\n| `mask` | Mask | Mask input | None | Optional mask for inpainting (white areas will be replaced) |\n\n## Usage Examples\n\n### Text-to-Image Example\n\nThe image below contains a simple text-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI openai-gpt-image-1 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/text2image.png) The corresponding workflow is very simple: ![ComfyUI openai-gpt-image-1 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg) You only need to load the `OpenAI GPT-Image-1` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.\n\n### Image-to-Image Example\n\nThe image below contains a simple image-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI openai-gpt-image-1 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/image2image.png) We will use the image below as input: ![ComfyUI openai-gpt-image-1 workflow input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/input.webp) In this workflow, we use the `OpenAI GPT-Image-1` node to generate images and the `Load Image` node to load the input image, then connect it to the `image` input of the `OpenAI GPT-Image-1` node. ![ComfyUI openai-gpt-image-1 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg)\n\n### Multiple Image Input Example\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow. ![Multiple Image Input Example](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/multiple_image_input.png) Use the hat image below as an additional input image. ![Hat](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/hat.webp) The corresponding workflow is shown in the image below: ![Multiple Image Input Example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png) The `Batch Images` node is used to load multiple images into the `OpenAI GPT-Image-1` node.\n\n### Inpainting Workflow\n\nGPT-Image-1 also supports image editing functionality, allowing you to specify areas to replace using a mask. Below is a simple inpainting workflow example: Download the image below and drag it into ComfyUI to load the corresponding workflow. We will continue to use the input image from the image-to-image workflow section. ![ComfyUI openai-gpt-image-1 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/inpaint.png) The corresponding workflow is shown in the image ![ComfyUI openai-gpt-image-1 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg) Compared to the image-to-image workflow, we use the MaskEditor in the `Load Image` node through the right-click menu to draw a mask, then connect it to the `mask` input of the `OpenAI GPT-Image-1` node to complete the workflow. **Notes**\n\n*   The mask and image must be the same size\n*   When inputting large images, the node will automatically resize the image to an appropriate size\n\n## FAQs"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/runway/video-generation",
  "markdown": "# Runway API Node Video Generation ComfyUI Official Example\n\nRunway is a company focused on generative AI, providing powerful video generation capabilities. Currently, ComfyUI has integrated the Runway API, allowing you to directly use the related nodes in ComfyUI for video generation. Currently, ComfyUI natively integrates the following Runway video generation models:\n\n*   Runway Gen3a turbo\n*   Runway Gen4 turbo\n*   Runway First Last Frame to video\n\n## Gen3a turbo Image-to-Video Workflow\n\n### 1\\. Workflow File Download\n\nThe video below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/runway_image_to_video_gen3a_turbo.json)\n\nDownload the image below as input image ![ComfyUI Runway gen3a turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/steampunk.png)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway gen3a turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg) You can refer to the numbers in the image to complete the basic image-to-video workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  In the `Runway Gen3a turbo` node, set the `prompt` to describe video content, modify the `duration` parameter to set video length, modify the `ratio` parameter to set video aspect ratio\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.\n4.  After waiting for the API to return results, you can view the generated video in the `Save Video` node (right-click to save). The corresponding video will also be saved to the `ComfyUI/output/` directory.\n\n## Gen4 turbo Image-to-Video Workflow\n\n### 1\\. Workflow File Download\n\nThe video below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/runway_gen4_turo_image_to_video.json)\n\nDownload the image below as input image ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/godfather.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway gen4 turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg) You can refer to the numbers in the image to complete the basic image-to-video workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  In the `Runway Gen4 turbo` node, set the `prompt` to describe video content, modify the `duration` parameter to set video length, modify the `ratio` parameter to set video aspect ratio\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.\n4.  After waiting for the API to return results, you can view the generated video in the `Save Video` node (right-click to save). The corresponding video will also be saved to the `ComfyUI/output/` directory.\n\n## First-Last Frame Video Generation Workflow\n\n### 1\\. Workflow File Download\n\nThe video below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/runway_first_last_frame.json)\n\nDownload the images below as input images ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/first.jpg) ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/last.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway gen4 turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg) You can refer to the numbers in the image to complete the basic first-last frame to video workflow execution:\n\n1.  In the `Load Image` node, load the starting frame\n2.  In the `Load Image` node, load the ending frame\n3.  In the `Runway First-Last-Frame to Video` node, set the `prompt` to describe video content, modify the `duration` parameter to set video length, modify the `ratio` parameter to set video aspect ratio\n4.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.\n5.  After waiting for the API to return results, you can view the generated video in the `Save Video` node (right-click to save). The corresponding video will also be saved to the `ComfyUI/output/` directory."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template",
  "markdown": "# PixVerse Template - ComfyUI Native Node Documentation\n\n![ComfyUI Native PixVerse Template Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg) The PixVerse Template node lets you choose from predefined video generation templates to control the style and effects of PixVerse video generation nodes. This helper node connects to PixVerse video generation nodes, allowing users to quickly apply preset video styles without manually adjusting complex parameter combinations.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| template | Select | Choose a template from available video presets |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| pixverse\\_template | PixverseIO.TEMPLATE | Configuration object containing the selected template ID |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\n\nclass PixverseTemplateNode:\n    \"\"\"\n    Select template for Pixverse Video generation.\n    \"\"\"\n\n    RETURN_TYPES = (PixverseIO.TEMPLATE,)\n    RETURN_NAMES = (\"pixverse_template\",)\n    FUNCTION = \"create_template\"\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"template\": (list(pixverse_templates.keys()), ),\n            }\n        }\n\n    def create_template(self, template: str):\n        template_id = pixverse_templates.get(template, None)\n        if template_id is None:\n            raise Exception(f\"Template '{template}' is not recognized.\")\n        # just return the integer\n        return (template_id,)\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video",
  "markdown": "# Luma Image to Video API Node ComfyUI Official Example\n\nThe [Luma Image to Video](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video) node allows you to convert static images into smooth, dynamic videos using Luma AI’s advanced technology, bringing life and motion to your images. In this guide, we’ll show you how to set up a workflow for image-to-video conversion using this node.\n\n## Luma Image to Video Node Documentation\n\nCheck out the following documentation to learn more about the node’s parameters:\n\n[\n\nLuma Image to Video API node documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)[\n\n## Luma Concepts Node Docs\n\nLuma Concepts API node documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)\n\n## Image to Video Workflow with Luma API Node\n\nThe Luma Image to Video node requires at least one image input (`first_image` or `last_image`) along with text prompts to determine the video’s motion effects. In this guide, we’ve created an example using `first_image` and `luma_concepts` to showcase Luma AI’s video generation capabilities.\n\n### 1\\. Download the Workflow\n\nThe workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow. ![Luma Image to Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/luma_i2v.mp4) Download the following image to use as input: ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/input.png)\n\n### 2\\. Follow the Workflow Steps\n\n![Luma Image to Video Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg) Follow these basic steps to run the workflow:\n\n1.  Upload your input image in the `first_image` node\n2.  (Optional) Write prompts in the Luma Image to Video node to describe how you want the image animated\n3.  (Optional) Modify the `Luma Concepts` node to control camera movement for professional cinematography\n4.  Click `Run` or use `Ctrl(cmd) + Enter` to generate the video\n5.  Once the API returns results, view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory\n\n### 3\\. Additional Notes\n\n*   **Image Input Requirements**: At least one of `first_image` or `last_image` is required, with a maximum of 1 image per input\n*   **Luma Concepts**: Controls camera movement for professional video effects\n*   **Seed Parameter**: Only determines if the node should rerun, doesn’t affect generation results\n*   **Enable Input Nodes**: Right-click on purple “Bypass” mode nodes and set “mode” to “always” to enable inputs\n*   **Model Selection**: Different video generation models have unique characteristics, adjustable via the model parameter\n*   **Resolution and Duration**: Adjust output video resolution and length using resolution and duration parameters"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video",
  "markdown": "# MiniMax Image to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native MiniMax Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg) The MiniMax Image to Video node uses MiniMax’s API to generate videos from input images and text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image used as the first frame of video |\n| prompt\\_text | string | \"\"  | Text prompt to guide video generation |\n| model | select | ”I2V-01” | Available models: “I2V-01-Director”, “I2V-01”, “I2V-01-live” |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| seed | integer | Random seed for noise generation |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass MinimaxImageToVideoNode(MinimaxTextToVideoNode):\n    \"\"\"\n    Generates videos synchronously based on an image and prompt, and optional parameters using Minimax's API.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Image to use as first frame of video generation\"\n                    },\n                ),\n                \"prompt_text\": (\n                    \"STRING\",\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt to guide the video generation\",\n                    },\n                ),\n                \"model\": (\n                    [\n                        \"I2V-01-Director\",\n                        \"I2V-01\",\n                        \"I2V-01-live\",\n                    ],\n                    {\n                        \"default\": \"I2V-01\",\n                        \"tooltip\": \"Model to use for video generation\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from an image and prompts using Minimax's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Minimax\"\n    API_NODE = True\n    OUTPUT_NODE = True\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video",
  "markdown": "# Pika 2.2 Text to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native Pika 2.2 Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg) The Pika 2.2 Text to Video node uses Pika’s 2.2 API to create videos from text descriptions. It connects to Pika’s text-to-video API, allowing users to generate videos using text prompts with various control parameters.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt\\_text | String | \"\"  | Text prompt describing the video content |\n| negative\\_prompt | String | \"\"  | Elements to exclude from the video |\n| seed | Integer | 0   | Random seed for generation |\n| resolution | Select | ”1080p” | Output video resolution |\n| duration | Select | ”5s” | Length of generated video |\n| aspect\\_ratio | Float | 1.7777777777777777 | Video aspect ratio, range 0.4-2.5, step 0.001 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\n\nclass PikaTextToVideoNodeV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Text to Video Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                **cls.get_base_inputs_types(PikaBodyGenerate22T2vGenerate22T2vPost),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.FLOAT,\n                    PikaBodyGenerate22T2vGenerate22T2vPost,\n                    \"aspectRatio\",\n                    step=0.001,\n                    min=0.4,\n                    max=2.5,\n                    default=1.7777777777777777,\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Sends a text prompt to the Pika API v2.2 to generate a video.\"\n\n    def api_call(\n        self,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        aspect_ratio: float,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika 2.2 Text to Video.\"\"\"\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_TEXT_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22T2vGenerate22T2vPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=PikaBodyGenerate22T2vGenerate22T2vPost(\n                promptText=prompt_text,\n                negativePrompt=negative_prompt,\n                seed=seed,\n                resolution=resolution,\n                duration=duration,\n                aspectRatio=aspect_ratio,\n            ),\n            auth_token=auth_token,\n            content_type=\"application/x-www-form-urlencoded\",\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fbuilt-in-nodes%2Fapi-node%2Fvideo%2Fkwai_vgi%2Fkling-camera-control-i2v",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video",
  "markdown": "# Luma Text to Video API Node ComfyUI Official Guide\n\nThe [Luma Text to Video](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video) node allows you to create high-quality, smooth videos from text descriptions using Luma AI’s innovative video generation technology. In this guide, we’ll show you how to set up a text-to-video workflow using this node.\n\n## Luma Text to Video Node Documentation\n\nCheck out the following documentation to learn more about the node parameters:\n\n[\n\nDocumentation for the Luma Text to Video API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)[\n\n## Luma Concepts Node Docs\n\nDocumentation for the Luma Concepts API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)\n\n## Text to Video Workflow with Luma API Node\n\nThe Luma Text to Video node requires text prompts to describe the video content. In this guide, we’ve created examples using `prompt` and `luma_concepts` to showcase Luma AI’s excellent video generation capabilities.\n\n### 1\\. Download the Workflow\n\nThe workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow. ![Luma Text to Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2v/luma_t2v.mp4)\n\n### 2\\. Follow the Steps\n\n![Luma Text to Video Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg) Follow these basic steps to run the workflow:\n\n1.  Write your prompt in the `Luma Text to Video` node to describe the video content you want\n2.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the video\n3.  After the API returns results, you can view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory\n\n> (Optional) Modify the `Luma Concepts` node to control camera movements and add professional cinematography\n\n### 3\\. Additional Notes\n\n*   **Writing Prompts**: Describe scenes, subjects, actions, and mood in detail for best results\n*   **Luma Concepts**: Mainly used for camera control to create professional video shots\n*   **Seed Parameter**: Only determines if the node should rerun, doesn’t affect generation results\n*   **Model Selection**: Different video models have different features, adjustable via the model parameter\n*   **Resolution and Duration**: Adjust output video resolution and length using these parameters\n*   **Ray 1.6 Model Note**: Duration and resolution parameters don’t work when using the Ray 1.6 model\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/interface/settings/comfy-desktop",
  "markdown": "# Comfy Desktop General Settings - ComfyUI\n\n### Window Style\n\n**Function**: Controls the title bar style of the application window\n\n### Automatically check for updates\n\n**Function**: Automatically checks for ComfyUI Desktop updates and will remind you to update when updates are available\n\n### Send anonymous usage metrics\n\n**Function**: Sends anonymous usage statistics to help improve the software. Changes to this setting require a restart to take effect\n\n## UV\n\nThis section is mainly for users in China, because many of the original mirrors used by Desktop are outside of China, so access may not be friendly for domestic users. You can set your own mirror sources here to improve access speed and ensure that corresponding packages can be accessed and downloaded normally.\n\n### Python Install Mirror\n\n**Function**:\n\n*   Managed Python installation packages are downloaded from the Astral python-build-standalone project\n*   Can set mirror URL to use different Python installation sources\n*   The provided URL will replace the default GitHub download address\n*   Supports using file:// protocol to read distribution packages from local directories  \n    **Validation**: Automatically checks mirror reachability\n\n### Pypi Install Mirror\n\n**Function**: Default pip package installation mirror source\n\n### Torch Install Mirror\n\n**Function**: PyTorch-specific pip installation mirror source"
},
{
  "url": "https://docs.comfy.org/interface/settings/3d",
  "markdown": "# ComfyUI 3D Settings - ComfyUI\n\nThis section of settings is mainly used to control the initialization settings of 3D-related components in ComfyUI, including camera, lighting, scene, etc. When creating new 3D components, they will be initialized according to these settings. After creation, these settings can still be adjusted individually.\n\n## Camera\n\n### Initial Camera Type\n\n*   **Options**:\n    *   `perspective`\n    *   `orthographic`\n*   **Function**: Controls whether the default camera is perspective or orthographic when creating new 3D components. This default setting can still be switched individually for each component after creation\n\n![摄像机类型](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/camera_type.jpg)\n\n## Light\n\nThe light settings in this section are used to set the default lighting settings for 3D components. The corresponding settings in the 3D settings in ComfyUI can also be modified. ![light](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/light.jpg)\n\n### Light Adjustment Increment\n\n*   **Default Value**: 0.5\n*   **Function**: Controls the step size when adjusting light intensity in 3D scenes. Smaller step values allow for finer light adjustments, while larger values make each adjustment more noticeable\n\n### Light Intensity Minimum\n\n*   **Default Value**: 1\n*   **Function**: Sets the minimum light intensity value allowed in 3D scenes. This defines the lowest brightness that can be set when adjusting the lighting of any 3D control\n\n### Light Intensity Maximum\n\n*   **Default Value**: 10\n*   **Function**: Sets the maximum light intensity value allowed in 3D scenes. This defines the upper limit of brightness that can be set when adjusting the lighting of any 3D control\n\n### Initial Light Intensity\n\n*   **Default Value**: 3\n*   **Function**: Sets the default brightness level of lights in 3D scenes. This value determines the intensity with which lights illuminate objects when creating new 3D controls, but each control can be adjusted individually after creation\n\n## Scene\n\n### Initial Background Color\n\n*   **Function**: Controls the default background color of 3D scenes. This setting determines the background appearance when creating new 3D components, but each component can be adjusted individually after creation\n*   **Default Value**: `282828` (dark gray)\n\nChange the background color, which can also be adjusted in the canvas. ![Initial Background Color vs Modified](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/background_color.jpg) \n\n### Initial Preview Visibility\n\n*   **Function**: Controls whether the preview screen is displayed by default when creating new 3D components. This default setting can still be toggled individually for each component after creation\n*   **Default Value**: true (enabled)\n\n![Preview vs hide preview](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/hide_preview.jpg)\n\n### Initial Grid Visibility\n\n*   **Function**: Controls whether the grid is displayed by default when creating new 3D components. This default setting can still be toggled individually for each component after creation\n*   **Default Value**: true (enabled)\n\nHide or show the grid on initialization ![Show gird vs hide gird](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/hide_grid.jpg)"
},
{
  "url": "https://docs.comfy.org/interface/settings/comfy",
  "markdown": "# Comfy Settings - ComfyUI\n\n## API Nodes\n\n### Show API node pricing badge\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display pricing badges on API nodes, helping users identify the usage cost of API nodes\n\n![Show API node pricing badge](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/api_node_pricing_badge.jpg)\n\n> For more information about API nodes, please refer to [API Nodes](https://docs.comfy.org/tutorials/api-nodes/overview)\n\n## Dev Mode\n\n### Enable dev mode options (API save, etc.)\n\n*   **Default Value**: Disabled\n*   **Function**: Enables development mode options (such as API save, etc.)\n\n## Edit Token Weight\n\n### Ctrl+up/down precision\n\n*   **Default Value**: 0.01\n*   **Function**: When using CLIPTextEncode type nodes or text input node widgets, use Ctrl+up/down to quickly adjust weights. This option changes the weight value for each adjustment\n\n## Locale\n\n### Language\n\n*   **Options**: English, 中文 (Chinese),日本語 (Japanese), 한국어 (Korean), Русский (Russian), Español (Spanish), Français (French)\n*   **Default Value**: Auto-detect browser language\n*   **Function**: Modify the display language of ComfyUI interface\n\n*   **Default Value**: Top\n*   **Function**: Select menu interface and position, currently only supports Top, Bottom, Disabled\n\nThe menu interface will be displayed at the top of the workspace ![Top Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/UseNewMenu_top.jpg) \n\n## Model Library\n\nModel Library refers to the model management function in the ComfyUI sidebar menu. You can use this function to view models in your `ComfyUI/models` and additionally configured model folders. ![Model Library](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/sidepanel/model_library.jpg)\n\n### What name to display in the model library tree view\n\n*   **Default Value**: title\n*   **Function**: Select the name format to display in the model library tree view, currently only supports filename and title\n\n### Automatically load all model folders\n\n*   **Default Value**: Disabled\n*   **Function**: Whether to automatically detect model files in all folders when clicking the model library. Enabling may cause loading delays (requires traversing all folders). When disabled, files in the corresponding folder will only be loaded when clicking the folder name.\n\n## Node\n\nDuring the iteration process of ComfyUI, we will adjust some nodes and enable some nodes. These nodes may undergo major changes or be removed in future versions. However, to ensure compatibility, deprecated nodes have not been removed. You can use the settings below to enable whether to display **experimental nodes** and **deprecated nodes**.\n\n### Show deprecated nodes in search\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to display deprecated nodes in search. Deprecated nodes are hidden by default in the UI, but remain effective in existing workflows.\n\n![Show deprecated nodes in search](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/depr_node.jpg)\n\n### Show experimental nodes in search\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display experimental nodes in search. Experimental nodes are some new feature support, but are not fully stable and may change or be removed in future versions.\n\n![Show experimental nodes in search](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/beta_node.jpg)\n\n## Node Search Box\n\n### Number of nodes suggestions\n\n*   **Default Value**: 5\n*   **Function**: Used to modify the number of recommended nodes in the related node context menu. The larger the value, the more related recommended nodes are displayed.\n\n![Number of nodes suggestions](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_suggestions.jpg)\n\n### Show node frequency in search results\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to display node usage frequency in search results\n\n![Show node frequency in search results](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_frequency.png)\n\n### Show node id name in search results\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to display node ID names in search results\n\n![Show node id name in search results](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_id_name.jpg)\n\n### Show node category in search results\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display node categories in search results, helping users understand node classification information\n\n### Node preview\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display node previews in search results, making it convenient for you to quickly preview nodes\n\n### Node search box implementation\n\n*   **Default Value**: default\n*   **Function**: Select the implementation method of the node search box (experimental feature). If you select `litegraph (legacy)`, it will switch to the early ComfyUI search box\n\n### Widget control mode\n\n*   **Options**: before, after\n*   **Function**: Controls whether the timing of node widget value updates is before or after workflow execution, such as updating seed values\n\n### Textarea widget spellcheck\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether text area widgets enable spellcheck, providing spellcheck functionality during text input. This functionality is implemented through the browser’s spellcheck attribute\n\n## Queue\n\n### Queue history size\n\n*   **Default Value**: 100\n*   **Function**: Controls the queue history size recorded in the sidebar queue history panel. The larger the value, the more queue history is recorded. When the number is large, loading the page will also consume more memory\n\n## Queue Button\n\n### Batch count limit\n\n*   **Default Value**: 100\n*   **Function**: Sets the maximum number of tasks added to the queue in a single click, preventing accidentally adding too many tasks to the queue\n\n## Validation\n\n### Validate node definitions (slow)\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to validate all node definitions at startup (slow). Only recommended for node developers. When enabled, the system will use Zod schemas to strictly validate each node definition. This functionality will consume more memory and time\n*   **Error Handling**: Failed node definitions will be skipped and warning information will be output to the console\n\n### Validate workflows\n\n*   **Default Value**: Enabled\n*   **Function**: Ensures the structural and connection correctness of workflows. If enabled, the system will call `useWorkflowValidation().validateWorkflow()` to validate workflow data\n*   **Validation Process**: The validation process includes two steps:\n    *   Schema validation: Use Zod schemas to validate workflow structure\n    *   Link repair: Check and repair connection issues between nodes\n*   **Error Handling**: When validation fails, error prompts will be displayed, but workflow loading will not be blocked\n\n## Window\n\n### Show confirmation when closing window\n\n*   **Default Value**: Enabled\n*   **Function**: When there are modified but unsaved workflows, controls whether to display confirmation when closing the browser window or tab, preventing accidental window closure that leads to loss of unsaved workflows\n\n## Workflow\n\n### Persist workflow state and restore on page (re)load\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to restore workflow state on page (re)load, maintaining workflow content after page refresh\n\n### Auto Save\n\n*   **Default Value**: off\n*   **Function**: Controls the auto-save behavior of workflows, automatically saving workflow changes to avoid data loss\n\n### Auto Save Delay (ms)\n\n*   **Default Value**: 1000\n*   **Function**: Sets the delay time for auto-save, only effective when auto-save is set to “after delay”\n\n### Show confirmation when deleting workflows\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display a confirmation dialog when deleting workflows in the sidebar, preventing accidental deletion of important workflows\n\n### Save and restore canvas position and zoom level in workflows\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to save and restore canvas position and zoom level in workflows, restoring the previous view state when reopening workflows\n\n### Opened workflows position\n\n*   **Options**: Sidebar, Topbar, Topbar (Second Row)\n*   **Default Value**: Topbar\n*   **Function**: Controls the display position of opened workflow tabs, currently only supports Sidebar, Topbar, Topbar (Second Row)\n\n### Prompt for filename when saving workflow\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to prompt for filename input when saving workflows, allowing users to customize workflow filenames\n\n### Sort node IDs when saving workflow\n\n*   **Default Value**: Disabled\n*   **Function**: Determines whether to sort node IDs when saving workflows, making workflow file format more standardized and convenient for version control\n\n### Show missing nodes warning\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display warnings for missing nodes in workflows, helping users identify unavailable nodes in workflows\n\n### Show missing models warning\n\n*   **Default Value**: Enabled\n*   **Function**: We support adding model link information to widget values in workflow files for prompts when loading model files. When enabled, if you don’t have the corresponding model files locally, warnings for missing models in workflows will be displayed\n\n### Require confirmation when clearing workflow\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display a confirmation dialog when clearing workflows, preventing accidental clearing of workflow content\n\n### Save node IDs to workflow\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to save node IDs when saving workflows, making workflow file format more standardized and convenient for version control"
},
{
  "url": "https://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1",
  "markdown": "# ComfyUI ACE-Step Native Example - ComfyUI\n\nACE-Step is an open-source foundational music generation model jointly developed by Chinese team StepFun and ACE Studio, aimed at providing music creators with efficient, flexible and high-quality music generation and editing tools. The model is released under the [Apache-2.0](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-license) license and is free for commercial use. As a powerful music generation foundation, ACE-Step provides rich extensibility. Through fine-tuning techniques like LoRA and ControlNet, developers can customize the model according to their actual needs. Whether it’s audio editing, vocal synthesis, accompaniment production, voice cloning or style transfer applications, ACE-Step provides stable and reliable technical support. This flexible architecture greatly simplifies the development process of music AI applications, allowing more creators to quickly apply AI technology to music creation. Currently, ACE-Step has released related training code, including LoRA model training, and the corresponding ControlNet training code will be released in the future. You can visit their [Github](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-roadmap) to learn more details.\n\n## ACE-Step ComfyUI Text-to-Audio Generation Workflow Example\n\n### 1\\. Download Workflow and Related Models\n\nClick the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information. Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_t2m.json)\n\nYou can also manually download [ace\\_step\\_v1\\_3.5b.safetensors](https://huggingface.co/Comfy-Org/ACE-Step_ComfyUI_repackaged/blob/main/all_in_one/ace_step_v1_3.5b.safetensors) and save it to the `ComfyUI/models/checkpoints` folder\n\n### 2\\. Complete the Workflow Step by Step\n\n![Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg)\n\n1.  Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model\n2.  (Optional) In the `EmptyAceStepLatentAudio` node, you can set the duration of the music to be generated\n3.  (Optional) In the `LatentOperationTonemapReinhard` node, you can adjust the `multiplier` to control the volume of the vocals (higher numbers result in more prominent vocals)\n4.  (Optional) Input corresponding music styles etc. in the `tags` field of `TextEncodeAceStepAudio`\n5.  (Optional) Input corresponding lyrics in the `lyrics` field of `TextEncodeAceStepAudio`\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation\n7.  After the workflow completes,, you can preview the generated audio in the `Save Audio` node. You can click to play and listen to it, and the audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).\n\n## ACE-Step ComfyUI Audio-to-Audio Workflow\n\nSimilar to image-to-image workflows, you can input a piece of music and use the workflow below to resample and generate music. You can also adjust the difference from the original audio by controlling the `denoise` parameter in the `Ksampler`.\n\n### 1\\. Download Workflow File\n\nClick the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_m2m_editing.json)\n\nDownload the following audio file as the input audio:\n\n[\n\nDownload Example Audio File for Input\n\n](https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/audio/ace-step/input.mp3)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ACE-Step Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg)\n\n1.  Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model\n2.  Upload the provided audio file in the `LoadAudio` node\n3.  (Optional) Input corresponding music styles and lyrics in the `tags` and `lyrics` fields of `TextEncodeAceStepAudio`. Providing lyrics is very important for audio editing\n4.  (Optional) Modify the `denoise` parameter in the `Ksampler` node to adjust the noise added during sampling to control similarity with the original audio (smaller values result in more similarity to the original audio; setting it to `1.00` is approximately equivalent to having no audio input)\n5.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation\n6.  After the workflow completes, you can preview the generated audio in the `Save Audio` node. You can click to play and listen to it, and the audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).\n\nYou can also implement the lyrics modification and editing functionality from the ACE-Step project page, modifying the original lyrics to change the audio effect.\n\n### 3\\. Additional Workflow Notes\n\n1.  In the example workflow, you can change the `tags` in `TextEncodeAceStepAudio` from `male voice` to `female voice` to generate female vocals.\n2.  You can also modify the `lyrics` in `TextEncodeAceStepAudio` to change the lyrics and thus the generated audio. Refer to the examples on the ACE-Step project page for more details.\n\n## ACE-Step Prompt Guide\n\nACE currently uses two types of prompts: `tags` and `lyrics`.\n\n*   `tags`: Mainly used to describe music styles, scenes, etc. Similar to prompts we use for other generations, they primarily describe the overall style and requirements of the audio, separated by English commas\n*   `lyrics`: Mainly used to describe lyrics, supporting lyric structure tags such as \\[verse\\], \\[chorus\\], and \\[bridge\\] to distinguish different parts of the lyrics. You can also input instrument names for purely instrumental music\n\nYou can find rich examples of `tags` and `lyrics` on the [ACE-Step model homepage](https://ace-step.github.io/). You can refer to these examples to try corresponding prompts. This document’s prompt guide is organized based on the project to help you quickly try combinations to achieve your desired effect.\n\n### Tags (prompt)\n\n#### Mainstream Music Styles\n\nUse short tag combinations to generate specific music styles\n\n*   electronic\n*   rock\n*   pop\n*   funk\n*   soul\n*   cyberpunk\n*   Acid jazz\n*   electro\n*   em (electronic music)\n*   soft electric drums\n*   melodic\n\n#### Scene Types\n\nCombine specific usage scenarios and atmospheres to generate music that matches the corresponding mood\n\n*   background music for parties\n*   radio broadcasts\n*   workout playlists\n\n#### Instrumental Elements\n\n*   saxophone\n*   jazz\n*   piano, violin\n\n#### Vocal Types\n\n*   female voice\n*   male voice\n*   clean vocals\n\n#### Professional Terms\n\nUse some professional terms commonly used in music to precisely control music effects\n\n*   110 bpm (beats per minute is 110)\n*   fast tempo\n*   slow tempo\n*   loops\n*   fills\n*   acoustic guitar\n*   electric bass\n\n### Lyrics\n\n#### Lyric Structure Tags\n\n*   \\[outro\\]\n*   \\[verse\\]\n*   \\[chorus\\]\n*   \\[bridge\\]\n\n#### Multilingual Support\n\n*   ACE-Step V1 supports multiple languages. When used, ACE-Step converts different languages into English letters and then generates music.\n*   In ComfyUI, we haven’t fully implemented the conversion of all languages to English letters. Currently, only [Japanese hiragana and katakana characters](https://github.com/comfyanonymous/ComfyUI/commit/5d3cc85e13833aeb6ef9242cdae243083e30c6fc) are implemented. So if you need to use multiple languages for music generation, you need to first convert the corresponding language to English letters, and then input the language code abbreviation at the beginning of the `lyrics`, such as Chinese `[zh]`, Korean `[ko]`, etc.\n\nFor example:\n\n```\n[verse]\n\n[zh]wo3zou3guo4shen1ye4de5jie1dao4\n[zh]leng3feng1chui1luan4si1nian4de5piao4liang4wai4tao4\n[zh]ni3de5wei1xiao4xiang4xing1guang1hen3xuan4yao4\n[zh]zhao4liang4le5wo3gu1du2de5mei3fen1mei3miao3\n\n[chorus]\n\n[verse]​\n[ko]hamkke si-kkeuleo-un sesang-ui sodong-eul pihae​\n[ko]honja ogsang-eseo dalbich-ui eolyeompus-ileul balaboda​\n[ko]niga salang-eun lideum-i ganghan eum-ag gatdago malhaess-eo​\n[ko]han ta han tamada ma-eum-ui ondoga eolmana heojeonhanji ijge hae\n\n[bridge]\n[es]cantar mi anhelo por ti sin ocultar\n[es]como poesía y pintura, lleno de anhelo indescifrable\n[es]tu sombra es tan terca como el viento, inborrable\n[es]persiguiéndote en vuelo, brilla como cruzar una mar de nubes\n\n[chorus]\n[fr]que tu sois le vent qui souffle sur ma main\n[fr]un contact chaud comme la douce pluie printanière\n[fr]que tu sois le vent qui s'entoure de mon corps\n[fr]un amour profond qui ne s'éloignera jamais\n```\n\nCurrently, ACE-Step supports 19 languages, but the following ten languages have better support:\n\n*   English\n*   Chinese: \\[zh\\]\n*   Russian: \\[ru\\]\n*   Spanish: \\[es\\]\n*   Japanese: \\[ja\\]\n*   German: \\[de\\]\n*   French: \\[fr\\]\n*   Portuguese: \\[pt\\]\n*   Italian: \\[it\\]\n*   Korean: \\[ko\\]\n\n*   [Project Page](https://ace-step.github.io/)\n*   [Hugging Face](https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B)\n*   [GitHub](https://github.com/ace-step/ACE-Step)\n*   [Training Scripts](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-train)"
},
{
  "url": "https://docs.comfy.org/interface/settings/lite-graph",
  "markdown": "# ComfyUI LiteGraph (Canvas) Settings - ComfyUI\n\nLiteGraph is the underlying graphics rendering engine of ComfyUI. The settings in this category mainly control the behavior and appearance of graphical interfaces such as canvas, nodes, and links.\n\n### Show selection toolbox\n\n*   **Default Value**: Enabled\n*   **Function**: The selection toolbox is a floating quick action toolbar that appears on nodes after they are selected, providing common quick operations such as partial execution, pinning, deletion, color modification, etc.\n\n![Show selection toolbox](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/selection-toolbox.jpg)\n\n### Low quality rendering zoom threshold\n\n*   **Default Value**: 0.6\n*   **Range**: 0.1 - 1.0\n*   **Function**: When rendering the interface, especially when the workflow is particularly complex and the entire canvas is particularly large, the frontend rendering of corresponding elements will consume a lot of memory and cause lag. By lowering this threshold, you can control elements to enter low quality rendering mode when scaled to a specific percentage, thereby reducing memory consumption. The corresponding different rendering modes are shown below\n\n![Low quality rendering](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/render-mode.jpg)\n\n### Maximum FPS\n\n*   **Default Value**: 0 (use screen refresh rate)\n*   **Range**: 0 - 120\n*   **Function**: Limits the rendering frame rate of the canvas. 0 means using the screen refresh rate. Higher FPS will make the canvas rendering smoother, but will also consume more performance. Too low values will cause more obvious stuttering.\n\n### Always snap to grid\n\n*   **Default Value**: Disabled\n*   **Function**: When this option is not enabled, you can hold the `Shift` key to align node edges with the grid. When enabled, node edges will automatically align with the grid without holding the `Shift` key.\n\n### Snap to grid size\n\n*   **Range**: 1 - 500\n*   **Function**: When auto-snap is enabled or when moving nodes while holding the `Shift` key, this parameter determines the grid size for snapping. The default value is 10, and you can adjust it according to your needs.\n\n### Enable fast-zoom shortcut (Ctrl + Shift + Drag)\n\n*   **Default Value**: Enabled\n*   **Function**: Enables the `Ctrl + Shift + Left Mouse Button Drag` fast zoom function, providing a faster zoom operation method\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display the canvas menu in the bottom right corner\n\nThe canvas menu is located in the bottom right corner of the entire ComfyUI interface, containing operations such as canvas zooming, temporarily hiding all connections, quickly scaling the workflow to fit the canvas, etc., as shown in the image below ![Show graph canvas menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/canvas_menu.jpg)\n\n### Canvas zoom speed\n\n*   **Default Value**: 1.1\n*   **Range**: 1.01 - 2.5\n*   **Function**: Controls the speed of canvas zooming, adjusts the sensitivity of mouse wheel zooming\n\n### Show canvas info on bottom left corner (fps, etc.)\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display canvas information in the bottom left corner, showing performance metrics like FPS\n\n![Canvas info](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/canvas-info.jpg)\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to scale node combo widget menus (lists) when zoomed in, allowing users to select node combo widgets\n\n## Graph\n\n### Link Render Mode\n\n*   **Default Value**: 2 (Spline)\n*   **Options**: Straight, Linear, Spline, Hidden\n*   **Function**: Sets the rendering style of connections, controlling the visual style of links between nodes\n\n![Link Render Mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/link-render-mode.jpg)\n\n## Group\n\nThis section of settings is mainly related to node group functionality ![Node Group](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-group.png)\n\n### Double click group title to edit\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether you can double-click the node title to edit it, allowing users to rename nodes, marked as part `1` in the image\n\n### Group selected nodes padding\n\n*   **Default Value**: 10\n*   **Range**: 0 - 100\n*   **Function**: Sets the inner padding when grouping selected nodes, controlling the spacing between the group frame and nodes, marked as the arrow annotation part `2` in the image\n\n## Link\n\n### Link midpoint markers\n\n*   **Default Value**: Circle\n*   **Options**: None, Circle, Arrow\n*   **Function**: Sets the marker style at link midpoints, displaying direction indicators at link midpoints\n\n![Link midpoint markers](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/link-midpoint.jpg)\n\n## Link Release\n\nThis menu section currently mainly controls related operations when link connections are released. The current two related operations are: **A node recommendation list related to the current input/output will appear after release**\n\n**A search box will be launched after release**\n\n### Action on link release (Shift)\n\n*   **Default Value**: search box\n*   **Options**: context menu, search box, no action\n*   **Function**: Sets the action when releasing links while holding the Shift key, special behavior when releasing links while holding Shift\n\n### Action on link release (No modifier)\n\n*   **Default Value**: context menu\n*   **Options**: context menu, search box, no action\n*   **Function**: Sets the default action when releasing links, controls the behavior after dragging and releasing links\n\n## Node\n\n### Always shrink new nodes\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to automatically shrink when creating new nodes, so nodes can always display the minimum size, but may cause some text display to be truncated when adding, requiring manual adjustment of node size\n\n### Enable DOM element clipping (enabling may reduce performance)\n\n*   **Default Value**: Enabled\n*   **Function**: Enables DOM element clipping (may affect performance), optimizes rendering but may reduce performance\n\n### Middle-click creates a new Reroute node\n\n*   **Default Value**: Enabled\n*   **Function**: Creates a new reroute node when middle-clicking, quickly creates reroute nodes for organizing connections\n\n### Keep all links when deleting nodes\n\n*   **Default Value**: Enabled\n*   **Function**: Automatically bypasses connections when deleting intermediate nodes, attempts to reconnect input and output links when deleting nodes\n\n### Snap highlights node\n\n*   **Default Value**: Enabled\n*   **Function**: Highlights nodes when dragging links to them, provides visual feedback, shows connectable nodes. When enabled, the effect is as shown in the image below, the corresponding side of the link will display highlighted style\n\n![Snap highlights node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/highlights-node.jpg)\n\n### Auto snap link to node slot\n\n*   **Default Value**: Enabled\n*   **Function**: Automatically snaps to available slots when dragging links to nodes, simplifies connection operations, automatically finds suitable input slots\n\n### Enable Tooltips\n\n*   **Default Value**: Enabled\n*   **Function**: Some node information will contain tooltips, including parameter descriptions, etc. When enabled, these tooltips will be displayed when hovering the mouse, as shown in the image below\n\n![Enable Tooltips](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/tooltips.jpg)\n\n### Tooltip Delay\n\n*   **Default Value**: 500\n*   **Function**: Controls the delay time for tooltips, in milliseconds. Setting to 0 means displaying tooltips immediately\n\n### Node life cycle badge mode\n\n*   **Default Value**: Show all\n*   **Function**: Controls the display of node lifecycle markers, showing node status information\n\n### Node ID badge mode\n\n*   **Default Value**: Show all\n*   **Function**: Controls the display of node ID markers, showing node unique identifiers\n\n![Node ID badge mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-id-badge.jpg)\n\n### Node source badge mode\n\n*   **Options**:\n    *   None\n    *   Hide built-in\n    *   Show all\n*   **Function**: Controls the display mode of node source markers, showing node source information. The corresponding display effect is shown in the image below. If show all is selected, it will display labels for both custom nodes and built-in nodes, making it convenient for you to determine the corresponding node source. The corresponding fox logo represents ComfyUI built-in nodes\n\n![Node source badge mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-source-badge.jpg)\n\n### Double click node title to edit\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether you can double-click the node title to edit it, allowing users to rename nodes\n\n### Float widget rounding decimal places \\[0 = auto\\]\n\n*   **Default Value**: 0 (auto)\n*   **Range**: 0 - 6\n*   **Function**: Sets the decimal places for float widget rounding, 0 means auto, requires page reload\n\n### Disable default float widget rounding\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to disable default float widget rounding, requires page reload, cannot be disabled when the node backend has set rounding\n\n### Disable node widget sliders\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to disable slider controls in node widgets, forcing text input instead of sliders\n\n### Preview image format\n\n*   **Default Value**: Empty string (use original format)\n*   **Function**: Sets the format for preview images in image widgets, converts to lightweight formats like webp, jpeg, etc.\n\n### Show width × height below the image preview\n\n*   **Default Value**: Enabled\n*   **Function**: Displays width × height information below image previews, showing image dimension information\n\n![Show width × height below the image preview](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/show-size.jpg)\n\n## Pointer\n\n### Enable trackpad gestures\n\n*   **Default Value**: Enabled\n*   **Function**: This setting enables trackpad mode for the canvas, allowing two-finger pinch zoom and drag.\n\n### Double click interval (maximum)\n\n*   **Default Value**: 300\n*   **Function**: Maximum time (milliseconds) between two clicks of a double-click. Increasing this value helps solve issues where double-clicks are sometimes not recognized.\n\n### Pointer click drift delay\n\n*   **Default Value**: 150\n*   **Function**: Maximum time (milliseconds) to ignore pointer movement after pressing the pointer button. Helps prevent accidental mouse movement when clicking.\n\n### Pointer click drift (maximum distance)\n\n*   **Default Value**: 6\n*   **Function**: If the pointer moves more than this distance while holding the button, it is considered a drag (rather than a click). Helps prevent accidental mouse movement when clicking.\n\n## Reroute\n\n### Reroute spline offset\n\n*   **Default Value**: 20\n*   **Function**: Used to determine the smoothness of curves on both sides of reroute nodes. Larger values make curves smoother, smaller values make curves sharper.\n\n![Reroute spline offset](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/reroute-spline-offset.jpg)"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/moonvalley/moonvalley-video-generation",
  "markdown": "# Moonvalley API Node Official ComfyUI Example\n\nMoonvalley Marey Realism v1.5 is an AI video generation model designed for cinematic-level creation. The model is **trained entirely with commercially licensed content**, ensuring **copyright compliance and commercial safety**.\n\n## Product Highlights\n\n*   Exceptional prompt comprehension: Accurately interprets complex prompt instructions.\n*   Native 1080p HD quality: The training dataset is based on **1080P** videos, resulting in fine and detailed output.\n*   Realistic physics and dynamic performance: Precisely simulates physical motion models and natural dynamics, delivering professional-grade realism.\n*   Complex scene layering and advanced lighting effects: Supports foreground, midground, and background layering in complex scenes, with intelligent spatial relationship understanding.\n*   Production-level control features such as motion and pose transfer: Automatically generates realistic lighting for composite scenes.\n\nCurrently, Moonvalley-related API nodes are natively supported in ComfyUI. You can use the corresponding text-to-video, image-to-video, and video-to-video capabilities directly in ComfyUI.\n\n## Moonvalley Text-to-Video Workflow\n\n### 1\\. Download the Workflow File\n\n[\n\nDownload the workflow file in JSON format\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_text_to_video.json)\n\n### 2\\. Follow the Steps to Run the Workflow\n\n![Text-to-Video Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg)\n\n1.  Enter the positive prompt (content you want to appear in the video)\n2.  Enter the negative prompt (content you do not want to appear in the video)\n3.  Modify the video output resolution\n4.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to start video generation\n5.  After the API returns the result, you can view the generated video in the `Save Video` node. The video will also be saved in the `ComfyUI/output/` directory\n\n## Moonvalley Image-to-Video Workflow\n\n### 1\\. Download the Workflow File\n\n[\n\nDownload the workflow file in JSON format\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video.json)\n\nDownload the image below as the input image ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video_input.webp)\n\n### 2\\. Follow the Steps to Run the Workflow\n\n![Image-to-Video Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg)\n\n1.  Load the input image in the `Load Image` node\n2.  Enter the positive prompt (content you want to appear in the video)\n3.  Enter the negative prompt (content you do not want to appear in the video)\n4.  Modify the video output resolution\n5.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to start video generation\n6.  After the API returns the result, you can view the generated video in the `Save Video` node. The video will also be saved in the `ComfyUI/output/` directory\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/interface/shortcuts",
  "markdown": "# ComfyUI Keyboard Shortcuts and Custom Settings\n\nCurrently, ComfyUI supports custom keyboard shortcuts. You can set the shortcuts by clicking on `Settings (gear icon)` —> `Keybinding`. ![ComfyUI Shortcut Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/keybinding.jpg) In the corresponding menu, you can see all the current shortcut settings for ComfyUI. Click the `edit icon` before the corresponding command to customize the shortcut. Below is the current list of shortcuts for ComfyUI, which you can customize as needed.\n\n| Shortcut | Command |\n| --- | --- |\n| Ctrl + Enter | Queue prompt |\n| Ctrl + Shift + Enter | Queue prompt (Front) |\n| Ctrl + Alt + Enter | Interrupt |\n| Ctrl + Z / Ctrl + Y | Undo/Redo |\n| Ctrl + S | Save workflow |\n| Ctrl + O | Load workflow |\n| Ctrl + A | Select all nodes |\n| Alt + C | Collapse/uncollapse selected nodes |\n| Ctrl + M | Mute/unmute selected nodes |\n| Ctrl + B | Bypass/unbypass selected nodes |\n| Delete  <br>Backspace | Delete selected nodes |\n| Backspace | Clear workflow |\n| Space | Move canvas when holding and moving cursor |\n| Ctrl + Click  <br>Shift + Click | Add clicked node to selection |\n| Ctrl + C/Ctrl + V | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes) |\n| Ctrl + C/Ctrl + Shift + V | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |\n| Shift + Drag | Move multiple selected nodes at the same time |\n| Ctrl + G | Add frame to selected nodes |\n| Ctrl + , | Show settings dialog |\n| Alt + = | Zoom in (canvas) |\n| Alt + - | Zoom out (canvas) |\n| .   | Fit view to selected nodes |\n| P   | Pin/unpin selected items |\n| Q   | Toggle queue sidebar |\n| W   | Toggle workflow sidebar |\n| N   | Toggle node library sidebar |\n| M   | Toggle model library sidebar |\n| Ctrl + \\` | Toggle log bottom panel |\n| F   | Toggle focus mode (full screen) |\n| R   | Refresh node definitions |\n| Double-Click LMB | Quick search for nodes to add |"
},
{
  "url": "https://docs.comfy.org/interface/settings/extension",
  "markdown": "# Extension Settings - ComfyUI\n\nThe Extension settings panel is a special management panel in the ComfyUI frontend settings system, specifically used to manage the enable/disable status of frontend extension plugins. Unlike Custom Nodes, this panel is only used to manage frontend extensions registered by custom nodes, not to disable custom nodes themselves. ![extension](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/extension/extension.jpg) These frontend extension plugins are used to enhance the ComfyUI experience, such as providing shortcuts, settings, UI components, menu items, and other features. Extension status changes require a page reload to take effect:\n\n## Extension Settings Panel Features\n\n### 1\\. Extension List Management\n\nDisplays all registered extensions, including:\n\n*   Extension Name\n*   Core extension identification (displays “Core” label)\n*   Enable/disable status\n\n### 2\\. Search Functionality\n\nProvides a search box to quickly find specific extensions:\n\n### 3\\. Enable/Disable Control\n\nEach extension has an independent toggle switch:\n\n### 4\\. Batch Operations\n\nProvides right-click menu for batch operations:\n\n*   Enable All extensions\n*   Disable All extensions\n*   Disable 3rd Party extensions (keep core extensions)\n\n## Notes\n\n*   Extension status changes require a page reload to take effect\n*   Some core extensions cannot be disabled\n*   The system will automatically disable known problematic extensions\n*   Extension settings are automatically saved to the user configuration file\n\nThis Extension settings panel is essentially a “frontend plugin manager” that allows users to flexibly control ComfyUI’s functional modules."
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter",
  "markdown": "# ComfyUI Depth T2I Adapter Usage Example\n\n## Introduction to T2I Adapter\n\n[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) is a lightweight adapter developed by [Tencent ARC Lab](https://github.com/TencentARC) designed to enhance the structural, color, and style control capabilities of text-to-image generation models (such as Stable Diffusion). It works by aligning external conditions (such as edge detection maps, depth maps, sketches, or color reference images) with the model’s internal features, achieving high-precision control without modifying the original model structure. With only about 77M parameters (approximately 300MB in size), its inference speed is about 3 times faster than [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly), and it supports multiple condition combinations (such as sketch + color grid). Application scenarios include line art to image conversion, color style transfer, multi-element scene generation, and more.\n\n### Comparison Between T2I Adapter and ControlNet\n\nAlthough their functions are similar, there are notable differences in implementation and application:\n\n1.  **Lightweight Design**: T2I Adapter has fewer parameters and a smaller memory footprint\n2.  **Inference Speed**: T2I Adapter is typically about 3 times faster than ControlNet\n3.  **Control Precision**: ControlNet offers more precise control in certain scenarios, while T2I Adapter is more suitable for lightweight control\n4.  **Multi-condition Combination**: T2I Adapter shows more significant resource advantages when combining multiple conditions\n\n### Main Types of T2I Adapter\n\nT2I Adapter provides various types to control different aspects:\n\n*   **Depth**: Controls the spatial structure and depth relationships in images\n*   **Line Art (Canny/Sketch)**: Controls image edges and lines\n*   **Keypose**: Controls character poses and actions\n*   **Segmentation (Seg)**: Controls scene layout through semantic segmentation\n*   **Color**: Controls the overall color scheme of images\n\nIn ComfyUI, using T2I Adapter is similar to [ControlNet](https://docs.comfy.org/tutorials/controlnet/controlnet) in terms of interface and workflow. In this example, we will demonstrate how to use a depth T2I Adapter to control an interior scene. ![ComfyUI Depth T2I Adapter Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)\n\n## Value of Depth T2I Adapter Applications\n\nDepth maps have several important applications in image generation:\n\n1.  **Spatial Layout Control**: Accurately describes three-dimensional spatial structures, suitable for interior design and architectural visualization\n2.  **Object Positioning**: Controls the relative position and size of objects in a scene, suitable for product showcases and scene construction\n3.  **Perspective Relationships**: Maintains reasonable perspective and proportions, suitable for landscape and urban scene generation\n4.  **Light and Shadow Layout**: Natural light and shadow distribution based on depth information, enhancing realism\n\nWe will use interior design as an example to demonstrate how to use the depth T2I Adapter, but these techniques are applicable to other scenarios as well.\n\n## ComfyUI Depth T2I Adapter Workflow Example Explanation\n\n### 1\\. Depth T2I Adapter Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Depth T2I Adapter](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)\n\nPlease download the image below, which we will use as input: ![ComfyUI Interior Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)\n\n### 2\\. Model Installation\n\n*   [interiordesignsuperm\\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [t2iadapter\\_depth\\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── interiordesignsuperm_v2.safetensors\n│   └── controlnet/\n│       └── t2iadapter_depth_sd15v2.pth\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - Depth T2I Adapter Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg)\n\n1.  Ensure that `Load Checkpoint` can load **interiordesignsuperm\\_v2.safetensors**\n2.  Ensure that `Load ControlNet` can load **t2iadapter\\_depth\\_sd15v2.pth**\n3.  Click `Upload` in the `Load Image` node to upload the input image provided earlier\n4.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n## General Tips for Using T2I Adapter\n\n### Input Image Quality Optimization\n\nRegardless of the application scenario, high-quality input images are key to successfully using T2I Adapter:\n\n1.  **Moderate Contrast**: Control images (such as depth maps, line art) should have clear contrast, but not excessively extreme\n2.  **Clear Boundaries**: Ensure that major structures and element boundaries are clearly distinguishable in the control image\n3.  **Noise Control**: Try to avoid excessive noise in control images, especially for depth maps and line art\n4.  **Reasonable Layout**: Control images should have a reasonable spatial layout and element distribution\n\n## Characteristics of T2I Adapter Usage\n\nOne major advantage of T2I Adapter is its ability to easily combine multiple conditions for complex control effects:\n\n1.  **Depth + Edge**: Control spatial layout while maintaining clear structural edges, suitable for architecture and interior design\n2.  **Line Art + Color**: Control shapes while specifying color schemes, suitable for character design and illustrations\n3.  **Pose + Segmentation**: Control character actions while defining scene areas, suitable for complex narrative scenes\n\nMixing different T2I Adapters, or combining them with other control methods (such as ControlNet, regional prompts, etc.), can further expand creative possibilities. To achieve mixing, simply chain multiple `Apply ControlNet` nodes together in the same way as described in [Mixing ControlNet](https://docs.comfy.org/tutorials/controlnet/mixing-controlnets)."
},
{
  "url": "https://docs.comfy.org/interface/settings/about",
  "markdown": "# About Page - ComfyUI\n\nThe About page is an information display panel in the ComfyUI settings system, used to show application version information, related links, and system statistics. These settings can provide us with critical information when you submit feedback or report issues. ![about](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/settings-about.jpg)\n\n### Version Information Badges\n\nThe About page displays the following core version information:\n\n*   **ComfyUI Version**: Shows the backend ComfyUI version number, linked to the official GitHub repository\n*   **ComfyUI\\_frontend Version**: Shows the frontend interface version number, linked to the frontend GitHub repository\n*   **Discord Community**: Provides a link to the ComfyOrg Discord server\n*   **Official Website**: Links to the ComfyOrg official website\n\n### Custom Node Badges\n\nIf custom nodes are installed, the About page will also display additional badge information provided by custom nodes. These badges are registered by each custom node through the `aboutPageBadges` property.\n\n### System Info\n\nThe bottom of the page displays detailed system statistics, including:\n\n*   Hardware configuration information\n*   Software environment information\n*   System performance data\n\n## Extension Developer Guide\n\nExtension developers can add custom badges to the About page by adding the `aboutPageBadges` property to their extension configuration:\n\n```\napp.registerExtension({\n  name: 'MyExtension',\n  aboutPageBadges: [\n    {\n      label: 'My Extension v1.0.0',\n      url: 'https://github.com/myuser/myextension',\n      icon: 'pi pi-github'\n    }\n  ]\n})\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/depth-controlnet",
  "markdown": "# ComfyUI Depth ControlNet Usage Example\n\n## Introduction to Depth Maps and Depth ControlNet\n\nA depth map is a special type of image that uses grayscale values to represent the distance between objects in a scene and the observer or camera. In a depth map, the grayscale value is inversely proportional to distance: brighter areas (closer to white) indicate objects that are closer, while darker areas (closer to black) indicate objects that are farther away. ![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png) Depth ControlNet is a ControlNet model specifically trained to understand and utilize depth map information. It helps AI correctly interpret spatial relationships, ensuring that generated images conform to the spatial structure specified by the depth map, thereby enabling precise control over three-dimensional spatial layouts.\n\n### Application Scenarios for Depth Maps with ControlNet\n\nDepth maps have numerous applications in various scenarios:\n\n1.  **Portrait Scenes**: Control the spatial relationship between subjects and backgrounds, avoiding distortion in critical areas such as faces\n2.  **Landscape Scenes**: Control the hierarchical relationships between foreground, middle ground, and background\n3.  **Architectural Scenes**: Control the spatial structure and perspective relationships of buildings\n4.  **Product Showcase**: Control the separation and spatial positioning of products against their backgrounds\n\nIn this example, we will use a depth map to generate an architectural visualization scene.\n\n## ComfyUI ControlNet Workflow Example Explanation\n\n### 1\\. ControlNet Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![Depth Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_controlnet.png)\n\nPlease download the image below, which we will use as input: ![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)\n\n### 2\\. Model Installation\n\n*   [architecturerealmix\\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [control\\_v11f1p\\_sd15\\_depth\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── architecturerealmix_v11.safetensors\n│   └── controlnet/\n│       └── control_v11f1p_sd15_depth_fp16.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - Depth ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth.jpg)\n\n1.  Ensure that `Load Checkpoint` can load **architecturerealmix\\_v11.safetensors**\n2.  Ensure that `Load ControlNet` can load **control\\_v11f1p\\_sd15\\_depth\\_fp16.safetensors**\n3.  Click `Upload` in the `Load Image` node to upload the depth image provided earlier\n4.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n## Combining Depth Control with Other Techniques\n\nBased on different creative needs, you can combine Depth ControlNet with other types of ControlNet to achieve better results:\n\n1.  **Depth + Lineart**: Maintain spatial relationships while reinforcing outlines, suitable for architecture, products, and character design\n2.  **Depth + Pose**: Control character posture while maintaining correct spatial relationships, suitable for character scenes\n\nFor more information on using multiple ControlNet models together, please refer to the [Mixing ControlNet](https://docs.comfy.org/tutorials/controlnet/mixing-controlnets) example."
},
{
  "url": "https://docs.comfy.org/interface/appearance",
  "markdown": "# Customizing ComfyUI Appearance - ComfyUI\n\nComfyUI offers flexible appearance customization options that allow you to personalize the interface to your preferences.\n\n## Color Palette System\n\nThe primary way to customize ComfyUI’s appearance is through the built-in color palette system. ![Color Palette](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/color-palette.jpg) This allows you to:\n\n1.  Switch ComfyUI themes\n2.  Export the currently selected theme as JSON format\n3.  Load custom theme configuration from JSON file\n4.  Delete custom theme configuration\n\n### How to Customize Color Themes\n\nThe color palette allows you to modify many specific properties. Here are some of the most commonly customized elements, with colors represented in hexadecimal format:\n\n```\n{\n  \"id\": \"dark\",                     // Must be unique, cannot be the same as other theme IDs\n  \"name\": \"Dark (Default)\",         // Theme name, displayed in theme selector\n  \"colors\": {\n    \"node_slot\": {                  // Node connection slot color configuration\n      \"CLIP\": \"#FFD500\",            // CLIP model connection slot color\n      \"CLIP_VISION\": \"#A8DADC\",     // CLIP Vision model connection slot color\n      \"CLIP_VISION_OUTPUT\": \"#ad7452\", // CLIP Vision output connection slot color\n      \"CONDITIONING\": \"#FFA931\",     // Conditioning control connection slot color\n      \"CONTROL_NET\": \"#6EE7B7\",     // ControlNet model connection slot color\n      \"IMAGE\": \"#64B5F6\",           // Image data connection slot color\n      \"LATENT\": \"#FF9CF9\",          // Latent space connection slot color\n      \"MASK\": \"#81C784\",            // Mask data connection slot color\n      \"MODEL\": \"#B39DDB\",           // Model connection slot color\n      \"STYLE_MODEL\": \"#C2FFAE\",     // Style model connection slot color\n      \"VAE\": \"#FF6E6E\",             // VAE model connection slot color\n      \"NOISE\": \"#B0B0B0\",           // Noise data connection slot color\n      \"GUIDER\": \"#66FFFF\",          // Guider connection slot color\n      \"SAMPLER\": \"#ECB4B4\",         // Sampler connection slot color\n      \"SIGMAS\": \"#CDFFCD\",          // Sigmas data connection slot color\n      \"TAESD\": \"#DCC274\"            // TAESD model connection slot color\n    },\n    \"litegraph_base\": {             // LiteGraph base interface configuration\n      \"BACKGROUND_IMAGE\": \"\",        // Background image, default is empty\n      \"CLEAR_BACKGROUND_COLOR\": \"#222\", // Main canvas background color\n      \"NODE_TITLE_COLOR\": \"#999\",    // Node title text color\n      \"NODE_SELECTED_TITLE_COLOR\": \"#FFF\", // Selected node title color\n      \"NODE_TEXT_SIZE\": 14,          // Node text size\n      \"NODE_TEXT_COLOR\": \"#AAA\",     // Node text color\n      \"NODE_TEXT_HIGHLIGHT_COLOR\": \"#FFF\", // Node text highlight color\n      \"NODE_SUBTEXT_SIZE\": 12,       // Node subtext size\n      \"NODE_DEFAULT_COLOR\": \"#333\",   // Node default color\n      \"NODE_DEFAULT_BGCOLOR\": \"#353535\", // Node default background color\n      \"NODE_DEFAULT_BOXCOLOR\": \"#666\", // Node default border color\n      \"NODE_DEFAULT_SHAPE\": 2,        // Node default shape\n      \"NODE_BOX_OUTLINE_COLOR\": \"#FFF\", // Node border outline color\n      \"NODE_BYPASS_BGCOLOR\": \"#FF00FF\", // Node bypass background color\n      \"NODE_ERROR_COLOUR\": \"#E00\",    // Node error state color\n      \"DEFAULT_SHADOW_COLOR\": \"rgba(0,0,0,0.5)\", // Default shadow color\n      \"DEFAULT_GROUP_FONT\": 24,       // Group default font size\n      \"WIDGET_BGCOLOR\": \"#222\",       // Widget background color\n      \"WIDGET_OUTLINE_COLOR\": \"#666\", // Widget outline color\n      \"WIDGET_TEXT_COLOR\": \"#DDD\",    // Widget text color\n      \"WIDGET_SECONDARY_TEXT_COLOR\": \"#999\", // Widget secondary text color\n      \"WIDGET_DISABLED_TEXT_COLOR\": \"#666\", // Widget disabled state text color\n      \"LINK_COLOR\": \"#9A9\",          // Connection line color\n      \"EVENT_LINK_COLOR\": \"#A86\",    // Event connection line color\n      \"CONNECTING_LINK_COLOR\": \"#AFA\", // Connecting line color\n      \"BADGE_FG_COLOR\": \"#FFF\",      // Badge foreground color\n      \"BADGE_BG_COLOR\": \"#0F1F0F\"    // Badge background color\n    },\n    \"comfy_base\": {                  // ComfyUI base interface configuration\n      \"fg-color\": \"#fff\",            // Foreground color\n      \"bg-color\": \"#202020\",         // Background color\n      \"comfy-menu-bg\": \"#353535\",    // Menu background color\n      \"comfy-menu-secondary-bg\": \"#303030\", // Secondary menu background color\n      \"comfy-input-bg\": \"#222\",      // Input field background color\n      \"input-text\": \"#ddd\",          // Input text color\n      \"descrip-text\": \"#999\",        // Description text color\n      \"drag-text\": \"#ccc\",           // Drag text color\n      \"error-text\": \"#ff4444\",       // Error text color\n      \"border-color\": \"#4e4e4e\",     // Border color\n      \"tr-even-bg-color\": \"#222\",    // Table even row background color\n      \"tr-odd-bg-color\": \"#353535\",  // Table odd row background color\n      \"content-bg\": \"#4e4e4e\",       // Content area background color\n      \"content-fg\": \"#fff\",          // Content area foreground color\n      \"content-hover-bg\": \"#222\",    // Content area hover background color\n      \"content-hover-fg\": \"#fff\",    // Content area hover foreground color\n      \"bar-shadow\": \"rgba(16, 16, 16, 0.5) 0 0 0.5rem\" // Bar shadow effect\n    }\n  }\n}\n```\n\n## Canvas\n\n### Background Image\n\n*   **Requirements**: ComfyUI frontend version 1.20.5 or newer\n*   **Function**: Set a custom background image for the canvas to provide a more personalized workspace. You can upload images or use web images to set the background for the canvas.\n\n![Set Background Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/set-as-bg.jpg)\n\n## Node\n\n### Node Opacity\n\n*   **Function**: Set the opacity of nodes, where 0 represents completely transparent and 1 represents completely opaque.\n\n![Node Opacity](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/node-opacity.jpg)\n\n### Textarea Widget Font Size\n\n*   **Range**: 8 - 24\n*   **Function**: Set the font size in textarea widgets. Adjusts the text display size in text input boxes to improve readability.\n\n![Textarea Widget Font Size](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/textarea-font-size.jpg)\n\n*   **Function**: When enabled, the sidebar width will be unified to a consistent width when switching between different sidebars. If disabled, different sidebars can maintain their custom widths when switching.\n\n*   **Function**: Control the size of the sidebar, can be set to normal or small.\n\n*   **Function**: Control whether the sidebar is displayed on the left or right side of the interface, allowing users to adjust the sidebar position according to their usage habits.\n\n## Tree Explorer\n\n### Tree Explorer Item Padding\n\n*   **Function**: Set the padding of items in the tree explorer (sidebar panel), adjusting the spacing between items in the tree structure.\n\n![Tree Explorer Item Padding](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/padding.jpg)\n\n## Advanced Customization with user.css\n\nFor cases where the color palette doesn’t provide enough control, you can use custom CSS via a user.css file. This method is recommended for advanced users who need to customize elements that aren’t available in the color palette system.\n\n### Requirements\n\n*   ComfyUI frontend version 1.20.5 or newer\n\n### Setting Up user.css\n\n1.  Create a file named `user.css` in your ComfyUI user directory (same location as your workflows and settings - see location details below)\n2.  Add your custom CSS rules to this file\n3.  Restart ComfyUI or refresh the page to apply changes\n\n### User Directory Location\n\nThe ComfyUI user directory is where your personal settings, workflows, and customizations are stored. The location depends on your installation type:\n\n```\nC:\\Users\\<your username>\\AppData\\Roaming\\ComfyUI\\user\n```\n\nThis location contains your workflows, settings, and other user-specific files. After finding the above folder location, please copy the corresponding CSS file to the corresponding user directory, such as the default user folder being `ComfyUI/user/default`, then restart ComfyUI or refresh the page to apply changes.\n\n### user.css Examples and Related Instructions\n\nThe `user.css` file is loaded early in the application startup process. So you may need to use `!important` in your CSS rules to ensure they override the default styles. **user.css Customization Examples**\n\n```\n/* Increase font size in inputs and menus for better readability */\n.comfy-multiline-input, .litecontextmenu .litemenu-entry {\n    font-size: 20px !important;\n}\n\n/* Make context menu entries larger for easier selection */\n.litegraph .litemenu-entry,\n.litemenu-title {\n  font-size: 24px !important; \n}\n\n/* Custom styling for specific elements not available in the color palette */\n.comfy-menu {\n    border: 1px solid rgb(126, 179, 189) !important;\n    border-radius: 0px 0px 0px 10px !important;\n    backdrop-filter: blur(2px);\n}\n```\n\n**Best Practices**\n\n1.  **Start with the color palette** for most customizations\n2.  Use **user.css only when necessary** for elements not covered by the color palette\n3.  **Export your theme** before making significant changes so you can revert if needed\n4.  **Share your themes** with the community to inspire others\n\n**Troubleshooting**\n\n*   If your color palette changes don’t appear, try refreshing the page\n*   If CSS customizations don’t work, check that you’re using frontend version 1.20.5+\n*   Try adding `!important` to user.css rules that aren’t being applied\n*   Keep backups of your customizations to easily restore them\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass",
  "markdown": "# ComfyUI Pose ControlNet Usage Example\n\n## Introduction to OpenPose\n\n[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is an open-source real-time multi-person pose estimation system developed by Carnegie Mellon University (CMU), representing a significant breakthrough in the field of computer vision. The system can simultaneously detect multiple people in an image, capturing:\n\n*   **Body skeleton**: 18 keypoints, including head, shoulders, elbows, wrists, hips, knees, and ankles\n*   **Facial expressions**: 70 facial keypoints for capturing micro-expressions and facial contours\n*   **Hand details**: 21 hand keypoints for precisely expressing finger positions and gestures\n*   **Foot posture**: 6 foot keypoints, recording standing postures and movement details\n\n![OpenPose Example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/openpose_example.jpg) In AI image generation, skeleton structure maps generated by OpenPose serve as conditional inputs for ControlNet, enabling precise control over the posture, actions, and expressions of generated characters. This allows us to generate realistic human figures with expected poses and actions, greatly improving the controllability and practical value of AI-generated content. Particularly for early Stable Diffusion 1.5 series models, skeletal maps generated by OpenPose can effectively prevent issues with distorted character actions, limbs, and expressions.\n\n### 1\\. Pose ControlNet Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)\n\nPlease download the image below, which we will use as input: ![ComfyUI Pose Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)\n\n### 2\\. Manual Model Installation\n\n*   [control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)\n*   [majicmixRealistic\\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model&format=SafeTensor&size=pruned&fp=fp16)\n*   [japaneseStyleRealistic\\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model&format=SafeTensor&size=pruned&fp=fp16)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── majicmixRealistic_v7.safetensors\n│   │   └── japaneseStyleRealistic_v20.safetensors\n│   ├── vae/\n│   │   └── vae-ft-mse-840000-ema-pruned.safetensors\n│   └── controlnet/\n│       └── control_v11p_sd15_openpose_fp16.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - Pose ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg) Follow these steps according to the numbered markers in the image:\n\n1.  Ensure that `Load Checkpoint` can load **majicmixRealistic\\_v7.safetensors**\n2.  Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**\n3.  Ensure that `Load ControlNet Model` can load **control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors**\n4.  Click the select button in the `Load Image` node to upload the pose input image provided earlier, or use your own OpenPose skeleton map\n5.  Ensure that `Load Checkpoint` can load **japaneseStyleRealistic\\_v20.safetensors**\n6.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n## Explanation of the Pose ControlNet 2-Pass Workflow\n\nThis workflow uses a two-pass image generation approach, dividing the image creation process into two phases:\n\n### First Phase: Basic Pose Image Generation\n\nIn the first phase, the **majicmixRealistic\\_v7** model is combined with Pose ControlNet to generate an initial character pose image:\n\n1.  First, load the majicmixRealistic\\_v7 model via the `Load Checkpoint` node\n2.  Load the pose control model through the `Load ControlNet Model` node\n3.  The input pose image is fed into the `Apply ControlNet` node and combined with positive and negative prompt conditions\n4.  The first `KSampler` node (typically using 20-30 steps) generates a basic character pose image\n5.  The pixel-space image for the first phase is obtained through `VAE Decode`\n\nThis phase primarily focuses on correct character posture, pose, and basic structure, ensuring that the generated character conforms to the input skeletal pose.\n\n### Second Phase: Style Optimization and Detail Enhancement\n\nIn the second phase, the output image from the first phase is used as a reference, with the **japaneseStyleRealistic\\_v20** model performing stylization and detail enhancement:\n\n1.  The image generated in the first phase creates a larger resolution latent space through the `Upscale latent` node\n2.  The second `Load Checkpoint` loads the japaneseStyleRealistic\\_v20 model, which focuses on details and style\n3.  The second `KSampler` node uses a lower `denoise` strength (typically 0.4-0.6) for refinement, preserving the basic structure from the first phase\n4.  Finally, a higher quality, larger resolution image is output through the second `VAE Decode` and `Save Image` nodes\n\nThis phase primarily focuses on style consistency, detail richness, and enhancing overall image quality.\n\n## Advantages of 2-Pass Image Generation\n\nCompared to single-pass generation, the two-pass image generation method offers the following advantages:\n\n1.  **Higher Resolution**: Two-pass processing can generate high-resolution images beyond the capabilities of single-pass generation\n2.  **Style Blending**: Can combine advantages of different models, such as using a realistic model in the first phase and a stylized model in the second phase\n3.  **Better Details**: The second phase can focus on optimizing details without having to worry about overall structure\n4.  **Precise Control**: Once pose control is completed in the first phase, the second phase can focus on refining style and details\n5.  **Reduced GPU Load**: Generating in two passes allows for high-quality large images with limited GPU resources"
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/mixing-controlnets",
  "markdown": "# ComfyUI Mixing ControlNet Examples - ComfyUI\n\nIn AI image generation, a single control condition often fails to meet the requirements of complex scenes. Mixing multiple ControlNets allows you to control different regions or aspects of an image simultaneously, achieving more precise control over image generation. In certain scenarios, mixing ControlNets can leverage the characteristics of different control conditions to achieve more refined conditional control:\n\n1.  **Scene Complexity**: Complex scenes require multiple control conditions working together\n2.  **Fine-grained Control**: By adjusting the strength parameter of each ControlNet, you can precisely control the degree of influence for each part\n3.  **Complementary Effects**: Different types of ControlNets can complement each other, compensating for the limitations of single controls\n4.  **Creative Expression**: Combining different controls can produce unique creative effects\n\n### How to Mix ControlNets\n\nWhen mixing multiple ControlNets, each ControlNet influences the image generation process according to its applied area. ComfyUI enables multiple ControlNet conditions to be applied sequentially in a layered manner through chain connections in the `Apply ControlNet` node: ![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)\n\n## ComfyUI ControlNet Regional Division Mixing Example\n\nIn this example, we will use a combination of **Pose ControlNet** and **Scribble ControlNet** to generate a scene containing multiple elements: a character on the left controlled by Pose ControlNet and a cat on a scooter on the right controlled by Scribble ControlNet.\n\n### 1\\. ControlNet Mixing Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Mixing ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets.png) \n\nInput pose image (controls the character pose on the left): ![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input.png) Input scribble image (controls the cat and scooter on the right): ![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input_scribble.png)\n\n### 2\\. Manual Model Installation\n\n*   [awpainting\\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)\n*   [control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── awpainting_v14.safetensors\n│   ├── controlnet/\n│   │   └── control_v11p_sd15_scribble_fp16.safetensors\n│   │   └── control_v11p_sd15_openpose_fp16.safetensors\n│   ├── vae/\n│   │   └── vae-ft-mse-840000-ema-pruned.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - Mixing ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg) Follow these steps according to the numbered markers in the image:\n\n1.  Ensure that `Load Checkpoint` can load **awpainting\\_v14.safetensors**\n2.  Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**\n\nFirst ControlNet group using the Openpose model: 3. Ensure that `Load ControlNet Model` loads **control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors** 4. Click `Upload` in the `Load Image` node to upload the pose image provided earlier Second ControlNet group using the Scribble model: 5. Ensure that `Load ControlNet Model` loads **control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors** 6. Click `Upload` in the `Load Image` node to upload the scribble image provided earlier 7. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n## Workflow Explanation\n\n#### Strength Balance\n\nWhen controlling different regions of an image, balancing the strength parameters is particularly important:\n\n*   If the ControlNet strength for one region is significantly higher than another, it may cause that region’s control effect to overpower and suppress the other region\n*   It’s recommended to set similar strength values for ControlNets controlling different regions, for example, both set to 1.0\n\n#### Prompt Techniques\n\nFor regional division mixing, the prompt needs to include descriptions of both regions:\n\n```\n\"A woman in red dress, a cat riding a scooter, detailed background, high quality\"\n```\n\nSuch a prompt covers both the character and the cat on the scooter, ensuring the model pays attention to both control regions.\n\n## Multi-dimensional Control Applications for a Single Subject\n\nIn addition to the regional division mixing shown in this example, another common mixing approach is to apply multi-dimensional control to the same subject. For example:\n\n*   **Pose + Depth**: Control character posture and spatial sense\n*   **Pose + Canny**: Control character posture and edge details\n*   **Pose + Reference**: Control character posture while referencing a specific style\n\nIn this type of application, reference images for multiple ControlNets should be aligned to the same subject, and their strengths should be adjusted to ensure proper balance. By combining different types of ControlNets and specifying their control regions, you can achieve precise control over elements in your image."
},
{
  "url": "https://docs.comfy.org/tutorials/flux/flux-1-controlnet",
  "markdown": "# ComfyUI Flux.1 ControlNet Examples - ComfyUI\n\n ![Flux.1 Canny Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-canny-controlnet.png) ![Flux.1 Depth Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-depth-controlnet.png)\n\n## FLUX.1 ControlNet Model Introduction\n\nFLUX.1 Canny and Depth are two powerful models from the [FLUX.1 Tools](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/). This toolkit is designed to add control and guidance capabilities to FLUX.1, enabling users to modify and recreate real or generated images. **FLUX.1-Depth-dev** and **FLUX.1-Canny-dev** are both 12B parameter Rectified Flow Transformer models that can generate images based on text descriptions while maintaining the structural features of the input image. The Depth version maintains the spatial structure of the source image through depth map extraction techniques, while the Canny version uses edge detection techniques to preserve the structural features of the source image, allowing users to choose the appropriate control method based on different needs. Both models have the following features:\n\n*   Top-tier output quality and detail representation\n*   Excellent prompt following ability while maintaining consistency with the original image\n*   Trained using guided distillation techniques for improved efficiency\n*   Open weights for the research community\n*   API interfaces (pro version) and open-source weights (dev version)\n\nAdditionally, Black Forest Labs also provides **FLUX.1-Depth-dev-lora** and **FLUX.1-Canny-dev-lora** adapter versions extracted from the complete models. These can be applied to the FLUX.1 \\[dev\\] base model to provide similar functionality with smaller file size, especially suitable for resource-constrained environments. We will use the full version of **FLUX.1-Canny-dev** and **FLUX.1-Depth-dev-lora** to complete the workflow examples.\n\n## FLUX.1-Canny-dev Complete Version Workflow\n\n### 1\\. Workflow and Asset\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow ![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev.png) Please download the image below, which we will use as the input image ![ComfyUI Flux.1 Canny Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev-input.png)\n\n### 2\\. Manual Models Installation\n\nComplete model list:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) (Please ensure you have agreed to the corresponding repo’s terms)\n\nFile storage location:\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── t5xxl_fp16.safetensors\n│   ├── vae/\n│   │   └── ae.safetensors\n│   └── diffusion_models/\n│       └── flux1-canny-dev.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Flux.1 Canny Controlnet Step Process](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg)\n\n1.  Make sure `ae.safetensors` is loaded in the `Load VAE` node\n2.  Make sure `flux1-canny-dev.safetensors` is loaded in the `Load Diffusion Model` node\n3.  Make sure the following models are loaded in the `DualCLIPLoader` node:\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n4.  Upload the provided input image in the `Load Image` node\n5.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n### 4\\. Start Your Experimentation\n\nTry using the [FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) model to complete the Depth version of the workflow You can use the image below as input ![ComfyUI Indoor Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png) Or use the following custom nodes to complete image preprocessing:\n\n*   [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n*   [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n## FLUX.1-Depth-dev-lora Workflow\n\nThe LoRA version workflow builds on the complete version by adding the LoRA model. Compared to the [complete version of the Flux workflow](https://docs.comfy.org/tutorials/flux/flux-1-text-to-image), it adds nodes for loading and using the corresponding LoRA model.\n\n### 1\\. Workflow and Asset\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow ![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora.png) Please download the image below, which we will use as the input image ![ComfyUI Flux.1 Depth Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora-input.png)\n\n### 2\\. Manual Model Download\n\nComplete model list:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)\n*   [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)\n\nFile storage location:\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── t5xxl_fp16.safetensors\n│   ├── vae/\n│   │   └── ae.safetensors\n│   ├── diffusion_models/\n│   │   └── flux1-dev.safetensors\n│   └── loras/\n│       └── flux1-depth-dev-lora.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Flux.1 Depth Controlnet Step Process](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg)\n\n1.  Make sure `flux1-dev.safetensors` is loaded in the `Load Diffusion Model` node\n2.  Make sure `flux1-depth-dev-lora.safetensors` is loaded in the `LoraLoaderModelOnly` node\n3.  Make sure the following models are loaded in the `DualCLIPLoader` node:\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n4.  Upload the provided input image in the `Load Image` node\n5.  Make sure `ae.safetensors` is loaded in the `Load VAE` node\n6.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n### 4\\. Start Your Experimentation\n\nTry using the [FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) model to complete the Canny version of the workflow Use [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) or [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to complete image preprocessing\n\nXLab and InstantX + Shakker Labs have released Controlnets for Flux. **InstantX:**\n\n*   [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)\n*   [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)\n*   [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)\n\n**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections) Place these files in the `ComfyUI/models/controlnet` directory. You can visit [Flux Controlnet Example](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png) to get the corresponding workflow image, and use the image from [here](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png) as the input image."
},
{
  "url": "https://docs.comfy.org/tutorials/flux/flux-1-fill-dev",
  "markdown": "# ComfyUI Flux.1 fill dev Example\n\n![Flux.1 fill dev](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-fill-dev-demo.jpeg)\n\n## Introduction to Flux.1 fill dev Model\n\nFlux.1 fill dev is one of the core tools in the [FLUX.1 Tools suite](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/), specifically designed for image inpainting and outpainting. Key features of Flux.1 fill dev:\n\n*   Powerful image inpainting and outpainting capabilities, with results second only to the commercial version FLUX.1 Fill \\[pro\\].\n*   Excellent prompt understanding and following ability, precisely capturing user intent while maintaining high consistency with the original image.\n*   Advanced guided distillation training technology, making the model more efficient while maintaining high-quality output.\n*   Friendly licensing terms, with generated outputs usable for personal, scientific, and commercial purposes, please refer to the [FLUX.1 \\[dev\\] non-commercial license](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) for details.\n\nOpen Source Repository: [FLUX.1 \\[dev\\]](https://huggingface.co/black-forest-labs/FLUX.1-dev) This guide will demonstrate inpainting and outpainting workflows based on the Flux.1 fill dev model. If you’re not familiar with inpainting and outpainting workflows, you can refer to [ComfyUI Layout Inpainting Example](https://docs.comfy.org/tutorials/basic/inpaint) and [ComfyUI Image Extension Example](https://docs.comfy.org/tutorials/basic/outpaint) for some related explanations.\n\nBefore we begin, let’s complete the installation of the Flux.1 Fill dev model files. The inpainting and outpainting workflows will use exactly the same model files. If you’ve previously used the full version of the [Flux.1 Text-to-Image workflow](https://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-fill-dev.safetensors** model file in this section. However, since downloading the corresponding model requires agreeing to the corresponding usage agreement, please visit the [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) page and make sure you have agreed to the corresponding agreement as shown in the image below. ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_fill_dev_agreement.jpg) Complete model list:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)\n\nFile storage location:\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │    ├── clip_l.safetensors\n│   │    └── t5xxl_fp16.safetensors\n│   ├── vae/\n│   │    └── ae.safetensors\n│   └── diffusion_models/\n│        └── flux1-fill-dev.safetensors\n```\n\n## Flux.1 Fill dev inpainting workflow\n\n### 1\\. Inpainting workflow and asset\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![ComfyUI Flux.1 inpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint.png) Please download the image below, we will use it as the input image ![ComfyUI Flux.1 inpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input.png) \n\n### 2\\. Steps to run the workflow\n\n![ComfyUI Flux.1 Fill dev Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_inpaint.jpg)\n\n1.  Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.\n2.  Ensure the `DualCLIPLoader` node has the following models loaded:\n    *   clip\\_name1: `t5xxl_fp16.safetensors`\n    *   clip\\_name2: `clip_l.safetensors`\n3.  Ensure the `Load VAE` node has `ae.safetensors` loaded.\n4.  Upload the input image provided in the document to the `Load Image` node; if you’re using the version without a mask, remember to complete the mask drawing using the mask editor\n5.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Flux.1 Fill dev Outpainting Workflow\n\n### 1\\. Outpainting workflow and asset\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![ComfyUI Flux.1 outpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint.png) Please download the image below, we will use it as the input image ![ComfyUI Flux.1 outpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint_input.png) \n\n### 2\\. Steps to run the workflow\n\n![ComfyUI Flux.1 Fill dev Outpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_outpaint.jpg)\n\n1.  Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.\n2.  Ensure the `DualCLIPLoader` node has the following models loaded:\n    *   clip\\_name1: `t5xxl_fp16.safetensors`\n    *   clip\\_name2: `clip_l.safetensors`\n3.  Ensure the `Load VAE` node has `ae.safetensors` loaded.\n4.  Upload the input image provided in the document to the `Load Image` node\n5.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow"
},
{
  "url": "https://docs.comfy.org/interface/settings/mask-editor",
  "markdown": "# ComfyUI Mask Editor Settings - ComfyUI\n\n## Brush Adjustment\n\n### Brush adjustment speed multiplier\n\n*   **Function**: Controls the speed of brush size and hardness changes during adjustment\n*   **Description**: Higher values mean faster changes\n\n### Lock brush adjustment to dominant axis\n\n*   **Function**: When enabled, brush adjustment will only affect size or hardness based on the direction you move\n*   **Description**: This feature allows users to more precisely control brush property adjustments\n\n## New Editor\n\n### Use new mask editor\n\n*   **Function**: Switch to the new brush editor interface\n*   **Description**: Allows users to switch between new and old editor interfaces\n\nThe new version has a better UI interface and interaction, with more complete functionality ![new](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/maskeditor/new-mask-editor.jpg) \n\n#### 0 reactions\n\nOn this page\n\n*   [Brush Adjustment](#brush-adjustment)\n*   [Brush adjustment speed multiplier](#brush-adjustment-speed-multiplier)\n*   [Lock brush adjustment to dominant axis](#lock-brush-adjustment-to-dominant-axis)\n*   [New Editor](#new-editor)\n*   [Use new mask editor](#use-new-mask-editor)"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/overview",
  "markdown": "# API Nodes - ComfyUI\n\nAPI Nodes are ComfyUI’s new way of calling closed-source models through API requests, providing ComfyUI users with access to external state-of-the-art AI models without complex API key setup.\n\n## What are API Nodes?\n\nAPI Nodes are a set of special nodes that connect to external API services, allowing you to use closed-source or third-party hosted AI models directly in your ComfyUI workflows. These nodes are designed to seamlessly integrate the capabilities of external models while maintaining the open-source nature of ComfyUI’s core. Currently supported models include:\n\n*   **Black Forest Labs**: Flux 1.1\\[pro\\] Ultra, Flux .1\\[pro\\], Flux .1 Kontext Pro, Flux .1 Kontext Max\n*   **Google**: Veo2, Gemini 2.5 Pro, Gemini 2.5 Flash\n*   **Ideogram**: V3, V2, V1\n*   **Kling**: 2.0, 1.6, 1.5 & Various Effects\n*   **Luma**: Photon, Ray2, Ray1.6\n*   **MiniMax**: Text-to-Video, Image-to-Video\n*   **OpenAI**: o1, o1-pro, o3, gpt-4o, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, DALL·E 2, DALL·E 3, GPT-Image-1\n*   **PixVerse**: V4 & Effects\n*   **Pika**: 2.2\n*   **Recraft**: V3, V2 & Various Tools\n*   **Rodin**: 3D Generation\n*   **Stability AI**: Stable Image Ultra, Stable Diffusion 3.5 Large, Image Upscale\n*   **Tripo**: v1-4, v2.0, v2.5\n\n## Prerequisites for Using API Nodes\n\nTo use API Nodes, the following requirements must be met:\n\n### 1\\. ComfyUI Version Requirements\n\nPlease update your ComfyUI to the latest version, as we may add more API support in the future, and corresponding nodes will be updated, so please keep your ComfyUI up to date.\n\n### 2\\. Account and Credits Requirements\n\nYou need to be logged into your ComfyUI with a [Comfy account](https://docs.comfy.org/interface/user) and have a credit balance of [credits](https://docs.comfy.org/interface/credits) greater than 0. Log in via `Settings` -> `User`: ![ComfyUI User Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user.jpg) Go to `Settings` -> `Credits` to purchase credits ![Credits Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/purchase-1.jpg) Please refer to the corresponding documentation for account and credits to ensure this requirement:\n\n*   [Comfy account](https://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.\n*   [Credits](https://docs.comfy.org/interface/credits): After logging in, the settings interface will show a credits menu where you can purchase credits. We use a prepaid system, so there will be no unexpected charges.\n\n### 3\\. Network Environment Requirements\n\nAPI access requires that your current requests are based on a secure network environment. The current requirements for API access are as follows:\n\n*   The local network only allows access from `127.0.0.1` or `localhost`, and you can directly use the login function.\n*   If you are accessing from a local area network or a website that is not on the whitelist, please log in with an API Key. Please refer to [Log in with an API Key](https://docs.comfy.org/interface/user#logging-in-with-an-api-key).\n*   You should be able to access our API service normally (in some regions, you may need to use a proxy service).\n*   Access should be carried out in an `https` environment to ensure the security of the requests.\n\n### 4\\. Using the Corresponding Nodes\n\n**Add to Workflow**: Add the API node to your workflow just like you would with other nodes. **Run**: Set the parameters and then run the workflow. ![API Nodes](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/sidebar.jpg)\n\n## Log in with API Key on non-whitelisted websites\n\nCurrently, we have set up a whitelist to restrict the websites where you can log in to your ComfyUI account. If you need to log in to your ComfyUI account on some non-whitelisted websites, please refer to the account management section to learn how to log in using an API Key. In this case, the corresponding website does not need to be on our whitelist.\n\n[\n\n## Account Management\n\nLearn how to log in with ComfyUI API Key\n\n\n\n](https://docs.comfy.org/interface/user#logging-in-with-an-api-key)\n\n![Select Comfy API Key Login](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user/user-login-api-1.jpg)\n\n## Use ComfyUI API Key Integration to call paid model API nodes\n\nCurrently, we support accessing our services through ComfyUI API Key Integration to call paid model API nodes. Please refer to the API Key Integration section to learn how to use API Key Integration to call paid model API nodes.\n\n[\n\n## API Key Integration\n\nPlease refer to the API Key Integration section to learn how to use API Key Integration to call paid model API nodes\n\n\n\n](https://docs.comfy.org/development/comfyui-server/api-key-integration)\n\n## Advantages of API Nodes\n\nAPI Nodes provide several important advantages for ComfyUI users:\n\n*   **Access to closed-source models**: Use state-of-the-art AI models without having to deploy them yourself\n*   **Seamless integration**: API nodes are fully compatible with other ComfyUI nodes and can be combined to create complex workflows\n*   **Simplified experience**: No need to manage API keys or handle complex API requests\n*   **Controlled costs**: The prepaid system ensures you have complete control over your spending with no unexpected charges\n\n## Pricing\n\n[\n\n## API Node Pricing\n\nPlease refer to the pricing page for the corresponding API pricing\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/pricing)\n\n## About Open Source and Opt-in\n\nIt’s important to note that **API Nodes are completely optional**. ComfyUI will always remain fully open-source and free for local users. API nodes are designed as an “opt-in” feature, providing convenience for those who want access to external SOTA (state-of-the-art) models.\n\n## Use Cases\n\nA powerful application of API Nodes is combining the output of external models with local nodes. For example:\n\n*   Using GPT-Image-1 to generate a base image, then transforming it into video with a local `wan` node\n*   Combining externally generated images with local upscaling or style transfer nodes\n*   Creating hybrid workflows that leverage the advantages of both closed-source and open-source models\n\nThis flexibility makes ComfyUI a truly universal generative AI interface, integrating various AI capabilities into a unified workflow, opening up more possibilities\n\n## FAQs"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/rodin/model-generation",
  "markdown": "# Rodin API Node Model Generation ComfyUI Official Example\n\nHyper3D Rodin (hyper3d.ai) is a platform focused on rapidly generating high-quality, production-ready 3D models and materials through artificial intelligence. ComfyUI has now natively integrated the corresponding Rodin model generation API, allowing you to conveniently use the related nodes in ComfyUI for model generation. Currently, ComfyUI’s API nodes support the following Rodin model generation capabilities:\n\n*   Single-view model generation\n*   Multi-view model generation\n*   Model generation with different levels of detail\n\n## Single-view Model Generation Workflow\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/rodin_image_to_model.json)\n\nDownload the image below as input image ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/doll.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Rodin Image to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  (Optional) In `Rodin 3D Generate - Regular Generate` adjust the corresponding parameters\n    *   polygon\\_count: You can set different polygon counts, the higher the value, the smoother and more detailed the model\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/Rodin` directory\n4.  In the `Preview 3D` node, click to expand the menu\n5.  Select `Export` to directly export the corresponding model\n\n## Multi-view Model Generation Workflow\n\nThe corresponding `Rodin 3D Generate - Regular Generate` allows up to 5 image inputs\n\n### 1\\. Workflow File Download\n\nYou can modify the single-view workflow to a multi-view workflow, or directly download the workflow file below Download the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/api_rodin_multiview_to_model.json)\n\nDownload the images below as input images ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/front.jpg) ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/back.jpg) ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/left.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Rodin Image to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Load Image` node, load the provided input images\n2.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/Rodin` directory\n3.  In the `Preview 3D` node, click to expand the menu\n4.  Select `Export` to directly export the corresponding model\n\nCurrently, Rodin provides different types of model generation nodes in ComfyUI, since the corresponding input conditions are the same as the workflow introduced in this article, you can enable them as needed. In addition, we have provided corresponding nodes in the corresponding templates, you can also modify the corresponding node mode as needed to enable them ![Rodin Other Related Nodes](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/other_nodes.jpg)"
},
{
  "url": "https://docs.comfy.org/tutorials/image/hidream/hidream-e1",
  "markdown": "# ComfyUI Native HiDream-E1, E1.1 Workflow Example\n\n![HiDream-E1 Demo](https://raw.githubusercontent.com/HiDream-ai/HiDream-E1/refs/heads/main/assets/demo.jpg) HiDream-E1 is an interactive image editing large model officially open-sourced by HiDream-ai, built based on HiDream-I1. It allows you to edit images using natural language. The model is released under the [MIT License](https://github.com/HiDream-ai/HiDream-E1?tab=MIT-1-ov-file), supporting use in personal projects, scientific research, and commercial applications. In combination with the previously released [hidream-i1](https://docs.comfy.org/tutorials/image/hidream/hidream-i1), it enables **creative capabilities from image generation to editing**.\n\n| Name | Update Date | Inference Steps | Resolution | HuggingFace Repository |\n| --- | --- | --- | --- | --- |\n| HiDream-E1-Full | 2025-4-28 | 28  | 768x768 | 🤗 [HiDream-E1-Full](https://huggingface.co/HiDream-ai/HiDream-E1-Full) |\n| HiDream-E1.1 | 2025-7-16 | 28  | Dynamic (1 Megapixel) | 🤗 [HiDream-E1.1](https://huggingface.co/HiDream-ai/HiDream-E1-1) |\n\n[HiDream E1 - Github](https://github.com/HiDream-ai/HiDream-E1)\n\nAll the models involved in this guide can be found [here](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files). Except for the Diffusion model, E1 and E1.1 use the same models. The corresponding workflow files also include the relevant model information. You can choose to manually download and save the models, or follow the workflow prompts to download them after loading the workflow. It is recommended to use E1.1. This model requires a large amount of VRAM to run. Please refer to the relevant sections for specific VRAM requirements. **Diffusion Model** You do not need to download both models. Since E1.1 is an iterative version based on E1, our tests show that its quality and performance are significantly improved compared to E1.\n\n*   [hidream\\_e1\\_1\\_bf16.safetensors (Recommended)](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_1_bf16.safetensors) 34.2GB\n*   [hidream\\_e1\\_full\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_full_bf16.safetensors) 34.2GB\n\n**Text Encoder**:\n\n*   [clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors) 236.12MB\n*   [clip\\_g\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors) 1.29GB\n*   [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) 4.8GB\n*   [llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors) 8.46GB\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) 319.77MB\n\n> This is the VAE model for Flux. If you have used the Flux workflow before, you may have already downloaded this file.\n\nModel Save Location\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 text_encoders/\n│   │   ├─── clip_l_hidream.safetensors\n│   │   ├─── clip_g_hidream.safetensors\n│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors\n│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors\n│   └── 📂 vae/\n│   │   └── ae.safetensors\n│   └── 📂 diffusion_models/\n│       ├── hidream_e1_1_bf16.safetensors\n│       └── hidream_e1_full_bf16.safetensors\n```\n\nE1.1 is an updated version released on July 16, 2025. This version supports dynamic 1-megapixel resolution, and the workflow uses the `Scale Image to Total Pixels` node to dynamically adjust the input image to 1 million pixels.\n\n### 1\\. HiDream E1.1 Workflow and Related Materials\n\nDownload the image below and drag it into ComfyUI with the corresponding workflow and models loaded: ![HiDream E1.1 Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/hidream_e1_1.png) Download the image below as input: ![HiDream E1.1 Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/input.webp) \n\n### 2\\. Step-by-step Guide to Running the HiDream-e1 Workflow\n\n![hidream_e1_1_guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/hidream/hidream-e1-1-guide.jpg) Follow these steps to run the workflow:\n\n1.  Make sure the `Load Diffusion Model` node loads the `hidream_e1_1_bf16.safetensors` model.\n2.  Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly:\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node uses the `ae.safetensors` file.\n4.  In the `Load Image` node, load the provided input or your desired image.\n5.  In the `Empty Text Encoder(Positive)` node, enter **the modifications you want to make to the image**.\n6.  In the `Empty Text Encoder(Negative)` node, enter **the content you do not want to appear in the image**.\n7.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute image generation.\n\n### 3\\. Additional Notes on the Workflow\n\n*   Since HiDream E1.1 supports dynamic input with a total of 1 million pixels, the workflow uses `Scale Image to Total Pixels` to process and convert all input images, which may cause the aspect ratio to differ from the original input image.\n*   When using the fp16 version of the model, in actual tests, the full version ran out of memory on both A100 40GB and 4090D 24GB, so the workflow is set by default to use `fp8_e4m3fn_fast` for inference.\n\n## HiDream E1 ComfyUI Native Workflow Example\n\nE1 is a model released on April 28, 2025. This model only supports 768\\*768 resolution.\n\n### 1\\. HiDream-e1 workflow\n\nPlease download the image below and drag it into ComfyUI. The workflow already contains model download information, and after loading, it will prompt you to download the corresponding models. ![ComfyUI Native HiDream-e1 Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/hidream_e1_full.png) Download this image below as input: ![ComfyUI Native HiDream-e1 Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/input.webp) \n\n### 2\\. Complete the HiDream-e1 Workflow Step by Step\n\n![hidream_e1_full_step_guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg) Follow these steps to complete the workflow:\n\n1.  Make sure the `Load Diffusion Model` node has loaded the `hidream_e1_full_bf16.safetensors` model\n2.  Ensure that the four corresponding text encoders are correctly loaded in the `QuadrupleCLIPLoader`\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node is using the `ae.safetensors` file\n4.  Load the input image we downloaded earlier in the `Load Image` node\n5.  (Important) Enter **the prompt for how you want to modify the image** in the `Empty Text Encoder(Positive)` node\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n\n### Additional Notes on ComfyUI HiDream-e1 Workflow\n\n*   You may need to modify the prompt multiple times or generate multiple times to get better results\n*   This model has difficulty maintaining consistency when changing image styles, so try to make your prompts as complete as possible\n*   As the model supports a resolution of 768\\*768, in actual testing with other dimensions, the image performance is poor or even significantly different at other dimensions"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/pricing",
  "markdown": "# Pricing - ComfyUI\n\nBFLFlux 1.1 \\[pro\\] Ultra ImageNA$0.06BFLFlux.1 Canny Control ImageNA$0.05BFLFlux.1 Depth Control ImageNA$0.05BFLFlux.1 Expand ImageNA$0.05BFLFlux.1 Fill ImageNA$0.05BFLFlux.1 Kontext \\[max\\] ImageNA$0.08BFLFlux.1 Kontext \\[pro\\] ImageNA$0.04BFLFlux.1 Kontext \\[pro\\] ImageNA$0.05KlingKling Image Generationkling-v1-5, 1, image to image$0.028KlingKling Image Generationkling-v1-5, 1, text to image$0.014KlingKling Image Generationkling-v1, 1, image to image$0.0035KlingKling Image Generationkling-v1, 1, text to image$0.0035KlingKling Image Generationkling-v2, 1, text to image$0.014KlingKling Virtual Try OnNA$0.07KlingKling, Text to Video (Camera Control)NA$0.49KlingKling Dual Character Video, Effectskling-v1-5, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1-5, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1-5, std, 5$0.28KlingKling Dual Character Video, Effectskling-v1-5, std, 10$0.56KlingKling Dual Character Video, Effectskling-v1-6, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1-6, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1-6, std, 5$0.28KlingKling Dual Character Video, Effectskling-v1-6, std, 10$0.56KlingKling Dual Character Video, Effectskling-v1, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1, std, 5$0.14KlingKling Dual Character Video, Effectskling-v1, std, 10$0.28KlingKling Image to Videokling-v1-5, pro, 5$0.49KlingKling Image to Videokling-v1-5, pro, 10$0.98KlingKling Image to Videokling-v1-5, std, 5$0.28KlingKling Image to Videokling-v1-5, std, 10$0.56KlingKling Image to Videokling-v1-6, pro, 5$0.49KlingKling Image to Videokling-v1-6, pro, 10$0.98KlingKling Image to Videokling-v1-6, std, 5$0.28KlingKling Image to Videokling-v1-6, std, 10$0.56KlingKling Image to Videokling-v1, pro, 5$0.49KlingKling Image to Videokling-v1, pro, 10$0.98KlingKling Image to Videokling-v1, std, 5$0.14KlingKling Image to Videokling-v1, std, 10$0.28KlingKling Image to Videokling-v2-maser, pro, 5s$1.4KlingKling Image to Videokling-v2-maser, pro, 10s$2.8KlingKling Image to Videokling-v2-maser, std, 5s$1.4KlingKling Image to Videokling-v2-maser, std, 10s$2.8KlingKling Lip Sync Video with, Audio5s$0.07KlingKling Lip Sync Video with, Audio10s$0.14KlingKling Lip Sync Video with Text5s$0.07KlingKling Lip Sync Video with Text10s$0.14KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1$0.49KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1-5$0.49KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1-6$0.49KlingKling Start-End Frame to Videopro mode / 10s duration / kling-v1-5$0.98KlingKling Start-End Frame to Videopro mode / 10s duration / kling-v1-6$0.98KlingKling Start-End Frame to Videostandard mode / 5s duration / kling-v1$0.14KlingKling Text to Videopro mode / 5s duration / kling-v1$0.49KlingKling Text to Videopro mode / 5s duration / kling-v2-master$1.4KlingKling Text to Videopro mode / 10s duration / kling-v1$0.98KlingKling Text to Videopro mode / 10s duration / kling-v2-master$2.8KlingKling Text to Videostandard mode / 5s duration / kling-v1$0.14KlingKling Text to Videostandard mode / 5s duration / kling-v1-6$0.28KlingKling Text to Videostandard mode / 5s duration / kling-v2-master$1.4KlingKling Text to Videostandard mode / 10s duration / kling-v1$0.28KlingKling Text to Videostandard mode / 10s duration / kling-v1-6$0.56KlingKling Text to Videostandard mode / 10s duration / kling-v2-master$2.8KlingKling Video Effectsdizzydizzy/bloombloom, 5$0.49KlingKling Video Effectsfuzzyfuzzy/squish/expansion, 5$0.28KlingKling Video ExtendNA$0.28LumaLuma, Text to Imagephoto-flash-1$0.0019LumaLuma, Text to Imagephoto-flash-1$0.0019LumaLuma Image to Imagephoton-1$0.0073LumaLuma Image to Imagephoton-1$0.0073LumaLuma Image to Videoray-1-6, 4k, 5s$3.19LumaLuma Image to Videoray-1-6, 4k, 9s$5.73LumaLuma Image to Videoray-1-6, 540p, 5s$0.2LumaLuma Image to Videoray-1-6, 540p, 9s$0.36LumaLuma Image to Videoray-1-6, 720p, 5s$0.35LumaLuma Image to Videoray-1-6, 720p, 9s$0.64LumaLuma Image to Videoray-1-6, 1080p, 5s$0.8LumaLuma Image to Videoray-1-6, 1080p, 9s$1.43LumaLuma Image to Videoray-2, 4k, 5s$6.37LumaLuma Image to Videoray-2, 4k, 9s$11.47LumaLuma Image to Videoray-2, 540p, 5s$0.4LumaLuma Image to Videoray-2, 540p, 9s$0.72LumaLuma Image to Videoray-2, 720p, 5s$0.71LumaLuma Image to Videoray-2, 720p, 9s$1.27LumaLuma Image to Videoray-2, 1080p, 5s$1.59LumaLuma Image to Videoray-2, 1080p, 9s$2.87LumaLuma Image to Videoray-flash-2, 4k, 5s$2.19LumaLuma Image to Videoray-flash-2, 4k, 9s$3.94LumaLuma Image to Videoray-flash-2, 540p, 5s$0.14LumaLuma Image to Videoray-flash-2, 540p, 9s$0.25LumaLuma Image to Videoray-flash-2, 720p, 5s$0.24LumaLuma Image to Videoray-flash-2, 720p, 9s$0.44LumaLuma Image to Videoray-flash-2, 1080p, 5s$0.55LumaLuma Image to Videoray-flash-2, 1080p, 9s$0.99LumaLuma Text-to-videoray-1-6, 4k, 5s$3.19LumaLuma Text-to-videoray-1-6, 4k, 9s$5.73LumaLuma Text-to-videoray-1-6, 540p, 5s$0.2LumaLuma Text-to-videoray-1-6, 540p, 9s$0.36LumaLuma Text-to-videoray-1-6, 720p, 5s$0.35LumaLuma Text-to-videoray-1-6, 720p, 9s$0.64LumaLuma Text-to-videoray-1-6, 1080p, 5s$0.8LumaLuma Text-to-videoray-1-6, 1080p, 9s$1.43LumaLuma Text-to-videoray-2, 4k, 5s$6.37LumaLuma Text-to-videoray-2, 4k, 9s$11.47LumaLuma Text-to-videoray-2, 540p, 5s$0.4LumaLuma Text-to-videoray-2, 540p, 9s$0.72LumaLuma Text-to-videoray-2, 720p, 5s$0.71LumaLuma Text-to-videoray-2, 720p, 9s$1.27LumaLuma Text-to-videoray-2, 1080p, 5s$1.59LumaLuma Text-to-videoray-2, 1080p, 9s$2.87LumaLuma Text-to-videoray-flash-2, 4k, 5s$2.19LumaLuma Text-to-videoray-flash-2, 4k, 9s$3.94LumaLuma Text-to-videoray-flash-2, 540p, 5s$0.14LumaLuma Text-to-videoray-flash-2, 540p, 9s$0.25LumaLuma Text-to-videoray-flash-2, 720p, 5s$0.24LumaLuma Text-to-videoray-flash-2, 720p, 9s$0.44LumaLuma Text-to-videoray-flash-2, 1080p, 5s$0.55LumaLuma Text-to-videoray-flash-2, 1080p, 9s$0.99GoogleGoogle Veo2 Video Generation5$2.5GoogleGoogle Veo2 Video Generation8$4GoogleGoogle Geminigemini-2.5-flash-preview-04-171.25/1Minputtokens+1.25/1M input tokens + 10/1M output tokens (< 200K tokens)GoogleGoogle Geminigemini-2.5-pro-preview-05-060.16/1Minputtokens+0.16/1M input tokens + 0.6/1M output tokens + $1/1M input audio tokens (< 200K tokens)MinimaxMinimax, Text to Video6s clip$0.43MinimaxMinimax Hailuo-02768P 6s$0.28MinimaxMinimax Hailuo-02768P 10s$0.56MinimaxMinimax Hailuo-021080P 6s$0.49MinimaxMinimax Image to Video6s clip$0.43RecraftRecraft, Creative Upscale ImageNA$0.25RecraftRecraft, Crisp Upscale ImageNA$0.004RecraftRecraft, Image Inpainting1$0.04RecraftRecraft, Image to Image1$0.04RecraftRecraft, Remove BackgroundNA$0.01RecraftRecraft, Replace Background1$0.04RecraftRecraft, Text to Image1$0.04RecraftRecraft, Text to Vector1$0.08RecraftRecraft, Vectorize ImageNA$0.01IdeogramIdeogram, V11, false$0.06IdeogramIdeogram, V11, true$0.02IdeogramIdeogram V21, false$0.08IdeogramIdeogram V21, true$0.05IdeogramIdeogram V31, Balanced$0.06IdeogramIdeogram V31, Quality$0.09IdeogramIdeogram V31, Turbo$0.03RunwayRuway, Text to ImageNA$0.08RunwayRunway, First-Last-Frame to Video5s$0.25RunwayRunway, First-Last-Frame to Video10s$0.5RunwayRunway Image to Video (Gen3a, Turbo)5s$0.25RunwayRunway Image to Video (Gen3a, Turbo)10s$0.5RunwayRunway Image to Video (Gen4, Turbo)5s$0.25RunwayRunway Image to Video (Gen4, Turbo)10s$0.5OpenAIGPT-Image-1 - Actualinput image tokens10/1Mtokens+,inputtexttokens10 / 1M tokens +, input text tokens5 / 1M tokens +,output tokens$40 / 1M tokensOpenAIGPT-Image-1 (Approximate price)high, 1024x1024$0.167OpenAIGPT-Image-1 (Approximate price)high, 1024x1536$0.25OpenAIGPT-Image-1 (Approximate price)high, 1536x1024$0.25OpenAIGPT-Image-1 (Approximate price)low, 1024x1024$0.011OpenAIGPT-Image-1 (Approximate price)low, 1024x1536$0.016OpenAIGPT-Image-1 (Approximate price)low, 1536x1024$0.016OpenAIGPT-Image-1 (Approximate price)medium, 1024x1024$0.042OpenAIGPT-Image-1 (Approximate price)medium, 1024x1536$0.063OpenAIGPT-Image-1 (Approximate price)medium, 1536x1024$0.063OpenAIImage Generation (DALL·E 2)size = 512 \\* 512$0.018OpenAIImage Generation (DALL·E 2)size = 1024 \\* 1024$0.02OpenAIImage Generation (DALL·E 2)size 256 \\* 256$0.016OpenAIImage Generation (DALL·E 3 HD)size = 1024 \\* 1024, hd$0.08OpenAIImage Generation (DALL·E 3 HD)size = 1024 \\* 1792, hd$0.12OpenAIImage Generation (DALL·E 3 HD)size = 1792 \\* 1024, hd$0.12OpenAIImage Generation (DALL·E 3 Std)size = 1024 \\* 1024,std$0.04OpenAIImage Generation (DALL·E 3 Std)size = 1024 \\* 1792, std$0.08OpenAIImage Generation (DALL·E 3 Std)size = 1792 \\* 1024, std$0.08PixversePixVerse, Text to Video360p fast 5s$0.9PixversePixVerse, Text to Video360p normal 5s$0.45PixversePixVerse, Text to Video360p normal 8s$0.9PixversePixVerse, Text to Video540p fast 5s$0.9PixversePixVerse, Text to Video540p normal 5s$0.45PixversePixVerse, Text to Video540p normal 8s$0.9PixversePixVerse, Text to Video720p fast 5s$1.2PixversePixVerse, Text to Video720p normal 5s$0.6PixversePixVerse, Text to Video720p normal 8s$1.2PixversePixVerse, Text to Video1080p normal 5s$1.2PixversePixVerse, Transition Video360p fast 5s$0.9PixversePixVerse, Transition Video360p normal 5s$0.45PixversePixVerse, Transition Video360p normal 8s$0.9PixversePixVerse, Transition Video540p fast 5s$0.9PixversePixVerse, Transition Video540p normal 5s$0.45PixversePixVerse, Transition Video540p normal 8s$0.9PixversePixVerse, Transition Video720p fast 5s$1.2PixversePixVerse, Transition Video720p normal 5s$0.6PixversePixVerse, Transition Video720p normal 8s$1.2PixversePixVerse, Transition Video1080p normal 5s$1.2PixversePixVerse,Image to Video360p fast 5s$0.9PixversePixVerse,Image to Video360p normal 5s$0.45PixversePixVerse,Image to Video360p normal 8s$0.9PixversePixVerse,Image to Video540p fast 5s$0.9PixversePixVerse,Image to Video540p normal 5s$0.45PixversePixVerse,Image to Video540p normal 8s$0.9PixversePixVerse,Image to Video720p fast 5s$1.2PixversePixVerse,Image to Video720p normal 5s$0.6PixversePixVerse,Image to Video720p normal 8s$1.2PixversePixVerse,Image to Video1080p normal 5s$1.2PikaPika, Scenes (Video Image Composition)720p, 5s$0.3PikaPika, Scenes (Video Image Composition)720p, 10s$0.4PikaPika, Scenes (Video Image Composition)1080p, 5s$0.5PikaPika, Scenes (Video Image Composition)1080p, 10s$1.5PikaPika, Start and End Frame to Video720p, 5s$0.2PikaPika, Start and End Frame to Video720p, 10s$0.25PikaPika, Start and End Frame to Video1080p, 5s$0.3PikaPika, Start and End Frame to Video1080p, 10s$1PikaPika, Text to Video720p, 5s$0.2PikaPika, Text to Video720p, 10s$0.6PikaPika, Text to Video1080p, 5s$0.45PikaPika, Text to Video1080p, 10s$1PikaPika,Image to Video720p, 5s$0.2PikaPika,Image to Video720p, 10s$0.6PikaPika,Image to Video1080p, 5s$0.45PikaPika,Image to Video1080p, 10s$1PikaPika Swaps, (Video Object Replacement)NA$0.3PikaPikadditios, (Video Object Insertion)NA$0.3PikaPikaffects, (Video Effects)NA$0.45MoonvalleyImage to video - 5sNA$1.5MoonvalleyText to video - 5sNA$1.5MoonvalleyVideo to video - 5sNA$2.25RodinRodin 3D, Generate - Regular GenerateNA$0.4RodinRodin 3D Generate - Detail, GenerateNA$0.4RodinRodin 3D Generate - Sketch, GenerateNA$0.4RodinRodin 3D Generate - Smooth, GenerateNA$0.4TripoTripo:, Text to Modelany style, false, any quality, false$0.15TripoTripo:, Text to Modelany style, false, any quality, true$0.2TripoTripo:, Text to Modelany style, true, detailed, false$0.35TripoTripo:, Text to Modelany style, true, detailed, true$0.4TripoTripo:, Text to Modelany style, true, standard, false$0.25TripoTripo:, Text to Modelany style, true, standard, true$0.3TripoTripo:, Text to Modelnone, false, any, quality, false$0.1TripoTripo:, Text to Modelnone, false, any quality, true$0.15TripoTripo:, Text to Modelnone, true, detailed, false$0.3TripoTripo:, Text to Modelnone, true, detailed, true$0.35TripoTripo:, Text to Modelnone, true, standard, false$0.2TripoTripo:, Text to Modelnone, true, standard, true$0.25TripoTripo:,Image to Model / Multiview to Modelany style, false, any quality, false$0.25TripoTripo:,Image to Model / Multiview to Modelany style, false, any quality, true$0.3TripoTripo:,Image to Model / Multiview to Modelany style, true, detailed, false$0.45TripoTripo:,Image to Model / Multiview to Modelany style, true, detailed, true$0.5TripoTripo:,Image to Model / Multiview to Modelany style, true, standard, false$0.35TripoTripo:,Image to Model / Multiview to Modelany style, true, standard, true$0.4TripoTripo:,Image to Model / Multiview to Modelnone, false, any, quality, false$0.2TripoTripo:,Image to Model / Multiview to Modelnone, false, any quality, true$0.25TripoTripo:,Image to Model / Multiview to Modelnone, true, detailed, false$0.4TripoTripo:,Image to Model / Multiview to Modelnone, true, detailed, true$0.45TripoTripo:,Image to Model / Multiview to Modelnone, true, standard, false$0.3TripoTripo:,Image to Model / Multiview to Modelnone, true, standard, true$0.35TripoTripo: Convert modelNA$0.1TripoTripo: Refine Draft modelNA$0.3TripoTripo: Retarget rigged modelNA$0.1TripoTripo: Rig modelNA$0.25TripoTripo: Texture modeldetailed$0.2TripoTripo: Texture modelstandard$0.1Stability AIStability, AI Stable Image UltraNA$0.08Stability AIStability AI Stable Diffusion, 3.5 Imagesd3.5-large$0.065Stability AIStability AI Stable Diffusion, 3.5 Imagesd3.5-medium$0.035Stability AIStability AI Upscale, ConservativeNA$0.25Stability AIStability AI Upscale CreativeNA$0.25Stability AIStability AI Upscale FastNA$0.01"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/server_overview",
  "markdown": "# Properties - ComfyUI\n\n### Simple Example\n\nHere’s the code for the Invert Image Node, which gives an overview of the key concepts in custom node development.\n\n```\nclass InvertImageNode:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": { \"image_in\" : (\"IMAGE\", {}) },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"image_out\",)\n    CATEGORY = \"examples\"\n    FUNCTION = \"invert\"\n\n    def invert(self, image_in):\n        image_out = 1 - image_in\n        return (image_out,)\n```\n\n### Main properties\n\nEvery custom node is a Python class, with the following key properties:\n\n#### INPUT\\_TYPES\n\n`INPUT_TYPES`, as the name suggests, defines the inputs for the node. The method returns a `dict` which _must_ contain the key `required`, and _may_ also include the keys `optional` and/or `hidden`. The only difference between `required` and `optional` inputs is that `optional` inputs can be left unconnected. For more information on `hidden` inputs, see [Hidden Inputs](https://docs.comfy.org/custom-nodes/backend/more_on_inputs#hidden-inputs). Each key has, as its value, another `dict`, in which key-value pairs specify the names and types of the inputs. The types are defined by a `tuple`, the first element of which defines the data type, and the second element of which is a `dict` of additional parameters. Here we have just one required input, named `image_in`, of type `IMAGE`, with no additional parameters. Note that unlike the next few attributes, this `INPUT_TYPES` is a `@classmethod`. This is so that the options in dropdown widgets (like the name of the checkpoint to be loaded) can be computed by Comfy at run time. We’ll go into this more later.\n\n#### RETURN\\_TYPES\n\nA `tuple` of `str` defining the data types returned by the node. If the node has no outputs this must still be provided `RETURN_TYPES = ()`\n\n#### RETURN\\_NAMES\n\nThe names to be used to label the outputs. This is optional; if omitted, the names are simply the `RETURN_TYPES` in lowercase.\n\n#### CATEGORY\n\nWhere the node will be found in the ComfyUI **Add Node** menu. Submenus can be specified as a path, eg. `examples/trivial`.\n\n#### FUNCTION\n\nThe name of the Python function in the class that should be called when the node is executed. The function is called with named arguments. All `required` (and `hidden`) inputs will be included; `optional` inputs will be included only if they are connected, so you should provide default values for them in the function definition (or capture them with `**kwargs`). The function returns a tuple corresponding to the `RETURN_TYPES`. This is required even if nothing is returned (`return ()`). Again, if you only have one output, remember that trailing comma `return (image_out,)`!\n\nA great feature of Comfy is that it caches outputs, and only executes nodes that might produce a different result than the previous run. This can greatly speed up lots of workflows. In essence this works by identifying which nodes produce an output (these, notably the Image Preview and Save Image nodes, are always executed), and then working backwards to identify which nodes provide data that might have changed since the last run. Two optional features of a custom node assist in this process.\n\n#### OUTPUT\\_NODE\n\nBy default, a node is not considered an output. Set `OUTPUT_NODE = True` to specify that it is.\n\n#### IS\\_CHANGED\n\nBy default, Comfy considers that a node has changed if any of its inputs or widgets have changed. This is normally correct, but you may need to override this if, for instance, the node uses a random number (and does not specify a seed - it’s best practice to have a seed input in this case so that the user can control reproducibility and avoid unnecessary execution), or loads an input that may have changed externally, or sometimes ignores inputs (so doesn’t need to execute just because those inputs changed).\n\n`IS_CHANGED` is passed the same arguments as the main function defined by `FUNCTION`, and can return any Python object. This object is compared with the one returned in the previous run (if any) and the node will be considered to have changed if `is_changed != is_changed_old` (this code is in `execution.py` if you need to dig). Since `True == True`, a node that returns `True` to say it has changed will be considered not to have! I’m sure this would be changed in the Comfy code if it wasn’t for the fact that it might break existing nodes to do so. To specify that your node should always be considered to have changed (which you should avoid if possible, since it stops Comfy optimising what gets run), `return float(\"NaN\")`. This returns a `NaN` value, which is not equal to anything, even another `NaN`. A good example of actually checking for changes is the code from the built-in LoadImage node, which loads the image and returns a hash\n\n```\n    @classmethod\n    def IS_CHANGED(s, image):\n        image_path = folder_paths.get_annotated_filepath(image)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n```\n\n### Other attributes\n\nThere are three other attributes that can be used to modify the default Comfy treatment of a node.\n\n#### INPUT\\_IS\\_LIST, OUTPUT\\_IS\\_LIST\n\nThese are used to control sequential processing of data, and are described [later](https://docs.comfy.org/custom-nodes/backend/lists).\n\n### VALIDATE\\_INPUTS\n\nIf a class method `VALIDATE_INPUTS` is defined, it will be called before the workflow begins execution. `VALIDATE_INPUTS` should return `True` if the inputs are valid, or a message (as a `str`) describing the error (which will prevent execution).\n\n#### Validating Constants\n\n`VALIDATE_INPUTS` is called with only the inputs that its signature requests (those returned by `inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args`). Any inputs which are received in this way will _not_ run through the default validation rules. For example, in the following snippet, the front-end will use the specified `min` and `max` values of the `foo` input, but the back-end will not enforce it.\n\n```\nclass CustomNode:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": { \"foo\" : (\"INT\", {\"min\": 0, \"max\": 10}) },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, foo):\n        # YOLO, anything goes!\n        return True\n```\n\nAdditionally, if the function takes a `**kwargs` input, it will receive _all_ available inputs and all of them will skip validation as if specified explicitly.\n\n#### Validating Types\n\nIf the `VALIDATE_INPUTS` method receives an argument named `input_types`, it will be passed a dictionary in which the key is the name of each input which is connected to an output from another node and the value is the type of that output. When this argument is present, all default validation of input types is skipped. Here’s an example making use of the fact that the front-end allows for the specification of multiple types:\n\n```\nclass AddNumbers:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"input1\" : (\"INT,FLOAT\", {\"min\": 0, \"max\": 1000})\n                \"input2\" : (\"INT,FLOAT\", {\"min\": 0, \"max\": 1000})\n            },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, input_types):\n        # The min and max of input1 and input2 are still validated because\n        # we didn't take `input1` or `input2` as arguments\n        if input_types[\"input1\"] not in (\"INT\", \"FLOAT\"):\n            return \"input1 must be an INT or FLOAT type\"\n        if input_types[\"input2\"] not in (\"INT\", \"FLOAT\"):\n            return \"input2 must be an INT or FLOAT type\"\n        return True\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/faq",
  "markdown": "# FAQs about API Nodes - ComfyUI\n\nThis article addresses common questions regarding the use of API nodes.\n\nWhy can't I find the API nodes?\n\nPlease update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.\n\nPlease note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.\n\nWhy can't I use / log in to the API Nodes?\n\nAPI access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:\n\n*   The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.\n*   Able to access our API service normally (a proxy service may be required in some regions).\n*   Your account does not have enough [credits](https://docs.comfy.org/interface/credits).\n\nWhy can't I use API node even after logging in, or why does it keep asking me to log in while using?\n\n*   Currently, only `127.0.0.1` or `localhost` access is supported.\n*   Ensure your account has enough credits.\n\nCan API Nodes be used for free?\n\nAPI Nodes require credits for API calls to closed-source models, so they do not support free usage.\n\nHow to purchase credits?\n\nPlease refer to the following documentation:\n\n1.  [Comfy Account](https://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.\n2.  [Credits](https://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.\n3.  Complete the payment through Stripe.\n4.  Check if the credits have been updated. If not, try restarting or refreshing the page.\n\nAre unused credits refundable?\n\nCurrently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).\n\nCan credits go negative?\n\nCredits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.\n\nWhere can I check usage and expenses?\n\nPlease visit the [Credits](https://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.\n\nIs it possible to use my own API Key?\n\nCurrently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.\n\nDo credits expire?\n\nNo, your credits do not expire.\n\nCan credits be transferred or shared?\n\nNo, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.\n\nCan I use the same account on different devices?\n\nWe do not limit the number of devices that can log in; you can use your account anywhere you want.\n\nHow can I request for my account or information to be deleted??\n\nEmail a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/lifecycle",
  "markdown": "# Lifecycle - ComfyUI\n\n## How Comfy loads custom nodes\n\nWhen Comfy starts, it scans the directory `custom_nodes` for Python modules, and attempts to load them. If the module exports `NODE_CLASS_MAPPINGS`, it will be treated as a custom node.\n\n### **init**.py\n\n`__init__.py` is executed when Comfy attempts to import the module. For a module to be recognized as containing custom node definitions, it needs to export `NODE_CLASS_MAPPINGS`. If it does (and if nothing goes wrong in the import), the nodes defined in the module will be available in Comfy. If there is an error in your code, Comfy will continue, but will report the module as having failed to load. So check the Python console! A very simple `__init__.py` file would look like this:\n\n```\nfrom .python_file import MyCustomNode\nNODE_CLASS_MAPPINGS = { \"My Custom Node\" : MyCustomNode }\n__all__ = [\"NODE_CLASS_MAPPINGS\"]\n```\n\n#### NODE\\_CLASS\\_MAPPINGS\n\n`NODE_CLASS_MAPPINGS` must be a `dict` mapping custom node names (unique across the Comfy install) to the corresponding node class.\n\n#### NODE\\_DISPLAY\\_NAME\\_MAPPINGS\n\n`__init__.py` may also export `NODE_DISPLAY_NAME_MAPPINGS`, which maps the same unique name to a display name for the node. If `NODE_DISPLAY_NAME_MAPPINGS` is not provided, Comfy will use the unique name as the display name.\n\n#### WEB\\_DIRECTORY\n\nIf you are deploying client side code, you will also need to export the path, relative to the module, in which the JavaScript files are to be found. It is conventional to place these in a subdirectory of your custom node named `js`."
},
{
  "url": "https://docs.comfy.org/tutorials/video/hunyuan-video",
  "markdown": "# ComfyUI Hunyuan Video Examples - ComfyUI\n\nHunyuan Video series is developed and open-sourced by [Tencent](https://huggingface.co/tencent), featuring a hybrid architecture that supports both [Text-to-Video](https://github.com/Tencent/HunyuanVideo) and [Image-to-Video](https://github.com/Tencent/HunyuanVideo-I2V) generation with a parameter scale of 13B. Technical features:\n\n*   **Core Architecture:** Uses a DiT (Diffusion Transformer) architecture similar to Sora, effectively fusing text, image, and motion information to improve consistency, quality, and alignment between generated video frames. A unified full-attention mechanism enables multi-view camera transitions while ensuring subject consistency.\n*   **3D VAE:** The custom 3D VAE compresses videos into a compact latent space, making image-to-video generation more efficient.\n*   **Superior Image-Video-Text Alignment:** Utilizing MLLM text encoders that excel in both image and video generation, better following text instructions, capturing details, and performing complex reasoning.\n\nYou can learn more through the official repositories: [Hunyuan Video](https://github.com/Tencent/HunyuanVideo) and [Hunyuan Video-I2V](https://github.com/Tencent/HunyuanVideo-I2V). This guide will walk you through setting up both **Text-to-Video** and **Image-to-Video** workflows in ComfyUI.\n\n## Common Models for All Workflows\n\nThe following models are used in both Text-to-Video and Image-to-Video workflows. Please download and save them to the specified directories:\n\n*   [clip\\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)\n*   [llava\\_llama3\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)\n*   [hunyuan\\_video\\_vae\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)\n\nStorage location:\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── llava_llama3_fp8_scaled.safetensors\n│   ├── vae/\n│   │   └── hunyuan_video_vae_bf16.safetensors\n```\n\n## Hunyuan Text-to-Video Workflow\n\nHunyuan Text-to-Video was open-sourced in December 2024, supporting 5-second short video generation through natural language descriptions in both Chinese and English.\n\n### 1\\. Workflow\n\nDownload the image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Hunyuan Text-to-Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp) \n\n### 2\\. Manual Models Installation\n\nDownload [hunyuan\\_video\\_t2v\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models` folder. Ensure you have all these model files in the correct locations:\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors                       // Shared model\n│   │   └── llava_llama3_fp8_scaled.safetensors      // Shared model\n│   ├── vae/\n│   │   └── hunyuan_video_vae_bf16.safetensors       // Shared model\n│   └── diffusion_models/\n│       └── hunyuan_video_t2v_720p_bf16.safetensors  // T2V model\n```\n\n### 3\\. Steps to Run the Workflow\n\n![ComfyUI Hunyuan Video T2V Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg)\n\n1.  Ensure the `DualCLIPLoader` node has loaded these models:\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_t2v_720p_bf16.safetensors`\n3.  Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`\n4.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Hunyuan Image-to-Video Workflow\n\nHunyuan Image-to-Video model was open-sourced on March 6, 2025, based on the HunyuanVideo framework. It transforms static images into smooth, high-quality videos and also provides LoRA training code to customize special video effects like hair growth, object transformation, etc. Currently, the Hunyuan Image-to-Video model has two versions:\n\n*   v1 “concat”: Better motion fluidity but less adherence to the image guidance\n*   v2 “replace”: Updated the day after v1, with better image guidance but seemingly less dynamic compared to v1\n\nv1 “concat”\n\n![HunyuanVideo v1](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp)\n\nv2 “replace”\n\n![HunyuanVideo v2](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp)\n\n### Shared Model for v1 and v2 Versions\n\nDownload the following file and save it to the `ComfyUI/models/clip_vision` directory:\n\n*   [llava\\_llama3\\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)\n\n### V1 “concat” Image-to-Video Workflow\n\n#### 1\\. Workflow and Asset\n\nDownload the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Hunyuan Image-to-Video v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp) Download the image below, which we’ll use as the starting frame for the image-to-video generation: ![Starting Frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png) \n\n*   [hunyuan\\_video\\_image\\_to\\_video\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)\n\nEnsure you have all these model files in the correct locations:\n\n```\nComfyUI/\n├── models/\n│   ├── clip_vision/\n│   │   └── llava_llama3_vision.safetensors                     // I2V shared model\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors                                  // Shared model\n│   │   └── llava_llama3_fp8_scaled.safetensors                 // Shared model\n│   ├── vae/\n│   │   └── hunyuan_video_vae_bf16.safetensors                  // Shared model\n│   └── diffusion_models/\n│       └── hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 \"concat\" version model\n```\n\n#### 3\\. Steps to Run the Workflow\n\n![ComfyUI Hunyuan Video I2V v1 Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg)\n\n1.  Ensure that `DualCLIPLoader` has loaded these models:\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  Ensure that `Load CLIP Vision` has loaded `llava_llama3_vision.safetensors`\n3.  Ensure that `Load Image Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`\n4.  Ensure that `Load VAE` has loaded `vae_name: hunyuan_video_vae_bf16.safetensors`\n5.  Ensure that `Load Diffusion Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`\n6.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n### v2 “replace” Image-to-Video Workflow\n\nThe v2 workflow is essentially the same as the v1 workflow. You just need to download the **replace** model and use it in the `Load Diffusion Model` node.\n\n#### 1\\. Workflow and Asset\n\nDownload the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Hunyuan Image-to-Video v2](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp) Download the image below, which we’ll use as the starting frame for the image-to-video generation: ![Starting Frame](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png) \n\n*   [hunyuan\\_video\\_v2\\_replace\\_image\\_to\\_video\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)\n\nEnsure you have all these model files in the correct locations:\n\n```\nComfyUI/\n├── models/\n│   ├── clip_vision/\n│   │   └── llava_llama3_vision.safetensors                                // I2V shared model\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors                                             // Shared model\n│   │   └── llava_llama3_fp8_scaled.safetensors                            // Shared model\n│   ├── vae/\n│   │   └── hunyuan_video_vae_bf16.safetensors                             // Shared model\n│   └── diffusion_models/\n│       └── hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 \"replace\" version model\n```\n\n#### 3\\. Steps to Run the Workflow\n\n![ComfyUI Hunyuan Video I2V v2 Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg)\n\n1.  Ensure the `DualCLIPLoader` node has loaded these models:\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  Ensure the `Load CLIP Vision` node has loaded `llava_llama3_vision.safetensors`\n3.  Ensure the `Load Image Model` node has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`\n4.  Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`\n5.  Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`\n6.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Try it yourself\n\nHere are some images and prompts we provide. Based on that content or make an adjustment to create your own video. ![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png)\n\n```\nFuturistic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/samurai.png)\n\n```\nSamurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/a_flying_car.png)\n\n```\nflying car fastly moving and flying through the city\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png)\n\n```\ncyberpunk car race in night city, dynamic, super fast, fast shot\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/fun-camera",
  "markdown": "# ComfyUI Wan2.1 Fun Camera Official Examples\n\n**Wan2.1 Fun Camera** is a video generation project launched by the Alibaba team, focusing on controlling video generation effects through camera motion. **Model Weights Download**:\n\n*   [14B Version](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-14B-Control-Camera)\n*   [1.3B Version](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-1.3B-Control-Camera)\n\n**Code Repository**: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun) **ComfyUI now natively supports the Wan2.1 Fun Camera model**.\n\n## Model Installation\n\nThese models only need to be installed once. Additionally, model download information is included in the corresponding workflow images, so you can choose your preferred way to download the models. All of the following models can be found at [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) **Diffusion Models** choose either 1.3B or 14B:\n\n*   [wan2.1\\_fun\\_camera\\_v1.1\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors)\n*   [wan2.1\\_fun\\_camera\\_v1.1\\_14B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_14B_bf16.safetensors)\n\nIf you’ve used Wan2.1 related models before, you should already have the following models. If not, please download them: **Text Encoders** choose one:\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)\n\nFile Storage Location:\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│ ├── 📂 diffusion_models/\n│ │   ├── wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors # 1.3B version\n│ │   └── wan2.1_fun_camera_v1.1_14B_bf16.safetensors # 14B version\n│ ├── 📂 text_encoders/\n│ │   └── umt5_xxl_fp8_e4m3fn_scaled.safetensors\n│ ├── 📂 vae/\n│ │   └── wan_2.1_vae.safetensors\n│ └── 📂 clip_vision/\n│     └── clip_vision_h.safetensors\n```\n\n## ComfyUI Wan2.1 Fun Camera 1.3B Native Workflow Example\n\n#### 1.1 Workflow File\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow:\n\n[\n\nDownload Json Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B.json)\n\n#### 1.2 Input Image Download\n\nPlease download the image below, which we will use as the starting frame: ![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B_input.jpg)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Wan2.1 Fun Camera Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg)\n\n1.  Ensure the correct version of model file is loaded:\n    *   1.3B version: `wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors`\n    *   14B version: `wan2.1_fun_camera_v1.1_14B_bf16.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Load Image` node\n6.  Modify the Prompt if you’re using your own input image\n7.  Set camera motion in the `WanCameraEmbedding` node\n8.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute generation\n\n## ComfyUI Wan2.1 Fun Camera 14B Workflow and Input Image\n\n[\n\nDownload Json Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B.json)\n\n**Input Image** ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B_input.jpg) \n\n## Performance Reference\n\n**1.3B Version**:\n\n*   512×512 resolution on RTX 4090 takes about 72 seconds to generate 81 frames\n\n**14B Version**:\n\n*   RTX4090 24GB VRAM may experience insufficient memory when generating 512×512 resolution, and memory issues have also occurred on A100 when using larger sizes"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/manager",
  "markdown": "# Publishing to the Manager - ComfyUI\n\n### Using ComfyUI Manager\n\nTo make your custom node available through **ComfyUI Manager** you need to save it as a git repository (generally at `github.com`) and then submit a Pull Request on the **ComfyUI Manager** git, in which you have edited `custom-node-list.json` to add your node. [More details](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#how-to-register-your-custom-node-into-comfyui-manager). When a user installs the node, **ComfyUI Manager** will:\n\n### ComfyUI Manager files\n\nAs indicated above, there are a number of files and scripts that **ComfyUI Manager** will use to manage the lifecycle of a custom node. These are all optional.\n\n*   `requirements.txt` - Python dependencies as mentioned above\n*   `install.py`, `uninstall.py` - executed when the custom node is installed or uninstalled\n*   `disable.py`, `enable.py` - executed when a custom node is disabled or re-enabled\n*   `node_list.json` - only required if the custom nodes pattern of NODE\\_CLASS\\_MAPPINGS is not conventional.\n\nSee the [ComfyUI Manager guide](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#custom-node-support-guide) for official details."
},
{
  "url": "https://docs.comfy.org/tutorials/image/cosmos/cosmos-predict2-t2i",
  "markdown": "# Cosmos Predict2 Text-to-Image ComfyUI Official Example\n\nCosmos-Predict2 is NVIDIA’s next-generation physical world foundation model, specifically designed for high-quality visual generation and prediction tasks in physical AI scenarios. The model features exceptional physical accuracy, environmental interactivity, and detail reproduction capabilities, enabling realistic simulation of complex physical phenomena and dynamic scenes. Cosmos-Predict2 supports various generation methods including Text-to-Image (Text2Image) and Video-to-World (Video2World), and is widely used in industrial simulation, autonomous driving, urban planning, scientific research, and other fields. GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2) huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959) This guide will walk you through completing **text-to-image** workflow in ComfyUI. For the video generation section, please refer to the following part:\n\n[\n\n## Cosmos Predict2 Video Generation\n\nUsing Cosmos-Predict2 for video generation\n\n\n\n](https://docs.comfy.org/tutorials/video/cosmos/cosmos-predict2-video2world)\n\n## Cosmos Predict2 Video2World Workflow\n\nWhen testing the 2B version showed it uses around 16GB of VRAM.\n\n### 1\\. Workflow File\n\nPlease download the image below and drag it into ComfyUI to load the workflow. The workflow already has embedded model download links. ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/cosmos/predict2/cosmos_predict2_2B_t2i.png)\n\n### 2\\. Manual Model Installation\n\nIf the model download wasn’t successful, you can try to download them manually by yourself in this section. **Diffusion model**\n\n*   [cosmos\\_predict2\\_2B\\_t2i.safetensors](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged/resolve/main/cosmos_predict2_2B_t2i.safetensors)\n\nFor other weights, please visit [Cosmos\\_Predict2\\_repackaged](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged) to download **Text encoder** [oldt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/resolve/main/text_encoders/oldt5_xxl_fp8_e4m3fn_scaled.safetensors) **VAE** [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors) File Storage Location\n\n```\n📂 ComfyUI/\n├──📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └─── cosmos_predict2_2B_t2i.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── oldt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── 📂 vae/\n│       └──  wan_2.1_vae.safetensors\n```\n\n### 3\\. Complete Workflow Step by Step\n\n![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/cosmos/cosmos_predict2_2B_t2i_step_guide.jpg) Please follow the steps in the image to run the workflow:\n\n1.  Ensure the `Load Diffusion Model` node has loaded `cosmos_predict2_2B_t2i.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `oldt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Set the image size in `EmptySD3LatentImage`\n5.  Modify the prompts in the `ClipTextEncode` node\n6.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to run the worklfow\n7.  Once generation is complete, the image will automatically save to the `ComfyUI/output/` directory. You can also preview it in the `save image` node.\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/lists",
  "markdown": "# Data lists - ComfyUI\n\n## Length one processing\n\nInternally, the Comfy server represents data flowing from one node to the next as a Python `list`, normally length 1, of the relevant datatype. In normal operation, when a node returns an output, each element in the output `tuple` is separately wrapped in a list (length 1); then when the next node is called, the data is unwrapped and passed to the main function.\n\n## List processing\n\nIn some circumstance, multiple data instances are processed in a single workflow, in which case the internal data will be a list containing the data instances. An example of this might be processing a series of images one at a time to avoid running out of VRAM, or handling images of different sizes. By default, Comfy will process the values in the list sequentially:\n\n*   if the inputs are `list`s of different lengths, the shorter ones are padded by repeating the last value\n*   the main method is called once for each value in the input lists\n*   the outputs are `list`s, each of which is the same length as the longest input\n\nThe relevant code can be found in the method `map_node_over_list` in `execution.py`. However, as Comfy wraps node outputs into a `list` of length one, if the `tuple` returned by a custom node contains a `list`, that `list` will be wrapped, and treated as a single piece of data. In order to tell Comfy that the list being returned should not be wrapped, but treated as a series of data for sequential processing, the node should provide a class attribute `OUTPUT_IS_LIST`, which is a `tuple[bool]`, of the same length as `RETURN_TYPES`, specifying which outputs which should be so treated. A node can also override the default input behaviour and receive the whole list in a single call. This is done by setting a class attribute `INPUT_IS_LIST` to `True`. Here’s a (lightly annotated) example from the built in nodes - `ImageRebatch` takes one or more batches of images (received as a list, because `INPUT_IS_LIST - True`) and rebatches them into batches of the requested size.\n\n```\n\nclass ImageRebatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"images\": (\"IMAGE\",),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}) }}\n    RETURN_TYPES = (\"IMAGE\",)\n    INPUT_IS_LIST = True\n    OUTPUT_IS_LIST = (True, )\n    FUNCTION = \"rebatch\"\n    CATEGORY = \"image/batch\"\n\n    def rebatch(self, images, batch_size):\n        batch_size = batch_size[0]    # everything comes as a list, so batch_size is list[int]\n\n        output_list = []\n        all_images = []\n        for img in images:                    # each img is a batch of images\n            for i in range(img.shape[0]):     # each i is a single image\n                all_images.append(img[i:i+1])\n\n        for i in range(0, len(all_images), batch_size): # take batch_size chunks and turn each into a new batch\n            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))  # will die horribly if the image batches had different width or height!\n\n        return (output_list,)\n```\n\n#### INPUT\\_IS\\_LIST"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/more_on_inputs",
  "markdown": "# Hidden and Flexible inputs - ComfyUI\n\nAlongside the `required` and `optional` inputs, which create corresponding inputs or widgets on the client-side, there are three `hidden` input options which allow the custom node to request certain information from the server. These are accessed by returning a value for `hidden` in the `INPUT_TYPES` `dict`, with the signature `dict[str,str]`, containing one or more of `PROMPT`, `EXTRA_PNGINFO`, or `UNIQUE_ID`\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": {...},\n        \"optional\": {...},\n        \"hidden\": {\n            \"unique_id\": \"UNIQUE_ID\",\n            \"prompt\": \"PROMPT\", \n            \"extra_pnginfo\": \"EXTRA_PNGINFO\",\n        }\n    }\n```\n\n### UNIQUE\\_ID\n\n`UNIQUE_ID` is the unique identifier of the node, and matches the `id` property of the node on the client side. It is commonly used in client-server communications (see [messages](https://docs.comfy.org/development/comfyui-server/comms_messages#getting-node-id)).\n\n### PROMPT\n\n`PROMPT` is the complete prompt sent by the client to the server. See [the prompt object](https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking#prompt) for a full description.\n\n`EXTRA_PNGINFO` is a dictionary that will be copied into the metadata of any `.png` files saved. Custom nodes can store additional information in this dictionary for saving (or as a way to communicate with a downstream node).\n\n### DYNPROMPT\n\n`DYNPROMPT` is an instance of `comfy_execution.graph.DynamicPrompt`. It differs from `PROMPT` in that it may mutate during the course of execution in response to [Node Expansion](https://docs.comfy.org/custom-nodes/backend/expansion).\n\n## Flexible inputs\n\n### Custom datatypes\n\nIf you want to pass data between your own custom nodes, you may find it helpful to define a custom datatype. This is (almost) as simple as just choosing a name for the datatype, which should be a unique string in upper case, such as `CHEESE`. You can then use `CHEESE` in your node `INPUT_TYPES` and `RETURN_TYPES`, and the Comfy client will only allow `CHEESE` outputs to connect to a `CHEESE` input. `CHEESE` can be any python object. The only point to note is that because the Comfy client doesn’t know about `CHEESE` you need (unless you define a custom widget for `CHEESE`, which is a topic for another day), to force it to be an input rather than a widget. This can be done with the `forceInput` option in the input options dictionary:\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": { \"my_cheese\": (\"CHEESE\", {\"forceInput\":True}) }\n    }\n```\n\n### Wildcard inputs\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": { \"anything\": (\"*\",{})},\n    }\n\n@classmethod\ndef VALIDATE_INPUTS(s, input_types):\n    return True\n```\n\nThe frontend allows `*` to indicate that an input can be connected to any source. Because this is not officially supported by the backend, you can skip the backend validation of types by accepting a parameter named `input_types` in your `VALIDATE_INPUTS` function. (See [VALIDATE\\_INPUTS](https://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) for more information.) It’s up to the node to make sense of the data that is passed.\n\n### Dynamically created inputs\n\nIf inputs are dynamically created on the client side, they can’t be defined in the Python source code. In order to access this data we need an `optional` dictionary that allows Comfy to pass data with arbitrary names. Since the Comfy server\n\n```\nclass ContainsAnyDict(dict):\n    def __contains__(self, key):\n        return True\n...\n\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": {},\n        \"optional\": ContainsAnyDict()\n    }\n...\n\ndef main_method(self, **kwargs):\n    # the dynamically created input data will be in the dictionary kwargs\n\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/tensors",
  "markdown": "# Working with torch.Tensor - ComfyUI\n\n## pytorch, tensors, and torch.Tensor\n\nAll the core number crunching in Comfy is done by [pytorch](https://pytorch.org/). If your custom nodes are going to get into the guts of stable diffusion you will need to become familiar with this library, which is way beyond the scope of this introduction. However, many custom nodes will need to manipulate images, latents and masks, each of which are represented internally as `torch.Tensor`, so you’ll want to bookmark the [documentation for torch.Tensor](https://pytorch.org/docs/stable/tensors.html).\n\n### What is a Tensor?\n\n`torch.Tensor` represents a tensor, which is the mathematical generalization of a vector or matrix to any number of dimensions. A tensor’s _rank_ is the number of dimensions it has (so a vector has _rank_ 1, a matrix _rank_ 2); its _shape_ describes the size of each dimension. So an RGB image (of height H and width W) might be thought of as three arrays (one for each color channel), each measuring H x W, which could be represented as a tensor with _shape_ `[H,W,3]`. In Comfy images almost always come in a batch (even if the batch only contains a single image). `torch` always places the batch dimension first, so Comfy images have _shape_ `[B,H,W,3]`, generally written as `[B,H,W,C]` where C stands for Channels.\n\n### squeeze, unsqueeze, and reshape\n\nIf a tensor has a dimension of size 1 (known as a collapsed dimension), it is equivalent to the same tensor with that dimension removed (a batch with 1 image is just an image). Removing such a collapsed dimension is referred to as squeezing, and inserting one is known as unsqueezing.\n\nTo represent the same data in a different shape is referred to as reshaping. This often requires you to know the underlying data structure, so handle with care!\n\n### Important notation\n\n`torch.Tensor` supports most Python slice notation, iteration, and other common list-like operations. A tensor also has a `.shape` attribute which returns its size as a `torch.Size` (which is a subclass of `tuple` and can be treated as such). There are some other important bits of notation you’ll often see (several of these are less common standard Python notation, seen much more frequently when dealing with tensors)\n\n*   `torch.Tensor` supports the use of `None` in slice notation to indicate the insertion of a dimension of size 1.\n*   `:` is frequently used when slicing a tensor; this simply means ‘keep the whole dimension’. It’s like using `a[start:end]` in Python, but omitting the start point and end point.\n*   `...` represents ‘the whole of an unspecified number of dimensions’. So `a[0, ...]` would extract the first item from a batch regardless of the number of dimensions.\n*   in methods which require a shape to be passed, it is often passed as a `tuple` of the dimensions, in which a single dimension can be given the size `-1`, indicating that the size of this dimension should be calculated based on the total size of the data.\n\n```\n>>> a = torch.Tensor((1,2))\n>>> a.shape\ntorch.Size([2])\n>>> a[:,None].shape \ntorch.Size([2, 1])\n>>> a.reshape((1,-1)).shape\ntorch.Size([1, 2])\n```\n\n### Elementwise operations\n\nMany binary on `torch.Tensor` (including ’+’, ’-’, ’\\*’, ’/’ and ’==’) are applied elementwise (independently applied to each element). The operands must be _either_ two tensors of the same shape, _or_ a tensor and a scalar. So:\n\n```\n>>> import torch\n>>> a = torch.Tensor((1,2))\n>>> b = torch.Tensor((3,2))\n>>> a*b\ntensor([3., 4.])\n>>> a/b\ntensor([0.3333, 1.0000])\n>>> a==b\ntensor([False,  True])\n>>> a==1\ntensor([ True, False])\n>>> c = torch.Tensor((3,2,1)) \n>>> a==c\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n```\n\n### Tensor truthiness\n\nYou may be familiar with the truthy value of a Python list as `True` for any non-empty list, and `False` for `None` or `[]`. By contrast A `torch.Tensor` (with more than one elements) does not have a defined truthy value. Instead you need to use `.all()` or `.any()` to combine the elementwise truthiness:\n\n```\n>>> a = torch.Tensor((1,2))\n>>> print(\"yes\" if a else \"no\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n>>> a.all()\ntensor(False)\n>>> a.any()\ntensor(True)\n```\n\nThis also means that you need to use `if a is not None:` not `if a:` to determine if a tensor variable has been set."
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_overview",
  "markdown": "# Javascript Extensions - ComfyUI\n\n## Extending the Comfy Client\n\nComfy can be modified through an extensions mechanism. To add an extension you need to:\n\n*   Export `WEB_DIRECTORY` from your Python module,\n*   Place one or more `.js` files into that directory,\n*   Use `app.registerExtension` to register your extension.\n\nThese three steps are below. Once you know how to add an extension, look through the [hooks](https://docs.comfy.org/custom-nodes/js/javascript_hooks) available to get your code called, a description of various [Comfy objects](https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking) you might need, or jump straight to some [example code snippets](https://docs.comfy.org/custom-nodes/js/javascript_examples).\n\n### Exporting `WEB_DIRECTORY`\n\nThe Comfy web client can be extended by creating a subdirectory in your custom node directory, conventionally called `js`, and exporting `WEB_DIRECTORY` - so your `__init_.py` will include something like:\n\n```\nWEB_DIRECTORY = \"./js\"\n__all__ = [\"NODE_CLASS_MAPPINGS\", \"NODE_DISPLAY_NAME_MAPPINGS\", \"WEB_DIRECTORY\"]\n```\n\n### Including `.js` files\n\n_Only_ `.js` files will be added to the webpage. Other resources (such as `.css` files) can be accessed at `extensions/custom_node_subfolder/the_file.css` and added programmatically.\n\n### Registering an extension\n\nThe basic structure of an extension follows is to import the main Comfy `app` object, and call `app.registerExtension`, passing a dictionary that contains a unique `name`, and one or more functions to be called by hooks in the Comfy code. A complete, trivial, and annoying, extension might look like this:\n\n```\nimport { app } from \"../../scripts/app.js\";\napp.registerExtension({ \n\tname: \"a.unique.name.for.a.useless.extension\",\n\tasync setup() { \n\t\talert(\"Setup complete!\")\n\t},\n})\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/snippets",
  "markdown": "# Annotated Examples - ComfyUI\n\nA growing collection of fragments of example code…\n\n## Images and Masks\n\n### Load an image\n\nLoad an image into a batch of size 1 (based on `LoadImage` source code in `nodes.py`)\n\n```\ni = Image.open(image_path)\ni = ImageOps.exif_transpose(i)\nif i.mode == 'I':\n    i = i.point(lambda i: i * (1 / 255))\nimage = i.convert(\"RGB\")\nimage = np.array(image).astype(np.float32) / 255.0\nimage = torch.from_numpy(image)[None,]\n```\n\n### Save an image batch\n\nSave a batch of images (based on `SaveImage` source code in `nodes.py`)\n\n```\nfor (batch_number, image) in enumerate(images):\n    i = 255. * image.cpu().numpy()\n    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n    filepath = # some path that takes the batch number into account\n    img.save(filepath)\n```\n\n### Invert a mask\n\nInverting a mask is a straightforward process. Since masks are normalised to the range \\[0,1\\]:\n\n### Convert a mask to Image shape\n\n```\n# We want [B,H,W,C] with C = 1\nif len(mask.shape)==2: # we have [H,W], so insert B and C as dimension 1\n    mask = mask[None,:,:,None]\nelif len(mask.shape)==3 and mask.shape[2]==1: # we have [H,W,C]\n    mask = mask[None,:,:,:]\nelif len(mask.shape)==3:                      # we have [B,H,W]\n    mask = mask[:,:,:,None]\n```\n\n### Using Masks as Transparency Layers\n\nWhen used for tasks like inpainting or segmentation, the MASK’s values will eventually be rounded to the nearest integer so that they are binary — 0 indicating regions to be ignored and 1 indicating regions to be targeted. However, this doesn’t happen until the MASK is passed to those nodes. This flexibility allows you to use MASKs as you would in digital photography contexts as a transparency layer:\n\n```\n# Invert mask back to original transparency layer\nmask = 1.0 - mask\n\n# Unsqueeze the `C` (channels) dimension\nmask = mask.unsqueeze(-1)\n\n# Concatenate (\"cat\") along the `C` dimension\nrgba_image = torch.cat((rgb_image, mask), dim=-1)\n```\n\n## Noise\n\n### Creating noise variations\n\nHere’s an example of creating a noise object which mixes the noise from two sources. This could be used to create slight noise variations by varying `weight2`.\n\n```\nclass Noise_MixedNoise:\n    def __init__(self, nosie1, noise2, weight2):\n        self.noise1  = noise1\n        self.noise2  = noise2\n        self.weight2 = weight2\n\n    @property\n    def seed(self): return self.noise1.seed\n\n    def generate_noise(self, input_latent:torch.Tensor) -> torch.Tensor:\n        noise1 = self.noise1.generate_noise(input_latent)\n        noise2 = self.noise2.generate_noise(input_latent)\n        return noise1 * (1.0-self.weight2) + noise2 * (self.weight2)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/BasicScheduler",
  "markdown": "# BasicScheduler - ComfyUI 原生节点文档 - ComfyUI\n\n`BasicScheduler` 节点旨在根据提供的调度器、模型和去噪参数为扩散模型计算一系列 sigma 值。它根据去噪因子动态调整总步骤数，以微调扩散过程，在一些需要精细控制的高级的采样过程（比如分步采样）等等提供了精细的不同阶段的“配方”\n\n## 输入\n\n| 参数名称 | 数据类型 | 输入类型 | 默认值 | 范围  | 比喻说明 | 技术作用 |\n| --- | --- | --- | --- | --- | --- | --- |\n| `模型` | MODEL | Input | \\-  | \\-  | **画布类型**：不同材质的画布需要不同的颜料配方 | 扩散模型对象，决定sigma值的计算基础 |\n| `调度器` | COMBO\\[STRING\\] | Widget | \\-  | 9种选项 | **调色技法**：选择颜料浓度变化的方式 | 调度算法，控制噪声衰减模式 |\n| `步数` | INT | Widget | 20  | 1-10000 | **调色次数**：调色20次 vs 50次的精细度差异 | 采样步数，影响生成质量与速度 |\n| `降噪` | FLOAT | Widget | 1.0 | 0.0-1.0 | **创作强度**：从微调到重绘的控制力度 | 去噪强度，支持部分重绘场景 |\n\n### 调度器类型详解\n\n基于源码 `comfy.samplers.SCHEDULER_NAMES`，支持以下9种调度器：\n\n| 调度器名称 | 特点  | 适用场景 | 噪声衰减特性 |\n| --- | --- | --- | --- |\n| **normal** | 标准线性调度 | 通用场景，平衡效果 | 均匀递减 |\n| **karras** | 平滑过渡调度 | 高质量生成，细节丰富 | 平滑非线性递减 |\n| **exponential** | 指数递减调度 | 快速生成，效率优先 | 指数型快速递减 |\n| **sgm\\_uniform** | SGM均匀调度 | 特定模型优化 | SGM优化递减 |\n| **simple** | 简单调度 | 快速测试，基础应用 | 简化递减 |\n| **ddim\\_uniform** | DDIM均匀调度 | DDIM采样优化 | DDIM特定递减 |\n| **beta** | Beta分布调度 | 特殊分布需求 | Beta函数递减 |\n| **linear\\_quadratic** | 线性二次调度 | 复杂场景优化 | 二次函数递减 |\n| **kl\\_optimal** | KL最优调度 | 理论最优化 | KL散度优化递减 |\n\n## 输出结果\n\n| 参数名称 | 数据类型 | 输出类型 | 比喻说明 | 技术含义 |\n| --- | --- | --- | --- | --- |\n| `sigmas` | SIGMAS | Output | **调色配方表**：详细的颜料浓度清单，供画家逐步使用 | 噪声水平序列，指导扩散模型的去噪过程 |\n\n## 节点角色：画家的调色助手\n\n想象你是一位画家，正在从一团混乱的颜料（噪声）创作清晰的图像。`BasicScheduler` 就像是的**专业调色助手**，它的工作是为您准备一系列精确的颜料浓度配方：\n\n### 工作流程\n\n*   **第1步**：使用90%浓度的颜料（高噪声水平）\n*   **第2步**：使用80%浓度的颜料\n*   **第3步**：使用70%浓度的颜料\n*   **…**\n*   **最后一步**：使用0%浓度（纯净画布，无噪声）\n\n### 调色助手的特殊技能\n\n**不同的调色方法（scheduler）**：\n\n*   **“karras”调色法**：颜料浓度变化非常平滑，像专业画家的渐变技巧\n*   **“exponential”调色法**：颜料浓度快速递减，适合快速创作\n*   **“linear”调色法**：颜料浓度均匀递减，稳定可控\n\n**精细控制（steps）**：\n\n*   **20次调色**：快速作画，效率优先\n*   **50次调色**：精细作画，质量优先\n\n**创作强度（denoise）**：\n\n*   **1.0 = 全新创作**：完全从空白画布开始\n*   **0.5 = 半改造**：保留原画一半，改造一半\n*   **0.2 = 微调优化**：只对原画进行细微调整\n\n### 与其他节点的配合\n\n`BasicScheduler`（调色助手）→ 准备配方 → `SamplerCustom`（画家）→ 实际绘画 → 完成作品"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/Canny",
  "markdown": "# Canny - ComfyUI 原生节点文档 - ComfyUI\n\n从照片中提取所有边缘线条，就像用钢笔为照片描边一样，把物体的轮廓和细节边界都画出来。\n\n## 工作原理\n\n想象您是一位画家，要用钢笔为一张照片描边。Canny节点就像一个智能助手，帮您决定哪些地方需要画线（边缘），哪些地方不需要。 这个过程就像筛选工作：\n\n*   **高阈值**是”必须画线的标准”：只有非常明显、清晰的轮廓线才会被画出来，比如人物脸部轮廓、建筑物边框\n*   **低阈值**是”完全不画线的标准”：太微弱的边缘会被忽略，避免画出噪点和无意义的线条\n*   **中间区域**：介于两个标准之间的边缘，如果连着”必须画的线”就一起画出来，如果是孤立的就不画\n\n最终输出一张黑白图像，白色部分是检测到的边缘线条，黑色部分是没有边缘的区域。\n\n## 输入\n\n| 参数名称 | 数据类型 | 输入方式 | 默认值 | 取值范围 | 功能说明 |\n| --- | --- | --- | --- | --- | --- |\n| 图像  | IMAGE | 连接  | \\-  | \\-  | 需要提取边缘的原始照片 |\n| 低阈值 | FLOAT | 手动输入 | 0.4 | 0.01-0.99 | 低阈值，决定忽略多弱的边缘。数值越小保留的细节越多，但可能产生噪点 |\n| 高阈值 | FLOAT | 手动输入 | 0.8 | 0.01-0.99 | 高阈值，决定保留多强的边缘。数值越大只保留最明显的轮廓线 |\n\n## 输出\n\n| 输出名称 | 数据类型 | 说明  |\n| --- | --- | --- |\n| 图像  | IMAGE | 黑白边缘图像，白色线条为检测到的边缘，黑色区域为无边缘部分 |\n\n## 参数对比\n\n![原图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/canny/input.webp) ![参数对比](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/canny/compare.webp) **常见问题：**\n\n*   边缘断断续续：尝试降低高阈值\n*   出现很多噪点：提高低阈值\n*   丢失重要细节：降低低阈值\n*   边缘太粗糙：检查输入图像质量和分辨率"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/CheckpointLoaderSimple",
  "markdown": "# CheckpointLoaderSimple - ComfyUI 原生节点文档 - ComfyUI\n\n这是一个模型加载器节点，用于从指定位置加载模型文件，并将其分解为三个核心组件：主模型、文本编码器和图像编解码器。 这个节点会自动检测`ComfyUI/models/checkpoints`文件夹下的所有模型文件，以及你在extra\\_model\\_paths.yaml文件中配置的额外路径。\n\n1.  **模型兼容性**：确保所选模型与你的工作流程兼容，不同类型的模型（如SD1.5、SDXL、Flux等）需要配合相应的采样器和其他节点\n2.  **文件管理**：将模型文件放在`ComfyUI/models/checkpoints`文件夹中，或通过extra\\_model\\_paths.yaml配置其他路径\n3.  **界面刷新**：如果在ComfyUI运行期间添加了新的模型文件，需要刷新浏览器（Ctrl+R）才能在下拉列表中看到新文件\n\n## 输入\n\n| 参数名称 | 数据类型 | 输入方式 | 默认值 | 取值范围 | 功能说明 |\n| --- | --- | --- | --- | --- | --- |\n| checkpoint名称 | STRING | 下拉选择 | null | checkpoints文件夹中的所有模型文件 | 选择要加载的检查点模型文件名称，决定了后续生图使用的AI模型 |\n\n## 输出\n\n| 输出名称 | 数据类型 | 说明  |\n| --- | --- | --- |\n| 模型  | MODEL | 用于图像去噪生成的主要扩散模型，是AI绘画的核心组件 |\n| CLIP | CLIP | 用于编码文本提示词的模型，将文字描述转换为AI能理解的信息 |\n| VAE | VAE | 用于图像编解码的模型，负责在像素空间和潜在空间之间转换 |"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipLoader",
  "markdown": "# 加载CLIP - ComfyUI 原生节点文档 - ComfyUI\n\n该节点主要用于单独加载 CLIP 文本编码器模型。 支持检测以下路径的模型文件检测：\n\n*   “ComfyUI/models/text\\_encoders/”\n*   “ComfyUI/models/clip/”\n\n> 如果你是在 ComfyUI 启动后才保存的模型则需要刷新 ComfyUI 前端来获取最新的模型文件路径列表\n\n支持的模型格式有:\n\n*   `.ckpt`\n*   `.pt`\n*   `.pt2`\n*   `.bin`\n*   `.pth`\n*   `.safetensors`\n*   `.pkl`\n*   `.sft`\n\n更多最新模型文件加载详情请查阅[folder\\_paths](https://github.com/comfyanonymous/ComfyUI/blob/master/folder_paths.py)\n\n## 输入\n\n| 参数名称 | 数据类型 | 作用  |\n| --- | --- | --- |\n| `CLIP名称` | COMBO\\[STRING\\] | 指定要加载的 CLIP 模型的名称。此名称用于在预定义的目录结构内定位模型文件。 |\n| `类型` | COMBO\\[STRING\\] | 确定要加载的 CLIP 模型类型，随着 ComfyUI 支持的模型数量增加这里的类型也会新增对应的类型，请查看[node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py)中源码里关于`CLIPLoader` 类的相关定义 |\n| `设备` | COMBO\\[STRING\\] | 选择加载 CLIP 模型的设备，`default` 将会将对应的模型在 GPU 上运行，如果选择`CPU` 将强制在 CPU上进行加载 |\n\n### 不同`设备`选项的说明\n\n**选择 “default” 的情况**：\n\n*   有足够的 GPU 内存\n*   希望获得最佳性能\n*   让系统自动优化内存使用\n\n**选择 “cpu” 的情况**：\n\n*   GPU 内存不足\n*   需要为其他模型（如 UNet）保留 GPU 内存\n*   在低 VRAM 环境下运行\n*   调试或特殊用途需要\n\n**性能影响** CPU 运行会比 GPU 运行慢很多，但可以节省宝贵的 GPU 内存供其他更重要的模型组件使用。在内存受限的环境中，将 CLIP 模型放在 CPU 上是一个常见的优化策略。\n\n### 支持的搭配\n\n| 模型类型 | 对应编码器 |\n| --- | --- |\n| stable\\_diffusion | clip-l |\n| stable\\_cascade | clip-g |\n| sd3 | t5 xxl/ clip-g / clip-l |\n| stable\\_audio | t5 base |\n| mochi | t5 xxl |\n| cosmos | old t5 xxl |\n| lumina2 | gemma 2 2B |\n| wan | umt5 xxl |\n\n未来随着 ComfyUI 的更新，这个更新搭配可能会新增，详情请参考 [node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py)中源码里关于`CLIPLoader` 类的相关定义说明\n\n## 输出\n\n| 参数名称 | 数据类型 | 作用  |\n| --- | --- | --- |\n| `clip` | CLIP | 加载的 CLIP 模型，准备用于下游任务或进一步处理。 |\n\n## 其它扩展\n\nCLIP 模型在 ComfyUI 中扮演着文本编码器的核心角色，负责将文本提示转换为可供扩散模型理解的数值表示，你可以把它理解成翻译官，负责将你的文本翻译成大模型可以理解的语言，当然不同模型也存在着 “方言” ，所以在不同架构的模型之间需要不同的 CLIP 模型来完成文本编码的这一过程。"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipMergeSimple",
  "markdown": "# CLIP融合简易 - ComfyUI 原生节点文档 - ComfyUI\n\n`CLIP融合简易` 是一个高级模型合并节点，用于将两个 CLIP 文本编码器模型按指定比例进行合并. 此节点专门用于根据指定比例合并两个CLIP模型，有效地混合它们的特性。它有选择性地将一个模型的补丁应用到另一个模型上，排除像位置ID和对数尺度这样的特定组件，以创建一个结合了两个源模型特征的混合模型。\n\n## 输入\n\n| 参数名称 | 数据类型 | 作用  |\n| --- | --- | --- |\n| `clip1` | CLIP | 要合并的第一个CLIP模型。它作为合并过程的基础模型。 |\n| `clip2` | CLIP | 要合并的第二个CLIP模型。根据指定的比例，除位置ID和对数尺度外，其关键补丁将应用于第一个模型。 |\n| `比例` | FLOAT | 取值范围 `0.0 - 1.0` 确定从第二个模型融合到第一个模型中的特性比例。比例为1.0意味着完全采用第二个模型的特性，而0.0则仅保留第一个模型的特性。 |\n\n## 输出\n\n| 参数名称 | 数据类型 | 作用  |\n| --- | --- | --- |\n| `clip` | CLIP | 结果合并的CLIP模型，根据指定的比例整合了两个输入模型的特性。 |\n\n## 合并机制详解\n\n### 合并算法\n\n节点使用加权平均的方式合并两个模型：\n\n1.  **克隆基础模型**: 首先克隆 `clip1` 作为基础模型\n2.  **获取补丁**: 从 `clip2` 获取所有键值补丁 (key patches)\n3.  **过滤特殊键**: 跳过 `.position_ids` 和 `.logit_scale` 结尾的键\n4.  **应用加权合并**: 使用公式 `(1.0 - 比例) * clip1 + 比例 * clip2`\n\n### 比例 参数说明\n\n*   **比例 = 0.0**: 完全使用 clip1，忽略 clip2\n*   **比例 = 0.5**: 两个模型各占 50%\n*   **比例 = 1.0**: 完全使用 clip2，忽略 clip1\n\n## 使用场景\n\n1.  **模型风格融合**: 结合不同训练数据的 CLIP 模型特点\n2.  **性能优化**: 平衡不同模型的优缺点\n3.  **实验研究**: 探索不同 CLIP 编码器的组合效果"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipSave",
  "markdown": "# 保存CLIP - ComfyUI 原生节点文档 - ComfyUI\n\n`CLIP保存` 节点用于将 CLIP 文本编码器模型保存为 SafeTensors 格式文件, 该节点属于高级模型合并工作流的一部分，通常与 `CLIPMergeSimple`、`CLIPMergeAdd` 等节点配合使用。保存的文件采用 SafeTensors 格式，确保安全性和兼容性。\n\n## 输入\n\n| 参数名 | 类型  | 是否必需 | 默认值 | 描述  |\n| --- | --- | --- | --- | --- |\n| `clip` | CLIP | 必需  | \\-  | 要保存的 CLIP 模型 |\n| `文件名前缀` | STRING | 必需  | `\"clip/ComfyUI\"` | 保存文件的前缀路径 |\n| `prompt` | PROMPT | 隐藏参数 | \\-  | 工作流提示信息（用于元数据） |\n| `extra_pnginfo` | EXTRA\\_PNGINFO | 隐藏参数 | \\-  | 额外的PNG信息（用于元数据） |\n\n## 输出\n\n该节点没有定义输出类型, 会将处理后的文件保存到 `ComfyUI/output/` 文件夹下\n\n### 多文件保存策略\n\n节点会根据 CLIP 模型类型分别保存不同组件：\n\n| 前缀类型 | 文件名后缀 | 说明  |\n| --- | --- | --- |\n| `clip_l.` | `_clip_l` | CLIP-L 文本编码器 |\n| `clip_g.` | `_clip_g` | CLIP-G 文本编码器 |\n| 空前缀 | 无后缀 | 其他 CLIP 组件 |"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking",
  "markdown": "# Comfy Objects - ComfyUI\n\n## LiteGraph\n\nThe Comfy UI is built on top of [LiteGraph](https://github.com/jagenjo/litegraph.js). Much of the Comfy functionality is provided by LiteGraph, so if developing more complex nodes you will probably find it helpful to clone that repository and browse the documentation, which can be found at `doc/index.html`.\n\n## ComfyApp\n\nThe `app` object (always accessible by `import { app } from \"../../scripts/app.js\";`) represents the Comfy application running in the browser, and contains a number of useful properties and functions, some of which are listed below.\n\n### Properties\n\nImportant properties of `app` include (this is not an exhaustive list):\n\n| property | contents |\n| --- | --- |\n| `canvas` | An LGraphCanvas object, representing the current user interface. It contains some potentially interesting properties, such as `node_over` and `selected_nodes`. |\n| `canvasEl` | The DOM `<canvas>` element |\n| `graph` | A reference to the LGraph object describing the current graph |\n| `runningNodeId` | During execution, the node currently being executed |\n| `ui` | Provides access to some UI elements, such as the queue, menu, and dialogs |\n\n`canvas` (for graphical elements) and `graph` (for logical connections) are probably the ones you are most likely to want to access.\n\n### Functions\n\nAgain, there are many. A few significant ones are:\n\n| function | notes |\n| --- | --- |\n| graphToPrompt | Convert the graph into a prompt that can be sent to the Python server |\n| loadGraphData | Load a graph |\n| queuePrompt | Submit a prompt to the queue |\n| registerExtension | You’ve seen this one - used to add an extension |\n\n## LGraph\n\nThe `LGraph` object is part of the LiteGraph framework, and represents the current logical state of the graph (nodes and links). If you want to manipulate the graph, the LiteGraph documentation (at `doc/index.html` if you clone `https://github.com/jagenjo/litegraph.js`) describes the functions you will need. You can use `graph` to obtain details of nodes and links, for example:\n\n```\nconst ComfyNode_object_for_my_node = app.graph._nodes_by_id(my_node_id) \nComfyNode_object_for_my_node.inputs.forEach(input => {\n    const link_id = input.link;\n    if (link_id) {\n        const LLink_object = app.graph.links[link_id]\n        const id_of_upstream_node = LLink_object.origin_id\n        // etc\n    }\n});\n```\n\n## LLink\n\nThe `LLink` object, accessible through `graph.links`, represents a single link in the graph, from node `link.origin_id` output slot `link.origin_slot` to node `link.target_id` slot `link.target_slot`. It also has a string representing the data type, in `link.type`, and `link.id`. `LLink`s are created in the `connect` method of a `LGraphNode` (of which `ComfyNode` is a subclass).\n\n## ComfyNode\n\n`ComfyNode` is a subclass of `LGraphNode`, and the LiteGraph documentation is therefore helpful for more generic operations. However, Comfy has significantly extended the LiteGraph core behavior, and also does not make use of all LiteGraph functionality.\n\nA `ComfyNode` object represents a node in the current workflow. It has a number of important properties that you may wish to make use of, a very large number of functions that you may wish to use, or hijack to modify behavior. To get a more complete sense of the node object, you may find it helpful to insert the following code into your extension and place a breakpoint on the `console.log` command. When you then create a new node you can use your favorite debugger to interrogate the node.\n\n```\nasync nodeCreated(node) {\n    console.log(\"nodeCreated\")\n}\n```\n\n### Properties\n\n| property | contents |\n| --- | --- |\n| `bgcolor` | The background color of the node, or undefined for the default |\n| `comfyClass` | The Python class representing the node |\n| `flags` | A dictionary that may contain flags related to the state of the node. In particular, `flags.collapsed` is true for collapsed nodes. |\n| `graph` | A reference to the LGraph object |\n| `id` | A unique id |\n| `input_type` | A list of the input types (eg “STRING”, “MODEL”, “CLIP” etc). Generally matches the Python INPUT\\_TYPES |\n| `inputs` | A list of inputs (discussed below) |\n| `mode` | Normally 0, set to 2 if the node is muted and 4 if the node is bypassed. Values of 1 and 3 are not used by Comfy |\n| `order` | The node’s position in the execution order. Set by `LGraph.computeExecutionOrder()` when the prompt is submitted |\n| `pos` | The \\[x,y\\] position of the node on the canvas |\n| `properties` | A dictionary containing `\"Node name for S&R\"`, used by LiteGraph |\n| `properties_info` | The type and default value of entries in `properties` |\n| `size` | The width and height of the node on the canvas |\n| `title` | Display Title |\n| `type` | The unique name (from Python) of the node class |\n| `widgets` | A list of widgets (discussed below) |\n| `widgets_values` | A list of the current values of widgets |\n\n### Functions\n\nThere are a very large number of functions (85, last time I counted). A selection are listed below. Most of these functions are unmodified from the LiteGraph core code.\n\n#### Inputs, Outputs, Widgets\n\n| function | notes |\n| --- | --- |\n| Inputs / Outputs | Most have output methods with the equivalent names: s/In/Out/ |\n| `addInput` | Create a new input, defined by name and type |\n| `addInputs` | Array version of `addInput` |\n| `findInputSlot` | Find the slot index from the input name |\n| `findInputSlotByType` | Find an input matching the type. Options to prefer, or only use, free slots |\n| `removeInput` | By slot index |\n| `getInputNode` | Get the node connected to this input. The output equivalent is `getOutputNodes` and returns a list |\n| `getInputLink` | Get the LLink connected to this input. No output equivalent |\n| Widgets |     |\n| `addWidget` | Add a standard Comfy widget |\n| `addCustomWidget` | Add a custom widget (defined in the `getComfyWidgets` hook) |\n| `addDOMWidget` | Add a widget defined by a DOM element |\n| `convertWidgetToInput` | Convert a widget to an input if allowed by `isConvertableWidget` (in `widgetInputs.js`) |\n\n#### Connections\n\n| function | notes |\n| --- | --- |\n| `connect` | Connect this node’s output to another node’s input |\n| `connectByType` | Connect output to another node by specifying the type - connects to first available matching slot |\n| `connectByTypeOutput` | Connect input to another node output by type |\n| `disconnectInput` | Remove any link into the input (specified by name or index) |\n| `disconnectOutput` | Disconnect an output from a specified node’s input |\n| `onConnectionChange` | Called on each node. `side==1` if it’s an input on this node |\n| `onConnectInput` | Called _before_ a connection is made. If this returns `false`, the connection is refused |\n\n#### Display\n\n| function | notes |\n| --- | --- |\n| `setDirtyCanvas` | Specify that the foreground (nodes) and/or background (links and images) need to be redrawn |\n| `onDrawBackground` | Called with a `CanvasRenderingContext2D` object to draw the background. Used by Comfy to render images |\n| `onDrawForeground` | Called with a `CanvasRenderingContext2D` object to draw the node. |\n| `getTitle` | The title to be displayed. |\n| `collapse` | Toggles the collapsed state of the node. |\n\n#### Other\n\n| function | notes |\n| --- | --- |\n| `changeMode` | Use to set the node to bypassed (`mode == 4`) or not (`mode == 0`) |\n\nInputs and Widgets represent the two ways that data can be fed into a node. In general a widget can be converted to an input, but not all inputs can be converted to a widget (as many datatypes can’t be entered through a UI element). `node.inputs` is a list of the current inputs (colored dots on the left hand side of the node), specifying their `.name`, `.type`, and `.link` (a reference to the connected `LLink` in `app.graph.links`). If an input is a widget which has been converted, it also holds a reference to the, now inactive, widget in `.widget`. `node.widgets` is a list of all widgets, whether or not they have been converted to an input. A widget has:\n\n| property/function | notes |\n| --- | --- |\n| `callback` | A function called when the widget value is changed |\n| `last_y` | The vertical position of the widget in the node |\n| `name` | The (unique within a node) widget name |\n| `options` | As specified in the Python code (such as default, min, and max) |\n| `type` | The name of the widget type (see below) in lowercase |\n| `value` | The current widget value. This is a property with `get` and `set` methods |\n\n### Widget Types\n\n`app.widgets` is a dictionary of currently registered widget types, keyed in the UPPER CASE version of the name of the type. Build in Comfy widgets types include the self explanatory `BOOLEAN`, `INT`, and `FLOAT`, as well as `STRING` (which comes in two flavours, single line and multiline), `COMBO` for dropdown selection from a list, and `IMAGEUPLOAD`, used in Load Image nodes. Custom widget types can be added by providing a `getCustomWidgets` method in your extension.\n\n### Linked widgets\n\nWidgets can also be linked - the built in behavior of `seed` and `control_after_generate`, for example. A linked widget has `.type = 'base_widget_type:base_widget_name'`; so `control_after_generate` may have type `int:seed`.\n\n## Prompt\n\nWhen you press the `Queue Prompt` button in Comfy, the `app.graphToPrompt()` method is called to convert the current graph into a prompt that can be sent to the server. `app.graphToPrompt` returns an object (referred to herein as `prompt`) with two properties, `output` and `workflow`.\n\n### output\n\n`prompt.output` maps from the `node_id` of each node in the graph to an object with two properties.\n\n*   `prompt.output[node_id].class_type`, the unique name of the custom node class, as defined in the Python code\n*   `prompt.output[node_id].inputs`, which contains the value of each input (or widget) as a map from the input name to:\n    *   the selected value, if it is a widget, or\n    *   an array containing (`upstream_node_id`, `upstream_node_output_slot`) if there is a link connected to the input, or\n    *   undefined, if it is a widget that has been converted to an input and is not connected\n    *   other unconnected inputs are not included in `.inputs`\n\n### workflow\n\n`prompt.workflow` contains the following properties:\n\n*   `config` - a dictionary of additional configuration options (empty by default)\n*   `extra` - a dictionary containing extra information about the workflow. By default it contains:\n    *   `extra.ds` - describes the current view of the graph (`scale` and `offset`)\n*   `groups` - all groups in the workflow\n*   `last_link_id` - the id of the last link added\n*   `last_node_id` - the id of the last node added\n*   `links` - a list of all links in the graph. Each entry is an array of five integers and one string:\n    *   (`link_id`, `upstream_node_id`, `upstream_node_output_slot`, `downstream_node_id`, `downstream_node_input_slot`, `data type`)\n*   `nodes` - a list of all nodes in the graph. Each entry is a map of a subset of the properties of the node as described [above](#comfynode)\n    *   The following properties are included: `flags`, `id`, `inputs`, `mode`, `order`, `pos`, `properties`, `size`, `type`, `widgets_values`\n    *   In addition, unless a node has no outputs, there is an `outputs` property, which is a list of the outputs of the node, each of which contains:\n        *   `name` - the name of the output\n        *   `type` - the data type of the output\n        *   `links` - a list of the `link_id` of all links from this output (if there are no connections, may be an empty list, or null),\n        *   `shape` - the shape used to draw the output (default 3 for a dot)\n        *   `slot_index` - the slot number of the output\n*   `version` - the LiteGraph version number (at time of writing, `0.4`)"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_settings",
  "markdown": "# Settings - ComfyUI\n\nYou can provide a settings object to ComfyUI that will show up when the user opens the ComfyUI settings panel.\n\n## Basic operation\n\n### Add a setting\n\n```\nimport { app } from \"../../scripts/app.js\";\n\napp.registerExtension({\n    name: \"My Extension\",\n    settings: [\n        {\n            id: \"example.boolean\",\n            name: \"Example boolean setting\",\n            type: \"boolean\",\n            defaultValue: false,\n        },\n    ],\n});\n```\n\nThe `id` must be unique across all extensions and will be used to fetch values. If you do not [provide a category](#categories), then the `id` will be split by `.` to determine where it appears in the settings panel.\n\n*   If your `id` doesn’t contain any `.` then it will appear in the “Other” category and your `id` will be used as the section heading.\n*   If your `id` contains at least one `.` then the leftmost part will be used as the setting category and the second part will be used as the section heading. Any further parts are ignored.\n\n### Read a setting\n\n```\nimport { app } from \"../../scripts/app.js\";\n\nif (app.extensionManager.setting.get('example.boolean')) {\n    console.log(\"Setting is enabled.\");\n} else {\n    console.log(\"Setting is disabled.\");\n}\n```\n\n### React to changes\n\nThe `onChange()` event handler will be called as soon as the user changes the setting in the settings panel. This will also be called when the extension is registered, on every page load.\n\n```\n{\n    id: \"example.boolean\",\n    name: \"Example boolean setting\",\n    type: \"boolean\",\n    defaultValue: false,\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Write a setting\n\n```\nimport { app } from \"../../scripts/app.js\";\n\ntry {\n    await app.extensionManager.setting.set(\"example.boolean\", true);\n} catch (error) {\n    console.error(`Error changing setting: ${error}`);\n}\n```\n\nThe setting types are based on [PrimeVue](https://primevue.org/) components. Props described in the PrimeVue documentation can be defined for ComfyUI settings by adding them in an `attrs` field. For instance, this adds increment/decrement buttons to a number input:\n\n```\n{\n    id: \"example.number\",\n    name: \"Example number setting\",\n    type: \"number\",\n    defaultValue: 0,\n    attrs: {\n        showButtons: true,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n## Types\n\n### Boolean\n\nThis shows an on/off toggle. Based on the [ToggleSwitch PrimeVue component](https://primevue.org/toggleswitch/).\n\n```\n{\n    id: \"example.boolean\",\n    name: \"Example boolean setting\",\n    type: \"boolean\",\n    defaultValue: false,\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Text\n\nThis is freeform text. Based on the [InputText PrimeVue component](https://primevue.org/inputtext/).\n\n```\n{\n    id: \"example.text\",\n    name: \"Example text setting\",\n    type: \"text\",\n    defaultValue: \"Foo\",\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Number\n\nThis for entering numbers. To allow decimal places, set the `maxFractionDigits` attribute to a number greater than zero. Based on the [InputNumber PrimeVue component](https://primevue.org/inputnumber/).\n\n```\n{\n    id: \"example.number\",\n    name: \"Example number setting\",\n    type: \"number\",\n    defaultValue: 42,\n    attrs: {\n        showButtons: true,\n        maxFractionDigits: 1,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Slider\n\nThis lets the user enter a number directly or via a slider. Based on the [Slider PrimeVue component](https://primevue.org/slider/). Ranges are not supported.\n\n```\n{\n    id: \"example.slider\",\n    name: \"Example slider setting\",\n    type: \"slider\",\n    attrs: {\n        min: -10,\n        max: 10,\n        step: 0.5,\n    },\n    defaultValue: 0,\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Combo\n\nThis lets the user pick from a drop-down list of values. You can provide options either as a plain string or as an object with `text` and `value` fields. If you only provide a plain string, then it will be used for both. You can let the user enter freeform text by supplying the `editable: true` attribute, or search by supplying the `filter: true` attribute. Based on the [Select PrimeVue component](https://primevue.org/select/). Groups are not supported.\n\n```\n{\n    id: \"example.combo\",\n    name: \"Example combo setting\",\n    type: \"combo\",\n    defaultValue: \"first\",\n    options: [\n        { text: \"My first option\", value: \"first\" },\n        \"My second option\",\n    ],\n    attrs: {\n        editable: true,\n        filter: true,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Color\n\nThis lets the user select a color from a color picker or type in a hex reference. Note that the format requires six full hex digits - three digit shorthand does not work. Based on the [ColorPicker PrimeVue component](https://primevue.org/colorpicker/).\n\n```\n{\n    id: \"example.color\",\n    name: \"Example color setting\",\n    type: \"color\",\n    defaultValue: \"ff0000\",\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Image\n\nThis lets the user upload an image. The setting will be saved as a [data URL](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data). Based on the [FileUpload PrimeVue component](https://primevue.org/fileupload/).\n\n```\n{\n    id: \"example.image\",\n    name: \"Example image setting\",\n    type: \"image\",\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Hidden\n\nHidden settings aren’t displayed in the settings panel, but you can read and write to them from your code.\n\n```\n{\n    id: \"example.hidden\",\n    name: \"Example hidden setting\",\n    type: \"hidden\",\n}\n```\n\n## Other\n\n### Categories\n\nYou can specify the categorisation of your setting separately to the `id`. This means you can change the categorisation and naming without changing the `id` and losing the values that have already been set by users.\n\n```\n{\n    id: \"example.boolean\",\n    name: \"Example boolean setting\",\n    type: \"boolean\",\n    defaultValue: false,\n    category: [\"Category name\", \"Section heading\", \"Setting label\"],\n}\n```\n\n### Tooltips\n\nYou can add extra contextual help with the `tooltip` field. This adds a small ℹ︎ icon after the field name that will show the help text when the user hovers over it.\n\n```\n{\n    id: \"example.boolean\",\n    name: \"Example boolean setting\",\n    type: \"boolean\",\n    defaultValue: false,\n    tooltip: \"This is some helpful information\",\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncodeFlux",
  "markdown": "# CLIP文本编码Flux - ComfyUI 原生节点文档 - ComfyUI\n\n`CLIP文本编码Flux` 是 ComfyUI 中专为 Flux 架构设计的高级文本编码节点。它采用双文本编码器（CLIP-L 与 T5XXL）协同机制，能够同时处理结构化关键词和详细自然语言描述，为 Flux 模型提供更精准、更丰富的文本理解能力，提升文本到图像的生成质量。 该节点基于双编码器协作机制：\n\n1.  `clip_l` 输入会被 CLIP-L 编码器处理，提取风格、主题等关键词特征，适合简洁描述。\n2.  `t5xxl` 输入由 T5XXL 编码器处理，擅长理解复杂、细致的自然语言场景描述。\n3.  两路编码结果融合后，结合”引导”参数，生成统一的条件嵌入（CONDITIONING），用于下游的 Flux 采样器节点，控制生成内容与文本描述的契合度。\n\n## 输入\n\n| 参数名称 | 数据类型 | 输入方式 | 默认值 | 取值范围 | 功能说明 |\n| --- | --- | --- | --- | --- | --- |\n| `clip` | CLIP | 节点输入 | 无   | \\-  | 必须是支持 Flux 架构的 CLIP 模型，包含 CLIP-L 和 T5XXL 两个编码器 |\n| `clip_l` | STRING | 文本框 | 无   | 最多77个token | 适合输入简洁的关键词描述，如风格、主题等 |\n| `t5xxl` | STRING | 文本框 | 无   | 近乎无限制 | 适合输入详细的自然语言描述，表达复杂场景和细节 |\n| `引导` | FLOAT | 滑块  | 3.5 | 0.0 - 100.0 | 控制文本条件对生成过程的影响强度，数值越大越严格遵循文本描述 |\n\n## 输出\n\n| 输出名称 | 数据类型 | 说明  |\n| --- | --- | --- |\n| `条件` | CONDITIONING | 包含双编码器处理后的条件嵌入和引导参数，用于条件图像生成 |\n\n## 使用示例\n\n### 提示词示例\n\n*   **clip\\_l 输入框**（关键词风格）：\n    *   使用结构化、简洁的关键词组合\n    *   示例：`masterpiece, best quality, portrait, oil painting, dramatic lighting`\n    *   重点描述风格、质量、主题等核心元素\n*   **t5xxl 输入框**（自然语言描述）：\n    *   使用完整、流畅的场景描述\n    *   示例：`A highly detailed portrait in oil painting style, featuring dramatic chiaroscuro lighting that creates deep shadows and bright highlights, emphasizing the subject's features with renaissance-inspired composition.`\n    *   重点描述场景细节、空间关系、光影效果\n\n### 注意事项\n\n1.  确保使用兼容的 Flux 架构 CLIP 模型\n2.  建议同时填写 clip\\_l 和 t5xxl，以发挥双编码器优势\n3.  注意 clip\\_l 的词元数量限制（77个token）\n4.  根据生成效果调整”引导”参数"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncodeSdxl",
  "markdown": "# CLIP文本编码SDXL - ComfyUI内置节点文档 - ComfyUI\n\n此节点设计使用特别为SDXL架构定制的CLIP模型对文本输入进行编码。它使用双编码器系统（CLIP-L和CLIP-G）来处理文本描述，从而生成更准确的图像。\n\n## 输入\n\n| 参数名称 | 数据类型 | 作用  |\n| --- | --- | --- |\n| `clip` | `CLIP` | 用于编码文本的CLIP模型实例。 |\n| `宽度` | `INT` | 指定图像的宽度（以像素为单位），默认1024。 |\n| `高度` | `INT` | 指定图像的高度（以像素为单位），默认1024。 |\n| `裁剪宽` | `INT` | 裁剪区域的宽度（以像素为单位），默认0。 |\n| `裁剪高` | `INT` | 裁剪区域的高度（以像素为单位），默认0。 |\n| `目标宽度` | `INT` | 输出图像的目标宽度，默认1024。 |\n| `目标高度` | `INT` | 输出图像的目标高度，默认1024。 |\n| `text_g` | `STRING` | 全局文本描述，用于整体场景描述。 |\n| `text_l` | `STRING` | 局部文本描述，用于细节描述。 |\n\n## 输出\n\n| 参数名称 | 数据类型 | 作用  |\n| --- | --- | --- |\n| `条件` | `CONDITIONING` | 包含编码后的文本和图像生成所需的条件信息。 |"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncodeHunyuanDit",
  "markdown": "# CLIP文本编码混元DiT - ComfyUI内置节点文档 - ComfyUI\n\n`CLIP文本编码混元DiT` 节点的主要功能是将输入的文本转换为模型可以理解的形式。是一个高级条件化节点，专门用于 HunyuanDiT 模型的双文本编码器架构。 主要作用它就像一个翻译器，可以将我们的文字描述转换成 AI 模型能理解的”机器语言”。其中 `bert` 和 `mt5xl` 偏好不同类型的提示词输入\n\n## 输入\n\n| 参数  | 数据类型 | 描述  |\n| --- | --- | --- |\n| `clip` | CLIP | 一个 CLIP 模型实例，用于文本的标记化和编码，是生成条件的核心。 |\n| `bert` | STRING | 需要编码的文本输入，偏好短语、关键词，支持多行和动态提示。 |\n| `mt5xl` | STRING | 另一个需要编码的文本输入，支持多行和动态提示（多语言），可以使用完整的句子和复杂的描述。 |\n\n## 输出\n\n| 参数  | 数据类型 | 描述  |\n| --- | --- | --- |\n| `条件` | CONDITIONING | 编码后的条件输出，用于生成任务中的进一步处理。 |"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncodeSdxlRefiner",
  "markdown": "# CLIP文本编码SDXL精炼器 - ComfyUI内置节点文档 - ComfyUI\n\n此节点专门为 SDXL Refiner 模型设计，用于将文本提示转换为条件信息，通过纳入审美得分和维度信息来增强生成任务的条件，从而提升最终的精炼效果。它就像是一位专业的艺术指导，不仅传达您的创作意图，还能为作品注入精确的美学标准和规格要求。\n\n## 工作原理\n\nSDXL Refiner 是一个专门的精炼模型，它在 SDXL 基础模型的基础上，专注于提升图像的细节和质量。这个过程就像是一位艺术修饰师：\n\n1.  首先，它接收基础模型生成的初步图像或文本描述\n2.  然后，通过精确的美学评分和尺寸参数来指导精炼过程\n3.  最后，它专注于处理图像的高频细节，提升整体质量\n\nRefiner 可以通过两种方式使用：\n\n*   作为独立的精炼步骤，对基础模型生成的图像进行后期处理\n*   作为专家集成系统的一部分，在生成过程的低噪声阶段接管处理\n\n## 输入\n\n| 参数名称 | 数据类型 | 输入方式 | 默认值 | 取值范围 | 功能说明 |\n| --- | --- | --- | --- | --- | --- |\n| `CLIP` | CLIP | 必需  | \\-  | \\-  | 用于文本标记化和编码的 CLIP 模型实例，是将文本转换为模型可理解格式的核心组件 |\n| `美学分数` | FLOAT | 可选  | 6.0 | 0.0-1000.0 | 控制生成图像的视觉质量和美观程度，类似于为艺术作品设定质量标准：  <br>\\- 高分值(7.5-8.5)：追求更精美、细节丰富的效果  <br>\\- 中等分值(6.0-7.0)：平衡的质量控制  <br>\\- 低分值(2.0-3.0)：适用于负面提示 |\n| `宽度` | INT | 必需  | 1024 | 64-16384 | 指定输出图像的宽度（像素），需要是 8 的倍数。SDXL 在总像素量接近 1024×1024 (约100万像素) 时效果最佳 |\n| `高度` | INT | 必需  | 1024 | 64-16384 | 指定输出图像的高度（像素），需要是 8 的倍数。SDXL 在总像素量接近 1024×1024 (约100万像素) 时效果最佳 |\n| `text` | STRING | 必需  | \\-  | \\-  | 文本提示描述，支持多行输入和动态提示语法。在 Refiner 中，文本提示应更注重描述期望的视觉质量和细节特征 |\n\n## 输出\n\n| 输出名称 | 数据类型 | 说明  |\n| --- | --- | --- |\n| `条件` | CONDITIONING | 经过细化的条件输出，包含了文本语义、美学标准和尺寸信息的综合编码，专门用于指导 SDXL Refiner 模型进行精确的图像精炼 |\n\n## 注意事项\n\n1.  该节点专门为 SDXL Refiner 模型优化，与普通的 CLIPTextEncode 节点有所不同\n2.  美学分数建议使用 7.5 作为基准值，这是 SDXL 训练时的标准设置\n3.  所有尺寸参数必须是 8 的倍数，且建议总像素量接近 1024×1024（约100万像素）\n4.  Refiner 模型专注于提升图像细节和质量，因此文本提示应该更注重描述期望的视觉效果，而不是场景内容\n5.  在实际使用中，Refiner 通常用于生成过程的后期阶段（约最后20%的步骤），专注于细节优化"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_toast",
  "markdown": "# Toast API - ComfyUI\n\nThe Toast API provides a way to display non-blocking notification messages to users. These are useful for providing feedback without interrupting workflow.\n\n## Basic Usage\n\n### Simple Toast\n\n```\n// Display a simple info toast\napp.extensionManager.toast.add({\n  severity: \"info\",\n  summary: \"Information\",\n  detail: \"Operation completed successfully\",\n  life: 3000\n});\n```\n\n### Toast Types\n\n```\n// Success toast\napp.extensionManager.toast.add({\n  severity: \"success\",\n  summary: \"Success\",\n  detail: \"Data saved successfully\",\n  life: 3000\n});\n\n// Warning toast\napp.extensionManager.toast.add({\n  severity: \"warn\",\n  summary: \"Warning\",\n  detail: \"This action may cause problems\",\n  life: 5000\n});\n\n// Error toast\napp.extensionManager.toast.add({\n  severity: \"error\",\n  summary: \"Error\",\n  detail: \"Failed to process request\",\n  life: 5000\n});\n```\n\n### Alert Helper\n\n```\n// Shorthand for creating an alert toast\napp.extensionManager.toast.addAlert(\"This is an important message\");\n```\n\n## API Reference\n\n### Toast Message\n\n```\napp.extensionManager.toast.add({\n  severity?: \"success\" | \"info\" | \"warn\" | \"error\" | \"secondary\" | \"contrast\", // Message severity level (default: \"info\")\n  summary?: string,         // Short title for the toast\n  detail?: any,             // Detailed message content\n  closable?: boolean,       // Whether user can close the toast (default: true)\n  life?: number,            // Duration in milliseconds before auto-closing\n  group?: string,           // Group identifier for managing related toasts\n  styleClass?: any,         // Style class of the message\n  contentStyleClass?: any   // Style class of the content\n});\n```\n\n### Alert Helper\n\n```\napp.extensionManager.toast.addAlert(message: string);\n```\n\n### Additional Methods\n\n```\n// Remove a specific toast\napp.extensionManager.toast.remove(toastMessage);\n\n// Remove all toasts\napp.extensionManager.toast.removeAll();\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_sidebar_tabs",
  "markdown": "# Sidebar Tabs - ComfyUI\n\nThe Sidebar Tabs API allows extensions to add custom tabs to the sidebar of the ComfyUI interface. This is useful for adding features that require persistent visibility and quick access.\n\n## Basic Usage\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"customSidebar\",\n  icon: \"pi pi-compass\",\n  title: \"Custom Tab\",\n  tooltip: \"My Custom Sidebar Tab\",\n  type: \"custom\",\n  render: (el) => {\n    el.innerHTML = '<div>This is my custom sidebar content</div>';\n  }\n});\n```\n\n## Tab Configuration\n\nEach tab requires several properties:\n\n```\n{\n  id: string,              // Unique identifier for the tab\n  icon: string,            // Icon class for the tab button\n  title: string,           // Title text for the tab\n  tooltip?: string,        // Tooltip text on hover (optional)\n  type: string,            // Tab type (usually \"custom\")\n  render: (element) => void // Function that populates the tab content\n}\n```\n\nThe `render` function receives a DOM element where you should insert your tab’s content.\n\n## Icon Options\n\nSidebar tab icons can use various icon sets:\n\n*   PrimeVue icons: `pi pi-[icon-name]` (e.g., `pi pi-home`)\n*   Material Design icons: `mdi mdi-[icon-name]` (e.g., `mdi mdi-robot`)\n*   Font Awesome icons: `fa-[style] fa-[icon-name]` (e.g., `fa-solid fa-star`)\n\nEnsure the corresponding icon library is loaded before using these icons.\n\n## Stateful Tab Example\n\nYou can create tabs that maintain state:\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"statefulTab\",\n  icon: \"pi pi-list\",\n  title: \"Notes\",\n  type: \"custom\",\n  render: (el) => {\n    // Create elements\n    const container = document.createElement('div');\n    container.style.padding = '10px';\n    \n    const notepad = document.createElement('textarea');\n    notepad.style.width = '100%';\n    notepad.style.height = '200px';\n    notepad.style.marginBottom = '10px';\n    \n    // Load saved content if available\n    const savedContent = localStorage.getItem('comfyui-notes');\n    if (savedContent) {\n      notepad.value = savedContent;\n    }\n    \n    // Auto-save content\n    notepad.addEventListener('input', () => {\n      localStorage.setItem('comfyui-notes', notepad.value);\n    });\n    \n    // Assemble the UI\n    container.appendChild(notepad);\n    el.appendChild(container);\n  }\n});\n```\n\n## Using React Components\n\nYou can mount React components in sidebar tabs:\n\n```\n// Import React dependencies in your extension\nimport React from \"react\";\nimport ReactDOM from \"react-dom/client\";\n\n// Register sidebar tab with React content\napp.extensionManager.registerSidebarTab({\n  id: \"reactSidebar\",\n  icon: \"mdi mdi-react\",\n  title: \"React Tab\",\n  type: \"custom\",\n  render: (el) => {\n    const container = document.createElement(\"div\");\n    container.id = \"react-sidebar-container\";\n    el.appendChild(container);\n    \n    // Define a simple React component\n    function SidebarContent() {\n      const [count, setCount] = React.useState(0);\n      \n      return (\n        <div style={{ padding: \"10px\" }}>\n          <h3>React Sidebar</h3>\n          <p>Count: {count}</p>\n          <button onClick={() => setCount(count + 1)}>\n            Increment\n          </button>\n        </div>\n      );\n    }\n    \n    // Mount React component\n    ReactDOM.createRoot(container).render(\n      <React.StrictMode>\n        <SidebarContent />\n      </React.StrictMode>\n    );\n  }\n});\n```\n\nFor a real-world example of a React application integrated as a sidebar tab, check out the [ComfyUI-Copilot project on GitHub](https://github.com/AIDC-AI/ComfyUI-Copilot).\n\n## Dynamic Content Updates\n\nYou can update sidebar content in response to graph changes:\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"dynamicSidebar\",\n  icon: \"pi pi-chart-line\",\n  title: \"Stats\",\n  type: \"custom\",\n  render: (el) => {\n    const container = document.createElement('div');\n    container.style.padding = '10px';\n    el.appendChild(container);\n    \n    // Function to update stats\n    function updateStats() {\n      const stats = {\n        nodes: app.graph._nodes.length,\n        connections: Object.keys(app.graph.links).length\n      };\n      \n      container.innerHTML = `\n        <h3>Workflow Stats</h3>\n        <ul>\n          <li>Nodes: ${stats.nodes}</li>\n          <li>Connections: ${stats.connections}</li>\n        </ul>\n      `;\n    }\n    \n    // Initial update\n    updateStats();\n    \n    // Listen for graph changes\n    const api = app.api;\n    api.addEventListener(\"graphChanged\", updateStats);\n    \n    // Clean up listeners when tab is destroyed\n    return () => {\n      api.removeEventListener(\"graphChanged\", updateStats);\n    };\n  }\n});\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_topbar_menu",
  "markdown": "# Topbar Menu - ComfyUI\n\nThe Topbar Menu API allows extensions to add custom menu items to the ComfyUI’s top menu bar. This is useful for providing access to advanced features or less frequently used commands.\n\n## Basic Usage\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  // Define commands\n  commands: [\n    { \n      id: \"myCommand\", \n      label: \"My Command\", \n      function: () => { alert(\"Command executed!\"); } \n    }\n  ],\n  // Add commands to menu\n  menuCommands: [\n    { \n      path: [\"Extensions\", \"My Extension\"], \n      commands: [\"myCommand\"] \n    }\n  ]\n});\n```\n\nCommand definitions follow the same pattern as in the [Commands and Keybindings API](https://docs.comfy.org/custom-nodes/js/javascript_commands_keybindings). See that page for more detailed information about defining commands.\n\n## Command Configuration\n\nEach command requires an `id`, `label`, and `function`:\n\n```\n{\n  id: string,              // Unique identifier for the command\n  label: string,           // Display name for the command\n  function: () => void     // Function to execute when command is triggered\n}\n```\n\nThe `menuCommands` array defines where to place commands in the menu structure:\n\n```\n{\n  path: string[],          // Array representing menu hierarchy\n  commands: string[]       // Array of command IDs to add at this location\n}\n```\n\nThe `path` array specifies the menu hierarchy. For example, `[\"File\", \"Export\"]` would add commands to the “Export” submenu under the “File” menu.\n\n```\napp.registerExtension({\n  name: \"MenuExamples\",\n  commands: [\n    { \n      id: \"saveAsImage\", \n      label: \"Save as Image\", \n      function: () => { \n        // Code to save canvas as image\n      } \n    },\n    { \n      id: \"exportWorkflow\", \n      label: \"Export Workflow\", \n      function: () => { \n        // Code to export workflow\n      } \n    }\n  ],\n  menuCommands: [\n    // Add to File menu\n    { \n      path: [\"File\"], \n      commands: [\"saveAsImage\", \"exportWorkflow\"] \n    }\n  ]\n});\n```\n\n```\napp.registerExtension({\n  name: \"SubmenuExample\",\n  commands: [\n    { \n      id: \"option1\", \n      label: \"Option 1\", \n      function: () => { console.log(\"Option 1\"); } \n    },\n    { \n      id: \"option2\", \n      label: \"Option 2\", \n      function: () => { console.log(\"Option 2\"); } \n    },\n    { \n      id: \"suboption1\", \n      label: \"Sub-option 1\", \n      function: () => { console.log(\"Sub-option 1\"); } \n    }\n  ],\n  menuCommands: [\n    // Create a nested menu structure\n    { \n      path: [\"Extensions\", \"My Tools\"], \n      commands: [\"option1\", \"option2\"] \n    },\n    { \n      path: [\"Extensions\", \"My Tools\", \"Advanced\"], \n      commands: [\"suboption1\"] \n    }\n  ]\n});\n```\n\nYou can add the same command to multiple menu locations:\n\n```\napp.registerExtension({\n  name: \"MultiLocationExample\",\n  commands: [\n    { \n      id: \"helpCommand\", \n      label: \"Get Help\", \n      function: () => { window.open(\"https://docs.example.com\", \"_blank\"); } \n    }\n  ],\n  menuCommands: [\n    // Add to Help menu\n    { \n      path: [\"Help\"], \n      commands: [\"helpCommand\"] \n    },\n    // Also add to Extensions menu\n    { \n      path: [\"Extensions\"], \n      commands: [\"helpCommand\"] \n    }\n  ]\n});\n```\n\nCommands can work with other ComfyUI APIs like settings. For more information about the Settings API, see the [Settings API](https://docs.comfy.org/custom-nodes/js/javascript_settings) documentation."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/bfl/flux-1-1-pro-ultra-image",
  "markdown": "# Flux 1.1 \\[pro\\] Ultra Image - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Flux 1.1 [pro] Ultra Image 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg) Flux 1.1 \\[pro\\] Ultra Image 节点允许你通过文本提示词生成超高分辨率的图像，直接连接 Black Forest Labs 的最新图像生成 API。 此节点支持两种主要使用模式：\n\n1.  **文生图**：通过文本提示词生成高质量图像（不使用任何图像收入时）\n2.  **图生图**：将现有图像与提示词结合，创建融合两者特点的新图像（Remix 模式）\n\n此节点支持通过 API 调用 Ultra 模式，能够生成 4 倍于标准 Flux 1.1 \\[pro\\] 分辨率的图像（高达 4MP），同时不牺牲提示词遵循性，并且保持仅 10 秒的超快生成时间。与其他高分辨率模型相比，生成速度提高了 2.5 倍以上。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 生成图像的文本描述 |\n| prompt\\_upsampling | 布尔值 | False | 是否使用提示词上采样技术增强细节。启用后会自动修改提示词以获得更具创造性的生成，但结果会变得不确定（相同种子不会产生完全相同的结果） |\n| seed | 整数  | 0   | 随机种子值，控制生成的随机性 |\n| aspect\\_ratio | 字符串 | ”16:9” | 图像的宽高比，必须在 1:4 到 4:1 之间 |\n| raw | 布尔值 | False | 设置为 True 时，生成更少处理痕迹、更自然的图像 |\n\n### 可选参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image\\_prompt | 图像  | 无   | 可选输入，用于图生图（Remix）模式 |\n| image\\_prompt\\_strength | 浮点数 | 0.1 | 在有`image_prompt` 输入时生效，调节提示词与图像提示之间的混合程度，值越大输出越接近输入图像，范围为 0.0-1.0 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 生成的高分辨率图像结果 |\n\n## 使用示例\n\n请访问下面的教程查看对应的使用示例\n\n*   [Flux 1.1 Pro Ultra Image API 节点 ComfyUI 官方示例工作流](https://docs.comfy.org/zh-CN/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)\n\n## 工作原理\n\nFlux 1.1 \\[pro\\] Ultra 模式利用优化的深度学习架构和高效的 GPU 加速技术，在不牺牲速度的情况下实现高分辨率图像生成。当请求发送到 API 后，系统会解析提示词，应用适当的参数，然后并行计算图像，最终生成并返回高分辨率结果。 与常规模型相比，Ultra 模式特别关注细节保留和大尺寸下的一致性，以确保即使在 4MP 的高分辨率下也能保持令人印象深刻的质量。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass FluxProUltraImageNode(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    MINIMUM_RATIO = 1 / 4\n    MAXIMUM_RATIO = 4 / 1\n    MINIMUM_RATIO_STR = \"1:4\"\n    MAXIMUM_RATIO_STR = \"4:1\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"prompt_upsampling\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result).\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"16:9\",\n                        \"tooltip\": \"Aspect ratio of image; must be between 1:4 and 4:1.\",\n                    },\n                ),\n                \"raw\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"When True, generate less processed, more natural-looking images.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image_prompt\": (IO.IMAGE,),\n                \"image_prompt_strength\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.1,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Blend between the prompt and the image prompt.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, aspect_ratio: str):\n        try:\n            validate_aspect_ratio(\n                aspect_ratio,\n                minimum_ratio=cls.MINIMUM_RATIO,\n                maximum_ratio=cls.MAXIMUM_RATIO,\n                minimum_ratio_str=cls.MINIMUM_RATIO_STR,\n                maximum_ratio_str=cls.MAXIMUM_RATIO_STR,\n            )\n        except Exception as e:\n            return str(e)\n        return True\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/bfl\"\n\n    def api_call(\n        self,\n        prompt: str,\n        aspect_ratio: str,\n        prompt_upsampling=False,\n        raw=False,\n        seed=0,\n        image_prompt=None,\n        image_prompt_strength=0.1,\n        auth_token=None,\n        **kwargs,\n    ):\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/bfl/flux-pro-1.1-ultra/generate\",\n                method=HttpMethod.POST,\n                request_model=BFLFluxProUltraGenerateRequest,\n                response_model=BFLFluxProGenerateResponse,\n            ),\n            request=BFLFluxProUltraGenerateRequest(\n                prompt=prompt,\n                prompt_upsampling=prompt_upsampling,\n                seed=seed,\n                aspect_ratio=validate_aspect_ratio(\n                    aspect_ratio,\n                    minimum_ratio=self.MINIMUM_RATIO,\n                    maximum_ratio=self.MAXIMUM_RATIO,\n                    minimum_ratio_str=self.MINIMUM_RATIO_STR,\n                    maximum_ratio_str=self.MAXIMUM_RATIO_STR,\n                ),\n                raw=raw,\n                image_prompt=(\n                    image_prompt\n                    if image_prompt is None\n                    else convert_image_to_base64(image_prompt)\n                ),\n                image_prompt_strength=(\n                    None if image_prompt is None else round(image_prompt_strength, 2)\n                ),\n            ),\n            auth_token=auth_token,\n        )\n        output_image = handle_bfl_synchronous_operation(operation)\n        return (output_image,)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v1",
  "markdown": "# Ideogram V1 - ComfyUI 原生节点文档\n\n```\nclass IdeogramV1(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V1 model.\n\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"turbo\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to use turbo mode (faster generation, potentially lower quality)\",\n                    }\n                ),\n            },\n            \"optional\": {\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V2_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Description of what to exclude from the image\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v1\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        turbo=False,\n        aspect_ratio=\"1:1\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        negative_prompt=\"\",\n        num_images=1,\n        auth_token=None,\n    ):\n        # Determine the model based on turbo setting\n        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)\n        model = \"V_1_TURBO\" if turbo else \"V_1\"\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/ideogram/generate\",\n                method=HttpMethod.POST,\n                request_model=IdeogramGenerateRequest,\n                response_model=IdeogramGenerateResponse,\n            ),\n            request=IdeogramGenerateRequest(\n                image_request=ImageRequest(\n                    prompt=prompt,\n                    model=model,\n                    num_images=num_images,\n                    seed=seed,\n                    aspect_ratio=aspect_ratio if aspect_ratio != \"ASPECT_1_1\" else None,\n                    magic_prompt_option=(\n                        magic_prompt_option if magic_prompt_option != \"AUTO\" else None\n                    ),\n                    negative_prompt=negative_prompt if negative_prompt else None,\n                )\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v2",
  "markdown": "# Ideogram V2 - ComfyUI 原生节点文档\n\n```\n\nclass IdeogramV2(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V2 model.\n\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"turbo\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to use turbo mode (faster generation, potentially lower quality)\",\n                    }\n                ),\n            },\n            \"optional\": {\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V2_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation. Ignored if resolution is not set to AUTO.\",\n                    },\n                ),\n                \"resolution\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V1_RES_MAP.keys()),\n                        \"default\": \"Auto\",\n                        \"tooltip\": \"The resolution for image generation. If not set to AUTO, this overrides the aspect_ratio setting.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"style_type\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"GENERAL\", \"REALISTIC\", \"DESIGN\", \"RENDER_3D\", \"ANIME\"],\n                        \"default\": \"NONE\",\n                        \"tooltip\": \"Style type for generation (V2 only)\",\n                    },\n                ),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Description of what to exclude from the image\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n                #\"color_palette\": (\n                #    IO.STRING,\n                #    {\n                #        \"multiline\": False,\n                #        \"default\": \"\",\n                #        \"tooltip\": \"Color palette preset name or hex colors with weights\",\n                #    },\n                #),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v2\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        turbo=False,\n        aspect_ratio=\"1:1\",\n        resolution=\"Auto\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        style_type=\"NONE\",\n        negative_prompt=\"\",\n        num_images=1,\n        color_palette=\"\",\n        auth_token=None,\n    ):\n        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)\n        resolution = V1_V1_RES_MAP.get(resolution, None)\n        # Determine the model based on turbo setting\n        model = \"V_2_TURBO\" if turbo else \"V_2\"\n\n        # Handle resolution vs aspect_ratio logic\n        # If resolution is not AUTO, it overrides aspect_ratio\n        final_resolution = None\n        final_aspect_ratio = None\n\n        if resolution != \"AUTO\":\n            final_resolution = resolution\n        else:\n            final_aspect_ratio = aspect_ratio if aspect_ratio != \"ASPECT_1_1\" else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/ideogram/generate\",\n                method=HttpMethod.POST,\n                request_model=IdeogramGenerateRequest,\n                response_model=IdeogramGenerateResponse,\n            ),\n            request=IdeogramGenerateRequest(\n                image_request=ImageRequest(\n                    prompt=prompt,\n                    model=model,\n                    num_images=num_images,\n                    seed=seed,\n                    aspect_ratio=final_aspect_ratio,\n                    resolution=final_resolution,\n                    magic_prompt_option=(\n                        magic_prompt_option if magic_prompt_option != \"AUTO\" else None\n                    ),\n                    style_type=style_type if style_type != \"NONE\" else None,\n                    negative_prompt=negative_prompt if negative_prompt else None,\n                    color_palette=color_palette if color_palette else None,\n                )\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-image-to-image",
  "markdown": "# Luma Image to Image - ComfyUI 原生节点文档\n\n此节点连接到Luma AI的文本到图像API，让用户能够通过详细的文本提示词生成图像。Luma AI以其出色的真实感和细节表现而闻名，特别擅长生成照片级别的逼真内容和艺术风格图像。\n\nLuma Image to Image 节点分析输入图像并结合文本提示词来引导修改过程。它使用 Luma AI 的生成模型，根据提示词对图像进行创新性的变化。 节点流程：\n\n```\n\nclass LumaImageModifyNode(ComfyNodeABC):\n    \"\"\"\n    Modifies images synchronously based on prompt and aspect ratio.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE,),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"image_weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.02,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of the image; the closer to 0.0, the less the image will be modified.\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaImageModel],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {},\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        image: torch.Tensor,\n        image_weight: float,\n        seed,\n        auth_token=None,\n        **kwargs,\n    ):\n        # first, upload image\n        download_urls = upload_images_to_comfyapi(\n            image, max_images=1, auth_token=auth_token\n        )\n        image_url = download_urls[0]\n        # next, make Luma call with download url provided\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations/image\",\n                method=HttpMethod.POST,\n                request_model=LumaImageGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaImageGenerationRequest(\n                prompt=prompt,\n                model=model,\n                modify_image_ref=LumaModifyImageRef(\n                    url=image_url, weight=round(image_weight, 2)\n                ),\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        img_response = requests.get(response_poll.assets.image)\n        img = process_image_response(img_response)\n        return (img,)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v3",
  "markdown": "# Ideogram V3 - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Ideogram V3 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg) 此节点连接到Ideogram V3 API，来完成对应的图像生成任务。 目前此节点支持两种图像生成模式:\n\n*   **文生图模式** - 从纯文本提示词生成全新图像\n*   **局部重绘模式（Inpainting）** - 通过提供原始图像和遮罩来重新生成特定区域\n\n### 文生图模式\n\n这是默认模式，当没有提供图像和遮罩输入时激活。只需提供提示词和所需的参数：\n\n1.  在提示词字段中描述你想要的图像\n2.  选择适当的宽高比或分辨率\n3.  调整其他参数如魔法提示、种子和渲染质量\n4.  运行节点生成图像\n\n### 局部重绘模式\n\n**重要提示**：此模式要求同时提供图像和遮罩输入。如果只提供其中一个，节点将抛出错误。\n\n1.  将原始图像连接到`image`输入端口\n2.  创建一个与原图相同尺寸的遮罩，白色区域表示要重新生成的部分\n3.  将遮罩连接到`mask`输入端口\n4.  在提示词中描述要在遮罩区域生成的内容\n5.  运行节点执行局部编辑\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 描述要生成内容的文本提示词 |\n| aspect\\_ratio | 选择项 | ”1:1” | 图像宽高比（仅文生图模式有效） |\n| resolution | 选择项 | ”Auto” | 图像分辨率，设置后会覆盖宽高比设置 |\n| magic\\_prompt\\_option | 选择项 | ”AUTO” | 魔法提示增强选项：AUTO、ON或OFF |\n| seed | 整数  | 0   | 随机种子值，设为0则随机生成 |\n| num\\_images | 整数  | 1   | 生成图像数量(1-8) |\n| rendering\\_speed | 选择项 | ”BALANCED” | 渲染速度：BALANCED、TURBO或QUALITY |\n\n### 可选参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| image | 图像  | 用于局部重绘模式的输入图像（**必须与mask一起提供**） |\n| mask | 遮罩  | 用于局部重绘的遮罩，白色区域将被替换（**必须与image一起提供**） |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 生成的图像结果 |\n\n## 工作原理\n\nIdeogram V3节点使用最先进的AI模型处理用户输入，能够理解复杂的设计意图和文字排版需求。它支持两种主要模式：\n\n1.  **生成模式**：从文本提示创建全新图像\n2.  **编辑模式**：使用原始图像+遮罩组合，只替换遮罩指定的区域\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass IdeogramV3(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V3 model.\n\n    Supports both regular image generation from text prompts and image editing with mask.\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation or editing\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V3_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation. Ignored if resolution is not set to Auto.\",\n                    },\n                ),\n                \"resolution\": (\n                    IO.COMBO,\n                    {\n                        \"options\": V3_RESOLUTIONS,\n                        \"default\": \"Auto\",\n                        \"tooltip\": \"The resolution for image generation. If not set to Auto, this overrides the aspect_ratio setting.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n                \"rendering_speed\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"BALANCED\", \"TURBO\", \"QUALITY\"],\n                        \"default\": \"BALANCED\",\n                        \"tooltip\": \"Controls the trade-off between generation speed and quality\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v3\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        image=None,\n        mask=None,\n        resolution=\"Auto\",\n        aspect_ratio=\"1:1\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        num_images=1,\n        rendering_speed=\"BALANCED\",\n        auth_token=None,\n    ):\n        # Check if both image and mask are provided for editing mode\n        if image is not None and mask is not None:\n            # Edit mode\n            path = \"/proxy/ideogram/ideogram-v3/edit\"\n\n            # Process image and mask\n            input_tensor = image.squeeze().cpu()\n\n            # Validate mask dimensions match image\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n\n            # Process image\n            img_np = (input_tensor.numpy() * 255).astype(np.uint8)\n            img = Image.fromarray(img_np)\n            img_byte_arr = io.BytesIO()\n            img.save(img_byte_arr, format=\"PNG\")\n            img_byte_arr.seek(0)\n            img_binary = img_byte_arr\n            img_binary.name = \"image.png\"\n\n            # Process mask - white areas will be replaced\n            mask_np = (mask.squeeze().cpu().numpy() * 255).astype(np.uint8)\n            mask_img = Image.fromarray(mask_np)\n            mask_byte_arr = io.BytesIO()\n            mask_img.save(mask_byte_arr, format=\"PNG\")\n            mask_byte_arr.seek(0)\n            mask_binary = mask_byte_arr\n            mask_binary.name = \"mask.png\"\n\n            # Create edit request\n            edit_request = IdeogramV3EditRequest(\n                prompt=prompt,\n                rendering_speed=rendering_speed,\n            )\n\n            # Add optional parameters\n            if magic_prompt_option != \"AUTO\":\n                edit_request.magic_prompt = magic_prompt_option\n            if seed != 0:\n                edit_request.seed = seed\n            if num_images > 1:\n                edit_request.num_images = num_images\n\n            # Execute the operation for edit mode\n            operation = SynchronousOperation(\n                endpoint=ApiEndpoint(\n                    path=path,\n                    method=HttpMethod.POST,\n                    request_model=IdeogramV3EditRequest,\n                    response_model=IdeogramGenerateResponse,\n                ),\n                request=edit_request,\n                files={\n                    \"image\": img_binary,\n                    \"mask\": mask_binary,\n                },\n                content_type=\"multipart/form-data\",\n                auth_token=auth_token,\n            )\n\n        elif image is not None or mask is not None:\n            # If only one of image or mask is provided, raise an error\n            raise Exception(\"Ideogram V3 image editing requires both an image AND a mask\")\n        else:\n            # Generation mode\n            path = \"/proxy/ideogram/ideogram-v3/generate\"\n\n            # Create generation request\n            gen_request = IdeogramV3Request(\n                prompt=prompt,\n                rendering_speed=rendering_speed,\n            )\n\n            # Handle resolution vs aspect ratio\n            if resolution != \"Auto\":\n                gen_request.resolution = resolution\n            elif aspect_ratio != \"1:1\":\n                v3_aspect = V3_RATIO_MAP.get(aspect_ratio)\n                if v3_aspect:\n                    gen_request.aspect_ratio = v3_aspect\n\n            # Add optional parameters\n            if magic_prompt_option != \"AUTO\":\n                gen_request.magic_prompt = magic_prompt_option\n            if seed != 0:\n                gen_request.seed = seed\n            if num_images > 1:\n                gen_request.num_images = num_images\n\n            # Execute the operation for generation mode\n            operation = SynchronousOperation(\n                endpoint=ApiEndpoint(\n                    path=path,\n                    method=HttpMethod.POST,\n                    request_model=IdeogramV3Request,\n                    response_model=IdeogramGenerateResponse,\n                ),\n                request=gen_request,\n                auth_token=auth_token,\n            )\n\n        # Execute the operation and process response\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-reference",
  "markdown": "# Luma Reference - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Luma Reference 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-reference.jpg) Luma Reference 节点允许你设置参考图像和权重，用于指导Luma图像生成节点的创作过程，使得生成的图像更接近参考图像的特定特征。\n\n## 节点功能\n\n此节点作为Luma生成节点的辅助工具，允许用户提供参考图像来影响生成结果。它让用户能够设置参考图像的权重，以控制参考图像对最终结果的影响程度。 多个 Luma Reference 节点可以串联，根据对应的 API 要求，最多运行同时串联 4 个进行工作\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 作为参考的输入图像 |\n| weight | 浮点数 | 1.0 | 控制参考图像的影响强度 (0-1) |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| luma\\_ref | LUMA\\_REF | 包含图像和权重的参考对象 |\n\n## 使用示例\n\n[\n\n## Luma Text to Image 工作流示例\n\nLuma Text to Image 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-image)\n\n## 工作原理\n\nLuma Reference 节点接收图像输入并允许设置权重值。该节点不直接生成或修改图像，而是创建一个包含图像数据和权重信息的参考对象，后续传递给Luma生成节点。 在生成过程中，Luma AI 会分析参考图像的特征，并根据设定的权重将这些特征融入到生成结果中。较高的权重值意味着生成的图像将更接近参考图像的特征，而较低的权重值则表示参考图像只会轻微影响最终结果。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass LumaReferenceNode(ComfyNodeABC):\n    \"\"\"\n    Holds an image and weight for use with Luma Generate Image node.\n    \"\"\"\n\n    RETURN_TYPES = (LumaIO.LUMA_REF,)\n    RETURN_NAMES = (\"luma_ref\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_luma_reference\"\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Image to use as reference.\",\n                    },\n                ),\n                \"weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of image reference.\",\n                    },\n                ),\n            },\n            \"optional\": {\"luma_ref\": (LumaIO.LUMA_REF,)},\n        }\n\n    def create_luma_reference(\n        self, image: torch.Tensor, weight: float, luma_ref: LumaReferenceChain = None\n    ):\n        if luma_ref is not None:\n            luma_ref = luma_ref.clone()\n        else:\n            luma_ref = LumaReferenceChain()\n        luma_ref.add(LumaReference(image=image, weight=round(weight, 2)))\n        return (luma_ref,)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-text-to-image",
  "markdown": "# Luma Text to Image - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Luma Text to Image 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg) Luma Text to Image 节点允许你使用Luma AI的先进图像生成功能，通过文本描述创建高度逼真和艺术化的图像。\n\n## 节点功能\n\n此节点连接到Luma AI的文本到图像API，让用户能够通过详细的文本提示词生成图像。Luma AI以其出色的真实感和细节表现而闻名，特别擅长生成照片级别的逼真内容和艺术风格图像。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 描述要生成内容的文本提示词 |\n| model | 选择项 | \\-  | 选择使用的生成模型 |\n| aspect\\_ratio | 选择项 | 16:9 | 设置输出图像的宽高比 |\n| seed | 整数  | 0   | 种子值，用于确定节点是否应重新运行，但实际结果与种子无关 |\n| style\\_image\\_weight | 浮点数 | 1.0 | 样式图像的权重，范围0.0-1.0，仅在提供style\\_image时生效，越大风格参考越强 |\n\n### 可选参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| image\\_luma\\_ref | LUMA\\_REF | Luma参考节点连接，通过输入图像影响生成结果，最多可考虑4张图像 |\n| style\\_image | 图像  | 样式参考图像，仅使用1张图像 |\n| character\\_image | 图像  | 角色参考图像，可以是多张图像的批次，最多可考虑4张图像 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 生成的图像结果 |\n\n## 使用示例\n\n[\n\nLuma Text to Image 工作流详细使用指南\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-image)\n\n## 工作原理\n\nLuma Text to Image 节点分析用户提供的文本提示词，通过Luma AI的生成模型创建相应的图像。该过程利用深度学习技术理解文本描述并将其转换为视觉表现。用户可以通过调整各种参数来精细控制生成过程，包括分辨率、引导尺度和负面提示词。 此外，节点支持使用参考图像和概念引导来进一步影响生成结果，使创作者能够更精确地实现他们的创意愿景。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass LumaImageGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously based on prompt and aspect ratio.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaImageModel],),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in LumaAspectRatio],\n                    {\n                        \"default\": LumaAspectRatio.ratio_16_9,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n                \"style_image_weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of style image. Ignored if no style_image provided.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image_luma_ref\": (\n                    LumaIO.LUMA_REF,\n                    {\n                        \"tooltip\": \"Luma Reference node connection to influence generation with input images; up to 4 images can be considered.\"\n                    },\n                ),\n                \"style_image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"Style reference image; only 1 image will be used.\"},\n                ),\n                \"character_image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Character reference images; can be a batch of multiple, up to 4 images can be considered.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        aspect_ratio: str,\n        seed,\n        style_image_weight: float,\n        image_luma_ref: LumaReferenceChain = None,\n        style_image: torch.Tensor = None,\n        character_image: torch.Tensor = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # handle image_luma_ref\n        api_image_ref = None\n        if image_luma_ref is not None:\n            api_image_ref = self._convert_luma_refs(\n                image_luma_ref, max_refs=4, auth_token=auth_token\n            )\n        # handle style_luma_ref\n        api_style_ref = None\n        if style_image is not None:\n            api_style_ref = self._convert_style_image(\n                style_image, weight=style_image_weight, auth_token=auth_token\n            )\n        # handle character_ref images\n        character_ref = None\n        if character_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                character_image, max_images=4, auth_token=auth_token\n            )\n            character_ref = LumaCharacterRef(\n                identity0=LumaImageIdentity(images=download_urls)\n            )\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations/image\",\n                method=HttpMethod.POST,\n                request_model=LumaImageGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaImageGenerationRequest(\n                prompt=prompt,\n                model=model,\n                aspect_ratio=aspect_ratio,\n                image_ref=api_image_ref,\n                style_ref=api_style_ref,\n                character_ref=character_ref,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        img_response = requests.get(response_poll.assets.image)\n        img = process_image_response(img_response)\n        return (img,)\n\n    def _convert_luma_refs(\n        self, luma_ref: LumaReferenceChain, max_refs: int, auth_token=None\n    ):\n        luma_urls = []\n        ref_count = 0\n        for ref in luma_ref.refs:\n            download_urls = upload_images_to_comfyapi(\n                ref.image, max_images=1, auth_token=auth_token\n            )\n            luma_urls.append(download_urls[0])\n            ref_count += 1\n            if ref_count >= max_refs:\n                break\n        return luma_ref.create_api_model(download_urls=luma_urls, max_refs=max_refs)\n\n    def _convert_style_image(\n        self, style_image: torch.Tensor, weight: float, auth_token=None\n    ):\n        chain = LumaReferenceChain(\n            first_ref=LumaReference(image=style_image, weight=weight)\n        )\n        return self._convert_luma_refs(chain, max_refs=1, auth_token=auth_token)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/openai/openai-dalle3",
  "markdown": "# OpenAI DALL·E 3 - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Stability Stable Image Ultra 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg) 此节点连接到OpenAI的DALL·E 3 API，使用户能够通过详细的文本提示词生成高质量图像。DALL·E 3是 OpenAI 的图像生成模型，相比前代提供了显著提升的图像质量、更精确的提示词理解和更优秀的细节表现能力。\n\n```\n\nclass OpenAIDalle3(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's DALL·E 3 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for DALL·E\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"quality\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"standard\", \"hd\"],\n                        \"default\": \"standard\",\n                        \"tooltip\": \"Image quality\",\n                    },\n                ),\n                \"style\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"natural\", \"vivid\"],\n                        \"default\": \"natural\",\n                        \"tooltip\": \"Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"1024x1024\", \"1024x1792\", \"1792x1024\"],\n                        \"default\": \"1024x1024\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        style=\"natural\",\n        quality=\"standard\",\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"dall-e-3\"\n\n        # build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/openai/images/generations\",\n                method=HttpMethod.POST,\n                request_model=OpenAIImageGenerationRequest,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=OpenAIImageGenerationRequest(\n                model=model,\n                prompt=prompt,\n                quality=quality,\n                size=size,\n                style=style,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/vace",
  "markdown": "# ComfyUI Wan2.1 VACE Video Examples\n\n## About VACE\n\nVACE 14B is an open-source unified video editing model launched by the Alibaba Tongyi Wanxiang team. Through integrating multi-task capabilities, supporting high-resolution processing and flexible multi-modal input mechanisms, this model significantly improves the efficiency and quality of video creation. The model is open-sourced under the [Apache-2.0](https://github.com/ali-vilab/VACE?tab=Apache-2.0-1-ov-file) license and can be used for personal or commercial purposes. Here is a comprehensive analysis of its core features and technical highlights:\n\n*   Multi-modal input: Supports multiple input forms including text, images, video, masks, and control signals\n*   Unified architecture: Single model supports multiple tasks with freely combinable functions\n*   Motion transfer: Generates coherent actions based on reference videos\n*   Local replacement: Replaces specific areas in videos through masks\n*   Video extension: Completes actions or extends backgrounds\n*   Background replacement: Preserves subjects while changing environmental backgrounds\n\nCurrently VACE has released two versions - 1.3B and 14B. Compared to the 1.3B version, the 14B version supports 720P resolution output with better image details and stability.\n\n| Model | 480P | 720P |\n| --- | --- | --- |\n| [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B) | ✅   | ❌   |\n| [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B) | ✅   | ✅   |\n\nRelated model weights and code repositories:\n\n*   [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B)\n*   [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B)\n*   [Github](https://github.com/ali-vilab/VACE)\n*   [VACE Project Homepage](https://ali-vilab.github.io/VACE-Page/)\n\n## Model Download and Loading in Workflows\n\nSince the workflows covered in this document all use the same workflow template, we can first complete the model download and loading information introduction, then enable/disable different inputs through Bypassing different nodes to achieve different workflows. The model download information is already embedded in the workflow information in specific examples, so you can also complete the model download when downloading specific example workflows.\n\n### Model Download\n\n**diffusion\\_models** [wan2.1\\_vace\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_14B_fp16.safetensors) [wan2.1\\_vace\\_1.3B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_1.3B_fp16.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\nChoose one version from **Text encoders** to download\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\nFile save location\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └─── wan2.1_vace_14B_fp16.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors # or umt5_xxl_fp16.safetensors\n│   └── 📂 vae/\n│       └──  wan_2.1_vae.safetensors\n```\n\n### Model Loading\n\nSince the models used in the workflows covered in this document are consistent, the workflows are also the same, and only the nodes are bypassed to enable/disable different inputs, please refer to the following image to ensure that the corresponding models are correctly loaded in different workflows. ![Wan2.1 VACE Model Loading](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-model-loading.jpg)\n\n1.  Make sure the `Load Diffusion Model` node has loaded `wan2.1_vace_14B_fp16.safetensors`\n2.  Make sure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors` or `umt5_xxl_fp16.safetensors`\n3.  Make sure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n\n### How to toggle Node Bypass Status\n\nWhen a node is set to Bypass status, data passing through the node will not be affected by the node and will be output directly. We often set nodes to Bypass status when we don’t need them. Here are three ways to toggle a node’s Bypass status: ![Toggle Bypass](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/nodes/cancel-bypass.jpg)\n\n1.  After selecting the node, click the arrow in the indicator section of the selection toolbox to quickly toggle the node’s Bypass status\n2.  After selecting the node, right-click the node and select `Mode` -> `Always` to switch to Always mode\n3.  After selecting the node, right-click the node and select the `Bypass` option to toggle the Bypass status\n\n## VACE Text-to-Video Workflow\n\n### 1\\. Workflow Download\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow\n\n### 2\\. Complete the Workflow Step by Step\n\n![image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg) Please follow the numbered steps in the image to ensure smooth workflow execution\n\n1.  Enter positive prompts in the `CLIP Text Encode (Positive Prompt)` node\n2.  Enter negative prompts in the `CLIP Text Encode (Negative Prompt)` node\n3.  Set the image dimensions (640x640 resolution recommended for first run) and frame count (video duration) in `WanVaceToVideo`\n4.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n5.  Once generated, the video will automatically save to `ComfyUI/output/video` directory (subfolder location depends on `save video` node settings)\n\n## VACE Image-to-Video Workflow\n\nYou can continue using the workflow above, just unbypass the `Load image` node in **Load reference image** and input your image. You can also use the image below - in this file we’ve already set up the corresponding parameters.\n\n### 1\\. Workflow Download\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow\n\nPlease download the image below as input ![vace-i2v-input](https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/video/wan/vace/i2v/input.jpg)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg) Please follow the numbered steps in the image to ensure smooth workflow execution\n\n1.  Input the corresponding image in the `Load image` node\n2.  You can modify and edit prompts like in the text-to-video workflow\n3.  Set the image dimensions (640x640 resolution recommended for first run) and frame count (video duration) in `WanVaceToVideo`\n4.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n5.  Once generated, the video will automatically save to `ComfyUI/output/video` directory (subfolder location depends on `save video` node settings)\n\n### 3\\. Additional Workflow Notes\n\nVACE also supports inputting multiple reference images in a single image to generate corresponding videos. You can see related examples on the VACE project [page](https://ali-vilab.github.io/VACE-Page/)\n\n## VACE Video-to-Video Workflow\n\n### 1\\. Workflow Download\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow\n\nWe will use the following materials as input:\n\n1.  Input image for reference ![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/vace/v2v/input.jpg) \n2.  The video below has been preprocessed and will be used to control video generation\n\n3.  The video below is the original video. You can download these materials and use preprocessing nodes like [comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to preprocess the images\n\n### 2\\. Complete the Workflow Step by Step\n\n![Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg) Please follow the numbered steps in the image to ensure smooth workflow execution\n\n1.  Input the reference image in the `Load Image` node under `Load reference image`\n2.  Input the control video in the `Load Video` node under `Load control video`. Since the provided video is preprocessed, no additional processing is needed\n3.  If you need to preprocess the original video yourself, you can modify the `Image preprocessing` group or use `comfyui_controlnet_aux` nodes to complete the preprocessing\n4.  Modify prompts\n5.  Set the image dimensions (640x640 resolution recommended for first run) and frame count (video duration) in `WanVaceToVideo`\n6.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n7.  Once generated, the video will automatically save to `ComfyUI/output/video` directory (subfolder location depends on `save video` node settings)\n\n## VACE Video Outpainting Workflow\n\n\\[To be updated\\]\n\n## VACE First-Last Frame Video Generation\n\n\\[To be updated\\] To ensure that the first and last frames are effective, the video `length` setting must satisfy that `length-1` is divisible by 4. The corresponding `Batch_size` setting must satisfy `Batch_size = length - 2`\n\nPlease refer to the documentation below to learn about related nodes\n\n[\n\n## WanVaceToVideo Node Documentation\n\nWanVaceToVideo Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/conditioning/video-models/wan-vace-to-video)[\n\n## TrimVideoLatent Node Documentation\n\nComfyUI TrimVideoLatent Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/latent/video/trim-video-latent)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-creative-upscale",
  "markdown": "# Recraft Creative Upscale - ComfyUI 原生节点文档\n\n![ComfyUI 原生Recraft Creative Upscale节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg) Recraft Creative Upscale 节点使用 Recraft 的 API 增加图像分辨率，还创造性地增强和丰富图像细节。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 需要创意放大的输入图像 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 创意放大后的高分辨率图像 |\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass RecraftCreativeUpscaleNode(RecraftCrispUpscaleNode):\n    \"\"\"\n    Upscale image synchronously.\n    Enhances a given raster image using ‘creative upscale’ tool, boosting resolution with a focus on refining small details and faces.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_PATH = \"/proxy/recraft/images/creativeUpscale\"\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-controls",
  "markdown": "# Recraft Controls - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Recraft Controls 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg) Recraft Controls 节点允许你定义一系列控制参数（如颜色和背景颜色指导），用于精确指导Recraft的图像生成过程。这个节点将多种控制输入整合为一个统一的控制对象。\n\n## 参数说明\n\n### 可选参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| colors | Recraft Color | 用于生成图像的颜色控制 |\n| background\\_color | Recraft Color | 背景颜色控制 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| recraft\\_controls | Recraft Controls | 控制配置对象，连接到Recraft生成节点 |\n\n## 使用示例\n\n[\n\n## Recraft Text to Image 工作流示例\n\nRecraft Text to Image 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## 工作原理\n\n节点处理流程:\n\n1.  收集输入的控制参数（colors和background\\_color）\n2.  将这些参数整合到一个结构化的控制对象中\n3.  输出此控制对象，可连接到各种Recraft生成节点\n\n当连接到Recraft生成节点后，这些控制参数会影响AI的生成过程，使AI能够考虑多种因素，而不仅仅是文本提示的语义内容。如果配置了颜色输入，AI将尝试在生成的图像中合理地使用这些颜色。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass RecraftControlsNode:\n    \"\"\"\n    Create Recraft Controls for customizing Recraft generation.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.CONTROLS,)\n    RETURN_NAMES = (\"recraft_controls\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_controls\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n            },\n            \"optional\": {\n                \"colors\": (RecraftIO.COLOR,),\n                \"background_color\": (RecraftIO.COLOR,),\n            }\n        }\n\n    def create_controls(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None):\n        return (RecraftControls(colors=colors, background_color=background_color), )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale",
  "markdown": "# Recraft Crisp Upscale - ComfyUI 原生节点文档\n\n![ComfyUI 原生Recraft Crisp Upscale节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg) Recraft Crisp Upscale 节点利用 Recraft 的 API 增强图像分辨率和清晰度。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 需要放大的输入图像 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 放大和增强后的图像 |\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass RecraftCrispUpscaleNode:\n    \"\"\"\n    Upscale image synchronously.\n    Enhances a given raster image using ‘crisp upscale’ tool, increasing image resolution, making the image sharper and cleaner.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_PATH = \"/proxy/recraft/images/crispUpscale\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=self.RECRAFT_PATH,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-color-rgb",
  "markdown": "# Recraft Color RGB - ComfyUI 原生节点文档\n\n ![ComfyUI 原生 Recraft Color RGB 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg) Recraft Color RGB 节点允许你定义精确的RGB颜色值，用于控制Recraft图像生成过程中的颜色使用。\n\n## 节点功能\n\n此节点创建一个颜色配置对象，可以连接到Recraft Controls节点，用于指定生成图像中应使用的颜色。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| r   | 整数  | 0   | 红色通道值(0-255) |\n| g   | 整数  | 0   | 绿色通道值(0-255) |\n| b   | 整数  | 0   | 蓝色通道值(0-255) |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| recraft\\_color | Recraft Color | 颜色配置对象，连接到Recraft Controls节点 |\n\n## 使用示例\n\n[\n\n## Recraft Text to Image 工作流示例\n\nRecraft Text to Image 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass RecraftColorRGBNode:\n    \"\"\"\n    Create Recraft Color by choosing specific RGB values.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.COLOR,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    RETURN_NAMES = (\"recraft_color\",)\n    FUNCTION = \"create_color\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"r\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Red value of color.\"\n                }),\n                \"g\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Green value of color.\"\n                }),\n                \"b\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Blue value of color.\"\n                }),\n            },\n            \"optional\": {\n                \"recraft_color\": (RecraftIO.COLOR,),\n            }\n        }\n\n    def create_color(self, r: int, g: int, b: int, recraft_color: RecraftColorChain=None):\n        recraft_color = recraft_color.clone() if recraft_color else RecraftColorChain()\n        recraft_color.add(RecraftColor(r, g, b))\n        return (recraft_color, )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/openai/openai-dalle2",
  "markdown": "# OpenAI DALL·E 2 - ComfyUI 原生节点文档\n\n```\n\nclass OpenAIDalle2(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's DALL·E 2 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for DALL·E\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"256x256\", \"512x512\", \"1024x1024\"],\n                        \"default\": \"1024x1024\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"How many images to generate\",\n                    },\n                ),\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        image=None,\n        mask=None,\n        n=1,\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"dall-e-2\"\n        path = \"/proxy/openai/images/generations\"\n        content_type = \"application/json\"\n        request_class = OpenAIImageGenerationRequest\n        img_binary = None\n\n        if image is not None and mask is not None:\n            path = \"/proxy/openai/images/edits\"\n            content_type = \"multipart/form-data\"\n            request_class = OpenAIImageEditRequest\n\n            input_tensor = image.squeeze().cpu()\n            height, width, channels = input_tensor.shape\n            rgba_tensor = torch.ones(height, width, 4, device=\"cpu\")\n            rgba_tensor[:, :, :channels] = input_tensor\n\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n            rgba_tensor[:, :, 3] = 1 - mask.squeeze().cpu()\n\n            rgba_tensor = downscale_image_tensor(rgba_tensor.unsqueeze(0)).squeeze()\n\n            image_np = (rgba_tensor.numpy() * 255).astype(np.uint8)\n            img = Image.fromarray(image_np)\n            img_byte_arr = io.BytesIO()\n            img.save(img_byte_arr, format=\"PNG\")\n            img_byte_arr.seek(0)\n            img_binary = img_byte_arr  # .getvalue()\n            img_binary.name = \"image.png\"\n        elif image is not None or mask is not None:\n            raise Exception(\"Dall-E 2 image editing requires an image AND a mask\")\n\n        # Build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=path,\n                method=HttpMethod.POST,\n                request_model=request_class,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=request_class(\n                model=model,\n                prompt=prompt,\n                n=n,\n                size=size,\n                seed=seed,\n            ),\n            files=(\n                {\n                    \"image\": img_binary,\n                }\n                if img_binary\n                else None\n            ),\n            content_type=content_type,\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/controlnet",
  "markdown": "# ComfyUI ControlNet Usage Example - ComfyUI\n\nAchieving precise control over image creation in AI image generation cannot be done with just one click. It typically requires numerous generation attempts to produce a satisfactory image. However, the emergence of **ControlNet** has effectively addressed this challenge. ControlNet is a conditional control generation model based on diffusion models (such as Stable Diffusion), first proposed by [Lvmin Zhang](https://lllyasviel.github.io/) and Maneesh Agrawala et al. in 2023 in the paper [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543). ControlNet models significantly enhance the controllability of image generation and the ability to reproduce details by introducing multimodal input conditions, such as edge detection maps, depth maps, and pose keypoints. These conditioning constraints make image generation more controllable, allowing multiple ControlNet models to be used simultaneously during the drawing process for better results. Before ControlNet, we could only rely on the model to generate images repeatedly until we were satisfied with the results, which involved a lot of randomness. ![Images generated with random seeds in ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/generated_with_random_seed.jpg) With the advent of ControlNet, we can control image generation by introducing additional conditions. For example, we can use a simple sketch to guide the image generation process, producing images that closely align with our sketch. ![Sketch-controlled image generation in ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/scribble_example.jpg) In this example, we will guide you through installing and using ControlNet models in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), and complete a sketch-controlled image generation example. ![ComfyUI ControlNet Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)\n\n## ControlNet Image Preprocessing Information\n\nDifferent types of ControlNet models typically require different types of reference images: ![Reference Images](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/main/examples/CNAuxBanner.jpg?raw=true)\n\n> Image source: [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\nSince the current **Comfy Core** nodes do not include all types of **preprocessors**, in the actual examples in this documentation, we will provide pre-processed images. However, in practical use, you may need to use custom nodes to preprocess images to meet the requirements of different ControlNet models. Below are some relevant custom nodes:\n\n*   [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n*   [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n## ComfyUI ControlNet Workflow Example Explanation\n\n### 1\\. ControlNet Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)\n\nPlease download the image below, which we will use as input: ![ComfyUI Sketch Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_input.png)\n\n### 2\\. Manual Model Installation\n\n*   [dreamCreationVirtual3DECommerce\\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n*   [control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── dreamCreationVirtual3DECommerce_v10.safetensors\n│   ├── vae/\n│   │   └── vae-ft-mse-840000-ema-pruned.safetensors\n│   └── controlnet/\n│       └── control_v11p_sd15_scribble_fp16.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_scribble.png)\n\n1.  Ensure that `Load Checkpoint` can load **dreamCreationVirtual3DECommerce\\_v10.safetensors**\n2.  Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**\n3.  Click `Upload` in the `Load Image` node to upload the input image provided earlier\n4.  Ensure that `Load ControlNet` can load **control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors**\n5.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n### Load ControlNet Node Explanation\n\n![load controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_controlnet_model.jpg) Models located in `ComfyUI\\models\\controlnet` will be detected by ComfyUI and can be loaded through this node.\n\n### Apply ControlNet Node Explanation\n\n![apply controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg) This node accepts the ControlNet model loaded by `load controlnet` and generates corresponding control conditions based on the input image. **Input Types**\n\n| Parameter Name | Function |\n| --- | --- |\n| `positive` | Positive conditioning |\n| `negative` | Negative conditioning |\n| `control_net` | The ControlNet model to be applied |\n| `image` | Preprocessed image used as reference for ControlNet application |\n| `vae` | VAE model input |\n| `strength` | Strength of ControlNet application; higher values increase ControlNet’s influence on the generated image |\n| `start_percent` | Determines when to start applying ControlNet as a percentage; e.g., 0.2 means ControlNet guidance begins when 20% of diffusion is complete |\n| `end_percent` | Determines when to stop applying ControlNet as a percentage; e.g., 0.8 means ControlNet guidance stops when 80% of diffusion is complete |\n\n**Output Types**\n\n| Parameter Name | Function |\n| --- | --- |\n| `positive` | Positive conditioning data processed by ControlNet |\n| `negative` | Negative conditioning data processed by ControlNet |\n\nYou can use chain connections to apply multiple ControlNet models, as shown in the image below. You can also refer to the [Mixing ControlNet Models](https://docs.comfy.org/tutorials/controlnet/mixing-controlnets) guide to learn more about combining multiple ControlNet models. ![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg) \n\n## Start Your Exploration\n\n1.  Try creating similar sketches, or even draw your own, and use ControlNet models to generate images to experience the benefits of ControlNet.\n2.  Adjust the `Control Strength` parameter in the Apply ControlNet node to control the influence of the ControlNet model on the generated image.\n3.  Visit the [ControlNet-v1-1\\_fp16\\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) repository to download other types of ControlNet models and try using them to generate images."
},
{
  "url": "https://docs.comfy.org/specs/workflow_json",
  "markdown": "# Workflow JSON - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyWorkflow1_0\",\n  \"definitions\": {\n    \"ComfyWorkflow1_0\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"version\": {\n          \"type\": \"number\",\n          \"const\": 1\n        },\n        \"config\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"links_ontop\": {\n                      \"type\": \"boolean\"\n                    },\n                    \"align_to_grid\": {\n                      \"type\": \"boolean\"\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"state\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"lastGroupid\": {\n              \"type\": \"number\"\n            },\n            \"lastNodeId\": {\n              \"type\": \"number\"\n            },\n            \"lastLinkId\": {\n              \"type\": \"number\"\n            },\n            \"lastRerouteId\": {\n              \"type\": \"number\"\n            }\n          },\n          \"additionalProperties\": true\n        },\n        \"groups\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"title\": {\n                \"type\": \"string\"\n              },\n              \"bounding\": {\n                \"type\": \"array\",\n                \"minItems\": 4,\n                \"maxItems\": 4,\n                \"items\": [\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"font_size\": {\n                \"type\": \"number\"\n              },\n              \"locked\": {\n                \"type\": \"boolean\"\n              }\n            },\n            \"required\": [\n              \"title\",\n              \"bounding\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"nodes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"size\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"flags\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"collapsed\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"pinned\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"allow_interaction\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"horizontal\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"skip_repeated_outputs\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"order\": {\n                \"type\": \"number\"\n              },\n              \"mode\": {\n                \"type\": \"number\"\n              },\n              \"inputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"link\": {\n                      \"type\": [\n                        \"number\",\n                        \"null\"\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"outputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"links\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        {\n                          \"type\": \"null\"\n                        }\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"properties\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"Node name for S&R\": {\n                    \"type\": \"string\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"widgets_values\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\"\n                  },\n                  {\n                    \"type\": \"object\",\n                    \"additionalProperties\": {}\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"bgcolor\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"type\",\n              \"pos\",\n              \"size\",\n              \"flags\",\n              \"order\",\n              \"mode\",\n              \"properties\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"links\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"type\": \"number\"\n              },\n              \"origin_id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"origin_slot\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"target_id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"target_slot\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"string\"\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    }\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"parentId\": {\n                \"type\": \"number\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"origin_id\",\n              \"origin_slot\",\n              \"target_id\",\n              \"target_slot\",\n              \"type\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"reroutes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"type\": \"number\"\n              },\n              \"parentId\": {\n                \"type\": \"number\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"linkIds\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"number\"\n                    }\n                  },\n                  {\n                    \"type\": \"null\"\n                  }\n                ]\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"pos\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"extra\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"ds\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"scale\": {\n                          \"type\": \"number\"\n                        },\n                        \"offset\": {\n                          \"anyOf\": [\n                            {\n                              \"type\": \"object\",\n                              \"properties\": {\n                                \"0\": {\n                                  \"type\": \"number\"\n                                },\n                                \"1\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              \"required\": [\n                                \"0\",\n                                \"1\"\n                              ],\n                              \"additionalProperties\": true\n                            },\n                            {\n                              \"type\": \"array\",\n                              \"minItems\": 2,\n                              \"maxItems\": 2,\n                              \"items\": [\n                                {\n                                  \"type\": \"number\"\n                                },\n                                {\n                                  \"type\": \"number\"\n                                }\n                              ]\n                            }\n                          ]\n                        }\n                      },\n                      \"required\": [\n                        \"scale\",\n                        \"offset\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"info\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"name\": {\n                          \"type\": \"string\"\n                        },\n                        \"author\": {\n                          \"type\": \"string\"\n                        },\n                        \"description\": {\n                          \"type\": \"string\"\n                        },\n                        \"version\": {\n                          \"type\": \"string\"\n                        },\n                        \"created\": {\n                          \"type\": \"string\"\n                        },\n                        \"modified\": {\n                          \"type\": \"string\"\n                        },\n                        \"software\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"required\": [\n                        \"name\",\n                        \"author\",\n                        \"description\",\n                        \"version\",\n                        \"created\",\n                        \"modified\",\n                        \"software\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"linkExtensions\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"parentId\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    },\n                    \"reroutes\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          },\n                          \"pos\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"0\": {\n                                    \"type\": \"number\"\n                                  },\n                                  \"1\": {\n                                    \"type\": \"number\"\n                                  }\n                                },\n                                \"required\": [\n                                  \"0\",\n                                  \"1\"\n                                ],\n                                \"additionalProperties\": true\n                              },\n                              {\n                                \"type\": \"array\",\n                                \"minItems\": 2,\n                                \"maxItems\": 2,\n                                \"items\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"number\"\n                                  }\n                                ]\n                              }\n                            ]\n                          },\n                          \"linkIds\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              {\n                                \"type\": \"null\"\n                              }\n                            ]\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"pos\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"models\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"url\": {\n                \"type\": \"string\",\n                \"format\": \"uri\"\n              },\n              \"hash\": {\n                \"type\": \"string\"\n              },\n              \"hash_type\": {\n                \"type\": \"string\"\n              },\n              \"directory\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"name\",\n              \"url\",\n              \"directory\"\n            ],\n            \"additionalProperties\": false\n          }\n        }\n      },\n      \"required\": [\n        \"version\",\n        \"state\",\n        \"nodes\"\n      ],\n      \"additionalProperties\": true\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fcustom-nodes%2Fjs%2Fjavascript_bottom_panel_tabs",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fregistry%2Fspecifications",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-kontext",
  "markdown": "# ComfyUI Flux.1 Kontext Pro Image API Node Official Example\n\nFLUX.1 Kontext is a professional image-to-image editing model developed by Black Forest Labs, focusing on intelligent understanding of image context and precise editing. It can perform various editing tasks without complex descriptions, including object modification, style transfer, background replacement, character consistency editing, and text editing. The core advantage of Kontext lies in its excellent context understanding ability and character consistency maintenance, ensuring that key elements such as character features and composition layout remain stable even after multiple iterations of editing. Currently, ComfyUI has supported two models of Flux.1 Kontext:\n\n*   **Kontext Pro** is ideal for editing, composing, and remixing.\n*   **Kontext Max** pushes the limits on typography, prompt precision, and speed.\n\nIn this guide, we will briefly introduce how to use the Flux.1 Kontext API nodes to perform image editing through corresponding workflows.\n\n## Flux.1 Kontext Multiple Image Input Workflow\n\nWe have recently updated to support multiple image input workflows. Using the new `Image Stitch` node, you can stitch multiple images into a single image and edit it using Flux.1 Kontext.\n\n### 1\\. Workflow File Download\n\nThe `metadata` of the images below contains the workflow information. Please download and drag them into ComfyUI to load the corresponding workflow. ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/multiple_image_input.png) Download the following images for input or use your own images: ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/girl.jpg) ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/dog.jpg) ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/sofa.jpg)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ComfyUI Flux.1 Kontext Pro Image API Node Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg) You can follow the numbered steps in the image to complete the workflow:\n\n1.  Upload the provided images in the `Load image` node\n2.  Modify the necessary parameters in `Flux.1 Kontext Pro Image`:\n    *   `prompt` Enter the prompt for the image you want to edit\n    *   `aspect_ratio` Set the aspect ratio of the original image, which must be between 1:4 and 4:1\n    *   `prompt_upsampling` Set whether to use prompt upsampling. If enabled, it will automatically modify the prompt to get richer results, but the results are not reproducible\n3.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image editing\n4.  After waiting for the API to return results, you can view the edited image in the `Save Image` node, and the corresponding image will also be saved to the `ComfyUI/output/` directory\n\n### 1\\. Workflow File Download\n\nThe `metadata` of the image below contains the workflow information. Please download and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image.png) Download the image below for input or use your own image: ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image_input.png)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ComfyUI Flux.1 Kontext Pro Image API Node Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg) You can follow the numbered steps in the image to complete the workflow:\n\n1.  Load the image you want to edit in the `Load Image` node\n2.  (Optional) Modify the necessary parameters in `Flux.1 Kontext Pro Image`\n3.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image editing\n4.  After waiting for the API to return results, you can view the edited image in the `Save Image` node, and the corresponding image will also be saved to the `ComfyUI/output/` directory\n\n## Flux.1 Kontext Max Image API Node Workflow\n\n### 1\\. Workflow File Download\n\nThe `metadata` of the image below contains the workflow information. Please download and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI Flux.1 Kontext Max Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image.png) Download the image below for input or use your own image for demonstration: ![ComfyUI Flux.1 Kontext Max Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image_input.png)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ComfyUI Flux.1 Kontext Max Image API Node Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg) You can follow the numbered steps in the image to complete the workflow:\n\n1.  Load the image you want to edit in the `Load Image` node\n2.  (Optional) Modify the necessary parameters in `Flux.1 Kontext Max Image`\n3.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image editing\n4.  After waiting for the API to return results, you can view the edited image in the `Save Image` node, and the corresponding image will also be saved to the `ComfyUI/output/` directory\n\n## Flux Kontext Prompt Techniques\n\n### 1\\. Basic Modifications\n\n*   Simple and direct: `\"Change the car color to red\"`\n*   Maintain style: `\"Change to daytime while maintaining the same style of the painting\"`\n\n### 2\\. Style Transfer\n\n**Principles:**\n\n*   Clearly name style: `\"Transform to Bauhaus art style\"`\n*   Describe characteristics: `\"Transform to oil painting with visible brushstrokes, thick paint texture\"`\n*   Preserve composition: `\"Change to Bauhaus style while maintaining the original composition\"`\n\n### 3\\. Character Consistency\n\n**Framework:**\n\n*   Specific description: `\"The woman with short black hair\"` instead of “she”\n*   Preserve features: `\"while maintaining the same facial features, hairstyle, and expression\"`\n*   Step-by-step modifications: Change background first, then actions\n\n### 4\\. Text Editing\n\n*   Use quotes: `\"Replace 'joy' with 'BFL'\"`\n*   Maintain format: `\"Replace text while maintaining the same font style\"`\n\n## Common Problem Solutions\n\n### Character Changes Too Much\n\n❌ Wrong: `\"Transform the person into a Viking\"` ✅ Correct: `\"Change the clothes to be a viking warrior while preserving facial features\"`\n\n### Composition Position Changes\n\n❌ Wrong: `\"Put him on a beach\"` ✅ Correct: `\"Change the background to a beach while keeping the person in the exact same position, scale, and pose\"`\n\n### Style Application Inaccuracy\n\n❌ Wrong: `\"Make it a sketch\"` ✅ Correct: `\"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\"`\n\n## Core Principles\n\n1.  **Be Specific and Clear** - Use precise descriptions, avoid vague terms\n2.  **Step-by-step Editing** - Break complex modifications into multiple simple steps\n3.  **Explicit Preservation** - State what should remain unchanged\n4.  **Verb Selection** - Use “change”, “replace” rather than “transform”\n\n## Best Practice Templates\n\n**Object Modification:** `\"Change [object] to [new state], keep [content to preserve] unchanged\"` **Style Transfer:** `\"Transform to [specific style], while maintaining [composition/character/other] unchanged\"` **Background Replacement:** `\"Change the background to [new background], keep the subject in the exact same position and pose\"` **Text Editing:** `\"Replace '[original text]' with '[new text]', maintain the same font style\"`\n\n> **Remember:** The more specific, the better. Kontext excels at understanding detailed instructions and maintaining consistency."
},
{
  "url": "https://docs.comfy.org/tutorials/image/omnigen/omnigen2",
  "markdown": "# ComfyUI OmniGen2 Native Workflow Examples\n\n## About OmniGen2\n\nOmniGen2 is a powerful and efficient unified multimodal generation model with approximately **7B** total parameters (3B text model + 4B image generation model). Unlike OmniGen v1, OmniGen2 adopts an innovative dual-path Transformer architecture with completely independent text autoregressive model and image diffusion model, achieving parameter decoupling and specialized optimization.\n\n### Model Highlights\n\n*   **Visual Understanding**: Inherits the powerful image content interpretation and analysis capabilities of the Qwen-VL-2.5 foundation model\n*   **Text-to-Image Generation**: Creates high-fidelity and aesthetically pleasing images from text prompts\n*   **Instruction-guided Image Editing**: Performs complex, instruction-based image modifications, achieving state-of-the-art performance among open-source models\n*   **Contextual Generation**: Versatile capabilities to process and flexibly combine diverse inputs (including people, reference objects, and scenes), producing novel and coherent visual outputs\n\n### Technical Features\n\n*   **Dual-path Architecture**: Based on Qwen 2.5 VL (3B) text encoder + independent diffusion Transformer (4B)\n*   **Omni-RoPE Position Encoding**: Supports multi-image spatial positioning and identity distinction\n*   **Parameter Decoupling Design**: Avoids negative impact of text generation on image quality\n*   Support for complex text understanding and image understanding\n*   Controllable image generation and editing\n*   Excellent detail preservation capabilities\n*   Unified architecture supporting multiple image generation tasks\n*   Text generation capability: Can generate clear text content within images\n\n## OmniGen2 Model Download\n\nSince this article involves different workflows, the corresponding model files and installation locations are as follows. The download information for model files is also included in the corresponding workflows: **Diffusion Models**\n\n*   [omnigen2\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/omnigen2_fp16.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/resolve/main/split_files/vae/ae.safetensors)\n\n**Text Encoders**\n\n*   [qwen\\_2.5\\_vl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_fp16.safetensors)\n\nFile save location:\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └── omnigen2_fp16.safetensors\n│   ├── 📂 vae/\n│   │   └── ae.safetensors\n│   └── 📂 text_encoders/\n│       └── qwen_2.5_vl_fp16.safetensors\n```\n\n## ComfyUI OmniGen2 Text-to-Image Workflow\n\n### 1\\. Download Workflow File\n\n![Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_t2i.png)\n\n### 2\\. Complete Workflow Step by Step\n\n![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg) Please follow the numbered steps in the image for step-by-step confirmation to ensure smooth operation of the corresponding workflow:\n\n1.  **Load Main Model**: Ensure the `Load Diffusion Model` node loads `omnigen2_fp16.safetensors`\n2.  **Load Text Encoder**: Ensure the `Load CLIP` node loads `qwen_2.5_vl_fp16.safetensors`\n3.  **Load VAE**: Ensure the `Load VAE` node loads `ae.safetensors`\n4.  **Set Image Dimensions**: Set the generated image dimensions in the `EmptySD3LatentImage` node (recommended 1024x1024)\n5.  **Input Prompts**:\n    *   Input positive prompts in the first `CLipTextEncode` node (content you want to appear in the image)\n    *   Input negative prompts in the second `CLipTextEncode` node (content you don’t want to appear in the image)\n6.  **Start Generation**: Click the `Queue Prompt` button, or use the shortcut `Ctrl(cmd) + Enter` to execute text-to-image generation\n7.  **View Results**: After generation is complete, the corresponding images will be automatically saved to the `ComfyUI/output/` directory, and you can also preview them in the `SaveImage` node\n\n## ComfyUI OmniGen2 Image Editing Workflow\n\nOmniGen2 has rich image editing capabilities and supports adding text to images\n\n### 1\\. Download Workflow File\n\n![Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_image_edit.png) Download the image below, which we will use as the input image. ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/input_fairy.png) \n\n### 2\\. Complete Workflow Step by Step\n\n![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg)\n\n1.  **Load Main Model**: Ensure the `Load Diffusion Model` node loads `omnigen2_fp16.safetensors`\n2.  **Load Text Encoder**: Ensure the `Load CLIP` node loads `qwen_2.5_vl_fp16.safetensors`\n3.  **Load VAE**: Ensure the `Load VAE` node loads `ae.safetensors`\n4.  **Upload Image**: Upload the provided image in the `Load Image` node\n5.  **Input Prompts**:\n    *   Input positive prompts in the first `CLipTextEncode` node (content you want to appear in the image)\n    *   Input negative prompts in the second `CLipTextEncode` node (content you don’t want to appear in the image)\n6.  **Start Generation**: Click the `Queue Prompt` button, or use the shortcut `Ctrl(cmd) + Enter` to execute text-to-image generation\n7.  **View Results**: After generation is complete, the corresponding images will be automatically saved to the `ComfyUI/output/` directory, and you can also preview them in the `SaveImage` node\n\n### 3\\. Additional Workflow Instructions\n\n*   If you want to enable the second image input, you can use the shortcut **Ctrl + B** to enable the corresponding node inputs for nodes that are in pink/purple state in the workflow\n*   If you want to customize dimensions, you can delete the `Get image size` node linked to the `EmptySD3LatentImage` node and input custom dimensions"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image",
  "markdown": "# Luma Text to Image API Node ComfyUI Official Example\n\nThe [Luma Text to Image](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image) node allows you to generate high-quality images from text prompts using Luma AI’s advanced technology, capable of creating photorealistic content and artistic style images. In this guide, we’ll show you how to set up workflows using this node for text-to-image generation.\n\n## Luma Text to Image Node Documentation\n\nYou can refer to the following documentation for detailed parameter settings:\n\n[\n\n## Luma Text to Image Node Documentation\n\nLuma Text to Image API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)[\n\n## Luma Reference Node Documentation\n\nLuma Reference API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)\n\nWhen the `Luma Text to Image` node is used without any image inputs, it functions as a text-to-image workflow. In this guide, we’ve created examples using `style_image` and `image_luma_ref` to showcase Luma AI’s excellent image processing capabilities.\n\n### 1\\. Download Workflow Files\n\nThe workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow. ![Luma Text to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/luma_t2i.png) Please download these images for input: ![Input Image - Reference](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_ref.png) ![Input Image - Style](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_style.png)\n\n### 2\\. Follow Steps to Run the Workflow\n\n![Luma Text to Image Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg) Follow the numbered steps in the image to complete the basic workflow:\n\n1.  Upload the reference image in the `Load image` node\n2.  Upload the style reference image in the `Load image (renamed to styleref)` node\n3.  (Optional) Modify the prompts in the `Luma Text to Image` node\n4.  (Optional) Adjust the `style_image_weight` to control the style reference image’s influence\n5.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n6.  After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\n![Style Image Weight Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg)\n\n### 3\\. Additional Notes\n\n*   The [node](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image) allows up to 4 reference images and character references simultaneously.\n*   To enable multiple image inputs, right-click on the purple “Bypassed” nodes and set their `mode` to `always`"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcomfyui-server%2Fcomms_messages",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/video/ltxv",
  "markdown": "# LTX-Video - ComfyUI\n\n[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) is a very efficient video model by lightricks. The important thing with this model is to give it long descriptive prompts.\n\n## Multi Frame Control\n\nAllows you to control the video with a series of images. You can download the input images: [starting frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) and [ending frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png). ![LTX-Video Multi Frame Control](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/workflow.webp)\n\n## Image to Video\n\nAllows you to control the video with a first [frame image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png). ![LTX-Video Image to Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/workflow.webp)\n\n## Text to Video\n\n![LTX-Video Text to Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/t2v.webp)\n\n## Requirements\n\nDownload the following models and place them in the locations specified below:\n\n*   [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true)\n\n```\n├── checkpoints/\n│   └── ltx-video-2b-v0.9.5.safetensors\n└── text_encoders/\n    └── t5xxl_fp16.safetensors\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/openai/chat",
  "markdown": "# OpenAI Chat API Node ComfyUI Official Example\n\nOpenAI is a company focused on generative AI, providing powerful conversational capabilities. Currently, ComfyUI has integrated the OpenAI API, allowing you to directly use the related nodes in ComfyUI to complete conversational functions. In this guide, we will walk you through completing the corresponding conversational functionality.\n\n## OpenAI Chat Workflow\n\n### 1\\. Workflow File Download\n\nPlease download the Json file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/openai/api_openai_chat.json)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![OpenAI Chat Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg)\n\nYou can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Load Image` node, load the image you need AI to interpret\n2.  (Optional) If needed, you can modify the settings in `OpenAI Chat Advanced Options` to have AI execute specific tasks\n3.  In the `OpenAI Chat` node, you can modify `Prompt` to set the conversation prompt, or modify `model` to select different models\n4.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the conversation.\n5.  After waiting for the API to return results, you can view the corresponding AI returned content in the `Preview Any` node.\n\n### 3\\. Additional Notes\n\n*   Currently, the file input node `OpenAI Chat Input Files` requires files to be uploaded to the `ComfyUI/input/` directory first. This node is being improved, and we will modify the template after updates\n*   The workflow provides an example using `Batch Images` for input. If you have multiple images that need AI interpretation, you can refer to the step diagram and use right-click to set the corresponding node mode to `Always` to enable it"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/runway/image-generation",
  "markdown": "# Runway API Node Image Generation ComfyUI Official Example\n\nRunway is a company focused on generative AI, providing powerful image generation capabilities. Its models support features such as style transfer, image extension, and detail control. Currently, ComfyUI has integrated the Runway API, allowing you to directly use the related nodes in ComfyUI for image generation. In this guide, we will walk you through the following workflows:\n\n*   Text-to-image\n*   Reference-to-image\n\n## Runway Image Text-to-Image Workflow\n\n### 1\\. Workflow File Download\n\nThe image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI Runway Image Text to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/text_to_image.png)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway Image Text to Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg) You can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Runway Text to Image` node, input your prompt in the `prompt` field\n2.  (Optional) Adjust the `ratio` setting to set different output aspect ratios\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute image generation.\n4.  After waiting for the API to return results, you can view the generated image in the `Save Image` node (right-click to save). The corresponding image will also be saved to the `ComfyUI/output/` directory.\n\n## Runway Image Reference-to-Image Workflow\n\n### 1\\. Workflow and Input Image Download\n\nThe image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI Runway Image Reference to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/runway_reference_to_image.png) Download the image below for input ![ComfyUI Runway Image Reference to Image Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/input.png)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway Image Reference to Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg) You can refer to the numbers in the image to complete the basic reference-to-image workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  In the `Runway Text to Image` node, input your prompt in the `prompt` field and adjust dimensions\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute image generation.\n4.  After waiting for the API to return results, you can view the generated image in the `Save Image` node (right-click to save). The corresponding image will also be saved to the `ComfyUI/output/` directory."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image",
  "markdown": "# Stability AI Stable Diffusion 3.5 - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Stability AI Stable Diffusion 3.5 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg) Stability AI Stable Diffusion 3.5 Image 节点使用 Stability AI 的 Stable Diffusion 3.5 API 生成高质量图像。它支持文本到图像和图像到图像的生成，能够根据文本提示词创建详细的视觉内容。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 您希望在输出图像中看到的内容。强有力、描述性的提示词，清晰定义元素、颜色和主题将带来更好的结果 |\n| model | 选择项 | \\-  | 选择使用的Stability SD 3.5模型 |\n| aspect\\_ratio | 选择项 | ”1:1” | 生成图像的宽高比 |\n| style\\_preset | 选择项 | ”None” | 可选的期望图像风格预设 |\n| cfg\\_scale | 浮点数 | 4.0 | 扩散过程对提示文本的遵循程度（更高的值使图像更接近您的提示词）。范围：1.0 - 10.0，步长：0.1 |\n| seed | 整数  | 0   | 用于创建噪声的随机种子，范围0-4294967294 |\n\n### 可选参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 输入图像。当提供图像时，节点将切换到图像到图像模式 |\n| negative\\_prompt | 字符串 | \"\"  | 您不希望在输出图像中看到的关键词。这是一个高级功能 |\n| image\\_denoise | 浮点数 | 0.5 | 输入图像的去噪程度。0.0产生与输入完全相同的图像，1.0则相当于没有提供任何图像。范围：0.0 - 1.0，步长：0.01。仅在提供输入图像时有效 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 生成的图像 |\n\n## 使用示例\n\n[\n\nStability AI Stable Diffusion 3.5 Image 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)\n\n## 注意事项\n\n*   当提供输入图像时，节点将从文本到图像模式切换到图像到图像模式\n*   在图像到图像模式下，宽高比参数将被忽略\n*   模式选择会根据是否提供图像自动切换：\n    *   未提供图像：文本到图像模式\n    *   提供图像：图像到图像模式\n*   如果style\\_preset设置为”None”，则不会应用任何预设风格\n\n## 源码\n\n\\[节点源码 (更新于2025-05-07)\\]\n\n```\nclass StabilityStableImageSD_3_5Node:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Stability AI\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results.\"\n                    },\n                ),\n                \"model\": ([x.value for x in Stability_SD3_5_Model],),\n                \"aspect_ratio\": ([x.value for x in StabilityAspectRatio],\n                    {\n                        \"default\": StabilityAspectRatio.ratio_1_1,\n                        \"tooltip\": \"Aspect ratio of generated image.\",\n                    },\n                ),\n                \"style_preset\": (get_stability_style_presets(),\n                    {\n                        \"tooltip\": \"Optional desired style of generated image.\",\n                    },\n                ),\n                \"cfg_scale\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 4.0,\n                        \"min\": 1.0,\n                        \"max\": 10.0,\n                        \"step\": 0.1,\n                        \"tooltip\": \"How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 4294967294,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (IO.IMAGE,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"Keywords of what you do not wish to see in the output image. This is an advanced feature.\"\n                    },\n                ),\n                \"image_denoise\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(self, model: str, prompt: str, aspect_ratio: str, style_preset: str, seed: int, cfg_scale: float,\n                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,\n                 auth_token=None):\n        validate_string(prompt, strip_whitespace=False)\n        # prepare image binary if image present\n        image_binary = None\n        mode = Stability_SD3_5_GenerationMode.text_to_image\n        if image is not None:\n            image_binary = tensor_to_bytesio(image, total_pixels=1504*1504).read()\n            mode = Stability_SD3_5_GenerationMode.image_to_image\n            aspect_ratio = None\n        else:\n            image_denoise = None\n\n        if not negative_prompt:\n            negative_prompt = None\n        if style_preset == \"None\":\n            style_preset = None\n\n        files = {\n            \"image\": image_binary\n        }\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/stability/v2beta/stable-image/generate/sd3\",\n                method=HttpMethod.POST,\n                request_model=StabilityStable3_5Request,\n                response_model=StabilityStableUltraResponse,\n            ),\n            request=StabilityStable3_5Request(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                aspect_ratio=aspect_ratio,\n                seed=seed,\n                strength=image_denoise,\n                style_preset=style_preset,\n                cfg_scale=cfg_scale,\n                model=model,\n                mode=mode,\n            ),\n            files=files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.finish_reason != \"SUCCESS\":\n            raise Exception(f\"Stable Diffusion 3.5 Image generation failed: {response_api.finish_reason}.\")\n\n        image_data = base64.b64decode(response_api.image)\n        returned_image = bytesio_to_image_tensor(BytesIO(image_data))\n\n        return (returned_image,)\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftutorials%2Fbasic%2Fupscale",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/google/google-veo2-video",
  "markdown": "# Google Veo2 Video - ComfyUI 原生节点文档\n\n```\n\nclass VeoVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos from text prompts using Google's Veo API.\n\n    This node can create videos from text descriptions and optional image inputs,\n    with control over parameters like aspect ratio, duration, and more.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text description of the video\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"16:9\", \"9:16\"],\n                        \"default\": \"16:9\",\n                        \"tooltip\": \"Aspect ratio of the output video\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Negative text prompt to guide what to avoid in the video\",\n                    },\n                ),\n                \"duration_seconds\": (\n                    IO.INT,\n                    {\n                        \"default\": 5,\n                        \"min\": 5,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"Duration of the output video in seconds\",\n                    },\n                ),\n                \"enhance_prompt\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": True,\n                        \"tooltip\": \"Whether to enhance the prompt with AI assistance\",\n                    }\n                ),\n                \"person_generation\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"ALLOW\", \"BLOCK\"],\n                        \"default\": \"ALLOW\",\n                        \"tooltip\": \"Whether to allow generating people in the video\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFF,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation (0 for random)\",\n                    },\n                ),\n                \"image\": (IO.IMAGE, {\n                    \"default\": None,\n                    \"tooltip\": \"Optional reference image to guide video generation\",\n                }),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (IO.VIDEO,)\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Veo\"\n    DESCRIPTION = \"Generates videos from text prompts using Google's Veo API\"\n    API_NODE = True\n\n    def generate_video(\n        self,\n        prompt,\n        aspect_ratio=\"16:9\",\n        negative_prompt=\"\",\n        duration_seconds=5,\n        enhance_prompt=True,\n        person_generation=\"ALLOW\",\n        seed=0,\n        image=None,\n        auth_token=None,\n    ):\n        # Prepare the instances for the request\n        instances = []\n\n        instance = {\n            \"prompt\": prompt\n        }\n\n        # Add image if provided\n        if image is not None:\n            image_base64 = convert_image_to_base64(image)\n            if image_base64:\n                instance[\"image\"] = {\n                    \"bytesBase64Encoded\": image_base64,\n                    \"mimeType\": \"image/png\"\n                }\n\n        instances.append(instance)\n\n        # Create parameters dictionary\n        parameters = {\n            \"aspectRatio\": aspect_ratio,\n            \"personGeneration\": person_generation,\n            \"durationSeconds\": duration_seconds,\n            \"enhancePrompt\": enhance_prompt,\n        }\n\n        # Add optional parameters if provided\n        if negative_prompt:\n            parameters[\"negativePrompt\"] = negative_prompt\n        if seed > 0:\n            parameters[\"seed\"] = seed\n\n        # Initial request to start video generation\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/veo/generate\",\n                method=HttpMethod.POST,\n                request_model=Veo2GenVidRequest,\n                response_model=Veo2GenVidResponse\n            ),\n            request=Veo2GenVidRequest(\n                instances=instances,\n                parameters=parameters\n            ),\n            auth_token=auth_token\n        )\n\n        initial_response = initial_operation.execute()\n        operation_name = initial_response.name\n\n        logging.info(f\"Veo generation started with operation name: {operation_name}\")\n\n        # Define status extractor function\n        def status_extractor(response):\n            # Only return \"completed\" if the operation is done, regardless of success or failure\n            # We'll check for errors after polling completes\n            return \"completed\" if response.done else \"pending\"\n\n        # Define progress extractor function\n        def progress_extractor(response):\n            # Could be enhanced if the API provides progress information\n            return None\n\n        # Define the polling operation\n        poll_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=\"/proxy/veo/poll\",\n                method=HttpMethod.POST,\n                request_model=Veo2GenVidPollRequest,\n                response_model=Veo2GenVidPollResponse\n            ),\n            completed_statuses=[\"completed\"],\n            failed_statuses=[],  # No failed statuses, we'll handle errors after polling\n            status_extractor=status_extractor,\n            progress_extractor=progress_extractor,\n            request=Veo2GenVidPollRequest(\n                operationName=operation_name\n            ),\n            auth_token=auth_token,\n            poll_interval=5.0\n        )\n\n        # Execute the polling operation\n        poll_response = poll_operation.execute()\n\n        # Now check for errors in the final response\n        # Check for error in poll response\n        if hasattr(poll_response, 'error') and poll_response.error:\n            error_message = f\"Veo API error: {poll_response.error.message} (code: {poll_response.error.code})\"\n            logging.error(error_message)\n            raise Exception(error_message)\n\n        # Check for RAI filtered content\n        if (hasattr(poll_response.response, 'raiMediaFilteredCount') and\n            poll_response.response.raiMediaFilteredCount > 0):\n\n            # Extract reason message if available\n            if (hasattr(poll_response.response, 'raiMediaFilteredReasons') and\n                poll_response.response.raiMediaFilteredReasons):\n                reason = poll_response.response.raiMediaFilteredReasons[0]\n                error_message = f\"Content filtered by Google's Responsible AI practices: {reason} ({poll_response.response.raiMediaFilteredCount} videos filtered.)\"\n            else:\n                error_message = f\"Content filtered by Google's Responsible AI practices ({poll_response.response.raiMediaFilteredCount} videos filtered.)\"\n\n            logging.error(error_message)\n            raise Exception(error_message)\n\n        # Extract video data\n        video_data = None\n        if poll_response.response and hasattr(poll_response.response, 'videos') and poll_response.response.videos and len(poll_response.response.videos) > 0:\n            video = poll_response.response.videos[0]\n\n            # Check if video is provided as base64 or URL\n            if hasattr(video, 'bytesBase64Encoded') and video.bytesBase64Encoded:\n                # Decode base64 string to bytes\n                video_data = base64.b64decode(video.bytesBase64Encoded)\n            elif hasattr(video, 'gcsUri') and video.gcsUri:\n                # Download from URL\n                video_url = video.gcsUri\n                video_response = requests.get(video_url)\n                video_data = video_response.content\n            else:\n                raise Exception(\"Video returned but no data or URL was provided\")\n        else:\n            raise Exception(\"Video generation completed but no video was returned\")\n\n        if not video_data:\n            raise Exception(\"No video data was returned\")\n\n        logging.info(\"Video generation completed successfully\")\n\n        # Convert video data to BytesIO object\n        video_io = io.BytesIO(video_data)\n\n        # Return VideoFromFile object\n        return (VideoFromFile(video_io),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v",
  "markdown": "# Kling Text to Video (Camera Control) - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Kling Text to Video (Camera Control) 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg) Kling Text to Video (Camera Control) 节点允许用户将文本转换为具有专业摄像机动作的视频。该节点是标准 Kling Text to Video 节点的扩展版本，增加了摄像机控制功能。\n\n```\n\nclass KlingCameraControlT2VNode(KlingTextToVideoNode):\n    \"\"\"\n    Kling Text to Video Camera Control Node. This node is a text to video node, but it supports controlling the camera.\n    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingText2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingText2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"camera_control\": (\n                    \"CAMERA_CONTROL\",\n                    {\n                        \"tooltip\": \"Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Transform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text.\"\n\n    def api_call(\n        self,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        camera_control: Optional[CameraControl] = None,\n        auth_token: Optional[str] = None,\n    ):\n        return super().api_call(\n            model_name=\"kling-v1-5\",\n            cfg_scale=cfg_scale,\n            mode=\"pro\",\n            aspect_ratio=aspect_ratio,\n            duration=\"5\",\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            camera_control=camera_control,\n            auth_token=auth_token,\n        )\n\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video",
  "markdown": "# Kling Image to Video - ComfyUI 原生节点文档\n\n```\n\nclass KlingImage2VideoNode(KlingNodeBase):\n    \"\"\"Kling Image to Video Node\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"model_name\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"model_name\",\n                    enum_type=KlingVideoGenModelName,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"mode\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"mode\",\n                    enum_type=KlingVideoGenMode,\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=KlingVideoGenAspectRatio,\n                ),\n                \"duration\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"duration\",\n                    enum_type=KlingVideoGenDuration,\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (\"VIDEO\", \"STRING\", \"STRING\")\n    RETURN_NAMES = (\"VIDEO\", \"video_id\", \"duration\")\n    DESCRIPTION = \"Kling Image to Video Node\"\n\n    def get_response(self, task_id: str, auth_token: str) -> KlingImage2VideoResponse:\n        return poll_until_finished(\n            auth_token,\n            ApiEndpoint(\n                path=f\"{PATH_IMAGE_TO_VIDEO}/{task_id}\",\n                method=HttpMethod.GET,\n                request_model=KlingImage2VideoRequest,\n                response_model=KlingImage2VideoResponse,\n            ),\n        )\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        model_name: str,\n        cfg_scale: float,\n        mode: str,\n        aspect_ratio: str,\n        duration: str,\n        camera_control: Optional[KlingCameraControl] = None,\n        end_frame: Optional[torch.Tensor] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_I2V)\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_IMAGE_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=KlingImage2VideoRequest,\n                response_model=KlingImage2VideoResponse,\n            ),\n            request=KlingImage2VideoRequest(\n                model_name=KlingVideoGenModelName(model_name),\n                image=tensor_to_base64_string(start_frame),\n                image_tail=(\n                    tensor_to_base64_string(end_frame)\n                    if end_frame is not None\n                    else None\n                ),\n                prompt=prompt,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                cfg_scale=cfg_scale,\n                mode=KlingVideoGenMode(mode),\n                aspect_ratio=KlingVideoGenAspectRatio(aspect_ratio),\n                duration=KlingVideoGenDuration(duration),\n                camera_control=camera_control,\n            ),\n            auth_token=auth_token,\n        )\n\n        task_creation_response = initial_operation.execute()\n        validate_task_creation_response(task_creation_response)\n        task_id = task_creation_response.data.task_id\n\n        final_response = self.get_response(task_id, auth_token)\n        validate_video_result_response(final_response)\n\n        video = get_video_from_response(final_response)\n        return video_result_to_node_output(video)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video",
  "markdown": "# Kling Text to Video - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Kling Text to Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg) Kling Text to Video 节点通过连接Kling的API服务，实现文本到视频的生成功能。用户只需提供描述性文本，即可创建对应的视频内容。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 描述要生成视频内容的文本提示词 |\n| negative\\_prompt | 字符串 | \"\"  | 指定不希望在视频中出现的元素 |\n| cfg\\_scale | 浮点数 | 7.0 | 配置缩放值，控制对提示词的遵循程度 |\n| model\\_name | 选择项 | ”kling-v2-master” | 使用的视频生成模型 |\n| aspect\\_ratio | 选择项 | AspectRatio枚举值 | 输出视频的宽高比 |\n| duration | 选择项 | Duration枚举值 | 生成视频的持续时间 |\n| mode | 选择项 | Mode枚举值 | 视频生成模式 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n| Kling ID | 字符串 | 任务识别ID |\n| Duration (sec) | 字符串 | 视频时长（秒） |\n\n## 工作原理\n\n节点将文本提示词发送到Kling的API服务器，系统处理后返回生成的视频结果。处理过程包括初始请求和任务状态轮询，当任务完成后，节点会下载视频并输出结果。 用户可以通过调整各种参数来控制生成效果，包括负面提示词、配置缩放值以及视频属性等。系统会验证提示词长度，确保请求符合API要求。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass KlingTextToVideoNode(KlingNodeBase):\n    \"\"\"Kling Text to Video Node\"\"\"\n\n    @staticmethod\n    def poll_for_task_status(task_id: str, auth_token: str) -> KlingText2VideoResponse:\n        \"\"\"Polls the Kling API endpoint until the task reaches a terminal state.\"\"\"\n        polling_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"{PATH_TEXT_TO_VIDEO}/{task_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=KlingText2VideoResponse,\n            ),\n            completed_statuses=[\n                TaskStatus.succeed.value,\n            ],\n            failed_statuses=[TaskStatus.failed.value],\n            status_extractor=lambda response: (\n                response.data.task_status.value\n                if response.data and response.data.task_status\n                else None\n            ),\n            auth_token=auth_token,\n        )\n        return polling_operation.execute()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"negative_prompt\", multiline=True\n                ),\n                \"model_name\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"model_name\",\n                    enum_type=ModelName,\n                    default=\"kling-v2-master\",\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingText2VideoRequest, \"cfg_scale\"\n                ),\n                \"mode\": model_field_to_node_input(\n                    IO.COMBO, KlingText2VideoRequest, \"mode\", enum_type=Mode\n                ),\n                \"duration\": model_field_to_node_input(\n                    IO.COMBO, KlingText2VideoRequest, \"duration\", enum_type=Duration\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (\"VIDEO\", \"STRING\", \"STRING\")\n    RETURN_NAMES = (\"VIDEO\", \"Kling ID\", \"Duration (sec)\")\n    DESCRIPTION = \"Kling Text to Video Node\"\n\n    def api_call(\n        self,\n        prompt: str,\n        negative_prompt: str,\n        model_name: str,\n        cfg_scale: float,\n        mode: str,\n        duration: int,\n        aspect_ratio: str,\n        camera_control: Optional[CameraControl] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile, str, str]:\n        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_T2V)\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_TEXT_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=KlingText2VideoRequest,\n                response_model=KlingText2VideoResponse,\n            ),\n            request=KlingText2VideoRequest(\n                prompt=prompt if prompt else None,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                duration=Duration(duration),\n                mode=Mode(mode),\n                model_name=ModelName(model_name),\n                cfg_scale=cfg_scale,\n                aspect_ratio=AspectRatio(aspect_ratio),\n                camera_control=camera_control,\n            ),\n            auth_token=auth_token,\n        )\n\n        initial_response = initial_operation.execute()\n        if not is_valid_initial_response(initial_response):\n            error_msg = f\"Kling initial request failed. Code: {initial_response.code}, Message: {initial_response.message}, Data: {initial_response.data}\"\n            logging.error(error_msg)\n            raise KlingApiError(error_msg)\n\n        task_id = initial_response.data.task_id\n        final_response = self.poll_for_task_status(task_id, auth_token)\n        if not is_valid_video_response(final_response):\n            error_msg = (\n                f\"Kling task {task_id} succeeded but no video data found in response.\"\n            )\n            logging.error(error_msg)\n            raise KlingApiError(error_msg)\n\n        video = final_response.data.task_result.videos[0]\n        logging.debug(\"Kling task %s succeeded. Video URL: %s\", task_id, video.url)\n        return (\n            download_url_to_video_output(video.url),\n            str(video.id),\n            str(video.duration),\n        )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-concepts",
  "markdown": "# Luma Concepts - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Luma Concepts 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg) Luma Concepts 节点允许你将预定义的镜头概念应用到Luma生成过程中，提供更精确的镜头和视角控制，无需复杂的提示词描述。\n\n## 节点功能\n\n此节点作为Luma生成节点的辅助工具，让用户能够选择和应用预定义的镜头概念，这些概念包括不同的拍摄角度(如俯视、仰视)、镜头距离(如特写、远景)、运动方式(如推进、跟随)等摄影参数。它简化了创作工作流程，提供了一种直观的方式来控制生成结果的镜头效果。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| concept1 | 选择项 | 第一个镜头概念选择，包含多种预设镜头选项和”none” |\n| concept2 | 选择项 | 第二个镜头概念选择，包含多种预设镜头选项和”none” |\n| concept3 | 选择项 | 第三个镜头概念选择，包含多种预设镜头选项和”none” |\n| concept4 | 选择项 | 第四个镜头概念选择，包含多种预设镜头选项和”none” |\n\n### 可选参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPTS | 可选的额外Camera Concepts，会与此处选择的镜头概念合并 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPT | 包含所有选定镜头概念的组合对象 |\n\n## 使用示例\n\n[\n\n## Luma Text to video 工作流示例\n\nLuma Text to Video 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-video)[\n\n## Luma Text to Image 工作流示例\n\nLuma Image to Video 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-image-to-video)\n\n## 工作原理\n\nLuma Concepts 节点提供了丰富的预定义镜头概念供选择，包括但不限于:\n\n*   不同的拍摄距离(如特写、中景、远景)\n*   视角高度(如地平、俯视、仰视)\n*   运动方式(如推进、跟随、环绕)\n*   特殊效果(如手持、稳定、漂浮)\n\n用户可以从这些选项中选择最多4个概念组合使用。节点会创建一个包含所选镜头概念的对象，该对象随后传递给Luma生成节点。在生成过程中，Luma AI会根据这些镜头概念来影响生成结果的视角和构图，确保输出图像体现所选的摄影效果。 通过组合多个镜头概念，用户可以创建复杂的镜头指导，而无需撰写详细的提示词描述。这对于需要特定摄影视角或构图的场景特别有用。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass LumaConceptsNode(ComfyNodeABC):\n    \"\"\"\n    Holds one or more Camera Concepts for use with Luma Text to Video and Luma Image to Video nodes.\n    \"\"\"\n\n    RETURN_TYPES = (LumaIO.LUMA_CONCEPTS,)\n    RETURN_NAMES = (\"luma_concepts\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_concepts\"\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"concept1\": (get_luma_concepts(include_none=True),),\n                \"concept2\": (get_luma_concepts(include_none=True),),\n                \"concept3\": (get_luma_concepts(include_none=True),),\n                \"concept4\": (get_luma_concepts(include_none=True),),\n            },\n            \"optional\": {\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to add to the ones chosen here.\"\n                    },\n                ),\n            },\n        }\n\n    def create_concepts(\n        self,\n        concept1: str,\n        concept2: str,\n        concept3: str,\n        concept4: str,\n        luma_concepts: LumaConceptChain = None,\n    ):\n        chain = LumaConceptChain(str_list=[concept1, concept2, concept3, concept4])\n        if luma_concepts is not None:\n            chain = luma_concepts.clone_and_merge(chain)\n        return (chain,)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-image-to-video",
  "markdown": "# Luma Image to Video - ComfyUI 原生API节点文档\n\n![ComfyUI 原生 Luma Image to Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg) Luma Image to Video 节点允许你使用Luma AI的先进技术，将静态图像转换为流畅、动态的视频内容，为图像赋予生命力和动态特性。\n\n## 节点功能\n\n此节点连接到Luma AI的图像到视频API，让用户能够基于输入图像创建动态视频。它可以理解图像中的内容并生成自然、连贯的动作，同时保持原始图像的视觉风格和特性。结合文本提示词，用户可以精确控制生成视频的动态效果。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 描述视频动作和内容的文本提示词 |\n| model | 选择项 | \\-  | 使用的视频生成模型 |\n| resolution | 选择项 | ”540p” | 输出视频分辨率 |\n| duration | 选择项 | \\-  | 视频时长选项 |\n| loop | 布尔值 | False | 是否循环播放视频 |\n| seed | 整数  | 0   | 种子值，用于确定节点是否应重新运行，但实际结果与种子无关 |\n\n### 可选参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| first\\_image | 图像  | 视频的第一帧图像（与last\\_image至少需要提供一个） |\n| last\\_image | 图像  | 视频的最后一帧图像（与first\\_image至少需要提供一个） |\n| luma\\_concepts | LUMA\\_CONCEPTS | 用于控制相机运动和镜头效果的概念引导 |\n\n### 参数要求\n\n*   **first\\_image** 和 **last\\_image** 至少需要提供其中一个\n*   每个图像输入（first\\_image和last\\_image）最多只接受1张图片\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n\n## 使用示例\n\n[\n\nLuma Image to Video 工作流教程\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-image-to-video)\n\n## 工作原理\n\nLuma Image to Video 节点分析输入图像的内容和结构，然后结合文本提示词来确定如何为图像添加动态效果。它使用Luma AI的生成模型理解图像中的对象、人物或场景，并创建合理、连贯的动作序列。 用户可以通过提示词描述期望的动作类型、方向和强度，节点将据此生成相应的视频效果。通过设置不同的参数，如分辨率和时长，用户可以进一步定制输出视频的特性。 此外，通过提供起始帧和最后帧参考图像，用户可以指定视频的起始和结束状态，使动作朝特定方向发展。概念引导功能则允许用户进一步控制视频的整体风格、相机运动和美学效果。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass LumaImageToVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt, input images, and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaVideoModel],),\n                # \"aspect_ratio\": ([ratio.value for ratio in LumaAspectRatio], {\n                #     \"default\": LumaAspectRatio.ratio_16_9,\n                # }),\n                \"resolution\": (\n                    [resolution.value for resolution in LumaVideoOutputResolution],\n                    {\n                        \"default\": LumaVideoOutputResolution.res_540p,\n                    },\n                ),\n                \"duration\": ([dur.value for dur in LumaVideoModelOutputDuration],),\n                \"loop\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"first_image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"First frame of generated video.\"},\n                ),\n                \"last_image\": (IO.IMAGE, {\"tooltip\": \"Last frame of generated video.\"}),\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        resolution: str,\n        duration: str,\n        loop: bool,\n        seed,\n        first_image: torch.Tensor = None,\n        last_image: torch.Tensor = None,\n        luma_concepts: LumaConceptChain = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        if first_image is None and last_image is None:\n            raise Exception(\n                \"At least one of first_image and last_image requires an input.\"\n            )\n        keyframes = self._convert_to_keyframes(first_image, last_image, auth_token)\n        duration = duration if model != LumaVideoModel.ray_1_6 else None\n        resolution = resolution if model != LumaVideoModel.ray_1_6 else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations\",\n                method=HttpMethod.POST,\n                request_model=LumaGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaGenerationRequest(\n                prompt=prompt,\n                model=model,\n                aspect_ratio=LumaAspectRatio.ratio_16_9,  # ignored, but still needed by the API for some reason\n                resolution=resolution,\n                duration=duration,\n                loop=loop,\n                keyframes=keyframes,\n                concepts=luma_concepts.create_api_model() if luma_concepts else None,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.assets.video)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n    def _convert_to_keyframes(\n        self,\n        first_image: torch.Tensor = None,\n        last_image: torch.Tensor = None,\n        auth_token=None,\n    ):\n        if first_image is None and last_image is None:\n            return None\n        frame0 = None\n        frame1 = None\n        if first_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                first_image, max_images=1, auth_token=auth_token\n            )\n            frame0 = LumaImageReference(type=\"image\", url=download_urls[0])\n        if last_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                last_image, max_images=1, auth_token=auth_token\n            )\n            frame1 = LumaImageReference(type=\"image\", url=download_urls[0])\n        return LumaKeyframes(frame0=frame0, frame1=frame1)\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra",
  "markdown": "# Stability AI Stable Image Ultra API Node ComfyUI Official Example\n\nThe [Stability Stable Image Ultra](https://docs.comfy.org/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg) node allows you to use Stability AI’s Stable Image Ultra model to create high-quality, detailed image content through text prompts or reference images. In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.\n\n## Stability AI Stable Image Ultra Text-to-Image Workflow\n\n### 1\\. Workflow File Download\n\nThe workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow. ![Stability AI Stable Image Ultra Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_image_ultra_t2i.png)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![Stability AI Stable Image Ultra Text-to-Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg) You can follow the numbered steps in the image to complete the basic text-to-image workflow:\n\n1.  (Optional) Modify the `prompt` parameter in the `Stability AI Stable Image Ultra` node to input your desired image description. More detailed prompts often lead to better image quality. You can use the `(word:weight)` format to control specific word weights, for example: `The sky was crisp (blue:0.3) and (green:0.8)` indicates the sky is blue and green, but green is more prominent.\n2.  (Optional) Select the `style_preset` parameter to control the visual style of the image. Different preset styles will produce images with different stylistic characteristics, such as “cinematic”, “anime”, etc. Selecting “None” will not apply any specific style.\n3.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.\n4.  After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.\n\n### 3\\. Additional Notes\n\n*   **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions will lead to better results. It can include elements like scene, subject, colors, lighting, and style.\n*   **Style Preset**: Provides multiple preset styles such as cinematic, anime, digital art, etc., which can quickly define the overall style of the image.\n*   **Negative Prompt**: Used to specify elements you don’t want to appear in the generated image, helping avoid common issues like extra limbs or distorted faces.\n*   **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during the creative process.\n*   Currently, the `Load Image` node is in “Bypass” mode. To enable it, refer to the step guide and right-click on the corresponding node to set “Mode” to “Always” to enable input, which will switch to image-to-image mode.\n\n### 1\\. Workflow File Download\n\nThe workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow. ![Stability Stable Image Ultra Image-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/stable_image_ultra_i2i.png) Download the image below which we will use as input ![Stability Stable Image Ultra Image-to-Image Workflow Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/input.png) \n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![Stability Stable Image Ultra Image-to-Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg) You can follow the numbered steps in the image to complete the image-to-image workflow:\n\n1.  Load a reference image through the `Load Image` node, which will serve as the basis for generation.\n2.  (Optional) Modify the `prompt` parameter in the `Stability Stable Image Ultra` node to describe elements you want to change or enhance in the reference image.\n3.  (Optional) Adjust the `image_denoise` parameter (range 0.0-1.0) to control the degree of modification to the original image:\n    *   Values closer to 0.0 will make the generated image more similar to the input reference image\n    *   Values closer to 1.0 will make the generated image more like pure text-to-image generation\n4.  (Optional) You can also set `style_preset` and other parameters to further control the generation effect.\n5.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.\n6.  After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.\n\n### 3\\. Additional Notes\n\n**Image Denoise**: This parameter determines how much of the original image’s features are preserved during generation, and is the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths ![Stability Stable Image Ultra Image-to-Image Denoise Explanation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg)\n\n*   **Reference Image Selection**: Choosing images with clear subjects and good composition usually leads to better results.\n*   **Prompt Tips**: In image-to-image mode, prompts should focus more on what you want to change or enhance, rather than describing all elements already present in the image.\n\nYou can refer to the documentation below for detailed parameter settings and more information about the corresponding nodes\n\n[\n\n## Stability Stable Image Ultra Node Documentation\n\nStability Stable Image Ultra API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)"
},
{
  "url": "https://docs.comfy.org/tutorials/video/cosmos/cosmos-predict2-video2world",
  "markdown": "# Cosmos Predict2 Video2World ComfyUI Official Example\n\nCosmos-Predict2 is NVIDIA’s next-generation physical world foundation model, specifically designed for high-quality visual generation and prediction tasks in physical AI scenarios. The model features exceptional physical accuracy, environmental interactivity, and detail reproduction capabilities, enabling realistic simulation of complex physical phenomena and dynamic scenes. Cosmos-Predict2 supports various generation methods including Text-to-Image (Text2Image) and Video-to-World (Video2World), and is widely used in industrial simulation, autonomous driving, urban planning, scientific research, and other fields. It serves as a crucial foundational tool for promoting deep integration of intelligent vision and the physical world. GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2) huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959) This guide will walk you through completing **Video2World** generation in ComfyUI. For the text-to-image section, please refer to the following part:\n\n[\n\n## Cosmos Predict2 Text to Image\n\nUsing Cosmos-Predict2 for text-to-image generation\n\n\n\n](https://docs.comfy.org/tutorials/image/cosmos/cosmos-predict2-t2i)\n\nWhen testing the 2B version, it takes around 16GB VRAM.\n\n### 1\\. Workflow File\n\nPlease download the video below and drag it into ComfyUI to load the workflow. The workflow already has embedded model download links.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/cosmos/predict2/cosmos_predict2_2B_video2world_480p_16fps.json)\n\nPlease download the following image as input: ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/cosmos/predict2/input.png)\n\n### 2\\. Manual Model Installation\n\nIf the model download wasn’t successful, you can try to download them manually by yourself in this section. **Diffusion model**\n\n*   [cosmos\\_predict2\\_2B\\_video2world\\_480p\\_16fps.safetensors](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged/resolve/main/cosmos_predict2_2B_video2world_480p_16fps.safetensors)\n\nFor other weights, please visit [Cosmos\\_Predict2\\_repackaged](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged) to download **Text encoder** [oldt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/resolve/main/text_encoders/oldt5_xxl_fp8_e4m3fn_scaled.safetensors) **VAE** [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors) File Storage Location\n\n```\n📂 ComfyUI/\n├──📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └─── cosmos_predict2_2B_video2world_480p_16fps.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── oldt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── 📂 vae/\n│       └──  wan_2.1_vae.safetensors\n```\n\n### 3\\. Complete Workflow Step by Step\n\n![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/cosmos/cosmos_predict2_2B_video2world_480p_16fps_step_guide.jpg) Please follow the steps in the image to run the workflow:\n\n1.  Ensure the `Load Diffusion Model` node has loaded `cosmos_predict2_2B_video2world_480p_16fps.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `oldt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Upload the provided input image in the `Load Image` node\n5.  (Optional) If you need first and last frame control, use the shortcut `Ctrl(cmd) + B` to enable last frame input\n6.  (Optional) You can modify the prompts in the `ClipTextEncode` node\n7.  (Optional) Modify the size and frame count in the `CosmosPredict2ImageToVideoLatent` node\n8.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n9.  Once generation is complete, the video will automatically save to the `ComfyUI/output/` directory, you can also preview it in the `save video` node"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pika/pika-image-to-video",
  "markdown": "# Pika 2.2 Image to Video - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Pika 2.2 Image to Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg) Pika 2.2 Image to Video 节点通过连接Pika最新的2.2版本API，将静态图像转变为动态视频。该节点能够保留原始图像的视觉特征，同时根据文本提示词添加自然的动态效果。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 要转换为视频的输入图像 |\n| prompt\\_text | 字符串 | \"\"  | 描述视频动作和内容的文本提示词 |\n| negative\\_prompt | 字符串 | \"\"  | 指定不希望在视频中出现的元素 |\n| seed | 整数  | 0   | 生成过程的随机种子 |\n| resolution | 选择项 | ”1080p” | 生成视频的分辨率 |\n| duration | 选择项 | ”5s” | 生成视频的持续时间 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n\n## 工作流程\n\n节点将输入图像和相关参数（提示词、分辨率、持续时间等）通过多部分表单数据发送到Pika的API服务器。API处理后返回生成的视频结果。用户可以通过调整提示词、负面提示词、随机种子等参数来控制生成效果。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-05)\\]\n\n```\n\nclass PikaImageToVideoV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Image to Video Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"The image to convert to video\"},\n                ),\n                **cls.get_base_inputs_types(PikaBodyGenerate22I2vGenerate22I2vPost),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    DESCRIPTION = \"Sends an image and prompt to the Pika API v2.2 to generate a video.\"\n    RETURN_TYPES = (\"VIDEO\",)\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika 2.2 Image to Video.\"\"\"\n        # Convert image to BytesIO\n        image_bytes_io = tensor_to_bytesio(image)\n        image_bytes_io.seek(0)  # Reset stream position\n\n        # Prepare file data for multipart upload\n        pika_files = {\"image\": (\"image.png\", image_bytes_io, \"image/png\")}\n\n        # Prepare non-file data using the Pydantic model\n        pika_request_data = PikaBodyGenerate22I2vGenerate22I2vPost(\n            promptText=prompt_text,\n            negativePrompt=negative_prompt,\n            seed=seed,\n            resolution=resolution,\n            duration=duration,\n        )\n\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_IMAGE_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22I2vGenerate22I2vPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=pika_request_data,\n            files=pika_files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftutorials%2Fapi-nodes%2Fluma%2Fluma-text-to-video",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls",
  "markdown": "# Kling Camera Controls - ComfyUI 原生节点文档\n\n```\n\nclass KlingCameraControls(KlingNodeBase):\n    \"\"\"Kling Camera Controls Node\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"camera_control_type\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\n                            camera_control_type.value\n                            for camera_control_type in CameraType\n                        ],\n                        \"default\": \"simple\",\n                        \"tooltip\": \"Predefined camera movements type. simple: Customizable camera movement. down_back: Camera descends and moves backward. forward_up: Camera moves forward and tilts up. right_turn_forward: Rotate right and move forward. left_turn_forward: Rotate left and move forward.\",\n                    },\n                ),\n                \"horizontal_movement\": get_camera_control_input_config(\n                    \"Controls camera's movement along horizontal axis (x-axis). Negative indicates left, positive indicates right\"\n                ),\n                \"vertical_movement\": get_camera_control_input_config(\n                    \"Controls camera's movement along vertical axis (y-axis). Negative indicates downward, positive indicates upward.\"\n                ),\n                \"pan\": get_camera_control_input_config(\n                    \"Controls camera's rotation in vertical plane (x-axis). Negative indicates downward rotation, positive indicates upward rotation.\",\n                    default=0.5,\n                ),\n                \"tilt\": get_camera_control_input_config(\n                    \"Controls camera's rotation in horizontal plane (y-axis). Negative indicates left rotation, positive indicates right rotation.\",\n                ),\n                \"roll\": get_camera_control_input_config(\n                    \"Controls camera's rolling amount (z-axis). Negative indicates counterclockwise, positive indicates clockwise.\",\n                ),\n                \"zoom\": get_camera_control_input_config(\n                    \"Controls change in camera's focal length. Negative indicates narrower field of view, positive indicates wider field of view.\",\n                ),\n            }\n        }\n\n    DESCRIPTION = \"Kling Camera Controls Node. Not all model and mode combinations support camera control. Please refer to the Kling API documentation for more information.\"\n    RETURN_TYPES = (\"CAMERA_CONTROL\",)\n    RETURN_NAMES = (\"camera_control\",)\n    FUNCTION = \"main\"\n\n    @classmethod\n    def VALIDATE_INPUTS(\n        cls,\n        horizontal_movement: float,\n        vertical_movement: float,\n        pan: float,\n        tilt: float,\n        roll: float,\n        zoom: float,\n    ) -> bool | str:\n        if not is_valid_camera_control_configs(\n            [\n                horizontal_movement,\n                vertical_movement,\n                pan,\n                tilt,\n                roll,\n                zoom,\n            ]\n        ):\n            return \"Invalid camera control configs: at least one of the values must be non-zero\"\n        return True\n\n    def main(\n        self,\n        camera_control_type: str,\n        horizontal_movement: float,\n        vertical_movement: float,\n        pan: float,\n        tilt: float,\n        roll: float,\n        zoom: float,\n    ) -> tuple[CameraControl]:\n        return (\n            CameraControl(\n                type=CameraType(camera_control_type),\n                config=CameraConfig(\n                    horizontal=horizontal_movement,\n                    vertical=vertical_movement,\n                    pan=pan,\n                    roll=roll,\n                    tilt=tilt,\n                    zoom=zoom,\n                ),\n            ),\n        )\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pika/pika-scenes",
  "markdown": "# Pika 2.2 Scenes - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Pika 2.2 Scenes 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg) Pika 2.2 Scenes 节点允许你上传多张图像作为素材，并生成一个包含这些素材对象的高质量视频。节点利用Pika的2.2版本API，基于这些图像创建连贯的场景过渡视频。\n\nPika 2.2 Scenes 节点分析所有输入图像，然后创建一个包含这些图像元素的视频。节点将图像和参数发送到Pika的API服务器，处理完成后返回生成的视频结果。 用户可以通过提示词引导视频的风格和内容，通过负面提示词排除不需要的元素。节点支持上传最多5张图像作为素材，并会根据指定的组合模式、分辨率、持续时间和宽高比生成最终视频。\n\n```\n\nclass PikaScenesV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Scenes Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        image_ingredient_input = (\n            IO.IMAGE,\n            {\"tooltip\": \"Image that will be used as ingredient to create a video.\"},\n        )\n        return {\n            \"required\": {\n                **cls.get_base_inputs_types(\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                ),\n                \"ingredients_mode\": model_field_to_node_input(\n                    IO.COMBO,\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                    \"ingredientsMode\",\n                    enum_type=IngredientsMode,\n                    default=\"creative\",\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.FLOAT,\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                    \"aspectRatio\",\n                    step=0.001,\n                    min=0.4,\n                    max=2.5,\n                    default=1.7777777777777777,\n                ),\n            },\n            \"optional\": {\n                \"image_ingredient_1\": image_ingredient_input,\n                \"image_ingredient_2\": image_ingredient_input,\n                \"image_ingredient_3\": image_ingredient_input,\n                \"image_ingredient_4\": image_ingredient_input,\n                \"image_ingredient_5\": image_ingredient_input,\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    DESCRIPTION = \"Combine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them.\"\n    RETURN_TYPES = (\"VIDEO\",)\n\n    def api_call(\n        self,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        ingredients_mode: str,\n        aspect_ratio: float,\n        image_ingredient_1: Optional[torch.Tensor] = None,\n        image_ingredient_2: Optional[torch.Tensor] = None,\n        image_ingredient_3: Optional[torch.Tensor] = None,\n        image_ingredient_4: Optional[torch.Tensor] = None,\n        image_ingredient_5: Optional[torch.Tensor] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika Scenes 2.2.\"\"\"\n        all_image_bytes_io = []\n        for image in [\n            image_ingredient_1,\n            image_ingredient_2,\n            image_ingredient_3,\n            image_ingredient_4,\n            image_ingredient_5,\n        ]:\n            if image is not None:\n                image_bytes_io = tensor_to_bytesio(image)\n                image_bytes_io.seek(0)\n                all_image_bytes_io.append(image_bytes_io)\n\n        # Prepare files data for multipart upload\n        pika_files = [\n            (\"images\", (f\"image_{i}.png\", image_bytes_io, \"image/png\"))\n            for i, image_bytes_io in enumerate(all_image_bytes_io)\n        ]\n\n        # Prepare non-file data using the Pydantic model\n        pika_request_data = PikaBodyGenerate22C2vGenerate22PikascenesPost(\n            ingredientsMode=ingredients_mode,\n            promptText=prompt_text,\n            negativePrompt=negative_prompt,\n            seed=seed,\n            resolution=resolution,\n            duration=duration,\n            aspectRatio=aspect_ratio,\n        )\n\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_PIKASCENES,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=pika_request_data,\n            files=pika_files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video",
  "markdown": "# Kling Start-End Frame to Video - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Kling Start-End Frame to Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg) Kling Start-End Frame to Video 节点允许你提供开始和结束图像，生成在两者之间平滑过渡的视频序列。该节点会自动创建所有中间帧，产生流畅的变换效果。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| start\\_frame | 图像  | 视频的起始帧图像 |\n| end\\_frame | 图像  | 视频的结束帧图像 |\n| prompt | 字符串 | 描述视频内容和过渡效果的文本提示词 |\n| negative\\_prompt | 字符串 | 指定不希望在视频中出现的元素 |\n| cfg\\_scale | 浮点数 | 配置缩放值，控制对提示词的遵循程度 |\n| aspect\\_ratio | 选择项 | 输出视频的宽高比 |\n| mode | 选择项 | 视频生成配置，格式为”模式/时长/模型名称” |\n\n### Mode 选项\n\n节点支持以下模式选项：\n\n*   standard mode / 5s duration / kling-v1\n*   standard mode / 5s duration / kling-v1-5\n*   pro mode / 5s duration / kling-v1\n*   pro mode / 5s duration / kling-v1-5\n*   pro mode / 5s duration / kling-v1-6\n*   pro mode / 10s duration / kling-v1-5\n*   pro mode / 10s duration / kling-v1-6\n\n默认值为 “pro mode / 5s duration / kling-v1”\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n\n## 工作原理\n\nKling Start-End Frame to Video 节点分析起始帧和结束帧图像，然后创建连接这两个状态的平滑过渡序列。节点将图像和参数发送到Kling的API服务器，后者生成所有必要的中间帧，创建流畅的变换效果。 可以通过提示词引导过渡效果的风格和内容，而负面提示词则帮助避免不需要的元素。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\n\nclass KlingStartEndFrameNode(KlingImage2VideoNode):\n    \"\"\"\n    Kling First Last Frame Node. This node allows creation of a video from a first and last frame. It calls the normal image to video endpoint, but only allows the subset of input options that support the `image_tail` request field.\n    \"\"\"\n\n    @staticmethod\n    def get_mode_string_mapping() -> dict[str, tuple[str, str, str]]:\n        \"\"\"\n        Returns a mapping of mode strings to their corresponding (mode, duration, model_name) tuples.\n        Only includes config combos that support the `image_tail` request field.\n        \"\"\"\n        return {\n            \"standard mode / 5s duration / kling-v1\": (\"std\", \"5\", \"kling-v1\"),\n            \"standard mode / 5s duration / kling-v1-5\": (\"std\", \"5\", \"kling-v1-5\"),\n            \"pro mode / 5s duration / kling-v1\": (\"pro\", \"5\", \"kling-v1\"),\n            \"pro mode / 5s duration / kling-v1-5\": (\"pro\", \"5\", \"kling-v1-5\"),\n            \"pro mode / 5s duration / kling-v1-6\": (\"pro\", \"5\", \"kling-v1-6\"),\n            \"pro mode / 10s duration / kling-v1-5\": (\"pro\", \"10\", \"kling-v1-5\"),\n            \"pro mode / 10s duration / kling-v1-6\": (\"pro\", \"10\", \"kling-v1-6\"),\n        }\n\n    @classmethod\n    def INPUT_TYPES(s):\n        modes = list(KlingStartEndFrameNode.get_mode_string_mapping().keys())\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"end_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image_tail\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"mode\": (\n                    modes,\n                    {\n                        \"default\": modes[2],\n                        \"tooltip\": \"The configuration to use for the video generation following the format: mode / duration / model_name.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Generate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last.\"\n\n    def parse_inputs_from_mode(self, mode: str) -> tuple[str, str, str]:\n        \"\"\"Parses the mode input into a tuple of (model_name, duration, mode).\"\"\"\n        return KlingStartEndFrameNode.get_mode_string_mapping()[mode]\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        end_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        mode: str,\n        auth_token: Optional[str] = None,\n    ):\n        mode, duration, model_name = self.parse_inputs_from_mode(mode)\n        return super().api_call(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model_name=model_name,\n            start_frame=start_frame,\n            cfg_scale=cfg_scale,\n            mode=mode,\n            aspect_ratio=aspect_ratio,\n            duration=duration,\n            end_frame=end_frame,\n            auth_token=auth_token,\n        )\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pixverse/pixverse-transition-video",
  "markdown": "# PixVerse Transition Video - ComfyUI 原生节点文档\n\n![ComfyUI 原生 PixVerse Transition Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg) PixVerse Transition Video 节点连接到 PixVerse 的过渡视频生成 API，允许你提供开始和结束图像，生成在两者之间平滑过渡的视频序列。节点会自动创建所有中间帧，产生流畅的变换效果，特别适合创建变形、场景转换和对象演变等视觉效果。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| first\\_frame | 图像  | \\-  | 视频的起始帧图像 |\n| last\\_frame | 图像  | \\-  | 视频的结束帧图像 |\n| prompt | 字符串 | \"\"  | 描述视频内容和过渡效果的文本提示词 |\n| quality | 选择项 | ”PixverseQuality.res\\_540p” | 生成视频的质量级别 |\n| duration\\_seconds | 选择项 | \\-  | 生成视频的持续时间 |\n| motion\\_mode | 选择项 | \\-  | 视频的动作模式 |\n| seed | 整数  | 0   | 生成过程的随机种子，范围0-2147483647 |\n\n### 可选参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| negative\\_prompt | 字符串 | \"\"  | 指定不希望在视频中出现的元素 |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | 可选的PixVerse模板配置，影响生成风格 |\n\n### 参数限制说明\n\n*   当quality设置为1080p时，动作模式(motion\\_mode)会强制设为normal，持续时间(duration\\_seconds)会强制设为5秒\n*   当持续时间(duration\\_seconds)不等于5秒时，动作模式(motion\\_mode)会强制设为normal\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n\n## 源码参考\n\n```\n\nclass PixverseTransitionVideoNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"first_frame\": (\n                    IO.IMAGE,\n                ),\n                \"last_frame\": (\n                    IO.IMAGE,\n                ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"quality\": (\n                    [resolution.value for resolution in PixverseQuality],\n                    {\n                        \"default\": PixverseQuality.res_540p,\n                    },\n                ),\n                \"duration_seconds\": ([dur.value for dur in PixverseDuration],),\n                \"motion_mode\": ([mode.value for mode in PixverseMotionMode],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"pixverse_template\": (\n                    PixverseIO.TEMPLATE,\n                    {\n                        \"tooltip\": \"An optional template to influence style of generation, created by the Pixverse Template node.\"\n                    }\n                )\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        first_frame: torch.Tensor,\n        last_frame: torch.Tensor,\n        prompt: str,\n        quality: str,\n        duration_seconds: int,\n        motion_mode: str,\n        seed,\n        negative_prompt: str=None,\n        pixverse_template: int=None,\n        auth_token=None,\n        **kwargs,\n    ):\n        first_frame_id = upload_image_to_pixverse(first_frame, auth_token=auth_token)\n        last_frame_id = upload_image_to_pixverse(last_frame, auth_token=auth_token)\n\n        # 1080p is limited to 5 seconds duration\n        # only normal motion_mode supported for 1080p or for non-5 second duration\n        if quality == PixverseQuality.res_1080p:\n            motion_mode = PixverseMotionMode.normal\n            duration_seconds = PixverseDuration.dur_5\n        elif duration_seconds != PixverseDuration.dur_5:\n            motion_mode = PixverseMotionMode.normal\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/pixverse/video/transition/generate\",\n                method=HttpMethod.POST,\n                request_model=PixverseTransitionVideoRequest,\n                response_model=PixverseVideoResponse,\n            ),\n            request=PixverseTransitionVideoRequest(\n                first_frame_img=first_frame_id,\n                last_frame_img=last_frame_id,\n                prompt=prompt,\n                quality=quality,\n                duration=duration_seconds,\n                motion_mode=motion_mode,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                template_id=pixverse_template,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.Resp is None:\n            raise Exception(f\"Pixverse request failed: '{response_api.ErrMsg}'\")\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/pixverse/video/result/{response_api.Resp.video_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=PixverseGenerationStatusResponse,\n            ),\n            completed_statuses=[PixverseStatus.successful],\n            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],\n            status_extractor=lambda x: x.Resp.status,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.Resp.url)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftutorials%2Fapi-nodes%2Fmoonvalley%2Fmoonvalley-video-generation",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/flux/flux-1-kontext-dev",
  "markdown": "# ComfyUI Flux Kontext Dev Native Workflow Example\n\n## About FLUX.1 Kontext Dev\n\nFLUX.1 Kontext is a breakthrough multimodal image editing model from Black Forest Labs that supports simultaneous text and image input, intelligently understanding image context and performing precise editing. Its development version is an open-source diffusion transformer model with 12 billion parameters, featuring excellent context understanding and character consistency maintenance, ensuring that key elements such as character features and composition layout remain stable even after multiple iterative edits. It shares the same core capabilities as the FLUX.1 Kontext suite:\n\n*   Character Consistency: Preserves unique elements in images across multiple scenes and environments, such as reference characters or objects in the image.\n*   Editing: Makes targeted modifications to specific elements in the image without affecting other parts.\n*   Style Reference: Generates novel scenes while preserving the unique style of the reference image according to text prompts.\n*   Interactive Speed: Minimal latency in image generation and editing.\n\nWhile the previously released API version offers the highest fidelity and speed, FLUX.1 Kontext \\[Dev\\] runs entirely on local machines, providing unparalleled flexibility for developers, researchers, and advanced users who wish to experiment.\n\n### Version Information\n\n*   **\\[FLUX.1 Kontext \\[pro\\]** - Commercial version, focused on rapid iterative editing\n*   **FLUX.1 Kontext \\[max\\]** - Experimental version with stronger prompt adherence\n*   **FLUX.1 Kontext \\[dev\\]** - Open source version (used in this tutorial), 12B parameters, mainly for research\n\nCurrently in ComfyUI, you can use all these versions, where [Pro and Max versions](https://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-kontext) can be called through API nodes, while the Dev open source version please refer to the instructions in this guide.\n\n## Workflow Description\n\nIn this tutorial, we cover two types of workflows, which are essentially the same:\n\n*   A workflow using the **FLUX.1 Kontext Image Edit** group node, making the interface and workflow reuse simpler\n*   Another workflow without using group nodes, showing the complete original workflow.\n\nThe main advantage of using group nodes is workflow conciseness - you can reuse group nodes to implement complex workflows and quickly reuse node groups. Additionally, in the new version of the frontend, we’ve added a quick group node addition feature for Flux.1 Kontext Dev: ![Quick Add Group Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/selcetion_toolbox_edit.jpg)\n\n## Model Download\n\nTo run the workflows in this guide successfully, you first need to download the following model files. You can also directly get the model download links from the corresponding workflows, which already contain the model file download information. **Diffusion Model**\n\n*   [flux1-dev-kontext\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/flux1-kontext-dev_ComfyUI/resolve/main/split_files/diffusion_models/flux1-dev-kontext_fp8_scaled.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/blob/main/split_files/vae/ae.safetensors)\n\n**Text Encoder**\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/clip_l.safetensors)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors) or [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors)\n\nModel save location\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └── flux1-dev-kontext_fp8_scaled.safetensors\n│   ├── 📂 vae/\n│   │   └── ae.safetensor\n│   └── 📂 text_encoders/\n│       ├── clip_l.safetensors\n│       └── t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn_scaled.safetensors\n```\n\nThis workflow is a normal workflow, but it uses the `Load Image(from output)` node to load the image to be edited, making it more convenient for you to access the edited image for multiple rounds of editing.\n\n### 1\\. Workflow and Input Image Download\n\nDownload the following files and drag them into ComfyUI to load the corresponding workflow ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/flux_1_kontext_dev_basic.png) **Input Image** ![ComfyUI Flux Kontext Native Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/rabbit.jpg)\n\n### 2\\. Complete the workflow step by step\n\n ![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg) You can refer to the numbers in the image to complete the workflow run:\n\n1.  In the `Load Diffusion Model` node, load the `flux1-dev-kontext_fp8_scaled.safetensors` model\n2.  In the `DualCLIP Load` node, ensure that `clip_l.safetensors` and `t5xxl_fp16.safetensors` or `t5xxl_fp8_e4m3fn_scaled.safetensors` are loaded\n3.  In the `Load VAE` node, ensure that `ae.safetensors` model is loaded\n4.  In the `Load Image(from output)` node, load the provided input image\n5.  In the `CLIP Text Encode` node, modify the prompts, only English is supported\n6.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Flux.1 Kontext Dev Grouped Workflow\n\nThis workflow uses the **FLUX.1 Kontext Image Edit** group node, making the interface and workflow reuse simpler. This example also uses two images as input, using the `Image Stitch` node to combine two images into one, and then using Flux.1 Kontext for editing.\n\n### 1\\. Workflow and Input Image Download\n\nDownload the following files and drag them into ComfyUI to load the corresponding workflow ![ComfyUI Flux Kontext Native Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/flux_1_kontext_dev_grouped.png) **Input Images** ![ComfyUI Flux Kontext Native Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/doll_1.webp) ![ComfyUI Flux Kontext Native Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/doll_2.webp)\n\n### 2\\. Complete the workflow step by step\n\n ![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_1_kontext_dev_grouped_step_guide.jpg) You can refer to the numbers in the image to complete the workflow run:\n\n1.  In the `Load VAE` node, load the `ae.safetensors` model\n2.  In the `Load Image` node, load the first provided input image\n3.  In the `Load Image` node, load the second provided input image\n4.  Since other models and related nodes are packaged in the group node, you need to follow the reference in the step diagram to ensure that the corresponding models are correctly loaded and write prompts\n5.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## New Flux.1 Kontext Dev Selection Toolbox Feature\n\nTo make it easier for users to edit with Flux.1 Kontext Dev, we have added a selection toolbox feature. This feature allows users to quickly and conveniently add the `FLUX.1 Kontext Image Edit` group node. You can watch the video demo below. When you select the `Load Image` node, you can find the new edit button in the selection toolbox.\n\n## Flux Kontext Prompt Techniques\n\n### 1\\. Basic Modifications\n\n*   Simple and direct: `\"Change the car color to red\"`\n*   Maintain style: `\"Change to daytime while maintaining the same style of the painting\"`\n\n### 2\\. Style Transfer\n\n**Principles:**\n\n*   Clearly name style: `\"Transform to Bauhaus art style\"`\n*   Describe characteristics: `\"Transform to oil painting with visible brushstrokes, thick paint texture\"`\n*   Preserve composition: `\"Change to Bauhaus style while maintaining the original composition\"`\n\n### 3\\. Character Consistency\n\n**Framework:**\n\n*   Specific description: `\"The woman with short black hair\"` instead of “she”\n*   Preserve features: `\"while maintaining the same facial features, hairstyle, and expression\"`\n*   Step-by-step modifications: Change background first, then actions\n\n### 4\\. Text Editing\n\n*   Use quotes: `\"Replace 'joy' with 'BFL'\"`\n*   Maintain format: `\"Replace text while maintaining the same font style\"`\n\n## Common Problem Solutions\n\n### Character Changes Too Much\n\n❌ Wrong: `\"Transform the person into a Viking\"` ✅ Correct: `\"Change the clothes to be a viking warrior while preserving facial features\"`\n\n### Composition Position Changes\n\n❌ Wrong: `\"Put him on a beach\"` ✅ Correct: `\"Change the background to a beach while keeping the person in the exact same position, scale, and pose\"`\n\n### Style Application Inaccuracy\n\n❌ Wrong: `\"Make it a sketch\"` ✅ Correct: `\"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\"`\n\n## Core Principles\n\n1.  **Be Specific and Clear** - Use precise descriptions, avoid vague terms\n2.  **Step-by-step Editing** - Break complex modifications into multiple simple steps\n3.  **Explicit Preservation** - State what should remain unchanged\n4.  **Verb Selection** - Use “change”, “replace” rather than “transform”\n\n## Best Practice Templates\n\n**Object Modification:** `\"Change [object] to [new state], keep [content to preserve] unchanged\"` **Style Transfer:** `\"Transform to [specific style], while maintaining [composition/character/other] unchanged\"` **Background Replacement:** `\"Change the background to [new background], keep the subject in the exact same position and pose\"` **Text Editing:** `\"Replace '[original text]' with '[new text]', maintain the same font style\"`\n\n> **Remember:** The more specific, the better. Kontext excels at understanding detailed instructions and maintaining consistency."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video",
  "markdown": "# PixVerse Image to Video - ComfyUI 原生节点文档\n\n![ComfyUI 原生 PixVerse Image to Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg) PixVerse Image to Video 节点通过 PixVerse 的API服务，可以将静态图像转换为动态视频。它能保留原始图像的视觉特征，同时根据文本提示词添加自然的动态效果。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 要转换为视频的输入图像 |\n| prompt | 字符串 | \"\"  | 描述视频动作和内容的文本提示词 |\n| negative\\_prompt | 字符串 | \"\"  | 指定不希望在视频中出现的元素 |\n| seed | 整数  | \\-1 | 生成过程的随机种子，-1为随机 |\n| quality | 选择项 | ”high” | 生成视频的质量级别 |\n| aspect\\_ratio | 选择项 | ”r16\\_9” | 输出视频的宽高比 |\n| duration | 选择项 | ”seconds\\_4” | 生成视频的持续时间 |\n| motion\\_mode | 选择项 | ”standard” | 视频的动作模式 |\n\n### 可选参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | 可选的PixVerse模板配置 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-05)\\]\n\n```\nclass PixverseImageToVideoNode(ComfyNodeABC):\n    \"\"\"\n    Pixverse Image to Video\n\n    Generates videos from an image and prompts.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"prompt\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n                \"negative_prompt\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n                \"seed\": (\"INT\", {\"default\": -1, \"min\": -1, \"max\": 0xffffffffffffffff}),\n                \"quality\": (list(PixverseQuality.__members__.keys()), {\"default\": \"high\"}),\n                \"aspect_ratio\": (list(PixverseAspectRatio.__members__.keys()), {\"default\": \"r16_9\"}),\n                \"duration\": (list(PixverseDuration.__members__.keys()), {\"default\": \"seconds_4\"}),\n                \"motion_mode\": (list(PixverseMotionMode.__members__.keys()), {\"default\": \"standard\"}),\n            },\n            \"optional\": {\n                \"pixverse_template\": (\"PIXVERSE_TEMPLATE\",),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from an image and prompts using Pixverse's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Pixverse\"\n    API_NODE = True\n    OUTPUT_NODE = True\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/flux/flux-1-text-to-image",
  "markdown": "# ComfyUI Flux.1 Text-to-Image Workflow Example\n\n ![Flux](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_example.png) Flux is one of the largest open-source text-to-image generation models, with 12B parameters and an original file size of approximately 23GB. It was developed by [Black Forest Labs](https://blackforestlabs.ai/), a team founded by former Stable Diffusion team members. Flux is known for its excellent image quality and flexibility, capable of generating high-quality, diverse images. Currently, the Flux.1 model has several main versions:\n\n*   **Flux.1 Pro:** The best performing model, closed-source, only available through API calls.\n*   **[Flux.1 \\[dev\\]：](https://huggingface.co/black-forest-labs/FLUX.1-dev)** Open-source but limited to non-commercial use, distilled from the Pro version, with performance close to the Pro version.\n*   **[Flux.1 \\[schnell\\]：](https://huggingface.co/black-forest-labs/FLUX.1-schnell)** Uses the Apache2.0 license, requires only 4 steps to generate images, suitable for low-spec hardware.\n\n**Flux.1 Model Features**\n\n*   **Hybrid Architecture:** Combines the advantages of Transformer networks and diffusion models, effectively integrating text and image information, improving the alignment accuracy between generated images and prompts, with excellent fidelity to complex prompts.\n*   **Parameter Scale:** Flux has 12B parameters, capturing more complex pattern relationships and generating more realistic, diverse images.\n*   **Supports Multiple Styles:** Supports diverse styles, with excellent performance for various types of images.\n\nIn this example, we’ll introduce text-to-image examples using both Flux.1 Dev and Flux.1 Schnell versions, including the full version model and the simplified FP8 Checkpoint version.\n\n*   **Flux Full Version:** Best performance, but requires larger VRAM resources and installation of multiple model files.\n*   **Flux FP8 Checkpoint:** Requires only one fp8 version of the model, but quality is slightly reduced compared to the full version.\n\n## Flux.1 Full Version Text-to-Image Example\n\n### Flux.1 Dev\n\n#### 1\\. Workflow File\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Flux Dev Original Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_t5fp16.png) \n\n#### 2\\. Manual Model Installation\n\nPlease download the following model files:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) Recommended when your VRAM is greater than 32GB.\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)\n\nStorage location:\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── t5xxl_fp16.safetensors\n│   ├── vae/\n│   │   └── ae.safetensors\n│   └── diffusion_models/\n│       └── flux1-dev.safetensors\n```\n\n#### 3\\. Steps to Run the Workflow\n\nPlease refer to the image below to ensure all model files are loaded correctly ![ComfyUI Flux Dev Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg)\n\n1.  Ensure the `DualCLIPLoader` node has the following models loaded:\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n2.  Ensure the `Load Diffusion Model` node has `flux1-dev.safetensors` loaded\n3.  Make sure the `Load VAE` node has `ae.safetensors` loaded\n4.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n### Flux.1 Schnell\n\n#### 1\\. Workflow File\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Flux Schnell Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_t5fp8.png)\n\n#### 2\\. Manual Models Installation\n\nComplete model file list:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)\n\nFile storage location:\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── t5xxl_fp8_e4m3fn.safetensors\n│   ├── vae/\n│   │   └── ae.safetensors\n│   └── diffusion_models/\n│       └── flux1-schnell.safetensors\n```\n\n#### 3\\. Steps to Run the Workflow\n\n![Flux Schnell Version Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg)\n\n1.  Ensure the `DualCLIPLoader` node has the following models loaded:\n    *   clip\\_name1: t5xxl\\_fp8\\_e4m3fn.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n2.  Ensure the `Load Diffusion Model` node has `flux1-schnell.safetensors` loaded\n3.  Ensure the `Load VAE` node has `ae.safetensors` loaded\n4.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Flux.1 FP8 Checkpoint Version Text-to-Image Example\n\nThe fp8 version is a quantized version of the original Flux.1 fp16 version. To some extent, the quality of this version will be lower than that of the fp16 version, but it also requires less VRAM, and you only need to install one model file to try running it.\n\n### Flux.1 Dev\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Flux Dev fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_fp8.png) Please download [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory. Ensure that the corresponding `Load Checkpoint` node loads `flux1-dev-fp8.safetensors`, and you can try to run the workflow.\n\n### Flux.1 Schnell\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Flux Schnell fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_fp8.png) Please download [flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory. Ensure that the corresponding `Load Checkpoint` node loads `flux1-schnell-fp8.safetensors`, and you can try to run the workflow."
},
{
  "url": "https://docs.comfy.org/tutorials/image/hidream/hidream-i1",
  "markdown": "# ComfyUI Native HiDream-I1 Text-to-Image Workflow Example\n\n![HiDream-I1 Demo](https://raw.githubusercontent.com/HiDream-ai/HiDream-I1/main/assets/demo.jpg) HiDream-I1 is a text-to-image model officially open-sourced by HiDream-ai on April 7, 2025. The model has 17B parameters and is released under the [MIT license](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE), supporting personal projects, scientific research, and commercial use. It currently performs excellently in multiple benchmark tests.\n\n## Model Features\n\n**Hybrid Architecture Design** A combination of Diffusion Transformer (DiT) and Mixture of Experts (MoE) architecture:\n\n*   Based on Diffusion Transformer (DiT), with dual-stream MMDiT modules processing multimodal information and single-stream DiT modules optimizing global consistency.\n*   Dynamic routing mechanism flexibly allocates computing resources, enhancing complex scene processing capabilities and delivering excellent performance in color restoration, edge processing, and other details.\n\n**Multimodal Text Encoder Integration** Integrates four text encoders:\n\n*   OpenCLIP ViT-bigG, OpenAI CLIP ViT-L (visual semantic alignment)\n*   T5-XXL (long text parsing)\n*   Llama-3.1-8B-Instruct (instruction understanding) This combination achieves SOTA performance in complex semantic parsing of colors, quantities, spatial relationships, etc., with Chinese prompt support significantly outperforming similar open-source models.\n\n**Original Model Versions** HiDream-ai provides three versions of the HiDream-I1 model to meet different needs. Below are the links to the original model repositories:\n\n| Model Name | Description | Inference Steps | Repository Link |\n| --- | --- | --- | --- |\n| HiDream-I1-Full | Full version | 50  | [🤗 HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full) |\n| HiDream-I1-Dev | Distilled dev | 28  | [🤗 HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev) |\n| HiDream-I1-Fast | Distilled fast | 16  | [🤗 HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) |\n\n## About This Workflow Example\n\nIn this example, we will use the repackaged version from ComfyOrg. You can find all the model files we’ll use in this example in the [HiDream-I1\\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) repository.\n\nThe model requirements for different ComfyUI native HiDream-I1 workflows are basically the same, with only the [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) files being different. If you don’t know which version to choose, please refer to the following suggestions:\n\n*   **HiDream-I1-Full** can generate the highest quality images\n*   **HiDream-I1-Dev** balances high-quality image generation with speed\n*   **HiDream-I1-Fast** can generate images in just 16 steps, suitable for scenarios requiring real-time iteration\n\nFor the **dev** and **fast** versions, negative prompts are not needed, so please set the `cfg` parameter to `1.0` during sampling. We have noted the corresponding parameter settings in the relevant workflows.\n\n### Model Installation\n\nThe following model files are common files that we will use. Please click on the corresponding links to download and save them according to the model file save location. We will guide you to download the corresponding **diffusion models** in the corresponding workflows. **text\\_encoders**：\n\n*   [clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)\n*   [clip\\_g\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)\n*   [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.\n*   [llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux’s VAE model, if you have used Flux’s workflow before, you may have already downloaded this file.\n\n**diffusion models** We will guide you to download the corresponding model files in the corresponding workflows. Model file save location\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 text_encoders/\n│   │   ├─── clip_l_hidream.safetensors\n│   │   ├─── clip_g_hidream.safetensors\n│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors\n│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors\n│   └── 📂 vae/\n│   │   └── ae.safetensors\n│   └── 📂 diffusion_models/\n│       └── ...               # We will guide you to install in the corresponding version workflow       \n```\n\n### HiDream-I1 Full Version Workflow\n\n#### 1\\. Model File Download\n\nPlease select the appropriate version based on your hardware. Click the link and download the corresponding model file to save it to the `ComfyUI/models/diffusion_models/` folder.\n\n*   FP8 version: [hidream\\_i1\\_full\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) requires more than 16GB of VRAM\n*   Full version: [hidream\\_i1\\_full\\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) requires more than 27GB of VRAM\n\n#### 2\\. Workflow File Download\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![HiDream-I1 Full Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_full.png) \n\n#### 3\\. Complete the Workflow Step by Step\n\n![HiDream-I1 Full Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg) Complete the workflow execution step by step\n\n1.  Make sure the `Load Diffusion Model` node is using the `hidream_i1_full_fp8.safetensors` file\n2.  Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node is using the `ae.safetensors` file\n4.  For the **full** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`\n5.  For the `Ksampler` node, you need to make the following settings\n    *   Set `steps` to `50`\n    *   Set `cfg` to `5.0`\n    *   (Optional) Set `sampler` to `lcm`\n    *   (Optional) Set `scheduler` to `normal`\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n### HiDream-I1 Dev Version Workflow\n\n#### 1\\. Model File Download\n\nPlease select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.\n\n*   FP8 version: [hidream\\_i1\\_dev\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) requires more than 16GB of VRAM\n*   Full version: [hidream\\_i1\\_dev\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) requires more than 27GB of VRAM\n\n#### 2\\. Workflow File Download\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![HiDream-I1 Dev Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_dev.png)\n\n#### 3\\. Complete the Workflow Step by Step\n\n ![HiDream-I1 Dev Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg) Complete the workflow execution step by step\n\n1.  Make sure the `Load Diffusion Model` node is using the `hidream_i1_dev_fp8.safetensors` file\n2.  Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node is using the `ae.safetensors` file\n4.  For the **dev** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `6.0`\n5.  For the `Ksampler` node, you need to make the following settings\n    *   Set `steps` to `28`\n    *   (Important) Set `cfg` to `1.0`\n    *   (Optional) Set `sampler` to `lcm`\n    *   (Optional) Set `scheduler` to `normal`\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n### HiDream-I1 Fast Version Workflow\n\n#### 1\\. Model File Download\n\nPlease select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.\n\n*   FP8 version: [hidream\\_i1\\_fast\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 16GB of VRAM\n*   Full version: [hidream\\_i1\\_fast\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_bf16.safetensors?download=true) requires more than 27GB of VRAM\n\n#### 2\\. Workflow File Download\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![HiDream-I1 Fast Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_fast.png)\n\n#### 3\\. Complete the Workflow Step by Step\n\n![HiDream-I1 Fast Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg) Complete the workflow execution step by step\n\n1.  Make sure the `Load Diffusion Model` node is using the `hidream_i1_fast_fp8.safetensors` file\n2.  Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node is using the `ae.safetensors` file\n4.  For the **fast** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`\n5.  For the `Ksampler` node, you need to make the following settings\n    *   Set `steps` to `16`\n    *   (Important) Set `cfg` to `1.0`\n    *   (Optional) Set `sampler` to `lcm`\n    *   (Optional) Set `scheduler` to `normal`\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n### GGUF Version Models\n\n*   [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)\n*   [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)\n\nYou need to use the “Unet Loader (GGUF)” node in City96’s [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) to replace the “Load Diffusion Model” node.\n\n### NF4 Version Models\n\n*   [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)\n*   Use the [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) node to use the NF4 version model."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Finterface%2Fappearance",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video",
  "markdown": "# PixVerse Text to Video - ComfyUI 原生节点文档\n\n```\n\nclass PixverseTextToVideoNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in PixverseAspectRatio],\n                ),\n                \"quality\": (\n                    [resolution.value for resolution in PixverseQuality],\n                    {\n                        \"default\": PixverseQuality.res_540p,\n                    },\n                ),\n                \"duration_seconds\": ([dur.value for dur in PixverseDuration],),\n                \"motion_mode\": ([mode.value for mode in PixverseMotionMode],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"pixverse_template\": (\n                    PixverseIO.TEMPLATE,\n                    {\n                        \"tooltip\": \"An optional template to influence style of generation, created by the Pixverse Template node.\"\n                    }\n                )\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        aspect_ratio: str,\n        quality: str,\n        duration_seconds: int,\n        motion_mode: str,\n        seed,\n        negative_prompt: str=None,\n        pixverse_template: int=None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # 1080p is limited to 5 seconds duration\n        # only normal motion_mode supported for 1080p or for non-5 second duration\n        if quality == PixverseQuality.res_1080p:\n            motion_mode = PixverseMotionMode.normal\n            duration_seconds = PixverseDuration.dur_5\n        elif duration_seconds != PixverseDuration.dur_5:\n            motion_mode = PixverseMotionMode.normal\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/pixverse/video/text/generate\",\n                method=HttpMethod.POST,\n                request_model=PixverseTextVideoRequest,\n                response_model=PixverseVideoResponse,\n            ),\n            request=PixverseTextVideoRequest(\n                prompt=prompt,\n                aspect_ratio=aspect_ratio,\n                quality=quality,\n                duration=duration_seconds,\n                motion_mode=motion_mode,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                template_id=pixverse_template,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.Resp is None:\n            raise Exception(f\"Pixverse request failed: '{response_api.ErrMsg}'\")\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/pixverse/video/result/{response_api.Resp.video_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=PixverseGenerationStatusResponse,\n            ),\n            completed_statuses=[PixverseStatus.successful],\n            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],\n            status_extractor=lambda x: x.Resp.status,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.Resp.url)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v",
  "markdown": "# Kling Image to Video (Camera Control) - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Kling Image to Video (Camera Control) 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg) Kling Image to Video (Camera Control) 节点可将静态图像转换为具有专业镜头动作的视频，支持变焦、旋转、平移、倾斜和第一人称视角等摄像机控制功能，同时保持对原始图像内容的关注。\n\n```\n\nclass KlingCameraControlI2VNode(KlingImage2VideoNode):\n    \"\"\"\n    Kling Image to Video Camera Control Node. This node is a image to video node, but it supports controlling the camera.\n    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"camera_control\": (\n                    \"CAMERA_CONTROL\",\n                    {\n                        \"tooltip\": \"Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Transform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image.\"\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        camera_control: CameraControl,\n        auth_token: Optional[str] = None,\n    ):\n        return super().api_call(\n            model_name=\"kling-v1-5\",\n            start_frame=start_frame,\n            cfg_scale=cfg_scale,\n            mode=\"pro\",\n            aspect_ratio=aspect_ratio,\n            duration=\"5\",\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            camera_control=camera_control,\n            auth_token=auth_token,\n        )\n\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/comfy-cli/getting-started",
  "markdown": "# 快速入门 - ComfyUI\n\n### 概述\n\n`comfy-cli` 是一个 [命令行工具](https://github.com/Comfy-Org/comfy-cli)，可以帮助更轻松地安装和管理 Comfy。\n\n### 安装 CLI\n\n获取 shell 自动补全提示：\n\n```\ncomfy --install-completion\n```\n\n### 安装 ComfyUI\n\n使用任意高于 3.9 的 Python 版本创建一个虚拟环境。\n\n安装 ComfyUI\n\n### 运行 ComfyUI\n\n### 管理自定义节点\n\n```\ncomfy node install <NODE_NAME>\n```\n\n我们使用 `cm-cli` 来安装自定义节点。更多信息请参阅 [文档](https://github.com/ltdrdata/ComfyUI-Manager/blob/main/docs/en/cm-cli.md)。\n\n### 管理模型\n\n使用 `comfy-cli` 下载模型非常简单。只需运行以下命令：\n\n```\ncomfy model download <url> models/checkpoints\n```\n\n### 贡献\n\n我们鼓励对 comfy-cli 的贡献！如果您有建议、想法或错误报告，请在我们的 [GitHub 仓库](https://github.com/Comfy-Org/comfy-cli/issues) 上提交问题。如果您想贡献代码，请 fork 仓库并提交 pull request。 有关更多详细信息，请参阅 [开发指南](https://github.com/Comfy-Org/comfy-cli/blob/main/DEV_README.md)。\n\n### 数据分析\n\n我们会跟踪 CLI 的使用情况以改进用户体验。您可以通过运行以下命令禁用此功能：\n\n要重新启用跟踪，请运行："
},
{
  "url": "https://docs.comfy.org/zh-CN/comfy-cli/reference",
  "markdown": "# 参考 - ComfyUI\n\n## CLI\n\n## 节点\n\n**用法**:\n\n```\n$ comfy node [OPTIONS] COMMAND [ARGS]...\n```\n\n**选项**:\n\n*   `--install-completion`: 为当前 shell 安装自动补全功能。\n*   `--show-completion`: 显示当前 shell 的自动补全功能，可用于复制或自定义安装。\n*   `--help`: 显示此消息并退出。\n\n**命令**:\n\n*   `deps-in-workflow`\n*   `disable`\n*   `enable`\n*   `fix`\n*   `install`\n*   `install-deps`\n*   `reinstall`\n*   `restore-dependencies`\n*   `restore-snapshot`\n*   `save-snapshot`: 保存当前 ComfyUI 环境的快照…\n*   `show`\n*   `simple-show`\n*   `uninstall`\n*   `update`\n\n### `deps-in-workflow`\n\n**用法**:\n\n```\n$ deps-in-workflow [OPTIONS]\n```\n\n**选项**:\n\n*   `--workflow TEXT`: 工作流文件 (.json/.png) \\[必需\\]\n*   `--output TEXT`: 输出文件 (.json/.png) \\[必需\\]\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `disable`\n\n**用法**:\n\n```\n$ disable [OPTIONS] ARGS...\n```\n\n**参数**:\n\n*   `ARGS...`: 禁用自定义节点 \\[必需\\]\n\n**选项**:\n\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `enable`\n\n**用法**:\n\n```\n$ enable [OPTIONS] ARGS...\n```\n\n**参数**:\n\n*   `ARGS...`: 启用自定义节点 \\[必需\\]\n\n**选项**:\n\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `fix`\n\n**用法**:\n\n**参数**:\n\n*   `ARGS...`: 修复指定自定义节点的依赖项 \\[必需\\]\n\n**选项**:\n\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `install`\n\n**用法**:\n\n```\n$ install [OPTIONS] ARGS...\n```\n\n**参数**:\n\n*   `ARGS...`: 安装自定义节点 \\[必需\\]\n\n**选项**:\n\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `install-deps`\n\n**用法**:\n\n**选项**:\n\n*   `--deps TEXT`: 依赖项规范文件 (.json)\n*   `--workflow TEXT`: 工作流文件 (.json/.png)\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `reinstall`\n\n**用法**:\n\n```\n$ reinstall [OPTIONS] ARGS...\n```\n\n**参数**:\n\n*   `ARGS...`: 重新安装自定义节点 \\[必需\\]\n\n**选项**:\n\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `restore-dependencies`\n\n**用法**:\n\n```\n$ restore-dependencies [OPTIONS]\n```\n\n**选项**:\n\n*   `--help`: 显示此消息并退出。\n\n### `restore-snapshot`\n\n**用法**:\n\n```\n$ restore-snapshot [OPTIONS] PATH\n```\n\n**参数**:\n\n*   `PATH`: \\[必需\\]\n\n**选项**:\n\n*   `--help`: 显示此消息并退出。\n\n### `save-snapshot`\n\n保存当前 ComfyUI 环境的快照。 **用法**:\n\n```\n$ save-snapshot [OPTIONS]\n```\n\n**选项**:\n\n*   `--output TEXT`: 指定输出文件路径 (.json/.yaml)。\n*   `--help`: 显示此消息并退出。\n\n### `show`\n\n**用法**:\n\n**参数**:\n\n*   `ARGS...`: \\[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list\\] \\[必需\\]\n\n**选项**:\n\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `simple-show`\n\n**用法**:\n\n```\n$ simple-show [OPTIONS] ARGS...\n```\n\n**参数**:\n\n*   `ARGS...`: \\[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list\\] \\[必需\\]\n\n**选项**:\n\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `uninstall`\n\n**用法**:\n\n```\n$ uninstall [OPTIONS] ARGS...\n```\n\n**参数**:\n\n*   `ARGS...`: 卸载自定义节点 \\[必需\\]\n\n**选项**:\n\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n### `update`\n\n**用法**:\n\n```\n$ update [OPTIONS] ARGS...\n```\n\n**参数**:\n\n*   `ARGS...`: 更新自定义节点 \\[必需\\]\n\n**选项**:\n\n*   `--channel TEXT`: 指定操作模式\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: 显示此消息并退出。\n\n## 模型\n\n**用法**:\n\n```\n$ comfy model [OPTIONS] COMMAND [ARGS]...\n```\n\n**选项**:\n\n*   `--install-completion`: 为当前 shell 安装自动补全功能。\n*   `--show-completion`: 显示当前 shell 的自动补全功能，可用于复制或自定义安装。\n*   `--help`: 显示此消息并退出。\n\n**命令**:\n\n*   `download`: 下载模型到指定的相对路径…\n*   `list`: 显示当前所有已下载模型的列表…\n*   `remove`: 删除一个或多个已下载的模型，…\n\n### `download`\n\n如果模型尚未下载，则将其下载到指定的相对路径。 **用法**:\n\n**选项**:\n\n*   `--url TEXT`: 模型下载的 URL \\[必需\\]\n*   `--relative-path TEXT`: 从当前工作区到安装模型的相对路径。 \\[默认值: models/checkpoints\\]\n*   `--help`: 显示此消息并退出。\n\n### `list`\n\n以表格格式显示当前已下载的所有模型列表。 **用法**:\n\n**选项**:\n\n*   `--relative-path TEXT`: 从当前工作区到存储模型的相对路径。 \\[默认值: models/checkpoints\\]\n*   `--help`: 显示此消息并退出。\n\n### `remove`\n\n通过直接指定或交互式选择，删除一个或多个已下载的模型。 **用法**:\n\n**选项**:\n\n*   `--relative-path TEXT`: 从当前工作区到存储模型的相对路径。 \\[默认值: models/checkpoints\\]\n*   `--model-names TEXT`: 要删除的模型文件名列表，用空格分隔。\n*   `--help`: 显示此消息并退出。\n\n在此页面\n\n*   [CLI](#cli)\n*   [节点](#%E8%8A%82%E7%82%B9)\n*   [deps-in-workflow](#deps-in-workflow)\n*   [disable](#disable)\n*   [enable](#enable)\n*   [fix](#fix)\n*   [install](#install)\n*   [install-deps](#install-deps)\n*   [reinstall](#reinstall)\n*   [restore-dependencies](#restore-dependencies)\n*   [restore-snapshot](#restore-snapshot)\n*   [save-snapshot](#save-snapshot)\n*   [show](#show)\n*   [simple-show](#simple-show)\n*   [uninstall](#uninstall)\n*   [update](#update)\n*   [模型](#%E6%A8%A1%E5%9E%8B)\n*   [download](#download)\n*   [list](#list)\n*   [remove](#remove)"
},
{
  "url": "https://docs.comfy.org/zh-CN/comfy-cli/troubleshooting",
  "markdown": "# 开始 - ComfyUI\n\n### \n\n[​](#%E5%89%8D%E6%8F%90%E6%9D%A1%E4%BB%B6)\n\n前提条件\n\n你需要在你的系统上已经安装好了 Git。可以[在这里](https://git-scm.com/downloads)下载。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/datatypes",
  "markdown": "# 数据类型 - ComfyUI\n\n这些是最重要的内置数据类型。您也可以[定义自己的数据类型](https://docs.comfy.org/zh-CN/custom-nodes/backend/more_on_inputs#custom-datatypes)。 数据类型在客户端用于防止工作流将错误形式的数据传递给节点 - 有点像强类型。 JavaScript 客户端代码通常不允许将节点输出连接到不同数据类型的输入， 不过下面会提到一些例外情况。\n\n## Comfy 数据类型\n\n### COMBO\n\n*   `INPUT_TYPES` 中不需要额外参数\n*   Python 数据类型：定义为 `list[str]`，输出值为 `str`\n\n表示下拉菜单组件。 与其他数据类型不同，`COMBO` 在 `INPUT_TYPES` 中不是通过 `str` 指定的，而是通过 `list[str]` 指定， 对应下拉列表中的选项，默认选中第一个选项。 `COMBO` 输入通常在运行时动态生成。例如，在内置的 `CheckpointLoaderSimple` 节点中，您会看到\n\n```\n\"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), )\n```\n\nor they might just be a fixed list of options,\n\n```\n\"play_sound\": ([\"no\",\"yes\"], {}),\n```\n\n### 原始类型和重路由\n\n原始类型和重路由节点仅存在于客户端。它们没有固有的数据类型，但在连接时会采用所连接输入或输出的数据类型（这就是为什么它们不能连接到 `*` 输入…）\n\n## Python 数据类型\n\n### INT\n\n*   `INPUT_TYPES` 中的额外参数：\n    *   `default` 是必需的\n    *   `min` 和 `max` 是可选的\n*   Python 数据类型 `int`\n\n### FLOAT\n\n*   `INPUT_TYPES` 中的额外参数：\n    *   `default` 是必需的\n    *   `min`、`max`、`step` 是可选的\n*   Python 数据类型 `float`\n\n### STRING\n\n*   `INPUT_TYPES` 中的额外参数：\n    *   `default` 是必需的\n*   Python 数据类型 `str`\n\n### BOOLEAN\n\n*   `INPUT_TYPES` 中的额外参数：\n    *   `default` 是必需的\n*   Python 数据类型 `bool`\n\n## 张量数据类型\n\n### IMAGE\n\n*   `INPUT_TYPES` 中不需要额外参数\n*   Python 数据类型 `torch.Tensor`，形状为 \\[B,H,W,C\\]\n\n一批 `B` 张图像，高度 `H`，宽度 `W`，具有 `C` 个通道（通常 `C=3` 表示 `RGB`）。\n\n### LATENT\n\n*   `INPUT_TYPES` 中不需要额外参数\n*   Python 数据类型 `dict`，包含一个形状为 \\[B,C,H,W\\] 的 `torch.Tensor`\n\n传入的 `dict` 包含键 `samples`，这是一个形状为 \\[B,C,H,W\\] 的 `torch.Tensor`，表示 一批 `B` 个潜空间表示，具有 `C` 个通道（现有 stable diffusion 模型通常 `C=4`），高度 `H`，宽度 `W`。 高度和宽度是对应图像尺寸的 1/8（这是您在 Empty Latent Image 节点中设置的值）。 字典中的其他条目包含潜空间蒙版等内容。\n\n### MASK\n\n*   `INPUT_TYPES` 中不需要额外参数\n*   Python 数据类型 `torch.Tensor`，形状为 \\[H,W\\] 或 \\[B,C,H,W\\]\n\n### AUDIO\n\n*   `INPUT_TYPES` 中不需要额外参数\n*   Python 数据类型 `dict`，包含一个形状为 \\[B, C, T\\] 的 `torch.Tensor` 和采样率。\n\n传入的 `dict` 包含键 `waveform`，这是一个形状为 \\[B, C, T\\] 的 `torch.Tensor`，表示 一批 `B` 个音频样本，具有 `C` 个通道（`C=2` 表示立体声，`C=1` 表示单声道），以及 `T` 个时间步（即音频样本的数量）。 `dict` 还包含另一个键 `sample_rate`，表示音频的采样率。\n\n## 自定义采样数据类型\n\n### Noise\n\n`NOISE` 数据类型表示噪声的_来源_（而不是噪声本身）。它可以由任何提供生成噪声方法的 Python 对象表示， 方法签名为 `generate_noise(self, input_latent:Tensor) -> Tensor`，以及一个属性 `seed:Optional[int]`。\n\n当需要添加噪声时，潜空间表示会被传入这个方法，它应该返回一个包含噪声的相同形状的 `Tensor`。 参见[噪声混合示例](https://docs.comfy.org/zh-CN/custom-nodes/backend/snippets#creating-noise-variations)\n\n### Sampler\n\n`SAMPLER` 数据类型表示一个采样器，它由一个提供 `sample` 方法的 Python 对象表示。 Stable diffusion 采样超出了本指南的范围；如果您想深入研究这部分代码，请查看 `comfy/samplers.py`。\n\n### Sigmas\n\n`SIGMAS` 数据类型表示由调度器产生的采样过程中每个步骤前后的 sigma 值。 它表示为一个一维张量，长度为 `steps+1`，其中每个元素表示对应步骤之前预期的噪声量， 最后一个值表示最终步骤之后的噪声量。 对于 SDXL 模型，一个具有 20 步和去噪值为 1 的 `normal` 调度器会产生：\n\n```\ntensor([14.6146, 10.7468,  8.0815,  6.2049,  4.8557,  \n         3.8654,  3.1238,  2.5572,  2.1157,  1.7648,  \n         1.4806,  1.2458,  1.0481,  0.8784,  0.7297,  \n         0.5964,  0.4736,  0.3555,  0.2322,  0.0292,  0.0000])\n```\n\n### Guider\n\n`GUIDER` 是去噪过程的泛化，由提示词或任何其他形式的条件”引导”。在 Comfy 中，guider 由 一个提供 `__call__(*args, **kwargs)` 方法的可调用 Python 对象表示，该方法由采样器调用。 `__call__` 方法接收（在 `args[0]` 中）一批噪声潜空间表示（张量 `[B,C,H,W]`），并返回噪声的预测（相同形状的张量）。\n\n## 模型数据类型\n\n稳定扩散模型还有一些更技术性的数据类型。最重要的是 `MODEL`、`CLIP`、`VAE` 和 `CONDITIONING`。 目前这些内容超出了本指南的范围！\n\n## 附加参数\n\n以下是输入定义的”额外选项”部分可以使用的官方支持键的列表。\n\n| 键名  | 描述  |\n| --- | --- |\n| `default` | 控件的默认值 |\n| `min` | 数字类型(`FLOAT` 或 `INT`)的最小值 |\n| `max` | 数字类型(`FLOAT` 或 `INT`)的最大值 |\n| `step` | 控件的增减步长 |\n| `label_on` | 布尔值为 `True` 时在 UI 中显示的标签 (`BOOL`) |\n| `label_off` | 布尔值为 `False` 时在 UI 中显示的标签 (`BOOL`) |\n| `defaultInput` | 默认使用输入插槽而不是支持的控件 |\n| `forceInput` | 与 `defaultInput` 相同，且不允许转换为控件 |\n| `multiline` | 使用多行文本框 (`STRING`) |\n| `placeholder` | 当为空时在 UI 中显示的占位文本 (`STRING`) |\n| `dynamicPrompts` | 使前端评估动态提示词 |\n| `lazy` | 声明此输入使用[延迟求值](https://docs.comfy.org/zh-CN/custom-nodes/backend/lazy_evaluation) |\n| `rawLink` | 当存在链接时，您将收到链接而不是求值后的值（即 `[\"nodeId\", <outputIndex>]`）。主要在节点使用[节点扩展](https://docs.comfy.org/zh-CN/custom-nodes/backend/expansion)时有用。 |"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/expansion",
  "markdown": "# 节点扩展 - ComfyUI\n\n## 节点扩展\n\n通常，当节点执行时，执行函数会立即返回该节点的输出结果。“节点扩展”是一种相对高级的技术，允许节点返回一个新的子图节点，该子图将替代原节点在图中。这种技术使自定义节点能够实现循环功能。\n\n### 简单示例\n\n首先，这里是一个节点扩展的简单示例：\n\n```\ndef load_and_merge_checkpoints(self, checkpoint_path1, checkpoint_path2, ratio):\n    from comfy_execution.graph_utils import GraphBuilder # 通常在文件顶部\n    graph = GraphBuilder()\n    checkpoint_node1 = graph.node(\"CheckpointLoaderSimple\", checkpoint_path=checkpoint_path1)\n    checkpoint_node2 = graph.node(\"CheckpointLoaderSimple\", checkpoint_path=checkpoint_path2)\n    merge_model_node = graph.node(\"ModelMergeSimple\", model1=checkpoint_node1.out(0), model2=checkpoint_node2.out(0), ratio=ratio)\n    merge_clip_node = graph.node(\"ClipMergeSimple\", clip1=checkpoint_node1.out(1), clip2=checkpoint_node2.out(1), ratio=ratio)\n    return {\n        # 返回 (MODEL, CLIP, VAE) 输出\n        \"result\": (merge_model_node.out(0), merge_clip_node.out(0), checkpoint_node1.out(2)),\n        \"expand\": graph.finalize(),\n    }\n```\n\n虽然这个节点以前可以通过手动调用 ComfyUI 内部来实现，但使用扩展意味着每个子节点将单独缓存（所以如果你更改 `model2`，你不需要重新加载 `model1`）。\n\n### 要求\n\n为了执行节点扩展，一个节点必须返回一个包含以下键的字典：\n\n1.  `result`: 一个包含节点输出的元组。这可能是一个混合的最终值（像你从正常节点返回的那样）和节点输出。\n2.  `expand`: 要执行扩展的最终图。如果您不使用 `GraphBuilder`，请参见下文。\n\n#### 不使用 GraphBuilder 的额外要求\n\n`expand` 键的格式与 ComfyUI API 格式相同。以下要求由 `GraphBuilder` 处理，但如果您选择不使用它，则必须手动处理：\n\n1.  节点 ID 必须在整个图中唯一。（这包括在多次使用列表时由于使用列表而导致的同一节点的多次执行。）\n2.  节点 ID 必须确定且在多次执行图中一致（包括由于缓存而导致的部分执行）。\n\n即使您不想使用 `GraphBuilder` 来实际构建图（例如，因为您从文件加载了图的原始 JSON），您也可以使用 `GraphBuilder.alloc_prefix()` 函数生成一个前缀，并使用 `comfy.graph_utils.add_graph_prefix` 修复现有图以满足这些要求。\n\n### 高效的子图缓存\n\n虽然您可以向子图中的节点传递非文字输入（如 torch 张量），但这可能会抑制子图内部的缓存。当可能时，您应该传递子图对象的链接，而不是节点本身。（您可以在输入的[附加参数](https://docs.comfy.org/zh-CN/custom-nodes/backend/datatypes#additional-parameters)中声明一个 `rawLink` 来轻松实现这一点。）"
},
{
  "url": "https://docs.comfy.org/zh-CN/changelog/index",
  "markdown": "# 更新日志 - ComfyUI\n\n**高级采样与训练基础设施改进**本版本为AI研究人员和工作流程创建者引入了采样算法、训练功能和节点功能的重大增强：\n\n## 新的采样和生成功能\n\n*   **SA-Solver采样器**：新的重构SA-Solver采样算法，为复杂生成工作流提供增强的数值稳定性和质量\n*   **实验性CFGNorm节点**：高级无分类器引导标准化，用于改进生成质量和风格一致性的控制\n*   **嵌套双CFG支持**：为DualCFGGuider节点添加嵌套风格配置，提供更复杂的引导控制模式\n*   **SamplingPercentToSigma节点**：用于从采样百分比精确计算sigma的新实用节点，提高工作流程灵活性\n\n## 增强的训练功能\n\n*   **多图像-描述数据集支持**：LoRA训练节点现在可以同时处理多个图像-描述数据集，简化训练工作流程\n*   **更好的训练循环实现**：优化的训练算法，在模型微调过程中改善收敛性和稳定性\n*   **增强的错误检测**：为LoRA操作添加模型检测错误提示，在出现问题时提供更清晰的反馈\n\n## 平台和性能改进\n\n*   **异步节点支持**：完全支持异步节点函数，优化早期执行，改善I/O密集型操作的工作流程性能\n*   **Chroma灵活性**：在Chroma中取消硬编码的patch\\_size参数，允许更好地适应不同的模型配置\n*   **LTXV VAE解码器**：切换到改进的默认填充模式，提高LTXV模型的图像质量\n*   **Safetensors内存管理**：为mmap问题添加解决方案，提高加载大型模型文件时的可靠性\n\n## API和集成增强\n\n*   **自定义提示ID**：API现在允许指定提示ID，以便更好地跟踪和管理工作流程\n*   **Kling API优化**：增加轮询超时时间，防止视频生成工作流程中的用户超时\n*   **历史令牌清理**：从历史项目中删除敏感令牌以提高安全性\n*   **Python 3.9兼容性**：修复兼容性问题，确保更广泛的平台支持\n\n## 错误修复和稳定性\n\n*   **MaskComposite修复**：解决目标蒙版具有2个维度时的错误，提高蒙版工作流程可靠性\n*   **Fresca输入/输出**：修正Fresca模型工作流程的输入和输出处理\n*   **引用错误修复**：解决Gemini节点实现中的错误引用问题\n*   **行结束标准化**：自动检测和删除Windows行结束符，确保跨平台一致性\n\n## 开发者体验\n\n*   **警告系统**：添加torch导入错误警告，以捕获常见配置问题\n*   **模板更新**：多个模板版本更新（0.1.36、0.1.37、0.1.39），改进自定义节点开发\n*   **文档**：增强便携式配置中fast\\_fp16\\_accumulation的文档\n\n这些改进使ComfyUI在生产工作流程中更加稳健，同时引入了对高级AI研究和创意应用必不可少的强大新采样技术和训练功能。\n\n**高级采样和模型控制增强**此版本在采样算法和模型控制系统方面提供了重大改进，特别有利于高级AI研究人员和工作流创建者：\n\n## 新采样功能\n\n*   **TCFG节点**：增强的分类器无关引导控制，为您的工作流提供更细致的生成控制\n*   **ER-SDE采样器**：从VE迁移到VP算法，配备新的采样器节点，为复杂生成任务提供更好的数值稳定性\n*   **跳层引导（SLG）**：用于推理期间精确层级控制的替代实现，完美适用于高级模型导向工作流\n\n## 增强的开发工具\n\n*   **自定义节点管理**：新的`--whitelist-custom-nodes`参数与`--disable-all-custom-nodes`配对，提供精确的开发控制\n*   **性能优化**：双CFG节点现在在CFG为1.0时自动优化，减少计算开销\n*   **GitHub Actions集成**：自动化发布webhook通知让开发者及时了解新更新\n\n## 图像处理改进\n\n*   **新变换节点**：添加了ImageRotate和ImageFlip节点，增强图像操作工作流\n*   **ImageColorToMask修复**：修正了掩码值返回，提供更准确的基于颜色的掩码操作\n*   **3D模型支持**：上传3D模型到自定义子文件夹，为复杂项目提供更好的组织\n\n## 引导和条件增强\n\n*   **PerpNeg引导器**：更新了改进的前后CFG处理以及性能优化\n*   **潜在条件修复**：解决了多步骤工作流中索引 > 0 的条件问题\n*   **去噪步骤**：为多个采样器添加去噪步骤支持，获得更清洁的输出\n\n## 平台稳定性\n\n*   **PyTorch兼容性**：修复了PyTorch nightly构建的连续内存问题\n*   **FP8回退**：当FP8操作遇到异常时自动回退到常规操作\n*   **音频处理**：移除了已弃用的torchaudio.save函数依赖并修复警告\n\n## 模型集成\n\n*   **Moonvalley节点**：为Moonvalley模型工作流添加原生支持\n*   **调度器重新排序**：简单调度器现在默认优先，提供更好的用户体验\n*   **模板更新**：多个模板版本更新（0.1.31-0.1.35），改进自定义节点开发\n\n## 安全性和安全保护\n\n*   **安全加载**：在不安全加载文件时添加警告，文档说明检查点文件默认安全加载\n*   **文件验证**：增强检查点加载安全措施，确保工作流安全执行\n\n这些改进使ComfyUI在生产工作流中更加稳健，同时为使用高级采样技术和模型控制系统的AI艺术家扩展了创作可能性。\n\n**增强模型支持与工作流可靠性**本次发布在模型兼容性和工作流稳定性方面带来了重大改进：\n\n*   **扩展模型文档**：为 Flux Kontext 和 Omnigen 2 模型添加了全面的支持文档，让创作者更容易将这些强大的模型集成到他们的工作流中\n*   **VAE 编码改进**：移除了 VAE 编码过程中不必要的随机噪声注入，使工作流运行的输出更加一致和可预测\n*   **内存管理修复**：解决了专门影响 Kontext 模型使用的关键内存估算错误，防止内存不足错误并提高工作流稳定性\n\n这些变更提升了高级模型工作流的可靠性，同时保持了 ComfyUI 为从事前沿生成模型工作的 AI 艺术家和研究人员提供的灵活性。\n\n**主要模型支持新增**\n\n*   **Cosmos Predict2 支持**：全面实现文本到图像（2B 和 14B 模型）和图像到视频生成工作流，扩展视频创作功能\n*   **增强的 Flux 兼容性**：Chroma Text Encoder 现在能与常规 Flux 模型无缝协作，提升文本条件质量\n*   **LoRA 训练集成**：使用权重适配器方案的全新原生 LoRA 训练节点，支持在 ComfyUI 工作流中直接进行模型微调\n\n**性能和硬件优化**\n\n*   **AMD GPU 增强**：在 GFX1201 和其他兼容的 AMD GPU 上启用 FP8 操作和 PyTorch 注意力机制，加速推理\n*   **Apple Silicon 修复**：解决了 Apple 设备上长期存在的 FP16 注意力问题，提升 Mac 用户的稳定性\n*   **Flux 模型稳定性**：解决了特定 Flux 模型在 FP16 精度下生成黑色图像的问题\n\n**高级采样改进**\n\n*   **Rectified Flow (RF) 采样器**：新增支持 RF 的 SEEDS 和多步 DPM++ SDE 采样器，为前沿模型提供更多采样选项\n*   **ModelSamplingContinuousEDM**：新增 cosmos\\_rflow 选项，增强对 Cosmos 模型的采样控制\n*   **内存优化**：改进了支持无限分辨率的 Cosmos 模型的内存估算\n\n**开发者和集成功能**\n\n*   **SQLite 数据库支持**：增强自定义节点和工作流存储的数据管理功能\n*   **PyProject.toml 集成**：从 pyproject 文件自动注册 web 文件夹和配置设置\n*   **前端灵活性**：支持语义化版本后缀和预发布前端版本，适用于自定义部署\n*   **分词器增强**：通过 tokenizer\\_data 配置 min\\_length 设置，优化文本处理\n\n**使用体验改进**\n\n*   **Kontext 宽高比修复**：解决了仅限小组件的限制，现在在所有连接模式下都能正常工作\n*   **SaveLora 一致性**：统一所有保存节点的文件名格式，优化文件组织\n*   **Python 版本警告**：为过时的 Python 安装添加警报，防止兼容性问题\n*   **WebcamCapture 修复**：修正了 IS\\_CHANGED 签名，确保实时输入工作流的可靠性\n\n此版本显著扩展了 ComfyUI 的模型生态系统支持，同时提供了关键的稳定性改进和跨平台硬件兼容性增强。\n\n本次发布为 ComfyUI 创作者带来了强大的新工作流实用工具和性能优化：\n\n## 新的工作流工具\n\n*   **ImageStitch 节点**：在工作流中无缝拼接多个图像 - 非常适合创建对比网格或复合输出\n*   **GetImageSize 节点**：提取图像尺寸并支持批处理，对于动态调整大小的工作流至关重要\n*   **Regex Replace 节点**：高级文本处理功能，适用于提示词工程和字符串处理工作流\n\n## 增强的模型兼容性\n\n*   **改进的张量处理**：简化的列表处理使复杂的多模型工作流更加可靠\n*   **BFL API 优化**：完善了对 Kontext \\[pro\\] 和 \\[max\\] 模型的支持，提供更清晰的节点界面\n*   **性能提升**：在色度处理中使用融合乘加运算，加快生成速度\n\n## 开发者体验改进\n\n*   **自定义节点支持**：添加 pyproject.toml 支持，改善自定义节点依赖管理\n*   **帮助菜单集成**：在节点库侧边栏中新增帮助系统，加快节点发现速度\n*   **API 文档**：增强 API 节点文档，支持工作流自动化\n\n## 前端和 UI 增强\n\n*   **前端更新至 v1.21.7**：多项稳定性修复和性能改进\n*   **自定义 API 基础支持**：改进了自定义部署配置的子路径处理\n*   **安全加固**：修复 XSS 漏洞，确保工作流分享更安全\n\n## 错误修复和稳定性\n\n*   **Pillow 兼容性**：更新了已弃用的 API 调用，保持与最新图像处理库的兼容性\n*   **ROCm 支持**：改进了 AMD GPU 用户的版本检测\n*   **模板更新**：增强了自定义节点开发的项目模板\n\n这些更新强化了 ComfyUI 处理复杂 AI 工作流的基础，同时通过改进的文档和辅助工具让平台对新用户更加友好。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/images_and_masks",
  "markdown": "# 图像、潜变量和蒙版 - ComfyUI\n\n在处理这些数据类型时，你需要了解 `torch.Tensor` 类。  \n完整文档请见[这里](https://pytorch.org/docs/stable/tensors.html)，或者参考 Comfy 所需关键概念的[介绍](https://docs.comfy.org/zh-CN/custom-nodes/backend/tensors)。\n\n下方大部分概念都在[示例代码片段](https://docs.comfy.org/zh-CN/custom-nodes/backend/snippets)中有说明。\n\n## 图像（Images）\n\nIMAGE 是一个形状为 `[B,H,W,C]` 的 `torch.Tensor`，其中 `C=3`。如果你需要保存或加载图像，需要在 `PIL.Image` 格式和张量之间进行转换——请参见下方代码片段！注意，有些 `pytorch` 操作提供（或期望）`[B,C,H,W]`，即“通道优先”，这样做是为了计算效率。请务必小心区分。\n\n### 使用 PIL.Image\n\n如果你想加载和保存图像，你需要用到 PIL：\n\n```\nfrom PIL import Image, ImageOps\n```\n\n## 蒙版（Masks）\n\nMASK 是一个形状为 `[B,H,W]` 的 `torch.Tensor`。  \n在许多场景下，蒙版的值为二值（0 或 1），用于指示哪些像素需要进行特定操作。  \n有时蒙版的值会在 0 到 1 之间，用于表示遮罩的程度（例如调整透明度、滤镜或图层合成）。\n\n### Load Image 节点生成的蒙版\n\n`LoadImage` 节点会使用图像的 alpha 通道（即“RGBA”中的“A”）来创建蒙版。  \nalpha 通道的值会被归一化到 \\[0,1\\]（torch.float32），然后再取反。  \n`LoadImage` 节点在加载图像时总会生成一个 MASK 输出。许多图片（如 JPEG）没有 alpha 通道，这种情况下，`LoadImage` 会创建一个形状为 `[1, 64, 64]` 的默认蒙版。\n\n### 理解蒙版的形状\n\n在 `numpy`、`PIL` 等库中，单通道图像（如蒙版）通常表示为二维数组，形状为 `[H,W]`。  \n这意味着 `C`（通道）维度是隐式的，因此与 IMAGE 类型不同，蒙版的批量通常只有三维：`[B, H, W]`。  \n有时你会遇到 `B` 维被隐式 squeeze 的蒙版，变成了 `[H,W]` 的张量。 在使用 MASK 时，你经常需要通过 unsqueeze 匹配形状，变成 `[B,H,W,C]`，其中 `C=1`。  \n要给 `C` 维 unsqueeze，请用 `unsqueeze(-1)`，要给 `B` 维 unsqueeze，请用 `unsqueeze(0)`。  \n如果你的节点接收 MASK 作为输入，建议总是检查 `len(mask.shape)`。\n\n## 潜变量（Latents）\n\nLATENT 是一个 `dict`；潜变量样本通过键 `samples` 引用，形状为 `[B,C,H,W]`，其中 `C=4`。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/lazy_evaluation",
  "markdown": "# 延迟求值 - ComfyUI\n\n## 延迟求值\n\n默认情况下，在节点运行之前，所有 `required` 和 `optional` 输入都会被求值。然而，有时某些输入可能不会被使用，对其进行求值会导致不必要的处理。以下是一些可能从延迟求值中受益的节点示例：\n\n1.  一个 `ModelMergeSimple` 节点，其中比例要么是 `0.0`（这种情况下不需要加载第一个模型） 要么是 `1.0`（这种情况下不需要加载第二个模型）。\n2.  两个图像之间的插值，其中比例（或蒙版）要么完全是 `0.0` 要么完全是 `1.0`。\n3.  一个 Switch 节点，其中一个输入决定其他输入中的哪一个会被传递。\n\n### 创建延迟输入\n\n将输入设置为”延迟”输入需要两个步骤：\n\n1.  在 `INPUT_TYPES` 返回的字典中将输入标记为延迟\n2.  定义一个名为 `check_lazy_status` 的方法（注意：_不是_类方法），该方法将在求值之前被调用来确定是否需要更多输入。\n\n为了演示这些，我们将创建一个”MixImages”节点，它根据蒙版在两个图像之间进行插值。如果整个蒙版都是 `0.0`，我们不需要对第二个图像之前的任何部分进行求值。如果整个蒙版都是 `1.0`，我们可以跳过对第一个图像的求值。\n\n#### 定义 `INPUT_TYPES`\n\n将输入声明为延迟输入很简单，只需在输入的选项字典中添加一个 `lazy: True` 键值对即可。\n\n```\n@classmethod\ndef INPUT_TYPES(cls):\n    return {\n        \"required\": {\n            \"image1\": (\"IMAGE\",{\"lazy\": True}),\n            \"image2\": (\"IMAGE\",{\"lazy\": True}),\n            \"mask\": (\"MASK\",),\n        },\n    }\n```\n\n在这个例子中，`image1` 和 `image2` 都被标记为延迟输入，但 `mask` 总是会被求值。\n\n#### 定义 `check_lazy_status`\n\n一个 `check_lazy_status` 方法在存在一个或多个尚未可用的延迟输入时被调用。这个方法接收与标准执行函数相同的参数。所有可用的输入都会以其最终值传递，而不可用的延迟输入则会有一个 `None` 值。 `check_lazy_status` 方法的责任是返回一个列表，其中包含任何需要继续执行的延迟输入的名称。如果所有延迟输入都可用，该方法应返回一个空列表。 注意，`check_lazy_status` 方法可能会被多次调用。例如，你可能在评估一个延迟输入后发现需要评估另一个延迟输入。\n\n```\ndef check_lazy_status(self, mask, image1, image2):\n    mask_min = mask.min()\n    mask_max = mask.max()\n    needed = []\n    if image1 is None and (mask_min != 1.0 or mask_max != 1.0):\n        needed.append(\"image1\")\n    if image2 is None and (mask_min != 0.0 or mask_max != 0.0):\n        needed.append(\"image2\")\n    return needed\n```\n\n### 完整示例\n\n```\nclass LazyMixImages:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image1\": (\"IMAGE\",{\"lazy\": True}),\n                \"image2\": (\"IMAGE\",{\"lazy\": True}),\n                \"mask\": (\"MASK\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"mix\"\n\n    CATEGORY = \"Examples\"\n\n    def check_lazy_status(self, mask, image1, image2):\n        mask_min = mask.min()\n        mask_max = mask.max()\n        needed = []\n        if image1 is None and (mask_min != 1.0 or mask_max != 1.0):\n            needed.append(\"image1\")\n        if image2 is None and (mask_min != 0.0 or mask_max != 0.0):\n            needed.append(\"image2\")\n        return needed\n\n    # Not trying to handle different batch sizes here just to keep the demo simple\n    def mix(self, mask, image1, image2):\n        mask_min = mask.min()\n        mask_max = mask.max()\n        if mask_min == 0.0 and mask_max == 0.0:\n            return (image1,)\n        elif mask_min == 1.0 and mask_max == 1.0:\n            return (image2,)\n\n        result = image1 * (1. - mask) + image2 * mask,\n        return (result[0],)\n```\n\n## 执行阻塞\n\n虽然延迟求值是推荐的方式来”禁用”图的一部分，但有时你想要禁用一个没有实现延迟求值的 `OUTPUT` 节点。如果是你开发的节点，你应该按照以下方式添加延迟求值：\n\n1.  添加一个 `enabled` 的必需（如果这是一个新节点）或可选（如果你关心向后兼容性）输入，默认值为 `True`\n2.  将所有其他输入设置为延迟输入\n3.  仅在 `enabled` 为 `True` 时评估其他输入\n\n如果你无法控制该节点，你可以使用 `comfy_execution.graph.ExecutionBlocker`。这个特殊对象可以作为任何输出端口的返回值。任何接收到 `ExecutionBlocker` 作为输入的节点都会跳过执行，并将该 `ExecutionBlocker` 作为其所有输出返回。\n\n### 使用方法\n\n有两种方式可以构造和使用 `ExecutionBlocker`：\n\n1.  向构造函数传入 `None` 来静默阻止执行。这在阻止执行是成功运行的一部分时很有用——比如禁用某个输出。\n\n```\ndef silent_passthrough(self, passthrough, blocked):\n    if blocked:\n        return (ExecutionBlocker(None),)\n    else:\n        return (passthrough,)\n```\n\n2.  向构造函数传入一个字符串，当节点因接收到该对象而被阻止执行时显示错误消息。这在你想显示有意义的错误消息时很有用，比如当有人使用无意义的输出时——例如，加载不包含 VAE 的模型时的 `VAE` 输出。\n\n```\ndef load_checkpoint(self, ckpt_name):\n    ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n    model, clip, vae = load_checkpoint(ckpt_path)\n    if vae is None:\n        # 这个错误信息比在后续节点中出现 \"'NoneType' has no attribute\" 错误更有用\n        vae = ExecutionBlocker(f\"No VAE contained in the loaded model {ckpt_name}\")\n    return (model, clip, vae)\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/wan-video",
  "markdown": "# ComfyUI Wan2.1 Video Examples - ComfyUI\n\nWan2.1 Video series is a video generation model open-sourced by Alibaba in February 2025 under the [Apache 2.0 license](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). It offers two versions:\n\n*   14B (14 billion parameters)\n*   1.3B (1.3 billion parameters) Covering multiple tasks including text-to-video (T2V) and image-to-video (I2V). The model not only outperforms existing open-source models in performance but more importantly, its lightweight version requires only 8GB of VRAM to run, significantly lowering the barrier to entry.\n\n*   [Wan2.1 Code Repository](https://github.com/Wan-Video/Wan2.1)\n*   [Wan2.1 Model Repository](https://huggingface.co/Wan-AI)\n\n## Wan2.1 ComfyUI Native Workflow Examples\n\n## Model Installation\n\nAll models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). Below are the common models you’ll need for the examples in this guide, which you can download in advance: Choose one version from **Text encoders** to download:\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\nFile storage locations:\n\n```\nComfyUI/\n├── models/\n│   ├── diffusion_models/\n│   ├── ...                  # Let's download the models in the corresponding workflow\n│   ├── text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── vae/\n│   │   └──  wan_2.1_vae.safetensors\n│   └── clip_vision/\n│       └──  clip_vision_h.safetensors   \n```\n\n## Wan2.1 Text-to-Video Workflow\n\nBefore starting the workflow, please download [wan2.1\\_t2v\\_1.3B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.\n\n> If you need other t2v precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow: ![Wan2.1 Text-to-Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_t2v_1.3b.webp)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg)\n\n1.  Make sure the `Load Diffusion Model` node has loaded the `wan2.1_t2v_1.3B_fp16.safetensors` model\n2.  Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model\n3.  Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model\n4.  (Optional) You can modify the video dimensions in the `EmptyHunyuanLatentVideo` node if needed\n5.  (Optional) If you need to modify the prompts (positive and negative), make changes in the `CLIP Text Encoder` node at number `5`\n6.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation\n\n## Wan2.1 Image-to-Video Workflow\n\n**Since Wan Video separates the 480P and 720P models**, we’ll need to provide examples for both resolutions in this guide. In addition to using different models, they also have slight parameter differences.\n\n### 480P Version\n\n#### 1\\. Workflow and Input Image\n\nDownload the image below and drag it into ComfyUI to load the corresponding workflow: ![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_480P.webp) We’ll use the following image as input: ![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/flux_dev_example.png)\n\n#### 2\\. Model Download\n\nPlease download [wan2.1\\_i2v\\_480p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.\n\n#### 3\\. Complete the Workflow Step by Step\n\n![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg)\n\n1.  Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_480p_14B_fp16.safetensors` model\n2.  Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model\n3.  Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model\n4.  Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model\n5.  Upload the provided input image in the `Load Image` node\n6.  (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node\n7.  (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed\n8.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation\n\n### 720P Version\n\n#### 1\\. Workflow and Input Image\n\nDownload the image below and drag it into ComfyUI to load the corresponding workflow: ![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_720P.webp) We’ll use the following image as input: ![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/magician.png)\n\n#### 2\\. Model Download\n\nPlease download [wan2.1\\_i2v\\_720p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.\n\n#### 3\\. Complete the Workflow Step by Step\n\n![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg)\n\n1.  Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_720p_14B_fp16.safetensors` model\n2.  Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model\n3.  Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model\n4.  Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model\n5.  Upload the provided input image in the `Load Image` node\n6.  (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node\n7.  (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed\n8.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/wan-ati",
  "markdown": "# Wan ATI ComfyUI Native Workflow Tutorial\n\n**ATI (Any Trajectory Instruction)** is a controllable video generation framework proposed by the ByteDance team. ATI is implemented based on Wan2.1 and supports unified control of objects, local regions, and camera motion in videos through arbitrary trajectory instructions. Project URL: [https://github.com/bytedance/ATI](https://github.com/bytedance/ATI)\n\n## Key Features\n\n*   **Unified Motion Control**: Supports trajectory control for multiple motion types including objects, local regions, and camera movements.\n*   **Interactive Trajectory Editor**: Visual tool that allows users to freely draw and edit motion trajectories on images.\n*   **Wan2.1 Compatible**: Based on the official Wan2.1 implementation, compatible with environments and model structures.\n*   **Rich Visualization Tools**: Supports visualization of input trajectories, output videos, and trajectory overlays.\n\n## WAN ATI Trajectory Control Workflow Example\n\n### 1\\. Workflow Download\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow\n\nWe will use the following image as input: ![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/ati/input.jpg) \n\n### 2\\. Model Download\n\nIf you haven’t successfully downloaded the model files from the workflow, you can try downloading them manually using the links below **Diffusion Model**\n\n*   [Wan2\\_1-I2V-ATI-14B\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**Text encoders** Chose one of following model\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**clip\\_vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)\n\nFile save location\n\n```\nComfyUI/\n├───📂 models/\n│   ├───📂 diffusion_models/\n│   │   └───Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors\n│   ├───📂 text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors # or other version\n│   ├───📂 clip_vision/\n│   │   └─── clip_vision_h.safetensors\n│   └───📂 vae/\n│       └──  wan_2.1_vae.safetensors\n```\n\n### 3\\. Complete the workflow execution step by step\n\n![Workflow step diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan_ati_guide.jpg) Please follow the numbered steps in the image to ensure smooth execution of the corresponding workflow\n\n1.  Ensure the `Load Diffusion Model` node has loaded the `Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors` model\n2.  Ensure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model\n3.  Ensure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model\n4.  Ensure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model\n5.  Upload the provided input image in the `Load Image` node\n6.  Trajectory editing: Currently there is no corresponding trajectory editor in ComfyUI yet, you can use the following link to complete trajectory editing\n    *   [Online Trajectory Editing Tool](https://comfyui-wiki.github.io/Trajectory-Annotation-Tool/)\n7.  If you need to modify the prompts (positive and negative), please make changes in the `CLIP Text Encoder` node numbered `5`\n8.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/fun-control",
  "markdown": "# ComfyUI Wan2.1 Fun Control Video Examples\n\n**Wan2.1-Fun-Control** is an open-source video generation and control project developed by Alibaba team. It introduces innovative Control Codes mechanisms combined with deep learning and multimodal conditional inputs to generate high-quality videos that conform to preset control conditions. The project focuses on precisely guiding generated video content through multimodal control conditions. Currently, the Fun Control model supports various control conditions, including **Canny (line art), Depth, OpenPose (human posture), MLSD (geometric edges), and trajectory control.** The model also supports multi-resolution video prediction with options for 512, 768, and 1024 resolutions at 16 frames per second, generating videos up to 81 frames (approximately 5 seconds) in length. Model versions:\n\n*   **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**\n*   **14B** High-performance: Model size reaches 32GB+, offering better results but **requiring higher VRAM**\n\nHere are the relevant code repositories:\n\n*   [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)\n*   [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)\n*   Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)\n\nComfyUI now **natively supports** the Wan2.1 Fun Control model. Before starting this tutorial, please update your ComfyUI to ensure you’re using a version after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82). In this guide, we’ll provide two workflows:\n\n1.  A workflow using only native Comfy Core nodes\n2.  A workflow using custom nodes\n\n## Model Installation\n\nYou only need to install these models once. The workflow images also contain model download information, so you can choose your preferred download method. The following models can be found at [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) Click the corresponding links to download. If you’ve used Wan-related workflows before, you only need to download the **Diffusion models**. **Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:\n\n*   [wan2.1\\_fun\\_control\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)\n*   [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-Control.safetensors` after downloading\n\n**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\nFile storage location:\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └── wan2.1_fun_control_1.3B_bf16.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── 📂 vae/\n│   │   └── wan_2.1_vae.safetensors\n│   └── 📂 clip_vision/\n│       └──  clip_vision_h.safetensors                 \n```\n\n## ComfyUI Native Workflow\n\nIn this workflow, we use videos converted to **WebP format** since the `Load Image` node doesn’t currently support mp4 format. We also use **Canny Edge** to preprocess the original video. Because many users encounter installation failures and environment issues when installing custom nodes, this version of the workflow uses only native nodes to ensure a smoother experience. Thanks to our powerful ComfyUI authors who provide feature-rich nodes. If you want to directly check the related version, see [Workflow Using Custom Nodes](#workflow-using-custom-nodes).\n\n### 1\\. Workflow File Download\n\n#### 1.1 Workflow File\n\nDownload the image below and drag it into ComfyUI to load the workflow: ![Wan2.1 Fun Control Native Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_native.webp)\n\n#### 1.2 Input Images and Videos Download\n\nPlease download the following image and video for input: ![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_remix.png) ![Input Reference Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_video.webp)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Wan2.1 Fun Control Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_native_flow_diagram.png)\n\n1.  Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Load Image` node (renamed to `Start_image`)\n6.  Upload the control video to the second `Load Image` node. Note: This node currently doesn’t support mp4, only WebP videos\n7.  (Optional) Modify the prompt (both English and Chinese are supported)\n8.  (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions\n9.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n\n### 3\\. Usage Notes\n\n*   Since we need to input the same number of frames as the control video into the `WanFunControlToVideo` node, if the specified frame count exceeds the actual control video frames, the excess frames may display scenes not conforming to control conditions. We’ll address this issue in the [Workflow Using Custom Nodes](#workflow-using-custom-nodes)\n*   Avoid setting overly large dimensions, as this can make the sampling process very time-consuming. Try generating smaller images first, then upscale\n*   Use your imagination to build upon this workflow by adding text-to-image or other types of workflows to achieve direct text-to-video generation or style transfer\n*   Use tools like [ComfyUI-comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) for richer control options\n\n## Workflow Using Custom Nodes\n\nWe’ll need to install the following two custom nodes:\n\n*   [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n*   [ComfyUI-comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\nYou can use [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to install missing nodes or follow the installation instructions for each custom node package.\n\n### 1\\. Workflow File Download\n\n#### 1.1 Workflow File\n\nDownload the image below and drag it into ComfyUI to load the workflow: ![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.webp)\n\n#### 1.2 Input Images and Videos Download\n\nPlease download the following image and video for input: ![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-robot's_eye.png) \n\n### 2\\. Complete the Workflow Step by Step\n\n![Wan2.1 Fun Control Workflow Using Custom Nodes Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png)\n\n> The model part is essentially the same. If you’ve already experienced the native-only workflow, you can directly upload the corresponding images and run it.\n\n1.  Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Load Image` node\n6.  Upload an mp4 format video to the `Load Video(Upload)` custom node. Note that the workflow has adjusted the default `frame_load_cap`\n7.  For the current image, the `DWPose Estimator` only uses the `detect_face` option\n8.  (Optional) Modify the prompt (both English and Chinese are supported)\n9.  (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions\n10.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n\n### 3\\. Workflow Notes\n\nThanks to the ComfyUI community authors for their custom node packages:\n\n*   This example uses `Load Video(Upload)` to support mp4 videos\n*   The `video_info` obtained from `Load Video(Upload)` allows us to maintain the same `fps` for the output video\n*   You can replace `DWPose Estimator` with other preprocessors from the `ComfyUI-comfyui_controlnet_aux` node package\n*   Prompts support multiple languages\n\n## Usage Tips\n\n![Apply Multi Control Videos](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/apply_multi_control_videos.jpg)\n\n*   A useful tip is that you can combine multiple image preprocessing techniques and then use the `Image Blend` node to achieve the goal of applying multiple control methods simultaneously.\n*   You can use the `Video Combine` node from `ComfyUI-VideoHelperSuite` to save videos in mp4 format\n*   We use `SaveAnimatedWEBP` because we currently don’t support embedding workflow into **mp4** and some other custom nodes may not support embedding workflow too. To preserve the workflow in the video, we choose `SaveAnimatedWEBP` node.\n*   In the `WanFunControlToVideo` node, `control_video` is not mandatory, so sometimes you can skip using a control video, first generate a very small video size like 320x320, and then use them as control video input to achieve consistent results.\n*   [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n*   [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/fun-inp",
  "markdown": "# ComfyUI Wan2.1 Fun InP Video Examples\n\n## About Wan2.1-Fun-InP\n\n**Wan-Fun InP** is an open-source video generation model released by Alibaba, part of the Wan2.1-Fun series, focusing on generating videos from images with first and last frame control. **Key features**:\n\n*   **First and last frame control**: Supports inputting both first and last frame images to generate transitional video between them, enhancing video coherence and creative freedom. Compared to earlier community versions, Alibaba’s official model produces more stable and significantly higher quality results.\n*   **Multi-resolution support**: Supports generating videos at 512×512, 768×768, 1024×1024 and other resolutions to accommodate different scenario requirements.\n\n**Model versions**:\n\n*   **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**\n*   **14B** High-performance: Model size reaches 32GB+, offering better results but requiring **higher VRAM**\n\nBelow are the relevant model weights and code repositories:\n\n*   [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)\n*   [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)\n*   Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)\n\n## Wan2.1 Fun InP Workflow\n\nDownload the image below and drag it into ComfyUI to load the workflow: ![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_inp/wan2.1_fun_inp.webp)\n\n### 1\\. Workflow File Download\n\n### 2\\. Manual Model Installation\n\nIf automatic model downloading is ineffective, please download the models manually and save them to the corresponding folders. The following models can be found at [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) **Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:\n\n*   [wan2.1\\_fun\\_inp\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)\n*   [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-InP.safetensors` after downloading\n\n**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\nFile storage location:\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └── wan2.1_fun_inp_1.3B_bf16.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── 📂 vae/\n│   │   └── wan_2.1_vae.safetensors\n│   └── 📂 clip_vision/\n│       └──  clip_vision_h.safetensors                 \n```\n\n### 3\\. Complete the Workflow Step by Step\n\n![ComfyUI Wan2.1 Fun InP Video Generation Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_inp_flow_diagram.png)\n\n1.  Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_inp_1.3B_bf16.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Load Image` node (renamed to `Start_image`)\n6.  Upload the ending frame to the second `Load Image` node\n7.  (Optional) Modify the prompt (both English and Chinese are supported)\n8.  (Optional) Adjust the video size in `WanFunInpaintToVideo`, avoiding overly large dimensions\n9.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n\n### 4\\. Workflow Notes\n\n*   When using Wan Fun InP, you may need to frequently modify prompts to ensure the accuracy of the corresponding scene transitions.\n\n*   [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n*   [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n*   [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/wan-flf",
  "markdown": "# ComfyUI Wan2.1 FLF2V Native Example\n\nWan FLF2V (First-Last Frame Video Generation) is an open-source video generation model developed by the Alibaba Tongyi Wanxiang team. Its open-source license is [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). Users only need to provide two images as the starting and ending frames, and the model automatically generates intermediate transition frames, outputting a logically coherent and naturally flowing 720p high-definition video. **Core Technical Highlights**\n\n1.  **Precise First-Last Frame Control**: The matching rate of first and last frames reaches 98%, defining video boundaries through starting and ending scenes, intelligently filling intermediate dynamic changes to achieve scene transitions and object morphing effects.\n2.  **Stable and Smooth Video Generation**: Using CLIP semantic features and cross-attention mechanisms, the video jitter rate is reduced by 37% compared to similar models, ensuring natural and smooth transitions.\n3.  **Multi-functional Creative Capabilities**: Supports dynamic embedding of Chinese and English subtitles, generation of anime/realistic/fantasy and other styles, adapting to different creative needs.\n4.  **720p HD Output**: Directly generates 1280×720 resolution videos without post-processing, suitable for social media and commercial applications.\n5.  **Open-source Ecosystem Support**: Model weights, code, and training framework are fully open-sourced, supporting deployment on mainstream AI platforms.\n\n**Technical Principles and Architecture**\n\n1.  **DiT Architecture**: Based on diffusion models and Diffusion Transformer architecture, combined with Full Attention mechanism to optimize spatiotemporal dependency modeling, ensuring video coherence.\n2.  **3D Causal Variational Encoder**: Wan-VAE technology compresses HD frames to 1/128 size while retaining subtle dynamic details, significantly reducing memory requirements.\n3.  **Three-stage Training Strategy**: Starting from 480P resolution pre-training, gradually upgrading to 720P, balancing generation quality and computational efficiency through phased optimization.\n\n**Related Links**\n\n*   **GitHub Repository**: [GitHub](https://github.com/Wan-Video/Wan2.1)\n*   **Hugging Face Model Page**: [Hugging Face](https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P)\n*   **ModelScope Community**: [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)\n\n## Wan2.1 FLF2V 720P ComfyUI Native Workflow Example\n\n### 1\\. Download Workflow Files and Related Input Files\n\nPlease download the WebP file below, and drag it into ComfyUI to load the corresponding workflow. The workflow has embedded the corresponding model download file information. ![Wan2.1 FLF2V 720P f16 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/wan2.1_flf2v_720_f16.webp) Please download the two images below, which we will use as the starting and ending frames of the video ![start_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/start_image.png) ![end_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/end_image.png)\n\n### 2\\. Manual Model Installation\n\nIf corresponding All models involved in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). **diffusion\\_models** Choose one version based on your hardware conditions\n\n*   FP16:[wan2.1\\_flf2v\\_720p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp16.safetensors?download=true)\n*   FP8:[wan2.1\\_flf2v\\_720p\\_14B\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors)\n\nChoose one version from **Text encoders** for download,\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\nFile Storage Location\n\n```\nComfyUI/\n├── models/\n│   ├── diffusion_models/\n│   │   └─── wan2.1_flf2v_720p_14B_fp16.safetensors           # or FP8 version\n│   ├── text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors           # or your chosen version\n│   ├── vae/\n│   │   └──  wan_2.1_vae.safetensors\n│   └── clip_vision/\n│       └──  clip_vision_h.safetensors   \n```\n\n### 3\\. Complete Workflow Execution Step by Step\n\n![Wan2.1 FLF2V 720P Native Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg)\n\n1.  Ensure the `Load Diffusion Model` node has loaded `wan2.1_flf2v_720p_14B_fp16.safetensors` or `wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Start_image` node\n6.  Upload the ending frame to the `End_image` node\n7.  (Optional) Modify the positive and negative prompts, both Chinese and English are supported\n8.  (**Important**) In `WanFirstLastFrameToVideo` we use 720_1280 as default size.because it’s a 720P model, so using a small size will not yield good output. Please use size around 720_1280 for good generation.\n9.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/overview",
  "markdown": "# ComfyUI 内置节点 - ComfyUI\n\n内置节点是 ComfyUI 的默认节点，它们是 ComfyUI 的核心功能,你无需额外安装第三方自定义节点包，就可以使用的节点。\n\n## 节点文档更新说明\n\n我们目前已经支持了内置的节点帮助文档，所以此部分的文档内容是定期从 [这个仓库](https://github.com/Comfy-Org/embedded-docs) 中同步过来的，目前我们会每周进行一次人工同步和内容更新。\n\n## 贡献内容\n\n如果你发现我们的内容错误，或者想要补充我们缺失的内容，请在 [这个仓库](https://github.com/Comfy-Org/embedded-docs) 中提交 issue 或 pr 来帮助我们改进。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_about_panel_badges",
  "markdown": "# 关于面板徽章 - ComfyUI\n\n关于面板徽章 API 允许扩展为 ComfyUI 的关于页面添加自定义徽章。这些徽章可以显示有关你的扩展的信息，并包含指向文档、源代码或其他资源的链接。\n\n## 基本用法\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  aboutPageBadges: [\n    {\n      label: \"Documentation\",\n      url: \"https://example.com/docs\",\n      icon: \"pi pi-file\"\n    },\n    {\n      label: \"GitHub\",\n      url: \"https://github.com/username/repo\",\n      icon: \"pi pi-github\"\n    }\n  ]\n});\n```\n\n## 徽章配置\n\n每个徽章都需要以下所有属性：\n\n```\n{\n  label: string,           // 徽章上显示的文本\n  url: string,             // 点击徽章时打开的 URL\n  icon: string             // 图标类名（例如 PrimeVue 图标）\n}\n```\n\n## 图标选项\n\n徽章图标使用 PrimeVue 的图标集。以下是一些常用图标：\n\n*   文档：`pi pi-file` 或 `pi pi-book`\n*   GitHub：`pi pi-github`\n*   外部链接：`pi pi-external-link`\n*   信息：`pi pi-info-circle`\n*   下载：`pi pi-download`\n*   网站：`pi pi-globe`\n*   Discord：`pi pi-discord`\n\n完整的可用图标列表请参考 [PrimeVue 图标文档](https://primevue.org/icons/)。\n\n## 示例\n\n```\napp.registerExtension({\n  name: \"BadgeExample\",\n  aboutPageBadges: [\n    {\n      label: \"Website\",\n      url: \"https://example.com\",\n      icon: \"pi pi-home\"\n    },\n    {\n      label: \"Donate\",\n      url: \"https://example.com/donate\",\n      icon: \"pi pi-heart\"\n    },\n    {\n      label: \"Documentation\",\n      url: \"https://example.com/docs\",\n      icon: \"pi pi-book\"\n    }\n  ]\n});\n```\n\n徽章会显示在设置对话框的关于面板中，可以通过 ComfyUI 界面右上角的齿轮图标进入。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_bottom_panel_tabs",
  "markdown": "# 底部面板标签页 - ComfyUI\n\n底部面板标签页 API 允许扩展为 ComfyUI 界面的底部面板添加自定义标签页。这对于添加日志、调试工具或自定义面板等功能非常有用。\n\n## 基本用法\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  bottomPanelTabs: [\n    {\n      id: \"customTab\",\n      title: \"Custom Tab\",\n      type: \"custom\",\n      render: (el) => {\n        el.innerHTML = '<div>这是我的自定义标签页内容</div>';\n      }\n    }\n  ]\n});\n```\n\n## 标签页配置\n\n每个标签页都需要 `id`、`title` 和 `type`，以及一个渲染函数：\n\n```\n{\n  id: string,              // 标签页的唯一标识符\n  title: string,           // 标签页上显示的标题\n  type: string,            // 标签页类型（通常为 \"custom\"）\n  icon?: string,           // 图标类名（可选）\n  render: (element) => void // 用于填充标签页内容的函数\n}\n```\n\n`render` 函数会接收一个 DOM 元素，你应在其中插入标签页的内容。\n\n## 交互元素\n\n你可以添加如按钮等交互元素：\n\n```\napp.registerExtension({\n  name: \"InteractiveTabExample\",\n  bottomPanelTabs: [\n    {\n      id: \"controlsTab\",\n      title: \"Controls\",\n      type: \"custom\",\n      render: (el) => {\n        el.innerHTML = `\n          <div style=\"padding: 10px;\">\n            <button id=\"runBtn\">运行工作流</button>\n          </div>\n        `;\n        \n        // 添加事件监听器\n        el.querySelector('#runBtn').addEventListener('click', () => {\n          app.queuePrompt();\n        });\n      }\n    }\n  ]\n});\n```\n\n## 使用 React 组件\n\n你可以在底部面板标签页中挂载 React 组件：\n\n```\n// 在你的扩展中引入 React 依赖\nimport React from \"react\";\nimport ReactDOM from \"react-dom/client\";\n\n// 简单的 React 组件\nfunction TabContent() {\n  const [count, setCount] = React.useState(0);\n  \n  return (\n    <div style={{ padding: \"10px\" }}>\n      <h3>React 组件</h3>\n      <p>计数：{count}</p>\n      <button onClick={() => setCount(count + 1)}>递增</button>\n    </div>\n  );\n}\n\n// 注册带有 React 内容的扩展\napp.registerExtension({\n  name: \"ReactTabExample\",\n  bottomPanelTabs: [\n    {\n      id: \"reactTab\",\n      title: \"React Tab\",\n      type: \"custom\",\n      render: (el) => {\n        const container = document.createElement(\"div\");\n        container.id = \"react-tab-container\";\n        el.appendChild(container);\n        \n        // 挂载 React 组件\n        ReactDOM.createRoot(container).render(\n          <React.StrictMode>\n            <TabContent />\n          </React.StrictMode>\n        );\n      }\n    }\n  ]\n});\n```\n\n## 独立注册\n\n你也可以在 `registerExtension` 之外注册标签页：\n\n```\napp.extensionManager.registerBottomPanelTab({\n  id: \"standAloneTab\",\n  title: \"Stand-Alone Tab\",\n  type: \"custom\",\n  render: (el) => {\n    el.innerHTML = '<div>此标签页是独立注册的</div>';\n  }\n});\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_commands_keybindings",
  "markdown": "# 命令与快捷键绑定 - ComfyUI\n\n命令与快捷键绑定 API 允许扩展注册自定义命令，并将其与键盘快捷键关联。这样用户可以无需鼠标，快速触发操作。\n\n## 基本用法\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  // 注册命令\n  commands: [\n    {\n      id: \"myCommand\",\n      label: \"我的命令\",\n      function: () => {\n        console.log(\"命令已执行！\");\n      }\n    }\n  ],\n  // 将快捷键与命令关联\n  keybindings: [\n    {\n      combo: { key: \"k\", ctrl: true },\n      commandId: \"myCommand\"\n    }\n  ]\n});\n```\n\n## 命令配置\n\n每个命令都需要 `id`、`label` 和 `function`：\n\n```\n{\n  id: string,              // 命令的唯一标识符\n  label: string,           // 命令显示名称\n  function: () => void     // 命令被触发时执行的函数\n}\n```\n\n## 快捷键配置\n\n每个快捷键都需要 `combo` 和 `commandId`：\n\n```\n{\n  combo: {                 // 按键组合\n    key: string,           // 主键（单个字符或特殊按键）\n    ctrl?: boolean,        // 是否需要 Ctrl 键（可选）\n    shift?: boolean,       // 是否需要 Shift 键（可选）\n    alt?: boolean,         // 是否需要 Alt 键（可选）\n    meta?: boolean         // 是否需要 Meta/Command 键（可选）\n  },\n  commandId: string        // 要触发的命令 ID\n}\n```\n\n### 特殊按键\n\n对于非字符按键，请使用以下值之一：\n\n*   方向键：`\"ArrowUp\"`、`\"ArrowDown\"`、`\"ArrowLeft\"`、`\"ArrowRight\"`\n*   功能键：`\"F1\"` 到 `\"F12\"`\n*   其他特殊按键：`\"Escape\"`、`\"Tab\"`、`\"Enter\"`、`\"Backspace\"`、`\"Delete\"`、`\"Home\"`、`\"End\"`、`\"PageUp\"`、`\"PageDown\"`\n\n## 命令示例\n\n```\napp.registerExtension({\n  name: \"CommandExamples\",\n  commands: [\n    {\n      id: \"runWorkflow\",\n      label: \"运行工作流\",\n      function: () => {\n        app.queuePrompt();\n      }\n    },\n    {\n      id: \"clearWorkflow\",\n      label: \"清空工作流\",\n      function: () => {\n        if (confirm(\"确定要清空工作流吗？\")) {\n          app.graph.clear();\n        }\n      }\n    },\n    {\n      id: \"saveWorkflow\",\n      label: \"保存工作流\",\n      function: () => {\n        app.graphToPrompt().then(workflow => {\n          const blob = new Blob([JSON.stringify(workflow)], {type: \"application/json\"});\n          const url = URL.createObjectURL(blob);\n          const a = document.createElement(\"a\");\n          a.href = url;\n          a.download = \"workflow.json\";\n          a.click();\n          URL.revokeObjectURL(url);\n        });\n      }\n    }\n  ]\n});\n```\n\n## 快捷键示例\n\n```\napp.registerExtension({\n  name: \"KeybindingExamples\",\n  commands: [\n    /* 上述命令定义 */\n  ],\n  keybindings: [\n    // Ctrl+R 运行工作流\n    {\n      combo: { key: \"r\", ctrl: true },\n      commandId: \"runWorkflow\"\n    },\n    // Ctrl+Shift+C 清空工作流\n    {\n      combo: { key: \"c\", ctrl: true, shift: true },\n      commandId: \"clearWorkflow\"\n    },\n    // Ctrl+S 保存工作流\n    {\n      combo: { key: \"s\", ctrl: true },\n      commandId: \"saveWorkflow\"\n    },\n    // F5 运行工作流（备用）\n    {\n      combo: { key: \"F5\" },\n      commandId: \"runWorkflow\"\n    }\n  ]\n});\n```\n\n## 注意事项与限制\n\n*   ComfyUI 核心中定义的快捷键无法被扩展覆盖。请在以下源码文件中查看核心快捷键：\n    *   [核心命令](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/composables/useCoreCommands.ts)\n    *   [核心菜单命令](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreMenuCommands.ts)\n    *   [核心快捷键](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreKeybindings.ts)\n    *   [保留按键组合](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/reservedKeyCombos.ts)\n*   某些按键组合被浏览器保留（如 Ctrl+F 用于搜索），无法被覆盖\n*   如果多个扩展注册了相同的快捷键，行为未定义"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_dialog",
  "markdown": "# 对话框 API - ComfyUI\n\n对话框 API 提供了在桌面端和 Web 环境下都能一致工作的标准化对话框。扩展作者会发现 prompt 和 confirm 方法最为实用。\n\n## 基本用法\n\n### 输入对话框（Prompt Dialog）\n\n```\n// 显示一个输入对话框\napp.extensionManager.dialog.prompt({\n  title: \"用户输入\",\n  message: \"请输入你的姓名：\",\n  defaultValue: \"User\"\n}).then(result => {\n  if (result !== null) {\n    console.log(`输入内容: ${result}`);\n  }\n});\n```\n\n### 确认对话框（Confirm Dialog）\n\n```\n// 显示一个确认对话框\napp.extensionManager.dialog.confirm({\n  title: \"确认操作\",\n  message: \"你确定要继续吗？\",\n  type: \"default\"\n}).then(result => {\n  console.log(result ? \"用户已确认\" : \"用户已取消\");\n});\n```\n\n### Prompt\n\n```\napp.extensionManager.dialog.prompt({\n  title: string,             // 对话框标题\n  message: string,           // 显示的消息/问题\n  defaultValue?: string      // 输入框的初始值（可选）\n}).then((result: string | null) => {\n  // result 是输入的文本，若取消则为 null\n});\n```\n\n### Confirm\n\n```\napp.extensionManager.dialog.confirm({\n  title: string,             // 对话框标题\n  message: string,           // 显示的消息\n  type?: \"default\" | \"overwrite\" | \"delete\" | \"dirtyClose\" | \"reinstall\", // 对话框类型（可选）\n  itemList?: string[],       // 要显示的项目列表（可选）\n  hint?: string              // 显示的提示文本（可选）\n}).then((result: boolean | null) => {\n  // result 为 true 表示确认，false 表示拒绝，null 表示取消\n});\n```\n\n如需了解 ComfyUI 中其他专用对话框，扩展作者可参考源码中的 `dialogService.ts` 文件。"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Finterface%2Fsettings%2Fmask-editor",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_examples",
  "markdown": "# 带注释的示例 - ComfyUI\n\n不断增长的示例代码片段集合……\n\n## 右键菜单\n\n### 背景菜单\n\n主背景菜单（在画布上右键）是通过调用 `LGraph.getCanvasMenuOptions` 生成的。添加自定义菜单选项的一种方式是劫持这个调用：\n\n```\n/* 在 setup() 中 */\n    const original_getCanvasMenuOptions = LGraphCanvas.prototype.getCanvasMenuOptions;\n    LGraphCanvas.prototype.getCanvasMenuOptions = function () {\n        // 获取基础选项\n        const options = original_getCanvasMenuOptions.apply(this, arguments);\n        options.push(null); // 插入分隔线\n        options.push({\n            content: \"菜单的文本\",\n            callback: async () => {\n                // 执行任意操作\n            }\n        })\n        return options;\n    }\n```\n\n### 节点菜单\n\n当你在节点上右键时，菜单同样是通过 `node.getExtraMenuOptions` 生成的。但这次不是返回一个 options 对象，而是将其作为参数传入……\n\n```\n/* 在 beforeRegisterNodeDef() 中 */\nif (nodeType?.comfyClass==\"MyNodeClass\") { \n    const original_getExtraMenuOptions = nodeType.prototype.getExtraMenuOptions;\n    nodeType.prototype.getExtraMenuOptions = function(_, options) {\n        original_getExtraMenuOptions?.apply(this, arguments);\n        options.push({\n            content: \"做点有趣的事\",\n            callback: async () => {\n                // 有趣的操作\n            }\n        })\n    }   \n}\n```\n\n### 子菜单\n\n如果你想要子菜单，可以提供一个回调，使用 `LiteGraph.ContextMenu` 创建它：\n\n```\nfunction make_submenu(value, options, e, menu, node) {\n    const submenu = new LiteGraph.ContextMenu(\n        [\"选项 1\", \"选项 2\", \"选项 3\"],\n        { \n            event: e, \n            callback: function (v) { \n                // 用 v (==\"选项 x\") 做点什么\n            }, \n            parentMenu: menu, \n            node:node\n        }\n    )\n}\n\n/* ... */\n    options.push(\n        {\n            content: \"带选项的菜单\",\n            has_submenu: true,\n            callback: make_submenu,\n        }\n    )\n```\n\n## 捕获 UI 事件\n\n这和你预期的一样——在 DOM 中找到 UI 元素并添加 eventListener。`setup()` 是做这件事的好地方，因为此时页面已完全加载。例如，检测”队列”按钮的点击：\n\n```\nfunction queue_button_pressed() { console.log(\"队列按钮被按下！\") }\ndocument.getElementById(\"queue-button\").addEventListener(\"click\", queue_button_pressed);\n```\n\n## 检测工作流开始\n\n这是众多 `api` 事件之一：\n\n```\nimport { api } from \"../../scripts/api.js\";\n/* 在 setup() 中 */\n    function on_execution_start() { \n        /* 执行任意操作 */\n    }\n    api.addEventListener(\"execution_start\", on_execution_start);\n```\n\n## 检测工作流被中断\n\n这是一个劫持 api 的简单例子：\n\n```\nimport { api } from \"../../scripts/api.js\";\n/* 在 setup() 中 */\n    const original_api_interrupt = api.interrupt;\n    api.interrupt = function () {\n        /* 在调用原方法前做点什么 */\n        original_api_interrupt.apply(this, arguments);\n        /* 或者在之后 */\n    }\n```\n\n## 捕获节点点击\n\n`node` 有一个 mouseDown 方法可以被劫持。 这次我们注意传递任何返回值。\n\n```\nasync nodeCreated(node) {\n    if (node?.comfyClass === \"My Node Name\") {\n        const original_onMouseDown = node.onMouseDown;\n        node.onMouseDown = function( e, pos, canvas ) {\n            alert(\"哎呦！\");\n            return original_onMouseDown?.apply(this, arguments);\n        }        \n    }\n}\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftutorials%2Fimage%2Fcosmos%2Fcosmos-predict2-t2i",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncode",
  "markdown": "# CLIP文本编码 - ComfyUI 原生节点文档 - ComfyUI\n\n`CLIP文本编码 (CLIPTextEncode)` 这个节点就像一位翻译官，它能将你用文字描述的创作想法转换成AI能够理解的特殊”语言”，帮助AI准确理解你想要创作的图像内容。 想象你在和一位外国画家沟通，你需要一位翻译来帮助你准确传达你想要的画作效果。这个节点就像那位翻译，它使用CLIP模型（一个经过大量图文训练的AI模型）来理解你的文字描述，并将其转换成AI绘画模型能理解的”语言指令”。\n\n## 输入\n\n| 参数名称 | 数据类型 | 输入方式 | 默认值 | 取值范围 | 功能说明 |\n| --- | --- | --- | --- | --- | --- |\n| text | STRING | 文本输入框 | 空   | 任意文本 | 就像给画家的详细说明，在这里输入你想要生成的图像的文字描述。支持多行文本，可以非常详细地描述你想要的效果。 |\n| clip | CLIP | 模型选择 | 无   | 已加载的CLIP模型 | 相当于选择特定的翻译官，不同的CLIP模型就像不同的翻译官，他们对艺术风格的理解略有不同。 |\n\n## 输出\n\n| 输出名称 | 数据类型 | 说明  |\n| --- | --- | --- |\n| 条件  | CONDITIONING | 这是转换后的”绘画指令”，包含了AI能够理解的详细创作指引。这些指令会告诉AI模型应该如何绘制符合你描述的图像。 |\n\n## 使用建议\n\n1.  **文本提示的基本用法**\n    *   可以像写作文一样详细描述你想要的图像\n    *   越具体的描述，生成的图像越符合预期\n    *   可以使用英文逗号分隔不同的描述要素\n2.  **特殊功能：使用Embedding模型**\n    *   Embedding模型就像预设的艺术风格包，可以快速应用特定的艺术效果\n    *   目前支持 .safetensors、.pt、.bin 这三种文件格式，你不一定需要在使用的时候用完整的模型名称\n    *   使用方法：\n        \n        1.  将embedding模型文件(.pt格式)放入`ComfyUI/models/embeddings`文件夹\n        2.  在文本中使用`embedding:模型名称`来调用 例如：如果你有一个叫`EasyNegative.pt`的模型，可以这样使用：\n        \n        ```\n        a beautiful landscape, embedding:EasyNegative, high quality\n        ```\n        \n3.  **提示词权重调整**\n    *   可以用括号来调整某些描述的重要程度\n    *   例如：`(beautiful:1.2)`会让”beautiful”这个特征更突出\n    *   普通括号`()`的默认权重是1.1\n    *   使用键盘快捷键 `ctrl + 上/下方向键` 头可以快速调整权重\n    *   对应权重快速调整步长可以在设置中进行修改\n4.  **注意事项**\n    *   确保CLIP模型已正确加载\n    *   文本描述尽量使用正面、明确的词语\n    *   如果使用embedding模型，确保文件名称输入正确并且和当前主模型的架构吻合"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipVisionEncode",
  "markdown": "# CLIP视觉编码 - ComfyUI内置节点文档 - ComfyUI\n\n`CLIP视觉编码` 节点是 ComfyUI 中的图像编码节点，用于将输入图像通过 CLIP Vision 模型转换为视觉特征向量。该节点是连接图像和文本理解的重要桥梁，广泛用于各种 AI 图像生成和处理工作流中。 **节点功能**\n\n*   **图像特征提取**：将输入图像转换为高维特征向量\n*   **多模态桥接**：为图像和文本的联合处理提供基础\n*   **条件生成**：为基于图像的条件生成提供视觉条件\n\n## 输入参数\n\n| 参数名 | 类型  | 说明  |\n| --- | --- | --- |\n| `clip视觉` | CLIP\\_VISION | CLIP视觉模型，通常通过 CLIPVisionLoader 节点加载 |\n| `图像` | IMAGE | 需要编码的输入图像 |\n| `裁剪` | 下拉选择 | 图像裁剪方式，可选值：center（居中裁剪）、none（不裁剪） |\n\n## 输出参数\n\n| 参数名 | 类型  | 说明  |\n| --- | --- | --- |\n| CLIP视觉输出 | CLIP\\_VISION\\_OUTPUT | 编码后的视觉特征 |\n\n这个输出对象包含:\n\n*   `last_hidden_state`: 最后一层的隐藏状态\n*   `image_embeds`: 图像嵌入向量\n*   `penultimate_hidden_states`: 倒数第二层的隐藏状态\n*   `mm_projected`: 多模态投影结果（如果可用）"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipVisionLoader",
  "markdown": "# 加载CLIP视觉 - ComfyUI内置节点文档 - ComfyUI\n\n该节点会检测位于 `ComfyUI/models/clip_vision` 文件夹下的模型，同时也会读取在 `extra_model_paths.yaml` 文件中配置的额外路径的模型，如果你的模型是在 ComfyUI 启动后才添加的，请 **刷新 ComfyUI 界面** 保证前端能够获取到最新的模型文件列表\n\n## 输入\n\n| 参数名称 | 数据类型 | 作用  |\n| --- | --- | --- |\n| `clip名称` | COMBO\\[STRING\\] | 会获取`ComfyUI/models/clip_vision` 文件夹下受支持格式的模型文件列表 |\n\n## 输出\n\n| 参数名称 | 数据类型 | 作用  |\n| --- | --- | --- |\n| `CLIP视觉` | CLIP\\_VISION | 加载的CLIP视觉模型，准备用于编码图像或执行其他视觉相关任务。 |"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/Load3D",
  "markdown": "# 加载3D - ComfyUI内置节点文档 - ComfyUI\n\nLoad3D 节点用于加载和处理 3D 模型文件的核心节点，在加载节点时会自动获取 `ComfyUI/input/3d/` 可用的 3D 资源，你也可以通过上传功能将受支持的 3D 文件上传然后进行预览。 **支持格式** 目前该节点支持多种 3D 文件格式，包括 `.gltf`、`.glb`、`.obj`、`.fbx` 和 `.stl`。 **3D 节点预设** 3D 节点的一些相关偏好设置可以在 ComfyUI 的设置菜单中进行设置，请参考下面的文档了解对应的设置 [设置菜单 - 3D](https://docs.comfy.org/zh-CN/interface/settings/3d) 除了常规的节点输出之外， Load3D 有许多相关的 3D 视图相关操作是位于预览区域菜单, 3D 节点\n\n## 输入\n\n| 参数名 | 类型  | 描述  | 默认值 | 范围  |\n| --- | --- | --- | --- | --- |\n| 模型文件 | 文件选择 | 3D 模型文件路径，支持上传，默认读取 `ComfyUI/input/3d/` 下的模型文件 | \\-  | 受支持格式文件 |\n| 宽度  | INT | 画布渲染宽度 | 1024 | 1-4096 |\n| 高度  | INT | 画布渲染高度 | 1024 | 1-4096 |\n\n## 输出\n\n| 参数名称 | 数据类型 | 说明  |\n| --- | --- | --- |\n| image | IMAGE | 画布渲染渲染图像 |\n| mask | MASK | 包含当前模型位置的遮罩 |\n| mesh\\_path | STRING | 模型文件路径在`ComfyUI/input` 文件夹下的路径 |\n| normal | IMAGE | 法线贴图 |\n| lineart | IMAGE | 线稿图像输出，对应的 `edge_threshold` 可在画布的模型菜单中进行调节 |\n| camera\\_info | LOAD3D\\_CAMERA | 相机信息 |\n| recording\\_video | VIDEO | 录制视频（仅当有录制视频存在时） |\n\n对应所有的输出预览如下： ![视图操作演示](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/load3d_outputs.jpg) \n\n## 模型画布(Canvas)区说明\n\nLoad 3D 节点的 Canvas 区域包含了诸多的视图操作，包括：\n\n*   预览视图设置（网格、背景色、预览视图）\n*   相机控制: 控制FOV、相机类型\n*   全局光照强度: 调节光照强度\n*   视频录制: 录制视频并导出视频\n*   模型导出: 支持`GLB`、`OBJ`、`STL` 格式\n*   等\n\n![Load 3D 节点UI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/load3d_ui.jpg)\n\n1.  包含了 Load 3D 节点的多个菜单以及隐藏菜单\n2.  重新`缩放预览窗口大小`以及进行`画布视频录制`菜单\n3.  3D 视图操作轴\n4.  预览缩略图\n5.  预览尺寸设置，通过设置尺寸然后再缩放窗口大小来缩放预览视图显示\n\n### 1\\. 视图操作\n\n视图控制操作：\n\n*   鼠标左键点击 + 拖拽： 视图旋转控制\n*   鼠标右键 + 拖拽： 平移视图\n*   鼠标中键： 缩放控制\n*   坐标轴： 切换视图\n\n### 2\\. 左侧菜单功能\n\n![Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu.jpg) 在预览区域，有些视图操作相关的菜单被隐藏在了菜单里，点击菜单按钮可以展开对应不同的菜单\n\n*   1.  场景（Scene）: 包含预览窗口网格、背景色、缩略图设置\n*   2.  模型（Model）: 模型渲染模式、纹理材质、上方向设置\n*   3.  摄像机（Camera）: 轴测视图和透视视图切换、透视视角大小设置\n*   4.  灯光（Light）: 场景全局光照强度\n*   5.  导出（Export）: 导出模型为其它格式（GLB、OBJ、STL）\n\n#### 场景（Scene）\n\n![scene menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_scene.jpg) 场景菜单提供了对场景的一些基础设置功能\n\n1.  显示 / 隐藏网格\n2.  设置背景色\n3.  点击上传设置背景图片\n4.  隐藏预览图\n\n#### 模型(Model)\n\n![Menu_Scene](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_model.jpg) 模型菜单提供了一些模型的相关功能\n\n1.  **上方向(Up direction)**: 确定模型的哪个轴为上方向\n2.  **渲染模式（Material mode）**: 模型渲染模式切换 原始（Original）、法线(Normal)、线框(Wireframe)、线稿(Lineart)\n\n#### 摄像机（Camera）\n\n![menu_modelmenu_camera](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_camera.jpg) 该菜单提供了轴测视图和透视视图切换、透视视角大小设置\n\n1.  **相机（Camera）**: 在轴测视图和正交视图之间快速切换\n2.  **视场角(FOV)**: 调整 FOV 视角角度\n\n#### 灯光（Light）\n\n![menu_modelmenu_camera](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_light.jpg) 通过该菜单可以快速调节模型场景的全局光照强度\n\n#### 导出（Export）\n\n![menu_export](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_export.jpg) 该菜单提供了一个快速转换模型格式并导出的能力\n\n### 3\\. 右侧菜单功能\n\n右侧菜单的两个主要功能为：\n\n1.  **重设视图比例**： 点击按钮后视图将根据设定好的宽高按比例调整画布渲染区域比例\n2.  **视频录制**： 允许你将当前的 3D 视图操作录制为视频，允许导入，并可以作为 `recording_video` 输出给后续节点"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/sampling/ksampler",
  "markdown": "# Ksampler - ComfyUI 原生节点文档 - ComfyUI\n\n```\n\ndef common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n    latent_image = latent[\"samples\"]\n    latent_image = comfy.sample.fix_empty_latent_channels(model, latent_image)\n\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = comfy.sample.prepare_noise(latent_image, seed, batch_inds)\n\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]\n\n    callback = latent_preview.prepare_callback(model, steps)\n    disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,\n                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    out = latent.copy()\n    out[\"samples\"] = samples\n    return (out, )\n\n\nclass KSampler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"model\": (\"MODEL\", {\"tooltip\": \"The model used for denoising the input latent.\"}),\n                \"seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff, \"control_after_generate\": True, \"tooltip\": \"The random seed used for creating the noise.\"}),\n                \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000, \"tooltip\": \"The number of steps used in the denoising process.\"}),\n                \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01, \"tooltip\": \"The Classifier-Free Guidance scale balances creativity and adherence to the prompt. Higher values result in images more closely matching the prompt however too high values will negatively impact quality.\"}),\n                \"sampler_name\": (comfy.samplers.KSampler.SAMPLERS, {\"tooltip\": \"The algorithm used when sampling, this can affect the quality, speed, and style of the generated output.\"}),\n                \"scheduler\": (comfy.samplers.KSampler.SCHEDULERS, {\"tooltip\": \"The scheduler controls how noise is gradually removed to form the image.\"}),\n                \"positive\": (\"CONDITIONING\", {\"tooltip\": \"The conditioning describing the attributes you want to include in the image.\"}),\n                \"negative\": (\"CONDITIONING\", {\"tooltip\": \"The conditioning describing the attributes you want to exclude from the image.\"}),\n                \"latent_image\": (\"LATENT\", {\"tooltip\": \"The latent image to denoise.\"}),\n                \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01, \"tooltip\": \"The amount of denoising applied, lower values will maintain the structure of the initial image allowing for image to image sampling.\"}),\n            }\n        }\n\n    RETURN_TYPES = (\"LATENT\",)\n    OUTPUT_TOOLTIPS = (\"The denoised latent.\",)\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling\"\n    DESCRIPTION = \"Uses the provided model, positive and negative conditioning to denoise the latent image.\"\n\n    def sample(self, model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0):\n        return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/latent/video/trim-video-latent",
  "markdown": "# TrimVideoLatent 节点 - ComfyUI\n\n![ComfyUI TrimVideoLatent 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/latent/video/trim-video-latent.jpg) TrimVideoLatent 节点用于在潜在空间（LATENT）中裁剪视频帧。常用于处理视频潜变量序列时，去除前面不需要的帧，实现视频的“前向裁剪”。 基本用法：将需要裁剪的视频潜变量输入到 samples，设置 trim\\_amount 为要裁剪的帧数。节点会从视频的开头裁剪掉指定数量的帧，输出剩余的潜变量序列。 典型场景：用于视频生成、视频编辑等场景下，去除不需要的前置帧，或配合其他节点实现视频片段的拼接与处理。\n\n## 参数说明\n\n### 输入参数\n\n| 参数名 | 类型  | 是否必填 | 默认值 | 说明  |\n| --- | --- | --- | --- | --- |\n| samples | LATENT | 是   | 无   | 输入的潜在视频数据 |\n| trim\\_amount | INT | 是   | 0   | 需要裁剪掉的帧数（从前往后裁剪） |\n\n### 输出参数\n\n| 参数名 | 类型  | 说明  |\n| --- | --- | --- |\n| samples | LATENT | 裁剪后的视频潜变量 |\n\n## 使用示例\n\n[\n\n## Wan2.1 VACE 视频生成工作流示例\n\nWan2.1 VACE 视频生成工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/video/wan/vace)\n\n### 源码\n\n```\nclass TrimVideoLatent:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"trim_amount\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 99999}),\n                             }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/video\"\n\n    EXPERIMENTAL = True\n\n    def op(self, samples, trim_amount):\n        samples_out = samples.copy()\n\n        s1 = samples[\"samples\"]\n        samples_out[\"samples\"] = s1[:, :, trim_amount:]\n        return (samples_out,)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image",
  "markdown": "# Flux 1.1 Pro Ultra Image API 节点 ComfyUI 官方示例工作流\n\nFLUX 1.1 Pro Ultra 是由 BlackForestLabs 推出的高性能 AI 图像生成工具，主打超高分辨率与高效生成能力。它支持高达 4MP（标准版的 4 倍）的超清画质，同时将单张图像生成时间控制在 10 秒以内，速度比同类高分辨率模型快 2.5 倍。 该工具提供两种核心模式：\n\n*   **Ultra 模式**：专为高分辨率需求设计，适合广告、电商等需要细节放大的场景，能精准还原提示词并保持生成速度。\n*   **Raw 模式**：侧重自然真实感，优化人像肤色、光影及自然景观的细节，减少”AI 味”，适合摄影艺术和真实风格创作。\n\n目前在 ComfyUI 中我们已经支持了 Flux 1.1 Pro Ultra Image 节点，在本篇文档中我们将涉及以下内容：\n\n*   Flux 1.1 Pro 文生图\n*   Flux 1.1 Pro 图生图（Remix）\n\n你可查阅下面的文档了解对应节点的详细参数设置\n\n*   [Flux 1.1 Pro Ultra Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/bfl/flux-1-1-pro-ultra-image)\n\n## Flux 1.1 \\[pro\\] 文生图教程\n\n### 1\\. 工作流文件下载\n\n请下载下面的文件，并拖入 ComfyUI 以加载对应的工作流 ![Flux 1.1 pro 文生图工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_t2i.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg) 你可参考图片中的序号来完成最基础的工作流运行\n\n1.  (可选)在 `Flux 1.1 [pro] Ultra Image` 节点的 `prompt` 修改工作流的提示词\n2.  （可选）修改 `raw` 参数为 `false`,可以让输出的图像更真实\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n4.  等待 API 返回结果后，你可在`Save Image`节点中查看生成的图像, 对应的图像也会被保存至`ComfyUI/output/` 目录下\n\n## Flux 1.1\\[pro\\] 图生图教程\n\n当我们在节点输入中添加了 `image_prompt` 则对应输出结果将会融合输入图片特点进行混合（Remix） ，`image_prompt_strength` 的大小影响混合的比例:数值越大与输入图像越相似.\n\n### 1\\. 工作流文件下载\n\n请下载下面的文件，并拖入 ComfyUI 以加载对应的工作流，或者在 **文生图工作流** 中紫色的节点上右键设置 `模式（mode）` 为 `总是（always）` 来启用 `image_prompt` 输入 ![Flux 1.1 pro 图生图Remix](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_i2i.png) 我们会将下面的图片作为输入： ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png) \n\n### 2\\. 按步骤完成工作流的运行\n\n![工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg) 你可参考图片中的序号来完成最基础的工作流运行\n\n1.  在 `Load Image` 节点上点击 **Upload** 上传输入图像\n2.  （可选）修改 `Flux 1.1 [pro] Ultra Image` 中的 `image_prompt_strength` 的大小，来改变混合的比例\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n4.  等待 API 返回结果后，你可在`Save Image`节点中查看生成的图像, 对应的图像也会被保存至`ComfyUI/output/` 目录下\n\n下面是不同 `image_prompt_strength` 的输出结果对比 ![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/conditioning/video-models/wan-vace-to-video",
  "markdown": "# Wan Vace To Video - ComfyUI 原生节点文档\n\n![Wan Vace To Video](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg) Wan Vace To Video 节点允许您通过文本提示词生成视频，并支持多种输入方式，包括文本、图像、视频、遮罩和控制信号等。 该节点将输入的条件（提示词）、控制视频和遮罩组合起来，生成高质量的视频。它首先对输入进行预处理和编码，然后应用条件信息来生成最终的视频潜在表示。 当提供参考图像时，它会被作为视频的起始参考。控制视频和遮罩可以用来引导生成过程，让生成的视频更符合的预期。\n\n## 参数说明\n\n### 必选参数\n\n| 参数名 | 类型  | 默认值 | 范围  | 说明  |\n| --- | --- | --- | --- | --- |\n| positive | CONDITIONING | \\-  | \\-  | 正面提示词条件 |\n| negative | CONDITIONING | \\-  | \\-  | 负面提示词条件 |\n| vae | VAE | \\-  | \\-  | 用于编码/解码的VAE模型 |\n| width | INT | 832 | 16-MAX\\_RESOLUTION | 生成视频的宽度，步长为16 |\n| height | INT | 480 | 16-MAX\\_RESOLUTION | 生成视频的高度，步长为16 |\n| length | INT | 81  | 1-MAX\\_RESOLUTION | 生成视频的帧数，步长为4 |\n| batch\\_size | INT | 1   | 1-4096 | 批处理大小 |\n| strength | FLOAT | 1.0 | 0.0-1000.0 | 条件强度，步长为0.01 |\n\n### 可选参数\n\n| 参数名 | 类型  | 说明  |\n| --- | --- | --- |\n| control\\_video | IMAGE | 控制视频，用于引导生成过程 |\n| control\\_masks | MASK | 控制遮罩，定义视频中哪些区域应受到控制 |\n| reference\\_image | IMAGE | 参考图像，作为视频生成的起点或参考（单张） |\n\n### 输出参数\n\n| 参数名 | 类型  | 说明  |\n| --- | --- | --- |\n| positive | CONDITIONING | 处理后的正面提示词条件 |\n| negative | CONDITIONING | 处理后的负面提示词条件 |\n| latent | LATENT | 生成的视频潜在表示 |\n| trim\\_latent | INT | 裁剪潜在表示的参数，默认值为0。当提供参考图像时，该值会被设置为参考图像在潜在空间中的形状尺寸。它指示下游节点需要从生成的潜在表示中裁剪掉多少来自参考图像的内容，确保最终视频输出中参考图像的影响被适当控制。 |\n\n## 源码\n\n\\[源码更新时间: 2025-05-15\\]\n\n```\nclass WanVaceToVideo:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"vae\": (\"VAE\", ),\n                             \"width\": (\"INT\", {\"default\": 832, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                             \"height\": (\"INT\", {\"default\": 480, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                             \"length\": (\"INT\", {\"default\": 81, \"min\": 1, \"max\": nodes.MAX_RESOLUTION, \"step\": 4}),\n                             \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1000.0, \"step\": 0.01}),\n                },\n                \"optional\": {\"control_video\": (\"IMAGE\", ),\n                             \"control_masks\": (\"MASK\", ),\n                             \"reference_image\": (\"IMAGE\", ),\n                }}\n\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\", \"INT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\", \"trim_latent\")\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/video_models\"\n\n    EXPERIMENTAL = True\n\n    def encode(self, positive, negative, vae, width, height, length, batch_size, strength, control_video=None, control_masks=None, reference_image=None):\n        latent_length = ((length - 1) // 4) + 1\n        if control_video is not None:\n            control_video = comfy.utils.common_upscale(control_video[:length].movedim(-1, 1), width, height, \"bilinear\", \"center\").movedim(1, -1)\n            if control_video.shape[0] < length:\n                control_video = torch.nn.functional.pad(control_video, (0, 0, 0, 0, 0, 0, 0, length - control_video.shape[0]), value=0.5)\n        else:\n            control_video = torch.ones((length, height, width, 3)) * 0.5\n\n        if reference_image is not None:\n            reference_image = comfy.utils.common_upscale(reference_image[:1].movedim(-1, 1), width, height, \"bilinear\", \"center\").movedim(1, -1)\n            reference_image = vae.encode(reference_image[:, :, :, :3])\n            reference_image = torch.cat([reference_image, comfy.latent_formats.Wan21().process_out(torch.zeros_like(reference_image))], dim=1)\n\n        if control_masks is None:\n            mask = torch.ones((length, height, width, 1))\n        else:\n            mask = control_masks\n            if mask.ndim == 3:\n                mask = mask.unsqueeze(1)\n            mask = comfy.utils.common_upscale(mask[:length], width, height, \"bilinear\", \"center\").movedim(1, -1)\n            if mask.shape[0] < length:\n                mask = torch.nn.functional.pad(mask, (0, 0, 0, 0, 0, 0, 0, length - mask.shape[0]), value=1.0)\n\n        control_video = control_video - 0.5\n        inactive = (control_video * (1 - mask)) + 0.5\n        reactive = (control_video * mask) + 0.5\n\n        inactive = vae.encode(inactive[:, :, :, :3])\n        reactive = vae.encode(reactive[:, :, :, :3])\n        control_video_latent = torch.cat((inactive, reactive), dim=1)\n        if reference_image is not None:\n            control_video_latent = torch.cat((reference_image, control_video_latent), dim=2)\n\n        vae_stride = 8\n        height_mask = height // vae_stride\n        width_mask = width // vae_stride\n        mask = mask.view(length, height_mask, vae_stride, width_mask, vae_stride)\n        mask = mask.permute(2, 4, 0, 1, 3)\n        mask = mask.reshape(vae_stride * vae_stride, length, height_mask, width_mask)\n        mask = torch.nn.functional.interpolate(mask.unsqueeze(0), size=(latent_length, height_mask, width_mask), mode='nearest-exact').squeeze(0)\n\n        trim_latent = 0\n        if reference_image is not None:\n            mask_pad = torch.zeros_like(mask[:, :reference_image.shape[2], :, :])\n            mask = torch.cat((mask_pad, mask), dim=1)\n            latent_length += reference_image.shape[2]\n            trim_latent = reference_image.shape[2]\n\n        mask = mask.unsqueeze(0)\n        positive = node_helpers.conditioning_set_values(positive, {\"vace_frames\": control_video_latent, \"vace_mask\": mask, \"vace_strength\": strength})\n        negative = node_helpers.conditioning_set_values(negative, {\"vace_frames\": control_video_latent, \"vace_mask\": mask, \"vace_strength\": strength})\n\n        latent = torch.zeros([batch_size, 16, latent_length, height // 8, width // 8], device=comfy.model_management.intermediate_device())\n        out_latent = {}\n        out_latent[\"samples\"] = latent\n        return (positive, negative, out_latent, trim_latent)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipSetLastLayer",
  "markdown": "# 设置CLIP最后一层 - ComfyUI 原生节点文档 - ComfyUI\n\n`设置CLIP最后一层` 是 ComfyUI 中用于控制 CLIP 模型处理深度的核心节点。它允许用户精确控制 CLIP 文本编码器在哪一层停止处理，从而影响文本理解的深度和生成图像的风格。 想象 CLIP 模型是一个24层的智能大脑：\n\n*   浅层 (1-8层)：识别基本字母、单词\n*   中层 (9-16层)：理解语法、句子结构\n*   深层 (17-24层)：掌握抽象概念、复杂语义\n\n`设置CLIP最后一层` 就像一个 **“思考深度调节器”**： \\-1: 使用全部24层（最完整理解） -2: 停在第23层（稍微简化） -12: 停在第13层（中等理解） -24: 只用第1层（最基础理解）\n\n## 输入\n\n| 参数名称 | 数据类型 | 默认值 | 取值范围 | 功能说明 |\n| --- | --- | --- | --- | --- |\n| `clip` | CLIP | \\-  | \\-  | 要修改的CLIP模型 |\n| `停止在 CLIP层` | INT | \\-1 | \\-24 到 -1 | 指定停止处理的层级，-1表示使用全部层级，-24表示只用第一层 |\n\n## 输出\n\n| 输出名称 | 数据类型 | 说明  |\n| --- | --- | --- |\n| clip | CLIP | 已修改的CLIP模型，指定的层被设置为最后一层 |\n\n## 为什么需要设置最后一层\n\n*   **性能优化**：就像不需要博士学位来理解简单句子一样，有时浅层理解就够了，速度更快\n*   **风格控制**：不同层次的理解会产生不同的艺术风格\n*   **兼容性**：某些模型可能在特定层次上表现更好\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-replace-background",
  "markdown": "# Recraft Replace Background - ComfyUI 原生节点文档\n\n```\n\nclass RecraftReplaceBackgroundNode:\n    \"\"\"\n    Replace background on image, based on provided prompt.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt: str,\n        n: int,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n        )\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/replaceBackground\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-image",
  "markdown": "# Luma Text to Image API 节点 ComfyUI 官方示例\n\n[Luma Text to Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-text-to-image) 节点允许你使用Luma AI的先进技术根据文本提示词生成高质量的图像，能够创建照片级别的逼真内容和艺术风格图像。 本篇指南中，我们将引导你如何使用对应节点来进行文本生图的工作流设置。\n\n你可查阅下面的文档了解对应节点的详细参数设置等\n\n[\n\n## Luma Text to Image 节点文档\n\nLuma Text to Image API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-text-to-image)[\n\n## Luma Reference 节点文档\n\nLuma Reference API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-reference)\n\n## Luma Text to Image API 节点文本生图工作流\n\n在 `Luma Text to Image` 节点没有使用任何的图像输入时，对应的工作流则为文生图工作流，在本篇指南中，我们制作了使用`style_image` 和 `image_luma_ref` 的示例。 能够让你体验到 Luma AI 在相关图像处理上的优秀能力。\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![Luma 文本生图工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/luma_t2i.png) 请下载下面的图片，我们将会用作输入： ![输入图片-参考](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_ref.png) ![输入图片-风格](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_style.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![Luma 文本生图工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg) 你可参考图片中的序号来完成最基础的工作流运行  \n\n1.  在 `Load image` 节点中上传参考图像\n2.  在 `Load image（已重命名为 styleref）` 节点中上传风格参考图像\n3.  （可选）修改 `Luma Text to Image` 节点的提示词\n4.  （可选）修改 `style_image_weight` 的权重，来调整风格参考图像的权重\n5.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n6.  等待 API 返回结果后，你可在`Save Image`节点中查看生成的图像, 对应的图片也会被保存至`ComfyUI/output/` 目录下\n\n![不同 style_image_weight 效果对比](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg)\n\n### 3\\. 补充说明\n\n*   [对应的节点](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-text-to-image)同时允许最多同时分别输入 4 张参考图和角色参考。\n*   如果要启用多张图像输入参考，请在对应“紫色”的处于`绕过(Bypass)` 的节点上右键，将对应的 `模式（mode）` 设置为 `总是（always）`"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/save-svg",
  "markdown": "# Save SVG - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Save SVG 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/save-svg.jpg) Save SVG 节点允许你将从Recraft矢量生成节点获取的SVG数据保存为文件系统中的可用文件。这是处理和导出矢量图形的必要组件。\n\n## 节点功能\n\n此节点接收SVG矢量数据，并将其保存为文件系统中的标准SVG文件。它支持自动文件命名和保存路径指定，使得矢量图形可以被其他软件打开和编辑。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| svg | SVG | \\-  | 要保存的 SVG 矢量数据 |\n| filename\\_prefix | 字符串 | ”recraft” | 文件名前缀 |\n| output\\_dir | 字符串 | \\-  | 输出目录，默认为 ComfyUI 输出文件夹具体路径为 `ComfyUI/output/svg/` |\n| index | 整数  | \\-1 | 保存索引，-1 表示所有生成的 SVG |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| SVG | SVG | 传递输入的SVG数据 |\n\n## 使用示例\n\n[\n\n## Recraft Text to Image 工作流示例\n\nRecraft Text to Image 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass SaveSVGNode:\n    \"\"\"\n    Save SVG files on disk.\n    \"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n\n    RETURN_TYPES = ()\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"save_svg\"\n    CATEGORY = \"api node/image/Recraft\"\n    OUTPUT_NODE = True\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"svg\": (RecraftIO.SVG,),\n                \"filename_prefix\": (\"STRING\", {\"default\": \"svg/ComfyUI\", \"tooltip\": \"The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes.\"})\n            },\n            \"hidden\": {\n                \"prompt\": \"PROMPT\",\n                \"extra_pnginfo\": \"EXTRA_PNGINFO\"\n            }\n        }\n\n    def save_svg(self, svg: SVG, filename_prefix=\"svg/ComfyUI\", prompt=None, extra_pnginfo=None):\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n        results = list()\n\n        # Prepare metadata JSON\n        metadata_dict = {}\n        if prompt is not None:\n            metadata_dict[\"prompt\"] = prompt\n        if extra_pnginfo is not None:\n            metadata_dict.update(extra_pnginfo)\n\n        # Convert metadata to JSON string\n        metadata_json = json.dumps(metadata_dict, indent=2) if metadata_dict else None\n\n        for batch_number, svg_bytes in enumerate(svg.data):\n            filename_with_batch_num = filename.replace(\"%batch_num%\", str(batch_number))\n            file = f\"{filename_with_batch_num}_{counter:05}_.svg\"\n\n            # Read SVG content\n            svg_bytes.seek(0)\n            svg_content = svg_bytes.read().decode('utf-8')\n\n            # Inject metadata if available\n            if metadata_json:\n                # Create metadata element with CDATA section\n                metadata_element = f\"\"\"  <metadata>\n    <![CDATA[\n{metadata_json}\n    ]]>\n  </metadata>\n\"\"\"\n                # Insert metadata after opening svg tag using regex\n                import re\n                svg_content = re.sub(r'(<svg[^>]*>)', r'\\1\\n' + metadata_element, svg_content)\n\n            # Write the modified SVG to file\n            with open(os.path.join(full_output_folder, file), 'wb') as svg_file:\n                svg_file.write(svg_content.encode('utf-8'))\n\n            results.append({\n                \"filename\": file,\n                \"subfolder\": subfolder,\n                \"type\": self.type\n            })\n            counter += 1\n        return { \"ui\": { \"images\": results } }\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/minimax/minimax-image-to-video",
  "markdown": "# MiniMax Image to Video - ComfyUI 原生节点文档\n\n![ComfyUI 原生 MiniMax Image to Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg) MiniMax Image to Video 节点使用 MiniMax 的API，基于输入图像和提示词同步生成视频内容。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 用作视频生成第一帧的输入图像 |\n| prompt\\_text | 字符串 | \"\"  | 引导视频生成的文本提示词 |\n| model | 选择项 | ”I2V-01” | 可选模型包括”I2V-01-Director”、“I2V-01”、“I2V-01-live” |\n\n### 可选参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| seed | 整数  | 用于创建噪声的随机种子值 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass MinimaxImageToVideoNode(MinimaxTextToVideoNode):\n    \"\"\"\n    Generates videos synchronously based on an image and prompt, and optional parameters using Minimax's API.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Image to use as first frame of video generation\"\n                    },\n                ),\n                \"prompt_text\": (\n                    \"STRING\",\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt to guide the video generation\",\n                    },\n                ),\n                \"model\": (\n                    [\n                        \"I2V-01-Director\",\n                        \"I2V-01\",\n                        \"I2V-01-live\",\n                    ],\n                    {\n                        \"default\": \"I2V-01\",\n                        \"tooltip\": \"Model to use for video generation\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from an image and prompts using Minimax's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Minimax\"\n    API_NODE = True\n    OUTPUT_NODE = True\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/openai/openai-gpt-image1",
  "markdown": "# OpenAI GPT Image 1 - ComfyUI 原生节点文档\n\n![ComfyUI 原生OpenAI GPT Image 1  节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-gpt-image-1.jpg) 此节点连接到OpenAI的 GPT Image 1 API，让用户能够通过详细的文本提示词生成图像。GPT Image 1与传统的DALL·E模型不同，它利用了GPT-4的语言理解能力，可以处理更复杂和语境丰富的提示词，生成更符合用户意图的图像内容。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 详细描述要生成内容的文本提示词 |\n| quality | 选择项 | ”low” | 图像质量级别，可选值：“low”, “medium”, “high” |\n| size | 选择项 | ”auto” | 输出图像尺寸，可选值：“auto”, “1024x1024”, “1024x1536”, “1536x1024” |\n\n### 图像编辑参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| image | 图像  | 用于图像编辑的输入图像，支持批量输入多张图像 |\n| mask | 掩码  | 指定图像中要修改的区域(可选)，使用掩码时只能输入单张图像 |\n\n### 可选参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| background | 选择项 | 背景处理选项，可选值：“opaque”(不透明), “transparent”(透明) |\n| seed | 整数  | 生成的随机种子，当前在后端尚未实现 |\n| n   | 整数  | 生成的图像数量，范围1-8 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 生成的图像结果 |\n\n## 工作原理\n\nOpenAI GPT Image 1 节点结合了GPT-4的语言理解能力和图像生成技术。它首先分析用户提供的文本提示词，理解其语义内容和意图，然后生成符合描述的图像。 当提供输入图像时，节点可以在图像编辑模式下工作，允许对现有图像进行修改。通过额外提供掩码，用户可以精确控制哪些区域应该被修改，哪些应该保持不变。注意使用掩码时，只能提供单张图像输入。 用户可以通过调整各种参数来控制生成结果，包括质量级别、尺寸、背景处理和生成数量。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass OpenAIGPTImage1(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's GPT Image 1 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for GPT Image 1\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"quality\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"low\", \"medium\", \"high\"],\n                        \"default\": \"low\",\n                        \"tooltip\": \"Image quality, affects cost and generation time.\",\n                    },\n                ),\n                \"background\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"opaque\", \"transparent\"],\n                        \"default\": \"opaque\",\n                        \"tooltip\": \"Return image with or without background\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"auto\", \"1024x1024\", \"1024x1536\", \"1536x1024\"],\n                        \"default\": \"auto\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"How many images to generate\",\n                    },\n                ),\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        quality=\"low\",\n        background=\"opaque\",\n        image=None,\n        mask=None,\n        n=1,\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"gpt-image-1\"\n        path = \"/proxy/openai/images/generations\"\n        content_type=\"application/json\"\n        request_class = OpenAIImageGenerationRequest\n        img_binaries = []\n        mask_binary = None\n        files = []\n\n        if image is not None:\n            path = \"/proxy/openai/images/edits\"\n            request_class = OpenAIImageEditRequest\n            content_type =\"multipart/form-data\"\n\n            batch_size = image.shape[0]\n\n            for i in range(batch_size):\n                single_image = image[i : i + 1]\n                scaled_image = downscale_image_tensor(single_image).squeeze()\n\n                image_np = (scaled_image.numpy() * 255).astype(np.uint8)\n                img = Image.fromarray(image_np)\n                img_byte_arr = io.BytesIO()\n                img.save(img_byte_arr, format=\"PNG\")\n                img_byte_arr.seek(0)\n                img_binary = img_byte_arr\n                img_binary.name = f\"image_{i}.png\"\n\n                img_binaries.append(img_binary)\n                if batch_size == 1:\n                    files.append((\"image\", img_binary))\n                else:\n                    files.append((\"image[]\", img_binary))\n\n        if mask is not None:\n            if image.shape[0] != 1:\n                raise Exception(\"Cannot use a mask with multiple image\")\n            if image is None:\n                raise Exception(\"Cannot use a mask without an input image\")\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n            batch, height, width = mask.shape\n            rgba_mask = torch.zeros(height, width, 4, device=\"cpu\")\n            rgba_mask[:, :, 3] = 1 - mask.squeeze().cpu()\n\n            scaled_mask = downscale_image_tensor(rgba_mask.unsqueeze(0)).squeeze()\n\n            mask_np = (scaled_mask.numpy() * 255).astype(np.uint8)\n            mask_img = Image.fromarray(mask_np)\n            mask_img_byte_arr = io.BytesIO()\n            mask_img.save(mask_img_byte_arr, format=\"PNG\")\n            mask_img_byte_arr.seek(0)\n            mask_binary = mask_img_byte_arr\n            mask_binary.name = \"mask.png\"\n            files.append((\"mask\", mask_binary))\n\n        # Build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=path,\n                method=HttpMethod.POST,\n                request_model=request_class,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=request_class(\n                model=model,\n                prompt=prompt,\n                quality=quality,\n                background=background,\n                n=n,\n                seed=seed,\n                size=size,\n            ),\n            files=files if files else None,\n            content_type=content_type,\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image",
  "markdown": "# Recraft Style - Realistic Image - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Recraft Style - Realistic Image 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg) Recraft Style - Realistic Image 节点用于设置Recraft图像生成的真实照片风格，提供多种子风格选项，以控制生成图像的视觉特性。\n\n## 节点功能\n\n此节点创建一个风格配置对象，用于指导Recraft的图像生成过程朝向真实照片的视觉效果。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| substyle | 选择项 | None | 真实照片风格的具体子风格（必选） |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | 风格配置对象，连接到Recraft生成节点 |\n\n## 使用示例\n\n[\n\n## Recraft Text to Image 工作流示例\n\nRecraft Text to Image 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass RecraftStyleV3RealisticImageNode:\n    \"\"\"\n    Select realistic_image style and optional substyle.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.STYLEV3,)\n    RETURN_NAMES = (\"recraft_style\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_style\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_STYLE = RecraftStyleV3.realistic_image\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"substyle\": (get_v3_substyles(s.RECRAFT_STYLE),),\n            }\n        }\n\n    def create_style(self, substyle: str):\n        if substyle == \"None\":\n            substyle = None\n        return (RecraftStyle(self.RECRAFT_STYLE, substyle),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra",
  "markdown": "# Stability Stable Image Ultra - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Stability Stable Image Ultra 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg) Stability Stable Image Ultra 节点使用 Stability AI 的 Stable Diffusion Ultra API 生成高质量图像。它支持文本到图像和图像到图像的生成，能够根据文本提示词创建细节丰富、艺术表现力强的视觉内容。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 详细描述要生成内容的文本提示词。强大、描述性的提示词能明确定义元素、颜色和主题，从而获得更好的结果。可以使用`(词:权重)`格式控制特定词的权重，其中权重为0到1之间的值。例如：`天空是清爽的(蓝色:0.3)和(绿色:0.8)`表示天空是蓝色和绿色的，但绿色比蓝色更明显。 |\n| aspect\\_ratio | 选择项 | ”1:1” | 输出图像的宽高比 |\n| style\\_preset | 选择项 | ”None” | 可选的生成图像的预设风格 |\n| seed | 整数  | 0   | 用于创建噪声的随机种子，范围0-4294967294 |\n\n### 可选参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 用于图像到图像生成的输入图像 |\n| negative\\_prompt | 字符串 | \"\"  | 描述不希望在输出图像中看到的内容。这是一个高级功能 |\n| image\\_denoise | 浮点数 | 0.5 | 输入图像的去噪强度，范围0.0-1.0。0.0会产生与输入完全相同的图像，1.0则相当于没有提供任何图像 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 生成的图像结果 |\n\n## 使用示例\n\n[\n\nStability AI Stable Image Ultra 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)\n\n## 注意事项\n\n*   当未提供输入图像时，image\\_denoise参数不会生效\n*   如果style\\_preset设置为”None”，则不会应用任何预设风格\n*   当使用图像到图像功能时，输入图像会转换为适当的格式后发送到API\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass StabilityStableImageUltraNode:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/stability\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines\" +\n                                    \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines\" +\n                                    \"elements, colors, and subjects will lead to better results. \" +\n                                    \"To control the weight of a given word use the format `(word:weight)`,\" +\n                                    \"where `word` is the word you'd like to control the weight of and `weight`\" +\n                                    \"is a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\" +\n                                    \"would convey a sky that was blue and green, but more green than blue.\"\n                    },\n                ),\n                \"aspect_ratio\": ([x.value for x in StabilityAspectRatio],\n                    {\n                        \"default\": StabilityAspectRatio.ratio_1_1,\n                        \"tooltip\": \"Aspect ratio of generated image.\",\n                    },\n                ),\n                \"style_preset\": (get_stability_style_presets(),\n                    {\n                        \"tooltip\": \"Optional desired style of generated image.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 4294967294,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (IO.IMAGE,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"A blurb of text describing what you do not wish to see in the output image. This is an advanced feature.\"\n                    },\n                ),\n                \"image_denoise\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(self, prompt: str, aspect_ratio: str, style_preset: str, seed: int,\n                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,\n                 auth_token=None):\n        # prepare image binary if image present\n        image_binary = None\n        if image is not None:\n            image_binary = tensor_to_bytesio(image, 1504 * 1504).read()\n        else:\n            image_denoise = None\n\n        if not negative_prompt:\n            negative_prompt = None\n        if style_preset == \"None\":\n            style_preset = None\n\n        files = {\n            \"image\": image_binary\n        }\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/stability/v2beta/stable-image/generate/ultra\",\n                method=HttpMethod.POST,\n                request_model=StabilityStableUltraRequest,\n                response_model=StabilityStableUltraResponse,\n            ),\n            request=StabilityStableUltraRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                aspect_ratio=aspect_ratio,\n                seed=seed,\n                strength=image_denoise,\n                style_preset=style_preset,\n            ),\n            files=files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.finish_reason != \"SUCCESS\":\n            raise Exception(f\"Stable Image Ultra generation failed: {response_api.finish_reason}.\")\n\n        image_data = base64.b64decode(response_api.image)\n        returned_image = bytesio_to_image_tensor(BytesIO(image_data))\n\n        return (returned_image,)\n\n\n```\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-image-inpainting",
  "markdown": "# Recraft Image Inpainting - ComfyUI 原生节点文档\n\n```\nclass RecraftImageInpaintingNode:\n    \"\"\"\n    Modify image based on prompt and mask.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"mask\": (IO.MASK, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        mask: torch.Tensor,\n        prompt: str,\n        n: int,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n            random_seed=seed,\n        )\n\n        # prepare mask tensor\n        _, H, W, _ = image.shape\n        mask = mask.unsqueeze(-1)\n        mask = mask.movedim(-1,1)\n        mask = common_upscale(mask, width=W, height=H, upscale_method=\"nearest-exact\", crop=\"disabled\")\n        mask = mask.movedim(1,-1)\n        mask = (mask > 0.5).float()\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                mask=mask[i:i+1],\n                path=\"/proxy/recraft/images/inpaint\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-text-to-image",
  "markdown": "# Recraft Text to Image - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Recraft Text to Image 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg) Recraft Text to Image 节点允许你通过文本提示词生成高质量图像，直接连接 Recraft AI 的图像生成 API，创建各种风格的图像作品。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 生成图像的文本描述 |\n| size | 选择项 | 1024x1024 | 输出图像尺寸 |\n| n   | 整数  | 1   | 生成图像数量(1-6) |\n| seed | 整数  | 0   | 随机种子值 |\n\n### 可选参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | 设置生成图像的风格，默认为”真实照片” |\n| negative\\_prompt | 字符串 | 指定不希望出现的元素 |\n| recraft\\_controls | Recraft Controls | 附加控制参数(颜色等) |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 生成的图像结果 |\n\n## 使用示例\n\n## 工作原理\n\n本节点主要通过以下步骤处理请求：\n\n1.  整合输入参数，包括提示词、图像尺寸、生成数量和随机种子\n2.  如有连接，合并风格设置和控制参数\n3.  构建API请求并发送到Recraft服务器\n4.  接收返回的图像URL并下载图像数据\n5.  将图像数据转换为ComfyUI可用的tensor格式并输出\n\n节点使用同步操作模式，会在处理完成前阻塞工作流执行，直到所有请求的图像都生成完毕。该节点利用Recraft的V3模型进行图像生成，能支持各种详细的文本描述和风格变化。\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass RecraftTextToImageNode:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"size\": (\n                    [res.value for res in RecraftImageSize],\n                    {\n                        \"default\": RecraftImageSize.res_1024x1024,\n                        \"tooltip\": \"The size of the generated image.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        size: str,\n        n: int,\n        seed,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/recraft/image_generation\",\n                method=HttpMethod.POST,\n                request_model=RecraftImageGenerationRequest,\n                response_model=RecraftImageGenerationResponse,\n            ),\n            request=RecraftImageGenerationRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                model=RecraftModel.recraftv3,\n                size=size,\n                n=n,\n                style=recraft_style.style,\n                substyle=recraft_style.substyle,\n                style_id=recraft_style.style_id,\n                controls=controls_api,\n            ),\n            auth_token=auth_token,\n        )\n        response: RecraftImageGenerationResponse = operation.execute()\n        images = []\n        for data in response.data:\n            image = bytesio_to_image_tensor(\n                download_url_to_bytesio(data.url, timeout=1024)\n            )\n            if len(image.shape) < 4:\n                image = image.unsqueeze(0)\n            images.append(image)\n        output_image = torch.cat(images, dim=0)\n\n        return (output_image,)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-vectorize-image",
  "markdown": "# Recraft Vectorize Image - ComfyUI 原生节点文档\n\n![ComfyUI 原生Recraft Vectorize Image节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg) Recraft Vectorize Image 节点可以将通过 Recraft 的 API 将栅格图像(如照片、PNG或JPEG) 转换为矢量SVG格式。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 要转换为矢量的输入图像 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| SVG | 矢量图 | 转换后的SVG矢量图形，需连接到SaveSVG节点保存 |\n\n## 使用示例\n\n[\n\n## Recraft Text to Image 工作流示例\n\nRecraft Text to Image 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass RecraftVectorizeImageNode:\n    \"\"\"\n    Generates SVG synchronously from an input image.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.SVG,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        svgs = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/vectorize\",\n                auth_token=auth_token,\n            )\n            svgs.append(SVG(sub_bytes))\n            pbar.update(1)\n\n        return (SVG.combine_all(svgs), )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-remove-background",
  "markdown": "# Recraft Remove Background - ComfyUI 原生节点文档\n\n![ComfyUI 原生Recraft Remove Background节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg) Recraft Remove Background 节点通过 Recraft 的 API 能够智能识别并移除图像背景，生成带有透明背景的图像和对应的Alpha蒙版。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| image | 图像  | \\-  | 需要移除背景的输入图像 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| IMAGE | 图像  | 移除背景后的图像(带Alpha通道) |\n| MASK | 蒙版  | 主体对象的蒙版(白色区域为保留的主体) |\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass RecraftRemoveBackgroundNode:\n    \"\"\"\n    Remove background from image, and return processed image and mask.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE, IO.MASK)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/removeBackground\",\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        # use alpha channel as masks, in B,H,W format\n        masks_tensor = images_tensor[:,:,:,-1:].squeeze(-1)\n        return (images_tensor, masks_tensor)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-image-to-image",
  "markdown": "# Recraft Image to Image - ComfyUI 原生节点文档\n\n```\n\nclass RecraftImageToImageNode:\n    \"\"\"\n    Modify image based on prompt and strength.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"strength\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity.\"\n                    }\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt: str,\n        n: int,\n        strength: float,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            strength=round(strength, 2),\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n            controls=controls_api,\n            random_seed=seed,\n        )\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/imageToImage\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-text-to-vector",
  "markdown": "# Recraft Text to Vector - ComfyUI 原生节点文档\n\n```\n\nclass RecraftTextToVectorNode:\n    \"\"\"\n    Generates SVG synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.SVG,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"substyle\": (get_v3_substyles(RecraftStyleV3.vector_illustration),),\n                \"size\": (\n                    [res.value for res in RecraftImageSize],\n                    {\n                        \"default\": RecraftImageSize.res_1024x1024,\n                        \"tooltip\": \"The size of the generated image.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        substyle: str,\n        size: str,\n        n: int,\n        seed,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # create RecraftStyle so strings will be formatted properly (i.e. \"None\" will become None)\n        recraft_style = RecraftStyle(RecraftStyleV3.vector_illustration, substyle=substyle)\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/recraft/image_generation\",\n                method=HttpMethod.POST,\n                request_model=RecraftImageGenerationRequest,\n                response_model=RecraftImageGenerationResponse,\n            ),\n            request=RecraftImageGenerationRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                model=RecraftModel.recraftv3,\n                size=size,\n                n=n,\n                style=recraft_style.style,\n                substyle=recraft_style.substyle,\n                controls=controls_api,\n            ),\n            auth_token=auth_token,\n        )\n        response: RecraftImageGenerationResponse = operation.execute()\n        svg_data = []\n        for data in response.data:\n            svg_data.append(download_url_to_bytesio(data.url, timeout=1024))\n\n        return (SVG(svg_data),)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows",
  "markdown": "# 便携版(Windows) - ComfyUI\n\n**ComfyUI Portable(便携版)** 是一个独立封装完整的 ComfyUI Windows 版本，内部已经整合了 ComfyUI 运行所需的独立的 **Python(python\\_embeded)**,只需要解压即可使用,目前便携版本支持通过 **Nvidia** 显卡或者 **CPU** 运行。 本部分指南将引导你完成对应的安装。\n\n## 下载 ComfyUI Portable(便携版)\n\n您可通过点击下面的链接来获取最新的 **ComfyUI Portable(便携版)** 下载链接\n\n[\n\n下载 ComfyUI Portable(便携版)\n\n](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)\n\n下载后你可以使用类似解压软件如 [7-ZIP](https://7-zip.org/) 对压缩包进行解压 便携版解压后对应的文件结构及说明如下：\n\n```\nComfyUI_windows_portable\n├── 📂ComfyUI                   // ComfyUI 程序主体\n├── 📂python_embeded            // 独立的 Python 环境\n├── 📂update                    // 用于升级便携版安装包的批处理脚本\n├── README_VERY_IMPORTANT.txt   // 英文版本的 ComfyUI 便携版使用说明\n├── run_cpu.bat                 // 双击启动 ComfyUI（仅支持 CPU）\n└── run_nvidia_gpu.bat          // 双击启动 ComfyUI（仅支持 Nvidia 显卡）\n```\n\n根据你的电脑情况双击 `run_nvidia_gpu.bat` 或者 `run_cpu.bat` 来启动 ComfyUI，你会看到对应下图所示的命令的运行 ![ComfyUI便携版运行命令提示符](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui-portable-cmd.png) 当你看到类似图片中的\n\n```\nTo see the GUI go to: http://127.0.0.1:8188\n```\n\n此时你的 ComfyUI 服务已经启动，正常情况下 ComfyUI 会自动打开你的默认浏览器并访问 `http://127.0.0.1:8188` 地址，如果没有自动打开，请手动打开浏览器并访问该地址。\n\n## 添加外部模型路径\n\n如果你想要在 `ComfyUI/models` 之外管理你的模型文件，可能出于以下原因:\n\n*   你有多个 ComfyUI 实例，你想要让这些实例共享模型文件，从而减少磁盘占用\n*   你有多个不同的类型的 GUI 程序，如：WebUI, 你想要他们共用模型文件\n*   模型文件无法被识别或读取到\n\n我们提供了通过 `extra_model_paths.yaml` 配置文件来添加额外模型搜索路径的方法。\n\n### 不同 ComfyUI 版本配置文件位置\n\n对于[便携版](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)和[手动安装](https://docs.comfy.org/zh-CN/installation/manual_install)的 ComfyUI版本，你可以在 ComfyUI 的根目录下找到 `extra_model_paths.yaml.example` 的示例文件\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\n复制并重命名为 `extra_model_paths.yaml` 来使用, 并保持在 ComfyUI 的根目录下, 路径应该是 `ComfyUI/extra_model_paths.yaml`你也可以在 [这里](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 找到配置示例文件\n\n### 配置示例\n\n比如，你需要额外让 ComfyUI 识别的模型文件位于下面的文件夹:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 loras/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\n那么你可以进行如下的配置来让 ComfyUI 识别到你设备上的模型路径\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\n或者使用\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\n或者你也可以参考默认的 [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 来配置，保存之后， 需要 **重启 ComfyUI** 才能生效。 下面是完整的原始的配置配置示例:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### 添加额外自定义节点路径\n\n除了添加外部模型之外，你同样可以添加不在 ComfyUI 默认路径下的自定义节点路径\n\n下面是一个简单的配置示例（Mac 系统），请根据你的实际情况进行修改，并新增到对应的配置文件中，保存后需要 **重启 ComfyUI** 才能生效:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## 进行第一次图片生成\n\n安装成功后，你可以参考访问下面的章节，开始你的 ComfyUI 之路。\n\n[\n\n## 进行第一次图片生成\n\n本教程将引导你完成第一次的模型安装以及对应的文本到图片的生成\n\n\n\n](https://docs.comfy.org/zh-CN/get_started/first_generation)\n\n## 社区分发版本\n\n在中国早期有社区作者 [@秋葉aaaki](https://space.bilibili.com/12566101) 制作过独立分发的版本-秋叶整合包，有被广泛使用。 如果你是在中国使用，这个版本更改了 Github 的源地址，并配置了 Pypi 地址为为中国国内镜像地址，这可以让你在开始上手 ComfyUI 时可以避免一些因为网络导致的依赖和更新问题。\n\n[\n\n## 秋叶 ComfyUI 整合包\n\n访问秋叶整合包原始发布地址\n\n\n\n](https://www.bilibili.com/video/BV1Ew411776J)\n\n## 其它 ComfyUI 便携版相关说明\n\n### 1\\. ComfyUI 便携版升级\n\n你可以使用 **update** 文件夹下的相关批处理命令完成 ComfyUI 便携版的升级\n\n```\nComfyUI_windows_portable\n└─ 📂update\n   ├── update.py\n   ├── update_comfyui.bat            // 更新 ComfyUI 到最新的 Commit 版本\n   ├── update_comfyui_and_python_dependencies.bat  // 请仅在你的运行环境存在问题时使用\n   └── update_comfyui_stable.bat       // 更新 ComfyUI 为最新的 stable 版本\n```\n\n### 2\\. ComfyUI 便携版设置局域网访问\n\n如果你的 ComfyUI 运行在局域网内，想要其它的设备也可以访问到 ComfyUI，你可以通过记事本修改 `run_nvidia_gpu.bat` 或者 `run_cpu.bat` 文件来完成配置，主要通过添加`--listen`来添加监听地址 下面的示例是添加了 `--listen` 参数的 `run_nvidia_gpu.bat` 文件命令\n\n```\n.\\python_embeded\\python.exe -s ComfyUI\\main.py --listen --windows-standalone-build\npause\n```\n\n当启用 ComfyUI 后您会发现最后的运行地址会变为\n\n```\nStarting server\n\nTo see the GUI go to: http://0.0.0.0:8188\nTo see the GUI go to: http://[::]:8188\n```\n\n你可以通过 `WIN + R` 输入`cmd` 打开命令行，输入 `ipconfig` 来查看你的局域网 IP 地址，然后在其它设备上输入 `http://你的局域网IP:8188` 来访问 ComfyUI"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/desktop/linux",
  "markdown": "# Linux桌面版 - ComfyUI\n\n当Linux预建包可用时，你可以配置外部模型路径：\n\n## 添加外部模型路径\n\n如果你在计算机上的 ComfyUI 安装目录之外的其他位置存储了模型，可以通过配置 `extra_model_paths.yaml` 文件将它们添加到 ComfyUI 中。 对于 ComfyUI 桌面版，对应文件路径为：\n\n*   Windows：`C:\\Users\\<你的用户名>\\AppData\\Roaming\\ComfyUI\\extra_model_paths.yaml`\n*   macOS：`~/Library/Application Support/ComfyUI/extra_model_paths.yaml`\n*   Linux：`~/.config/ComfyUI/extra_model_paths.yaml`\n\n详细说明请参见[模型文档](https://docs.comfy.org/zh-CN/development/core-concepts/models#adding-external-model-paths)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster",
  "markdown": "# Recraft Style - Logo Raster - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Recraft Style - Logo Raster 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg) 此节点创建一个风格配置对象，用于指导Recraft的图像生成过程朝向专业标志设计的视觉效果。通过选择不同的子风格，可以定义生成标志的设计风格、复杂度和适用场景。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| substyle | 选择项 | \\-  | Logo栅格风格的具体子风格(必选) |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | 风格配置对象，连接到Recraft生成节点 |\n\n## 使用示例\n\n[\n\n## Recraft Text to Image 工作流示例\n\nRecraft Text to Image 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass RecraftStyleV3LogoRasterNode(RecraftStyleV3RealisticImageNode):\n    \"\"\"\n    Select vector_illustration style and optional substyle.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"substyle\": (get_v3_substyles(s.RECRAFT_STYLE, include_none=False),),\n            }\n        }\n\n    RECRAFT_STYLE = RecraftStyleV3.logo_raster\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/desktop/macos",
  "markdown": "# macOS 桌面版 - ComfyUI\n\n**ComfyUI 桌面版（Desktop）** 是一个独立的安装版本，可以像常规软件一样安装，支持快捷安装、自动配置 **Python 环境及依赖**，并支持导入已有的 ComfyUI 设置、模型、工作流和文件。 ComfyUI 桌面版是一个开源项目，完整代码请访问[这里](https://github.com/Comfy-Org/desktop)。\n\n本篇教程将引导你完成对应的软件安装，并提供相关的安装配置说明。\n\n请点击下面的按钮下载对应的 macOS 系统 **ComfyUI 桌面版** 安装包。\n\n[\n\nDownload for macOS\n\n](https://download.comfy.org/mac/dmg/arm64)\n\n## 通过 Homebrew 安装\n\nComfyUI 桌面版也可通过 [Homebrew](https://brew.sh/) 安装：\n\n## ComfyUI 桌面版安装步骤\n\n1.  双击下载到的安装包文件。\n2.  如图所示，请将 **ComfyUI** 程序按箭头所示拖入 **Applications** 文件夹。 ![ComfyUI 安装包](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0.png)\n3.  如果在打开安装包后，文件夹显示如下（图标上出现禁止符号），则说明你当前的系统版本与 **ComfyUI 桌面版** 不兼容。 ![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0-1.png)\n4.  然后在 **启动台 (Launchpad)** 中找到对应的 **ComfyUI 图标**，点击它即可进入 ComfyUI 的初始化设置。 ![ComfyUI Launchpad](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-1.jpg)\n\n## ComfyUI 桌面版初始化流程\n\n## 进行第一次图片生成\n\n安装成功后，你可以参考访问下面的章节，开始你的 ComfyUI 之路。\n\n[\n\n## 进行第一次图片生成\n\n本教程将引导你完成第一次的模型安装以及对应的文本到图片的生成\n\n\n\n](https://docs.comfy.org/zh-CN/get_started/first_generation)\n\n## 如何更新 ComfyUI 桌面版\n\n目前 ComfyUI 桌面版更新采用自动检测更新，请确保在设置中已经启用自动更新 ![ComfyUI 桌面版设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg) 你也可以在 `菜单` —> `帮助` —> `检查更新` 中选择手动检查是否有可用的更新 ![ComfyUI 桌面版检查更新](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_check_for_updates.jpg)\n\n## 添加外部模型路径\n\n如果你想要在 `ComfyUI/models` 之外管理你的模型文件，可能出于以下原因:\n\n*   你有多个 ComfyUI 实例，你想要让这些实例共享模型文件，从而减少磁盘占用\n*   你有多个不同的类型的 GUI 程序，如：WebUI, 你想要他们共用模型文件\n*   模型文件无法被识别或读取到\n\n我们提供了通过 `extra_model_paths.yaml` 配置文件来添加额外模型搜索路径的方法。\n\n### 不同 ComfyUI 版本配置文件位置\n\n对于[便携版](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)和[手动安装](https://docs.comfy.org/zh-CN/installation/manual_install)的 ComfyUI版本，你可以在 ComfyUI 的根目录下找到 `extra_model_paths.yaml.example` 的示例文件\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\n复制并重命名为 `extra_model_paths.yaml` 来使用, 并保持在 ComfyUI 的根目录下, 路径应该是 `ComfyUI/extra_model_paths.yaml`你也可以在 [这里](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 找到配置示例文件\n\n### 配置示例\n\n比如，你需要额外让 ComfyUI 识别的模型文件位于下面的文件夹:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 loras/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\n那么你可以进行如下的配置来让 ComfyUI 识别到你设备上的模型路径\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\n或者使用\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\n或者你也可以参考默认的 [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 来配置，保存之后， 需要 **重启 ComfyUI** 才能生效。 下面是完整的原始的配置配置示例:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### 添加额外自定义节点路径\n\n除了添加外部模型之外，你同样可以添加不在 ComfyUI 默认路径下的自定义节点路径\n\n下面是一个简单的配置示例（Mac 系统），请根据你的实际情况进行修改，并新增到对应的配置文件中，保存后需要 **重启 ComfyUI** 才能生效:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## 桌面端 Python 环境相关\n\n桌面端的安装将在你选择的安装目录下创建一个python 的虚拟环境，通常这是一个隐藏的 `.venv` 文件夹。 如果你需要为 ComfyUI 插件处理相关的依赖则需要在这个环境中进行处理，直接系统系统的命令行会有将对应依赖安装到系统环境的风险，请参考下面的指示完成对应环境的激活。\n\n### 如何使用 桌面端 的 python 环境？\n\n你可以使用桌面端自带的终端来使用 python 环境。 ![ComfyUI 桌面版终端](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_terminal.jpg) \n\n1.  点击菜单栏的 icon 打开底部面板\n2.  点击 `Terminal` 打开终端\n3.  如果你想要看对应环境的 python 安装位置，可以使用下面的命令\n\n```\n  python -c \"import sys; print(sys.executable)\"\n```\n\n## 如何卸载 ComfyUI 桌面版\n\n要卸载 **ComfyUI 桌面版**，你可以直接在 **Applications** 文件夹内删除 **ComfyUI** 程序。 如果你想要**完全删除** **ComfyUI 桌面版** 的所有文件，可以手动删除以下文件夹：\n\n```\n~/Library/Application Support/ComfyUI\n```\n\n以上操作**不会**删除你的以下文件夹。如果需要，请手动删除：\n\n*   Models（模型文件）\n*   Custom Nodes（自定义节点）\n*   Input/Output 目录（图片输入/输出目录）\n\n## 故障排除\n\n### 如何定位安装错误\n\n如果安装失败，你应该可以看到下面的界面显示 ![ComfyUI 安装失败](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg) 此时建议你采取以下几种方式查找错误原因\n\n1.  点击 `Show Teriminal` 查看错误问题输出\n2.  点击 `Open Logs` 查看安装过程日志\n3.  访问官方论坛查找错误反馈\n4.  点击`Reinstall`尝试重新安装\n\n建议在提交反馈之前，你可以将对应的**错误输出**以及 **log 文件**信息提供给类似 **GPT**一类的工具 ![ComfyUI 安装失败-错误日志](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg) ![ComfyUI 安装失败-GPT 反馈](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg) 如上图，询问对应错误的原因，或者完全删除 ComfyUI 后进行安装重试\n\n### 反馈错误\n\n如果在安装过程中，你发生了任何错误，请通过以下任意方式查看是否有类似错误反馈，或者向我们提交错误\n\n*   Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)\n*   Comfy 官方论坛: [https://forum.comfy.org/](https://forum.comfy.org/)\n\n请在提交错误时确保提交了以下日志以及配置文件，方便我们进行问题的定位和查找\n\n1.  日志文件\n\n| 文件名 | 描述  | 位置  |\n| --- | --- | --- |\n| main.log | 包含与桌面应用和服务器启动相关的日志，来自桌面的 Electron 进程。 |     |\n| comfyui.log | 包含与 ComfyUI 正常运行相关的日志，例如核心 ComfyUI 进程的终端输出。 |     |\n\n![ComfyUI 日志文件输出位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)\n\n2.  配置文件\n\n| 文件名 | 描述  | 位置  |\n| --- | --- | --- |\n| extra\\_model\\_paths.yaml | 包含 ComfyUI 将搜索模型和自定义节点的额外路径。 |     |\n| config.json | 包含应用配置。此文件通常不应直接编辑。 |     |\n\n![ComfyUI 配置文件位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/index",
  "markdown": "# ComfyUI 官方文档 - ComfyUI\n\n## 关于 ComfyUI\n\n由 [comfyanonymous](https://github.com/comfyanonymous) 和其他[贡献者](https://github.com/comfyanonymous/ComfyUI/graphs/contributors)开发。\n\n*   **ComfyUI** 是一个基于节点的生成式 AI 界面和推理引擎\n*   用户可以通过节点组合各种 AI 模型和操作，实现高度可定制和可控的内容生成\n*   ComfyUI 完全开源，可以在本地设备上运行"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image",
  "markdown": "# Recraft Text to Image API 节点 ComfyUI 官方示例\n\n本文将介绍如何在 ComfyUI 中使用 Recraft Text to Image API 节点的相关功能\n\n[Recraft Text to Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-text-to-image) 节点允许你使用Recraft AI的图像生成技术，通过文本描述创建高质量、风格多样的图像内容。 本篇指南中，我们将引导你如何使用对应节点来进行文本到图像的工作流设置。\n\n## Recraft Text to Image API 节点文本到图像工作流\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![Recraft 文本到图像工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/recraft/t2i/recraft_t2i.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![Recraft 文本到图像工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg) 你可参考图片中的序号来完成最基础的工作流运行：\n\n1.  (可选) 修改 `Color` 的 `Recraft Color RGB` 的颜色为你想要的颜色\n2.  (可选) 修改 `Recraft Style` 节点来控制图像的视觉风格，如数字插画、真实照片或Logo设计等，这个分组同时提供了其它的风格节点，你可以按需启用\n3.  (可选) 修改 `Recraft Text to Image` 节点中的 `prompt` 参数，你也可以通过修改`size`参数来改变\n4.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的生成\n5.  等待 API 返回结果后，你可在 `Save Image` 节点中查看生成的图像，对应的图像也会被保存至 `ComfyUI/output/` 目录下\n\n> (可选) 我们在工作流中提供了 **Convert to SVG** 的分组，由于该分组中的 `Recraft Vectorize Image` 节点也会额外消耗积分，你可按需启用，将生成的图像转换成 SVG 格式\n\n### 3\\. 补充说明\n\n*   **Recraft Style**：提供多种预设风格，如真实照片、数字插画、Logo栅格等\n*   **Seed 参数**：仅用于确定节点是否应重新运行，但实际生成结果与种子值无关\n\n## 相关节点详解\n\n你可查阅下面的文档了解对应节点的详细参数设置等\n\n[\n\n## Recraft Text to Image 节点文档\n\nRecraft Text to Image API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-text-to-image)[\n\n## Recraft Style 节点文档\n\nRecraft Style - Realistic Image API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)[\n\n## Recraft Controls 节点文档\n\nRecraft Controls API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-controls)\n\n在此页面\n\n*   [Recraft Text to Image API 节点文本到图像工作流](#recraft-text-to-image-api-%E8%8A%82%E7%82%B9%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E5%B7%A5%E4%BD%9C%E6%B5%81)\n*   [1\\. 工作流文件下载](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD)\n*   [2\\. 按步骤完成工作流的运行](#2-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C)\n*   [3\\. 补充说明](#3-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E)\n*   [相关节点详解](#%E7%9B%B8%E5%85%B3%E8%8A%82%E7%82%B9%E8%AF%A6%E8%A7%A3)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration",
  "markdown": "# Recraft Style - Digital Illustration - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Recraft Style Digital Illustration 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg) 此节点创建一个风格配置对象，用于指导Recraft的图像生成过程朝向数字插画的视觉效果。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| substyle | 选择项 | None | 数字插画风格的具体子风格 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | 风格配置对象，连接到Recraft生成节点 |\n\n## 使用示例\n\n[\n\n## Recraft Text to Image 工作流示例\n\nRecraft Text to Image 工作流示例\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\nclass RecraftStyleV3DigitalIllustrationNode(RecraftStyleV3RealisticImageNode):\n    \"\"\"\n    Select digital_illustration style and optional substyle.\n    \"\"\"\n\n    RECRAFT_STYLE = RecraftStyleV3.digital_illustration\n\n```\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image",
  "markdown": "# Stability AI Stable Diffusion 3.5 API 节点 ComfyUI 官方示例\n\n[Stability AI Stable Diffusion 3.5 Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image) 节点允许你使用 Stability AI 的 Stable Diffusion 3.5 模型，通过文本提示词或参考图像创建高质量、细节丰富的图像内容。 本篇指南中，我们将引导你如何使用对应节点来进行文生图和图生图的工作流设置。\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![Stability AI Stable Diffusion 3.5 文生图工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_diffusion_3-5-t2i.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![Stability AI Stable Diffusion 3.5 文生图步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  (可选)修改 `Stability AI Stable Diffusion 3.5 Image` 节点中的 `prompt` 参数，输入你想要生成的图像描述。提示词越详细，生成的图像质量往往越好。\n2.  (可选)选择 `model` 参数来选择使用的SD 3.5模型版本。\n3.  (可选)选择 `style_preset` 参数来控制图像的视觉风格。不同的预设风格会产生不同风格特点的图像，如”cinematic”（电影感）、“anime”（动漫风格）等。选择”None”则不应用任何特定风格。\n4.  (可选)编辑 `String(Multiline)` 来修改负向提示词，用于指定不希望在生成图像中出现的元素。\n5.  点击 `Run（运行）` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的生成。\n6.  等待 API 返回结果后，你可在 `Save Image` 节点中查看生成的图像，对应的图像也会被保存至 `ComfyUI/output/` 目录下。\n\n### 3\\. 补充说明\n\n*   **提示词(Prompt)**：提示词是生成过程中最重要的参数之一，详细、清晰的描述会带来更好的效果。可以包含场景、主体、颜色、光照、风格等元素。\n*   **CFG Scale**：控制生成器对提示词的遵循程度，值越高生成的图像越接近提示词描述，但太高可能会导致过度饱和或不自然的结果。\n*   **风格预设(Style Preset)**：提供多种预设风格，能够快速定义图像的整体风格。\n*   **负面提示词(Negative Prompt)**：用于指定不希望在生成图像中出现的元素\n*   **Seed 参数**：可以用于复现或微调生成结果，对于创作过程中的迭代很有帮助。\n*   当前 `Load Image` 节点为 “绕过（Bypass）” 模式，如需启用可以参考步骤图在对应节点上右键然后将”模式（Mode）“设置为”总是（Always）” 来启用输入,即可转为图生图模式。\n*   `image_denoise` 在没有输入图像时，该参数不起作用。\n\n## Stability AI Stable Diffusion 3.5 图生图工作流\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![Stability AI Stable Diffusion 3.5 图生图工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/stable_diffusion_3_5-i2i.png) 下载下面的图片我们将用于输入图片 ![Stability AI Stable Diffusion 3.5 图生图工作流输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/input.jpg) \n\n### 2\\. 按步骤完成工作流的运行\n\n![Stability AI Stable Diffusion 3.5 图生图步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg) 你可参考图片中的序号来完成图生图工作流运行：\n\n1.  通过 `Load Image` 节点加载一张参考图像，该图像将作为生成的基础。\n2.  (可选)修改 `Stability AI Stable Diffusion 3.5 Image` 节点中的 `prompt` 参数，描述你希望在参考图像基础上改变或增强的元素。\n3.  (可选)选择 `style_preset` 参数来控制图像的视觉风格，不同的预设风格会产生不同风格特点的图像。\n4.  (可选｜重要)调整 `image_denoise` 参数（范围0.0-1.0）来控制对原始图像的修改程度：\n    *   值越接近0.0，生成的图像越接近输入的参考图像（当 0.0 时，基本和原始图像一致）\n    *   值越接近1.0，生成的图像越接近纯文本生成的效果（当 1.0 时，相当于没有提供参考图像）\n5.  (可选)编辑 `String(Multiline)` 来修改负向提示词，用于指定不希望在生成图像中出现的元素。\n6.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的生成。\n7.  等待 API 返回结果后，你可在 `Save Image` 节点中查看生成的图像，对应的图像也会被保存至 `ComfyUI/output/` 目录下。\n\n### 3\\. 补充说明\n\n下图展示了使用相同参数设置下，有无输入图像的对比效果： ![Stability AI Stable Diffusion 3.5 有无图像输入对比](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg) **图像去噪强度(Image Denoise)**：这个参数决定了生成过程中保留原始图像特征的程度，是图生图模式中最关键的调节参数,下图是不同的去噪强度下生成的图像效果： ![Stability AI Stable Diffusion 3.5 图生图去噪强度说明](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg)\n\n*   **参考图像选择**：选择具有清晰主体和良好构图的图像通常能获得更好的结果。\n*   **提示词技巧**：在图生图模式中，提示词应该更多地关注你希望改变或增强的部分，而不需要描述已经存在于图像中的所有元素。\n*   **模式切换**：当提供输入图像时，节点会自动从文本到图像模式切换到图像到图像模式，并且会忽略宽高比参数设置。\n\n## 相关节点详解\n\n你可查阅下面的文档了解对应节点的详细参数设置等\n\n[\n\n## Stability Stable Diffusion 3.5 Image 节点文档\n\nStability Stable Diffusion 3.5 Image API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2Fapi-node%2Fimage%2Fopenai%2Fopenai-dalle2",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/desktop/windows",
  "markdown": "# Windows桌面版 - ComfyUI\n\n**ComfyUI 桌面版（Desktop）** 是一个独立的安装版本，可以像常规软件一样进行安装，支持快捷安装自动配置 **Python环境及依赖** ，支持导入已有的 ComfyUI 设置、模型、工作流和文件，可以快速从已有的[ComfyUI 便携版](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)迁移到桌面版 ComfyUI 桌面版是一个开源项目，完整代码请访问 [这里](https://github.com/Comfy-Org/desktop) ComfyUI 桌面版(Windows)硬件要求：\n\n*   NVIDIA 显卡\n\n本篇教程将引导你完成对应的软件安装，并说明相关安装配置说明。\n\n请点击下面的按钮下载对应的针对 Windows 系统的 **ComfyUI 桌面版** 安装包\n\n[\n\nDownload for Windows (NVIDIA)\n\n](https://download.comfy.org/windows/nsis/x64)\n\n## ComfyUI 桌面版安装步骤\n\n双击下载到的安装包文件，首先将会执行一次自动安装，并在桌面生成一个 **ComfyUI 桌面版** 的快捷方式 ![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-shortcut.jpg) 双击对应的快捷，进入 ComfyUI 的初始化设置\n\n### ComfyUI 桌面版初始化流程\n\n## 进行第一次图片生成\n\n安装成功后，你可以参考访问下面的章节，开始你的 ComfyUI 之路。\n\n[\n\n## 进行第一次图片生成\n\n本教程将引导你完成第一次的模型安装以及对应的文本到图片的生成\n\n\n\n](https://docs.comfy.org/zh-CN/get_started/first_generation)\n\n## 如何更新 ComfyUI 桌面版\n\n目前 ComfyUI 桌面版更新采用自动检测更新，请确保在设置中已经启用自动更新 ![ComfyUI 桌面版设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg) 你也可以在 `菜单` —> `帮助` —> `检查更新` 中选择手动检查是否有可用的更新 ![ComfyUI 桌面版检查更新](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_check_for_updates.jpg)\n\n## 添加外部模型路径\n\n如果你想要在 `ComfyUI/models` 之外管理你的模型文件，可能出于以下原因:\n\n*   你有多个 ComfyUI 实例，你想要让这些实例共享模型文件，从而减少磁盘占用\n*   你有多个不同的类型的 GUI 程序，如：WebUI, 你想要他们共用模型文件\n*   模型文件无法被识别或读取到\n\n我们提供了通过 `extra_model_paths.yaml` 配置文件来添加额外模型搜索路径的方法。\n\n### 不同 ComfyUI 版本配置文件位置\n\n对于[便携版](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)和[手动安装](https://docs.comfy.org/zh-CN/installation/manual_install)的 ComfyUI版本，你可以在 ComfyUI 的根目录下找到 `extra_model_paths.yaml.example` 的示例文件\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\n复制并重命名为 `extra_model_paths.yaml` 来使用, 并保持在 ComfyUI 的根目录下, 路径应该是 `ComfyUI/extra_model_paths.yaml`你也可以在 [这里](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 找到配置示例文件\n\n### 配置示例\n\n比如，你需要额外让 ComfyUI 识别的模型文件位于下面的文件夹:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 loras/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\n那么你可以进行如下的配置来让 ComfyUI 识别到你设备上的模型路径\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\n或者使用\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\n或者你也可以参考默认的 [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 来配置，保存之后， 需要 **重启 ComfyUI** 才能生效。 下面是完整的原始的配置配置示例:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### 添加额外自定义节点路径\n\n除了添加外部模型之外，你同样可以添加不在 ComfyUI 默认路径下的自定义节点路径\n\n下面是一个简单的配置示例（Mac 系统），请根据你的实际情况进行修改，并新增到对应的配置文件中，保存后需要 **重启 ComfyUI** 才能生效:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## 桌面端 Python 环境相关\n\n桌面端的安装将在你选择的安装目录下创建一个python 的虚拟环境，通常这是一个隐藏的 `.venv` 文件夹。 如果你需要为 ComfyUI 插件处理相关的依赖则需要在这个环境中进行处理，直接系统系统的命令行会有将对应依赖安装到系统环境的风险，请参考下面的指示完成对应环境的激活。\n\n### 如何使用 桌面端 的 python 环境？\n\n你可以使用桌面端自带的终端来使用 python 环境。 ![ComfyUI 桌面版终端](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_terminal.jpg) \n\n1.  点击菜单栏的 icon 打开底部面板\n2.  点击 `Terminal` 打开终端\n3.  如果你想要看对应环境的 python 安装位置，可以使用下面的命令\n\n```\n  python -c \"import sys; print(sys.executable)\"\n```\n\n## 如何卸载 ComfyUI 桌面版\n\n对于 **ComfyUI 桌面版** 你可以在 Windows 的系统设置中使用系统的卸载功能来完成对应软件的卸载操作 ![ComfyUI 桌面版卸载](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-uninstall-comfyui.jpg) 如果你想要完全删除 **ComfyUI 桌面版** 的所有文件，你可以手动删除以下文件夹：\n\n*   C:\\\\Users<你的用户名>\\\\AppData\\\\Local@comfyorgcomfyui-electron-updater\n*   C:\\\\Users<你的用户名>\\\\AppData\\\\Local\\\\Programs@comfyorgcomfyui-electron\n*   C:\\\\Users<你的用户名>\\\\AppData\\\\Roaming\\\\ComfyUI\n\n以上的操作并不会删除以下你的以下文件夹，如果你需要删除对应文件的话，请手动删除：\n\n*   models 模型文件\n*   custom nodes 自定义节点\n*   input/output directories. 图片输入/输出目录\n\n## 故障排除\n\n### 显示不支持的设备\n\n![ComfyUI 安装步骤 - 不支持的设备](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-0.jpg) 由于 ComfyUI 桌面版（Windows）仅支持可以使用 **CUDA 的 Nvdia 显卡** 所以如果你的设备不支持，可能会出现此界面\n\n*   请更换使用支持的设备\n*   或者考虑使用 [ComfyUI便携版](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows) 或者通过[手动安装](https://docs.comfy.org/zh-CN/installation/manual_install)来使用 ComfyUI\n\n### 如何定位安装错误\n\n如果安装失败，你应该可以看到下面的界面显示 ![ComfyUI 安装失败](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg) 此时建议你采取以下几种方式查找错误原因\n\n1.  点击 `Show Teriminal` 查看错误问题输出\n2.  点击 `Open Logs` 查看安装过程日志\n3.  访问官方论坛查找错误反馈\n4.  点击`Reinstall`尝试重新安装\n\n建议在提交反馈之前，你可以将对应的**错误输出**以及 **log 文件**信息提供给类似 **GPT**一类的工具 ![ComfyUI 安装失败-错误日志](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg) ![ComfyUI 安装失败-GPT 反馈](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg) 如上图，询问对应错误的原因，或者完全删除 ComfyUI 后进行安装重试\n\n### 反馈错误\n\n如果在安装过程中，你发生了任何错误，请通过以下任意方式查看是否有类似错误反馈，或者向我们提交错误\n\n*   Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)\n*   Comfy 官方论坛: [https://forum.comfy.org/](https://forum.comfy.org/)\n\n请在提交错误时确保提交了以下日志以及配置文件，方便我们进行问题的定位和查找\n\n1.  日志文件\n\n| 文件名 | 描述  | 位置  |\n| --- | --- | --- |\n| main.log | 包含与桌面应用和服务器启动相关的日志，来自桌面的 Electron 进程。 |     |\n| comfyui.log | 包含与 ComfyUI 正常运行相关的日志，例如核心 ComfyUI 进程的终端输出。 |     |\n\n![ComfyUI 日志文件输出位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)\n\n2.  配置文件\n\n| 文件名 | 描述  | 位置  |\n| --- | --- | --- |\n| extra\\_model\\_paths.yaml | 包含 ComfyUI 将搜索模型和自定义节点的额外路径。 |     |\n| config.json | 包含应用配置。此文件通常不应直接编辑。 |     |\n\n![ComfyUI 配置文件位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-video",
  "markdown": "# Luma Text to Video API 节点 ComfyUI 官方示例\n\n[Luma Text to Video](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-text-to-video) 节点允许你使用Luma AI的创新视频生成技术，通过文本描述创建高质量、流畅的视频内容。 本篇指南中，我们将引导你如何使用对应节点来进行文本到视频的工作流设置。\n\n你可查阅下面的文档了解对应节点的详细参数设置等\n\n[\n\n## Luma Text to Video 节点文档\n\nLuma Text to Video API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-text-to-video)[\n\n## Luma Concepts 节点文档\n\nLuma Concepts API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-concepts)\n\n## Luma Text to Video API 节点文本到视频工作流\n\nLuma Text to Video 节点需要提供文本提示词来描述生成视频内容。在本篇指南中，我们制作了使用`prompt`和`luma_concepts`的示例，让你体验Luma AI在视频生成上的优秀能力。\n\n### 1\\. 工作流文件下载\n\n下面的视频的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![Luma 文本到视频工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2v/luma_t2v.mp4)\n\n### 2\\. 按步骤完成工作流的运行\n\n![Luma 文本到视频工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg) 你可参考图片中的序号来完成最基础的工作流运行：\n\n1.  在 `Luma Text to Video` 节点中编写提示词，描述你希望生成的视频内容\n2.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频的生成\n3.  等待 API 返回结果后，你可在 `Save Video` 节点中查看生成的视频，对应的视频也会被保存至 `ComfyUI/output/` 目录下\n\n> (可选)修改 `Luma Concepts` 节点来控制相机运动效果，为视频添加专业的镜头语言\n\n### 3\\. 补充说明\n\n*   **提示词撰写**：尽可能详细地描述场景、主体、动作和氛围，以获得最佳生成效果\n*   **Luma Concepts**：主要用于控制相机运动，提供更专业的视频镜头效果\n*   **Seed 参数**：仅用于确定节点是否应重新运行，但实际生成结果与种子值无关\n*   **模型选择**：不同的视频生成模型有不同的特点，可以通过调整 model 参数来选择\n*   **分辨率与时长**：可以通过 resolution 和 duration 参数来调整输出视频的分辨率和时长\n*   **Ray 1.6 模型注意事项**：当使用 Ray 1.6 模型时，duration 和 resolution 参数将不会生效\n\n在此页面\n\n*   [Luma Text to Video 节点文档](#luma-text-to-video-%E8%8A%82%E7%82%B9%E6%96%87%E6%A1%A3)\n*   [Luma Text to Video API 节点文本到视频工作流](#luma-text-to-video-api-%E8%8A%82%E7%82%B9%E6%96%87%E6%9C%AC%E5%88%B0%E8%A7%86%E9%A2%91%E5%B7%A5%E4%BD%9C%E6%B5%81)\n*   [1\\. 工作流文件下载](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD)\n*   [2\\. 按步骤完成工作流的运行](#2-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C)\n*   [3\\. 补充说明](#3-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-text-to-video",
  "markdown": "# Luma Text to Video - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Luma Text to Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg) Luma Text to Video 节点允许你使用Luma AI的创新视频生成技术，通过文本描述创建高质量、流畅的视频内容。\n\n## 节点功能\n\n此节点连接到Luma AI的文本到视频API，让用户能够通过详细的文本提示词生成动态视频内容。\n\n## 参数说明\n\n### 基本参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt | 字符串 | \"\"  | 描述要生成视频内容的文本提示词 |\n| model | 选择项 | \\-  | 使用的视频生成模型 |\n| aspect\\_ratio | 选择项 | ”ratio\\_16\\_9” | 视频宽高比 |\n| resolution | 选择项 | ”res\\_540p” | 视频分辨率 |\n| duration | 选择项 | \\-  | 视频时长选项 |\n| loop | 布尔值 | False | 是否循环播放视频 |\n| seed | 整数  | 0   | 随机种子，用于决定节点是否需要重新运行；实际结果与种子无关 |\n\n### 可选参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPTS | 可选的摄像机概念，通过Luma Concepts节点控制摄像机运动 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n\n## 使用示例\n\n[](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-video)\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass LumaTextToVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaVideoModel],),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in LumaAspectRatio],\n                    {\n                        \"default\": LumaAspectRatio.ratio_16_9,\n                    },\n                ),\n                \"resolution\": (\n                    [resolution.value for resolution in LumaVideoOutputResolution],\n                    {\n                        \"default\": LumaVideoOutputResolution.res_540p,\n                    },\n                ),\n                \"duration\": ([dur.value for dur in LumaVideoModelOutputDuration],),\n                \"loop\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        aspect_ratio: str,\n        resolution: str,\n        duration: str,\n        loop: bool,\n        seed,\n        luma_concepts: LumaConceptChain = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        duration = duration if model != LumaVideoModel.ray_1_6 else None\n        resolution = resolution if model != LumaVideoModel.ray_1_6 else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations\",\n                method=HttpMethod.POST,\n                request_model=LumaGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaGenerationRequest(\n                prompt=prompt,\n                model=model,\n                resolution=resolution,\n                aspect_ratio=aspect_ratio,\n                duration=duration,\n                loop=loop,\n                concepts=luma_concepts.create_api_model() if luma_concepts else None,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.assets.video)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/minimax/minimax-text-to-video",
  "markdown": "# MiniMax Text to Video - ComfyUI 原生节点文档\n\n![ComfyUI 原生 MiniMax Text to Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg) MiniMax Text to Video 节点通过连接 MiniMax 的API，允许用户利用文本提示词生成高质量、流畅的视频内容。该节点支持不同的视频生成模型，可以创建各种风格和类型的短视频片段。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt\\_text | 字符串 | \"\"  | 用于指导视频生成的文本提示词 |\n| model | 选择项 | ”T2V-01” | 使用的视频生成模型，可选值包括”T2V-01”和”T2V-01-Director” |\n| seed | 整数  | 0   | 生成的随机种子，影响初始噪声创建，默认值为0 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-03)\\]\n\n```\n\nclass MinimaxTextToVideoNode:\n    \"\"\"\n    Generates videos synchronously based on a prompt, and optional parameters using Minimax's API.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt_text\": (\n                    \"STRING\",\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt to guide the video generation\",\n                    },\n                ),\n                \"model\": (\n                    [\n                        \"T2V-01\",\n                        \"T2V-01-Director\",\n                    ],\n                    {\n                        \"default\": \"T2V-01\",\n                        \"tooltip\": \"Model to use for video generation\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from prompts using Minimax's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Minimax\"\n    API_NODE = True\n    OUTPUT_NODE = True\n\n    def generate_video(\n        self,\n        prompt_text,\n        seed=0,\n        model=\"T2V-01\",\n        image: torch.Tensor=None, # used for ImageToVideo\n        subject: torch.Tensor=None, # used for SubjectToVideo\n        auth_token=None,\n    ):\n        '''\n        Function used between Minimax nodes - supports T2V, I2V, and S2V, based on provided arguments.\n        '''\n        # upload image, if passed in\n        image_url = None\n        if image is not None:\n            image_url = upload_images_to_comfyapi(image, max_images=1, auth_token=auth_token)[0]\n\n        # TODO: figure out how to deal with subject properly, API returns invalid params when using S2V-01 model\n        subject_reference = None\n        if subject is not None:\n            subject_url = upload_images_to_comfyapi(subject, max_images=1, auth_token=auth_token)[0]\n            subject_reference = [SubjectReferenceItem(image=subject_url)]\n\n\n        video_generate_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/video_generation\",\n                method=HttpMethod.POST,\n                request_model=MinimaxVideoGenerationRequest,\n                response_model=MinimaxVideoGenerationResponse,\n            ),\n            request=MinimaxVideoGenerationRequest(\n                model=Model(model),\n                prompt=prompt_text,\n                callback_url=None,\n                first_frame_image=image_url,\n                subject_reference=subject_reference,\n                prompt_optimizer=None,\n            ),\n            auth_token=auth_token,\n        )\n        response = video_generate_operation.execute()\n\n        task_id = response.task_id\n        if not task_id:\n            raise Exception(f\"Minimax generation failed: {response.base_resp}\")\n\n        video_generate_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/query/video_generation\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=MinimaxTaskResultResponse,\n                query_params={\"task_id\": task_id},\n            ),\n            completed_statuses=[\"Success\"],\n            failed_statuses=[\"Fail\"],\n            status_extractor=lambda x: x.status.value,\n            auth_token=auth_token,\n        )\n        task_result = video_generate_operation.execute()\n\n        file_id = task_result.file_id\n        if file_id is None:\n            raise Exception(\"Request was not successful. Missing file ID.\")\n        file_retrieve_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/files/retrieve\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=MinimaxFileRetrieveResponse,\n                query_params={\"file_id\": int(file_id)},\n            ),\n            request=EmptyRequest(),\n            auth_token=auth_token,\n        )\n        file_result = file_retrieve_operation.execute()\n\n        file_url = file_result.file.download_url\n        if file_url is None:\n            raise Exception(\n                f\"No video was found in the response. Full response: {file_result.model_dump()}\"\n            )\n        logging.info(f\"Generated video URL: {file_url}\")\n\n        video_io = download_url_to_bytesio(file_url)\n        if video_io is None:\n            error_msg = f\"Failed to download video from {file_url}\"\n            logging.error(error_msg)\n            raise Exception(error_msg)\n        return (VideoFromFile(video_io),)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/credits",
  "markdown": "# ComfyUI 积分管理 - ComfyUI\n\n积分系统是为了支持 `API Nodes` 节点而新增的，由于调用闭源 AI 模型需要消耗Token，所以对应的积分管理是很有必要的，在默认情况下积分界面并不会展示，请首先在`设置` -> `用户`中登录对应的 ComfyUI 账号，然后你就可以在 `设置` -> `积分` 中查看关联账号的积分信息了。 ![ComfyUI 积分界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/menu-credits.jpg)\n\n## 如何购买积分?\n\n下面是对应的积分购买演示视频：\n\n详细操作步骤如下：\n\n## 常见问题"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/appearance",
  "markdown": "# ComfyUI 外观设置 - ComfyUI\n\n这部分的设置主要用于自定义 ComfyUI 的外观，包括色彩主题、背景图片、节点样式等。\n\n## 色彩主题\n\n自定义 ComfyUI 外观的主要方式是通过内置的调色板系统。 ![色彩主题](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/color-palette.jpg)\n\n1.  切换 ComfyUI 主题\n2.  将当前选中的主题导出为 JSON 格式\n3.  从Json文件中载入自定义主题配置\n4.  删除自定义主题配置\n\n### 如何自定义颜色主题\n\n调色板允许您修改许多特定属性。以下是一些最常自定义的元素，颜色采用十六进制表示：\n\n```\n{\n  \"id\": \"dark\",                     // 必须是唯一的，不能和其它主题的id相同\n  \"name\": \"Dark (Default)\",         // 主题名称,显示在主题选择器中\n  \"colors\": {\n    \"node_slot\": {                  // 节点连接槽的颜色配置\n      \"CLIP\": \"#FFD500\",            // CLIP 模型连接槽颜色\n      \"CLIP_VISION\": \"#A8DADC\",     // CLIP Vision 模型连接槽颜色\n      \"CLIP_VISION_OUTPUT\": \"#ad7452\", // CLIP Vision 输出连接槽颜色\n      \"CONDITIONING\": \"#FFA931\",     // 条件控制连接槽颜色\n      \"CONTROL_NET\": \"#6EE7B7\",     // ControlNet 模型连接槽颜色\n      \"IMAGE\": \"#64B5F6\",           // 图像数据连接槽颜色\n      \"LATENT\": \"#FF9CF9\",          // 潜在空间连接槽颜色\n      \"MASK\": \"#81C784\",            // 蒙版数据连接槽颜色\n      \"MODEL\": \"#B39DDB\",           // 模型连接槽颜色\n      \"STYLE_MODEL\": \"#C2FFAE\",     // 风格模型连接槽颜色\n      \"VAE\": \"#FF6E6E\",             // VAE 模型连接槽颜色\n      \"NOISE\": \"#B0B0B0\",           // 噪声数据连接槽颜色\n      \"GUIDER\": \"#66FFFF\",          // 引导器连接槽颜色\n      \"SAMPLER\": \"#ECB4B4\",         // 采样器连接槽颜色\n      \"SIGMAS\": \"#CDFFCD\",          // Sigmas 数据连接槽颜色\n      \"TAESD\": \"#DCC274\"            // TAESD 模型连接槽颜色\n    },\n    \"litegraph_base\": {             // LiteGraph 基础界面配置\n      \"BACKGROUND_IMAGE\": \"\",        // 背景图片,默认为空\n      \"CLEAR_BACKGROUND_COLOR\": \"#222\", // 主画布背景色\n      \"NODE_TITLE_COLOR\": \"#999\",    // 节点标题文本颜色\n      \"NODE_SELECTED_TITLE_COLOR\": \"#FFF\", // 选中节点的标题颜色\n      \"NODE_TEXT_SIZE\": 14,          // 节点文本大小\n      \"NODE_TEXT_COLOR\": \"#AAA\",     // 节点文本颜色\n      \"NODE_TEXT_HIGHLIGHT_COLOR\": \"#FFF\", // 节点文本高亮颜色\n      \"NODE_SUBTEXT_SIZE\": 12,       // 节点子文本大小\n      \"NODE_DEFAULT_COLOR\": \"#333\",   // 节点默认颜色\n      \"NODE_DEFAULT_BGCOLOR\": \"#353535\", // 节点默认背景色\n      \"NODE_DEFAULT_BOXCOLOR\": \"#666\", // 节点默认边框颜色\n      \"NODE_DEFAULT_SHAPE\": 2,        // 节点默认形状\n      \"NODE_BOX_OUTLINE_COLOR\": \"#FFF\", // 节点边框轮廓颜色\n      \"NODE_BYPASS_BGCOLOR\": \"#FF00FF\", // 节点旁路背景色\n      \"NODE_ERROR_COLOUR\": \"#E00\",    // 节点错误状态颜色\n      \"DEFAULT_SHADOW_COLOR\": \"rgba(0,0,0,0.5)\", // 默认阴影颜色\n      \"DEFAULT_GROUP_FONT\": 24,       // 分组默认字体大小\n      \"WIDGET_BGCOLOR\": \"#222\",       // 小部件背景色\n      \"WIDGET_OUTLINE_COLOR\": \"#666\", // 小部件轮廓颜色\n      \"WIDGET_TEXT_COLOR\": \"#DDD\",    // 小部件文本颜色\n      \"WIDGET_SECONDARY_TEXT_COLOR\": \"#999\", // 小部件次要文本颜色\n      \"WIDGET_DISABLED_TEXT_COLOR\": \"#666\", // 小部件禁用状态文本颜色\n      \"LINK_COLOR\": \"#9A9\",          // 连接线颜色\n      \"EVENT_LINK_COLOR\": \"#A86\",    // 事件连接线颜色\n      \"CONNECTING_LINK_COLOR\": \"#AFA\", // 正在连接时的连接线颜色\n      \"BADGE_FG_COLOR\": \"#FFF\",      // 徽章前景色\n      \"BADGE_BG_COLOR\": \"#0F1F0F\"    // 徽章背景色\n    },\n    \"comfy_base\": {                  // ComfyUI 基础界面配置\n      \"fg-color\": \"#fff\",            // 前景色\n      \"bg-color\": \"#202020\",         // 背景色\n      \"comfy-menu-bg\": \"#353535\",    // 菜单背景色\n      \"comfy-menu-secondary-bg\": \"#303030\", // 次级菜单背景色\n      \"comfy-input-bg\": \"#222\",      // 输入框背景色\n      \"input-text\": \"#ddd\",          // 输入文本颜色\n      \"descrip-text\": \"#999\",        // 描述文本颜色\n      \"drag-text\": \"#ccc\",           // 拖拽文本颜色\n      \"error-text\": \"#ff4444\",       // 错误文本颜色\n      \"border-color\": \"#4e4e4e\",     // 边框颜色\n      \"tr-even-bg-color\": \"#222\",    // 表格偶数行背景色\n      \"tr-odd-bg-color\": \"#353535\",  // 表格奇数行背景色\n      \"content-bg\": \"#4e4e4e\",       // 内容区背景色\n      \"content-fg\": \"#fff\",          // 内容区前景色\n      \"content-hover-bg\": \"#222\",    // 内容区悬停背景色\n      \"content-hover-fg\": \"#fff\",    // 内容区悬停前景色\n      \"bar-shadow\": \"rgba(16, 16, 16, 0.5) 0 0 0.5rem\" // 栏阴影效果\n    }\n  }\n}\n```\n\n## 画布\n\n### 背景图片\n\n*   版本要求：ComfyUI 前端版本 1.20.5 或更新版本\n*   功能：为画布设置自定义背景图片，提供更加个性化的工作空间，你可以上传图片或者使用网络图片来为画布设置背景图片\n\n![设置背景图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/set-as-bg.jpg)\n\n## 节点\n\n### 节点不透明度\n\n*   功能：设置节点的不透明度，0表示完全透明，1表示完全不透明\n\n![节点不透明度](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/node-opacity.jpg)\n\n## 节点组件\n\n### 文本域小部件字体大小\\*\\*\n\n*   **范围**：8 - 24\n*   **功能**：设置文本域小部件中的字体大小，调整文本输入框中文字的显示大小，提升可读性 ![文本域小部件字体大小](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/textarea-font-size.jpg)\n\n## 侧边栏\n\n### 统一侧边栏宽度\n\n*   **功能**：启用后，当你在不同的侧边栏之间切换时，侧边栏的宽度将统一为一致的宽度，如果禁用，不同的侧边栏的宽度在切换时可以保持自定义的宽度\n\n### 侧边栏大小\n\n*   **功能**：控制侧边栏的尺寸大小，可以设置为正常或者小\n\n### 侧边栏位置\n\n*   **功能**：控制侧边栏显示在界面的左侧还是右侧，允许用户根据使用习惯调整侧边栏位置\n\n## 树形浏览器\n\n### 树形浏览器项目内边距\n\n*   **功能**：设置树形浏览器（侧边栏面板）中项目的内边距，调整树形结构中各项目之间的间距\n\n![树形浏览器项目内边距](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/padding.jpg)\n\n## 使用user.css进行高级外观自定义\n\n对于调色板不能提供足够控制的情况，您可以通过 user.css 文件使用自定义 CSS。此方法推荐给需要自定义调色板系统中不可用元素的高级用户。\n\n### 要求\n\n*   ComfyUI 前端版本 1.20.5 或更新版本\n\n### 设置 user.css\n\n1.  在 ComfyUI 用户目录（与工作流和设置相同位置 - 请参阅下面的位置详细信息）中创建一个名为 `user.css` 的文件\n2.  在此文件中添加您的自定义 CSS 规则\n3.  重启 ComfyUI 或刷新页面以应用更改\n\n### 用户目录位置\n\nComfyUI 用户目录是存储您的个人设置、工作流和自定义内容的地方。位置取决于您的安装类型：\n\n此位置包含您的工作流、设置和其他用户特定文件。 找到上述文件夹位置后，请将对应的 Css 文件复制到对应的用户目录中如默认用户文件夹为`ComfyUI/user/default`，然后重启 ComfyUI 或刷新页面以应用更改\n\n### user.css 示例及相关说明\n\n`user.css` 文件会在启动的早期就进行加载。所以能需要在 CSS 规则中使用 `!important` 来确保它们覆盖默认样式。 **user.css 自定义示例**\n\n```\n/* 增加输入框和菜单中的字体大小以提高可读性 */\n.comfy-multiline-input, .litecontextmenu .litemenu-entry {\n    font-size: 20px !important;\n}\n\n/* 使上下文菜单项更大，便于选择 */\n.litegraph .litemenu-entry,\n.litemenu-title {\n  font-size: 24px !important; \n}\n\n/* 为调色板中不可用的特定元素自定义样式 */\n.comfy-menu {\n    border: 1px solid rgb(126, 179, 189) !important;\n    border-radius: 0px 0px 0px 10px !important;\n    backdrop-filter: blur(2px);\n}\n```\n\n**最佳实践**\n\n1.  **首先使用调色板**进行大多数自定义\n2.  **仅在必要时使用 user.css**，用于调色板未涵盖的元素\n3.  **在进行重大更改前导出您的主题**，以便在需要时恢复\n4.  **与社区分享您的主题**，以启发他人\n\n**故障排除**\n\n*   如果您的调色板更改没有显示，尝试刷新页面\n*   如果 CSS 自定义不起作用，检查您是否使用前端版本 1.20.5+\n*   尝试在未应用的 user.css 规则中添加 `!important`\n*   保留您的自定义备份，以便轻松恢复"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-image-to-video",
  "markdown": "# Luma Image to Video API 节点 ComfyUI 官方示例\n\n[Luma Image to Video](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-image-to-video) 节点允许你使用Luma AI的先进技术将静态图像转换为流畅、动态的视频内容，为图像赋予生命力和动态特性。 本篇指南中，我们将引导你如何使用对应节点来进行图像到视频的工作流设置。\n\n你可查阅下面的文档了解对应节点的详细参数设置等\n\n[\n\n## Luma Image to Video 节点文档\n\nLuma Image to Video API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-image-to-video)[\n\n## Luma Concepts 节点文档\n\nLuma Concepts API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-concepts)\n\n## Luma Image to Video API 节点图像到视频工作流\n\nLuma Image to Video 节点需要至少提供一个图像输入（`first_image`或`last_image`），结合文本提示词来确定视频的动态效果。在本篇指南中，我们制作了使用`first_image`和`luma_concepts`的示例，让你体验Luma AI在视频生成上的优秀能力。\n\n### 1\\. 工作流文件下载\n\n下面的视频的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![Luma 图像到视频工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/luma_i2v.mp4) 请下载下面的图片，我们将会用作输入： ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/input.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![Luma 图像到视频工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg) 你可参考图片中的序号来完成最基础的工作流运行：\n\n1.  在 `first_image` 节点中上传你的输入图像\n2.  (可选)在 Luma Image to Video 节点中编写提示词，描述你希望视频如何动态展示图像\n3.  (可选)修改 `Luma Concepts` 节点来控制相机运动效果，为视频添加专业的镜头语言\n4.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频的生成\n5.  等待 API 返回结果后，你可在 `Save Video` 节点中查看生成的视频，对应的视频也会被保存至 `ComfyUI/output/` 目录下\n\n### 3\\. 补充说明\n\n*   **输入图像要求**：`first_image` 和 `last_image` 至少需要提供一个，每个输入最多只接受1张图片\n*   **Luma Concepts**：主要用于控制相机运动，提供更专业的视频镜头效果\n*   **Seed 参数**：仅用于确定节点是否应重新运行，但实际生成结果与种子值无关\n*   **启用输入节点**：要启用对应的输入请在目前紫色”绕过（Bypass）“模式的节点上右键，设置对应的”模式（mode）“为”总是（always）”\n*   **模型选择**：不同的视频生成模型有不同的特点，可以通过调整 model 参数来选择\n*   **分辨率与时长**：可以通过 resolution 和 duration 参数来调整输出视频的分辨率和时长"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pika/pika-text-to-video",
  "markdown": "# Pika 2.2 Text to Video - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Pika 2.2 Text to Video 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg) Pika 2.2 Text to Video 节点允许你使用Pika的2.2版本API，通过文本描述创建视频内容。此节点连接到Pika的文本到视频API，让用户能够通过文本提示词生成视频，并提供多种参数控制生成效果。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 默认值 | 说明  |\n| --- | --- | --- | --- |\n| prompt\\_text | 字符串 | \"\"  | 描述要生成视频内容的文本提示词 |\n| negative\\_prompt | 字符串 | \"\"  | 指定不希望在视频中出现的元素 |\n| seed | 整数  | 0   | 生成过程的随机种子 |\n| resolution | 选择项 | ”1080p” | 生成视频的分辨率 |\n| duration | 选择项 | ”5s” | 生成视频的持续时间 |\n| aspect\\_ratio | 浮点数 | 1.7777777777777777 | 输出视频的宽高比，范围0.4-2.5，步长0.001 |\n\n### 输出参数\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| VIDEO | 视频  | 生成的视频结果 |\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-05)\\]\n\n```\n\nclass PikaTextToVideoNodeV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Text to Video Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                **cls.get_base_inputs_types(PikaBodyGenerate22T2vGenerate22T2vPost),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.FLOAT,\n                    PikaBodyGenerate22T2vGenerate22T2vPost,\n                    \"aspectRatio\",\n                    step=0.001,\n                    min=0.4,\n                    max=2.5,\n                    default=1.7777777777777777,\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Sends a text prompt to the Pika API v2.2 to generate a video.\"\n\n    def api_call(\n        self,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        aspect_ratio: float,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika 2.2 Text to Video.\"\"\"\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_TEXT_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22T2vGenerate22T2vPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=PikaBodyGenerate22T2vGenerate22T2vPost(\n                promptText=prompt_text,\n                negativePrompt=negative_prompt,\n                seed=seed,\n                resolution=resolution,\n                duration=duration,\n                aspectRatio=aspect_ratio,\n            ),\n            auth_token=auth_token,\n            content_type=\"application/x-www-form-urlencoded\",\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pixverse/pixverse-template",
  "markdown": "# Pixverse Template - ComfyUI 原生节点文档\n\n![ComfyUI 原生 Pixverse Template 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg) Pixverse Template 节点允许你从预定义的视频生成模板中选择，用于控制Pixverse视频生成节点的输出风格和效果。这是一个辅助节点，可以连接到Pixverse的视频生成节点，让用户能够快速应用预设的视频风格，而无需手动调整复杂的参数组合。\n\n## 参数说明\n\n### 必需参数\n\n| 参数  | 类型  | 说明  |\n| --- | --- | --- |\n| template | 选择项 | 从可用的预设视频生成模板列表中选择一个模板 |\n\n### 输出\n\n| 输出  | 类型  | 说明  |\n| --- | --- | --- |\n| pixverse\\_template | PixverseIO.TEMPLATE | 包含所选模板ID的配置对象 |\n\n## 源码参考\n\n\\[节点源码 (更新于2025-05-05)\\]\n\n```\n\nclass PixverseTemplateNode:\n    \"\"\"\n    Select template for Pixverse Video generation.\n    \"\"\"\n\n    RETURN_TYPES = (PixverseIO.TEMPLATE,)\n    RETURN_NAMES = (\"pixverse_template\",)\n    FUNCTION = \"create_template\"\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"template\": (list(pixverse_templates.keys()), ),\n            }\n        }\n\n    def create_template(self, template: str):\n        template_id = pixverse_templates.get(template, None)\n        if template_id is None:\n            raise Exception(f\"Template '{template}' is not recognized.\")\n        # just return the integer\n        return (template_id,)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/about",
  "markdown": "# 关于页面 - ComfyUI\n\nAbout 页面是 ComfyUI 设置系统中的一个信息展示面板，用于显示应用程序版本信息、相关链接和系统统计数据，这些设置在向我们提交反馈问题时，可以提供给我们一些非常关键的信息。 ![about](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/settings-about.jpg)\n\n### 版本信息徽章\n\nAbout 页面显示以下核心版本信息：\n\n*   **ComfyUI 版本**：显示后端 ComfyUI 的版本号，链接到官方 GitHub 仓库\n*   **ComfyUI\\_frontend 版本**：显示前端界面的版本号，链接到前端 GitHub 仓库\n*   **Discord 社区**：提供 ComfyOrg Discord 服务器的链接\n*   **官方网站**：链接到 ComfyOrg 官方网站\n\n### 自定义节点徽章\n\n如果安装了自定义节点，About 页面还会显示自定义节点提供的额外徽章信息。这些徽章由各个自定义节点通过 `aboutPageBadges` 属性注册。\n\n### 系统统计信息\n\n页面底部显示详细的系统统计信息，包括：\n\n*   硬件配置信息\n*   软件环境信息\n*   系统性能数据\n\n## 扩展开发者指南\n\n扩展开发者可以通过在扩展配置中添加 `aboutPageBadges` 属性来向 About 页面添加自定义徽章：\n\n```\napp.registerExtension({\n  name: 'MyExtension',\n  aboutPageBadges: [\n    {\n      label: 'My Extension v1.0.0',\n      url: 'https://github.com/myuser/myextension',\n      icon: 'pi pi-github'\n    }\n  ]\n})\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/overview",
  "markdown": "# 概述 - ComfyUI\n\nComfyUI 是一个强大的 GenAI 推理引擎，可用于本地运行 AI 模型、创建工作流、开发自定义节点，以及部署为服务器。 ComfyUI 的主要功能包括：\n\n*   **[创建工作流](https://docs.comfy.org/zh-CN/development/core-concepts/workflow)**：工作流是一种编排 AI 模型和自动化任务的方式。它们是一系列相互连接形成管道的节点。\n*   **[自定义节点](https://docs.comfy.org/zh-CN/development/core-concepts/custom-nodes)**：任何人都可以编写自定义节点来扩展 ComfyUI 的功能。节点使用 Python 编写，并由社区发布。\n*   **扩展**：扩展是改进 ComfyUI 用户界面的第三方应用程序。\n*   **[部署](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_overview)**：ComfyUI 可以在您自己的环境中部署为 API 端点。"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/comms_messages",
  "markdown": "# 消息传递 - ComfyUI\n\n## 消息传递机制\n\n在工作流执行期间（或当执行队列状态发生变化时），`PromptExecutor` 会 通过 `PromptServer` 实例的 `send_sync` 方法向客户端回传消息。 这些消息由 `api.js` 文件中定义的 `socket` 事件监听器负责接收（截至本文撰写时，该监听器大致位于第 90 行，您也可以通过搜索 `this.socket.addEventListener` 找到它）。 该监听器会为每种已知的消息类型创建一个 `CustomEvent` 对象，并将其派发给所有已注册的相应监听器。 扩展程序可以遵循标准的 Javascript 模式来注册事件接收（此操作通常在 `setup()` 函数中完成）：\n\n```\napi.addEventListener(message_type, messageHandler);\n```\n\n如果 `message_type` 并非内置消息类型，系统会自动将其添加至已知消息类型列表。 注册的 `messageHandler` 函数在被调用时，会接收到一个 `CustomEvent` 对象。 该对象是对 `socket` 事件的扩展，额外增加了一个 `.detail` 属性，此属性是一个包含了服务器所发送数据的字典。因此，通常的使用方式如下：\n\n```\nfunction messageHandler(event) {\n    if (event.detail.node == aNodeIdThatIsInteresting) { // 判断是否为目标节点\n        // 利用 event.detail.other_things 中的数据执行相应操作\n    }\n}\n```\n\n### 内置消息类型\n\n在工作流执行期间（或当执行队列状态发生变化时），`PromptExecutor` 会通过 `PromptServer` 实例的 `send_sync` 方法向客户端发送以下类型的消息。 扩展程序可以注册监听这些消息中的任意一种。\n\n| 事件类型 (event) | 触发时机 | 数据内容 (data) |\n| --- | --- | --- |\n| `execution_start` | 当一个提示 (prompt) 即将开始执行时 | `prompt_id` (提示ID) |\n| `execution_error` | 当执行过程中发生错误时 | `prompt_id` (提示ID)，以及其他附加错误信息 |\n| `execution_interrupted` | 当某个节点抛出 `InterruptProcessingException` 异常导致执行中断时 | `prompt_id` (提示ID)、`node_id` (节点ID)、`node_type` (节点类型) 以及 `executed` (一个包含已执行节点ID的列表) |\n| `execution_cached` | 在执行开始阶段 | `prompt_id` (提示ID)、`nodes` (一个节点ID列表，这些节点的缓存输出将被复用，因此这些节点会被跳过执行) |\n| `execution_success` | 当提示中的所有节点都已成功执行时 | `prompt_id`, `timestamp`(时间戳) |\n| `executing` | 当一个新节点即将开始执行时 | `node` (当前执行的节点ID，若为 `None` 则表示整个提示执行完毕)、`prompt_id` (提示ID) |\n| `executed` | 当一个节点执行完毕并返回了用户界面 (UI) 元素时 | `node` (节点ID)、`prompt_id` (提示ID)、`output` (节点返回的UI数据) |\n| `progress` | 在执行某个实现了特定进度报告钩子 (hook) 的节点期间 | `node` (节点ID)、`prompt_id` (提示ID)、`value` (当前进度值)、`max` (最大进度值) |\n| `status` | 当执行队列的状态发生变化时 | `exec_info` (一个字典，其中包含 `queue_remaining`，表示队列中剩余的任务数量) |\n\n### 关于 `executed` 消息的使用\n\n值得注意的是，`executed` 消息并非在每个节点完成执行时都会发送（这一点与 `executing` 消息不同）， 它仅在节点执行后需要更新用户界面时才会触发。 要实现这一点，节点的Python主执行函数需要返回一个字典，而非通常的元组：\n\n```\n# 在主执行函数的末尾\n        return { \"ui\": a_new_dictionary, \"result\": the_tuple_of_output_values }\n```\n\n这样，`a_new_dictionary` 的内容便会作为 `executed` 消息中 `output` 字段的值发送给客户端。 如果节点本身没有输出（即不产生传递给下游节点的数据），那么返回字典中的 `result` 键可以省略（例如，可以参考 `nodes.py` 文件中 `SaveImage` 节点的实现方式）。\n\n### 自定义消息类型\n\n如前所述，在客户端，只需为自定义的消息类型名称注册一个监听器，即可轻松添加对新消息类型的处理。\n\n```\napi.addEventListener(\"my.custom.message\", messageHandler);\n```\n\n在服务器端，实现方式同样简洁：\n\n```\nfrom server import PromptServer\n# 然后，（通常）在您的节点主执行函数中\n        PromptServer.instance.send_sync(\"my.custom.message\", a_dictionary)\n```\n\n#### 获取当前节点 ID (node\\_id)\n\n大多数内置消息的 `node` 字段都包含了当前正在执行的节点 ID。在自定义消息中，您很可能也需要包含此信息。 在服务器端，可以通过一个隐藏输入来获取节点 ID。这需要在节点的 `INPUT_TYPES` 字典中添加一个 `hidden` 键来实现：\n\n```\n    @classmethod    \n    def INPUT_TYPES(s):\n        return {\"required\" : { }, # 此处填写您节点所需的常规输入\n                \"hidden\": { \"node_id\": \"UNIQUE_ID\" } } # 添加此 hidden 键以获取节点ID\n\n    def my_main_function(self, required_inputs, node_id): # node_id 会作为参数传入\n        # 执行某些操作...\n        PromptServer.instance.send_sync(\"my.custom.message\", {\"node\": node_id, \"other_things\": etc}) # 在消息中包含节点ID\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2Fapi-node%2Fvideo%2Fkwai_vgi%2Fkling-camera-control-i2v",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/comms_routes",
  "markdown": "# 路由 - ComfyUI\n\n## 路由\n\n服务器定义了一系列 `get` 和 `post` 方法， 这些方法可以通过在 `server.py` 中搜索 `@routes` 找到。当你在网页客户端提交工作流时， 它会被发送到 `/prompt` 端点，该端点会验证提示并将其添加到执行队列中， 返回 `prompt_id` 和 `number`（队列中的位置），如果验证失败则返回 `error` 和 `node_errors`。 提示队列定义在 `execution.py` 中，该文件还定义了 `PromptExecutor` 类。\n\n### 内置路由\n\n`server.py` 定义了以下路由：\n\n| 路径  | get/post | 用途  |\n| --- | --- | --- |\n| `/` | get | 加载 Comfy 网页 |\n| `/embeddings` | get | 获取可用的嵌入模型名称列表 |\n| `/extensions` | get | 获取注册了 `WEB_DIRECTORY` 的扩展列表 |\n| `/workflow_templates` | get | 获取自定义节点模块及其关联模板工作流的映射 |\n| `/upload/image` | post | 上传图片 |\n| `/upload/mask` | post | 上传蒙版 |\n| `/view` | get | 查看图片。更多选项请参见 `server.py` 中的 `@routes.get(\"/view\")` |\n| `/view_metadata`/ | get | 获取模型的元数据 |\n| `/system_stats` | get | 获取系统信息（Python 版本、设备、显存等） |\n| `/prompt` | get | 获取当前状态 |\n| `/prompt` | post | 提交提示到队列 |\n| `/object_info` | get | 获取所有节点类型的详细信息 |\n| `/object_info/{node_class}` | get | 获取特定节点类型的详细信息 |\n| `/history` | get | 获取队列历史记录 |\n| `/history/{prompt_id}` | get | 获取特定提示的队列历史记录 |\n| `/history` | post | 清除历史记录或删除历史记录项 |\n| `/queue` | get | 获取队列状态 |\n| `/interrupt` | post | 停止当前工作流 |\n| `/free` | post | 通过卸载指定模型释放内存 |\n\n### 自定义路由\n\n如果你想在执行过程中从客户端向服务器发送消息，你需要在服务器中添加一个自定义路由。 对于复杂的情况，你需要深入研究 [aiohttp 框架文档](https://docs.aiohttp.org/)，但大多数情况可以按以下方式处理：\n\n```\nfrom server import PromptServer\nfrom aiohttp import web\nroutes = PromptServer.instance.routes\n@routes.post('/my_new_path')\nasync def my_function(request):\n    the_data = await request.post()\n    # the_data now holds a dictionary of the values sent\n    MyClass.handle_my_message(the_data)\n    return web.json_response({})\n```\n\n客户端可以通过发送 `FormData` 对象来使用这个新路由，代码如下所示， 这将导致上面代码中的 `the_data` 包含 `message` 和 `node_id` 键：\n\n```\nimport { api } from \"../../scripts/api.js\";\nfunction send_message(node_id, message) {\n    const body = new FormData();\n    body.append('message',message);\n    body.append('node_id', node_id);\n    api.fetchApi(\"/my_new_path\", { method: \"POST\", body, });\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/api-key-integration",
  "markdown": "# 通过 API Key 集成来使用 ComfyUI API 节点\n\n从[PR #8041](https://github.com/comfyanonymous/ComfyUI/pull/8041)开始，ComfyUI 支持通过创建 API Key 来直接使用 ComfyUI 内置的 API 节点，无需特定的前端界面（甚至可以完全不使用前端）。 这意味着你可以创建工作流来组合：\n\n*   本地操作系统模型\n*   自定义节点社区的工具\n*   流行的付费模型\n\n并通过本地 Comfy webserver API 一起运行所有内容，让它处理所有的协调工作。 这对于将 Comfy 用作后端服务、通过命令行运行 Comfy、拥有自己的前端等用户都很有帮助。\n\n## 前提条件\n\n使用 API Key 来调用 ComfyUI 内置的 API 节点需要：\n\n*   确保你的 ComfyUI 版本 >= [PR #8041](https://github.com/comfyanonymous/ComfyUI/pull/8041)\n*   对应账户的 API Key\n*   足够的账户积分\n\n使用 API Key 来调用 ComfyUI 内置的 API 节点需要先在 [ComfyUI Platform](https://platform.comfy.org/login) 上注册一个账户，然后创建 API key\n\n[\n\n请参考用户界面章节了解如何使用 API Key 进行登录\n\n\n\n](https://docs.comfy.org/zh-CN/interface/user#%E4%BD%BF%E7%94%A8-api-key-%E8%BF%9B%E8%A1%8C%E7%99%BB%E5%BD%95)\n\n你需要确保你的 ComfyUI 账户有足够的积分来测试对应的功能。\n\n[](https://docs.comfy.org/zh-CN/interface/credits)\n\n## Python 示例\n\n以下是一个如何通过 Python 代码向 ComfyUI API 发送包含 API节点的工作流的示例：\n\n```\n\"\"\"在无头模式或使用替代前端运行 ComfyUI 时使用 API 节点\n\n你可以通过在 prompt 中包含 API key 来执行包含 API 节点的 ComfyUI 工作流。\nAPI key 需要添加到 payload 的 `extra_data` 字段中。\n下面我们展示一个如何实现的示例。\n\n更多信息请参考：\n\n- API 节点概述: https://docs.comfy.org/zh-CN/tutorials/api-nodes/overview\n- 要生成 API key，请登录这里: https://platform.comfy.org/login\n\"\"\"\n\nimport json\nfrom urllib import request\n\nSERVER_URL = \"http://127.0.0.1:8188\"\n\n# 我们有一个包含 API 节点的 prompt/job（API 格式的工作流）。\nworkflow_with_api_nodes = \"\"\"{\n  \"11\": {\n    \"inputs\": {\n      \"prompt\": \"A dreamy, surreal half-body portrait of a young woman meditating. She has a short, straight bob haircut dyed in pastel pink, with soft bangs covering her forehead. Her eyes are gently closed, and her hands are raised in a calm, open-palmed meditative pose, fingers slightly curved, as if levitating or in deep concentration. She wears a colorful dress made of patchwork-like pastel tiles, featuring clouds, stars, and rainbows. Around her float translucent, iridescent soap bubbles reflecting the rainbow hues. The background is a fantastical sky filled with cotton-candy clouds and vivid rainbow waves, giving the entire scene a magical, dreamlike atmosphere. Emphasis on youthful serenity, whimsical ambiance, and vibrant soft lighting.\",\n      \"prompt_upsampling\": false,\n      \"seed\": 589991183902375,\n      \"aspect_ratio\": \"1:1\",\n      \"raw\": false,\n      \"image_prompt_strength\": 0.4000000000000001,\n      \"image_prompt\": [\n        \"14\",\n        0\n      ]\n    },\n    \"class_type\": \"FluxProUltraImageNode\",\n    \"_meta\": {\n      \"title\": \"Flux 1.1 [pro] Ultra Image\"\n    }\n  },\n  \"12\": {\n    \"inputs\": {\n      \"filename_prefix\": \"ComfyUI\",\n      \"images\": [\n        \"11\",\n        0\n      ]\n    },\n    \"class_type\": \"SaveImage\",\n    \"_meta\": {\n      \"title\": \"Save Image\"\n    }\n  },\n  \"14\": {\n    \"inputs\": {\n      \"image\": \"example.png\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"Load Image\"\n    }\n  }\n}\"\"\"\n\n\nprompt = json.loads(workflow_with_api_nodes)\npayload = {\n    \"prompt\": prompt,\n    # 将 `api_key_comfy_org` 添加到 payload 中。\n    # 如果你需要处理多个客户端，可以先从关联的用户获取 key。\n    \"extra_data\": {\n        \"api_key_comfy_org\": \"comfyui-87d01e28d*******************************************************\"  # 替换为实际的 key\n    },\n}\ndata = json.dumps(payload).encode(\"utf-8\")\nreq = request.Request(f\"{SERVER_URL}/prompt\", data=data)\n\n# 发送请求\nrequest.urlopen(req)\n\n```\n\n## 相关文档\n\n*   [API节点概述](https://docs.comfy.org/zh-CN/tutorials/api-nodes/overview)\n*   [账户管理](https://docs.comfy.org/zh-CN/interface/user)\n*   [积分](https://docs.comfy.org/zh-CN/interface/credits)"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/overview",
  "markdown": "# 概述 - ComfyUI\n\n自定义节点允许你实现新功能并与更广泛的社区分享。 自定义节点就像任何 Comfy 节点一样：它接收输入，对其进行处理，然后产生输出。 虽然有些自定义节点执行非常复杂的任务，但许多节点只做一件事。下面是一个简单节点的例子，它接收一张图片并进行反色处理。 ![唯一图片节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/invert_image_node.png)\n\n## 客户端-服务器模型\n\nComfy 运行在客户端-服务器模型下。服务器端由 Python 编写，负责所有实际工作：数据处理、模型、图像扩散等。客户端由 Javascript 编写，负责用户界面。 Comfy 也可以以 API 模式使用，在该模式下，工作流由非 Comfy 客户端（如其他 UI 或命令行脚本）发送到服务器。 自定义节点可以分为以下四类：\n\n### 仅服务器端\n\n大多数自定义节点仅在服务器端运行，通过定义一个 Python 类来指定输入和输出类型，并提供一个可调用的函数来处理输入并生成输出。\n\n### 仅客户端\n\n少数自定义节点仅对客户端 UI 进行修改，但不添加核心功能。尽管名字如此，它们甚至可能不会向系统添加新节点。\n\n### 客户端与服务器端独立\n\n自定义节点可以同时提供额外的服务器功能和额外（相关的）UI 功能（例如用于新数据类型的新小部件）。在大多数情况下，客户端和服务器之间的通信可以通过 Comfy 的数据流控制来处理。\n\n### 客户端与服务器端联动\n\n在少数情况下，UI 功能和服务器需要直接相互通信。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/walkthrough",
  "markdown": "# 快速入门 - ComfyUI\n\n本页将带你一步步完成自定义节点的创建过程。 我们的示例将接收一批图片，并返回其中一张图片。最初，这个节点会返回平均颜色最亮的图片；随后我们会扩展它，支持多种选择标准，最后还会添加一些客户端代码。 本页假设你对 Python 或 Javascript 的了解很少。 完成本教程后，可以深入了解 [后端代码](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview) 和 [前端代码](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview)。\n\n## 编写基础节点\n\n### 前置条件\n\n*   一个可用的 ComfyUI [安装环境](https://docs.comfy.org/zh-CN/installation/manual_install)。开发建议手动安装 ComfyUI。\n*   一个可用的 comfy-cli [安装环境](https://docs.comfy.org/zh-CN/comfy-cli/getting-started)。\n\n### 环境搭建\n\n```\ncd ComfyUI/custom_nodes\ncomfy node scaffold\n```\n\n回答几个问题后，你会得到一个新的目录。\n\n```\n ~  % comfy node scaffold\nYou've downloaded .cookiecutters/cookiecutter-comfy-extension before. Is it okay to delete and re-download it? [y/n] (y): y\n  [1/9] full_name (): Comfy\n  [2/9] email (you@gmail.com): me@comfy.org\n  [3/9] github_username (your_github_username): comfy\n  [4/9] project_name (My Custom Nodepack): FirstComfyNode\n  [5/9] project_slug (firstcomfynode): \n  [6/9] project_short_description (A collection of custom nodes for ComfyUI): \n  [7/9] version (0.0.1): \n  [8/9] Select open_source_license\n    1 - GNU General Public License v3\n    2 - MIT license\n    3 - BSD license\n    4 - ISC license\n    5 - Apache Software License 2.0\n    6 - Not open source\n    Choose from [1/2/3/4/5/6] (1): 1\n  [9/9] include_web_directory_for_custom_javascript [y/n] (n): y\nInitialized empty Git repository in firstcomfynode/.git/\n✓ Custom node project created successfully!\n```\n\n### 定义节点\n\n将以下代码添加到 `src/nodes.py` 末尾：\n\n```\nclass ImageSelector:\n    CATEGORY = \"example\"\n    @classmethod    \n    def INPUT_TYPES(s):\n        return { \"required\":  { \"images\": (\"IMAGE\",), } }\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"choose_image\"\n```\n\n自定义节点通过 Python 类定义，必须包含以下四项：`CATEGORY`（指定新节点在添加节点菜单中的位置）、`INPUT_TYPES`（类方法，定义节点输入，详见[后文](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#input-types)）、`RETURN_TYPES`（定义节点输出）、`FUNCTION`（节点执行时调用的函数名）。\n\n### 主函数\n\n主函数 `choose_image` 会收到在 `INPUT_TYPES` 中定义的命名参数，并返回一个与 `RETURN_TYPES` 匹配的 `tuple`。由于我们处理的是图片，图片在内部以 `torch.Tensor` 存储，\n\n然后将函数添加到你的类中。图片的数据类型是形状为 `[B,H,W,C]` 的 `torch.Tensor`，其中 `B` 是批量大小，`C` 是通道数（RGB 为 3）。遍历该张量会得到 `B` 个形状为 `[H,W,C]` 的张量。`.flatten()` 方法将其变为一维张量，长度为 `H*W*C`，`torch.mean()` 求均值，`.item()` 将单值张量转为 Python 浮点数。\n\n```\ndef choose_image(self, images):\n    brightness = list(torch.mean(image.flatten()).item() for image in images)\n    brightest = brightness.index(max(brightness))\n    result = images[brightest].unsqueeze(0)\n    return (result,)\n```\n\n最后两行说明：\n\n*   `images[brightest]` 返回形状为 `[H,W,C]` 的张量。`unsqueeze` 用于在第 0 维插入一个长度为 1 的维度，得到 `[B,H,W,C]`，其中 `B=1`，即单张图片。\n*   `return (result,)` 末尾的逗号很重要，确保返回的是元组。\n\n### 注册节点\n\n要让 Comfy 识别新节点，必须在包级别可用。修改 `src/nodes.py` 末尾的 `NODE_CLASS_MAPPINGS` 变量。你需要重启 ComfyUI 才能看到更改。\n\n```\n\nNODE_CLASS_MAPPINGS = {\n    \"Example\" : Example,\n    \"Image Selector\" : ImageSelector,\n}\n\n# 可选：你可以在 `NODE_DISPLAY_NAME_MAPPINGS` 字典中重命名节点。\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"Example\": \"Example Node\",\n    \"Image Selector\": \"Image Selector\",\n}\n```\n\n## 添加选项\n\n这个节点可能有点无聊，所以我们可以加一些选项；比如一个小部件，让你选择最亮、最红、最绿或最蓝的图片。将你的 `INPUT_TYPES` 修改为：\n\n```\n@classmethod    \ndef INPUT_TYPES(s):\n    return { \"required\":  { \"images\": (\"IMAGE\",), \n                            \"mode\": ([\"brightest\", \"reddest\", \"greenest\", \"bluest\"],)} }\n```\n\n然后更新主函数。我们用一个很简单的“最红”定义，即像素的平均 R 值除以三色平均值。所以：\n\n```\ndef choose_image(self, images, mode):\n    batch_size = images.shape[0]\n    brightness = list(torch.mean(image.flatten()).item() for image in images)\n    if (mode==\"brightest\"):\n        scores = brightness\n    else:\n        channel = 0 if mode==\"reddest\" else (1 if mode==\"greenest\" else 2)\n        absolute = list(torch.mean(image[:,:,channel].flatten()).item() for image in images)\n        scores = list( absolute[i]/(brightness[i]+1e-8) for i in range(batch_size) )\n    best = scores.index(max(scores))\n    result = images[best].unsqueeze(0)\n    return (result,)\n```\n\n## 调整 UI\n\n也许我们想要一些可视化反馈，所以让我们发送一条文本消息进行显示。\n\n### 从服务器发送消息\n\n只需在 Python 代码中添加两行：\n\n```\nfrom server import PromptServer\n```\n\n在 `choose_image` 方法末尾添加一行，将消息发送到前端（`send_sync` 需要一个唯一的消息类型和一个字典）：\n\n```\nPromptServer.instance.send_sync(\"example.imageselector.textmessage\", {\"message\":f\"Picked image {best+1}\"})\nreturn (result,)\n```\n\n### 编写客户端扩展\n\n要为客户端添加 Javascript，在你的自定义节点目录下创建 `web/js` 子目录，并在 `__init__.py` 末尾导出 `WEB_DIRECTORY`：\n\n```\nWEB_DIRECTORY = \"./web/js\"\n__all__ = ['NODE_CLASS_MAPPINGS', 'WEB_DIRECTORY']\n```\n\n客户端扩展以 `.js` 文件保存在 `web/js` 子目录下，所以创建 `image_selector/web/js/imageSelector.js`，内容如下。（更多内容见 [客户端开发](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_overview)）\n\n```\napp.registerExtension({\n\tname: \"example.imageselector\",\n    async setup() {\n        function messageHandler(event) { alert(event.detail.message); }\n        app.api.addEventListener(\"example.imageselector.textmessage\", messageHandler);\n    },\n})\n```\n\n我们所做的就是注册一个扩展，并在 `setup()` 方法中为我们发送的消息类型添加监听器。它会读取我们发送的字典（存储在 `event.detail` 中）。 停止 Comfy 服务器，重新启动，刷新网页，运行你的工作流。\n\n### 完整示例\n\n完整示例见[这里](https://gist.github.com/robinjhuang/fbf54b7715091c7b478724fc4dffbd03)。你可以下载示例工作流 [JSON 文件](https://github.com/Comfy-Org/docs/blob/main/public/workflow.json) 或在下方查看：\n\n![Image Selector Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/firstnodeworkflow.png)"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/workflow_templates",
  "markdown": "# 工作流模板 - ComfyUI\n\n如果你的自定义节点包含示例工作流文件，ComfyUI 可以在模板浏览器（`工作流`/`浏览模板`菜单）中向用户展示这些文件。工作流模板是帮助用户快速上手你的节点的好方法。 作为节点开发者，你只需要创建一个 `example_workflows` 文件夹并在其中放置 `json` 文件即可。你还可以选择性地放置同名的 `jpg` 文件作为模板缩略图。 在底层，ComfyUI 会静态提供这些文件，并通过一个端点（`/api/workflow_templates`）返回工作流模板集合。\n\n## 示例\n\n在 `ComfyUI-MyCustomNodeModule/example_workflows/` 目录下：\n\n*   `My_example_workflow_1.json`\n*   `My_example_workflow_1.jpg`\n*   `My_example_workflow_2.json`\n\n在这个例子中，ComfyUI 的模板浏览器会显示一个名为 `ComfyUI-MyCustomNodeModule` 的类别，其中包含两个项目，其中一个带有缩略图。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/help_page",
  "markdown": "# 帮助页面 - ComfyUI\n\n## 使用 Markdown 创建节点文档\n\n自定义节点可以使用 Markdown 来创建富文本文档，这些文档信息将在 UI 中显示，取代常见的节点描述信息。可以为用户提供关于节点功能、参数和使用示例的详细信息。\n\n## 设置\n\n为你的节点添加节点文档：\n\n1.  在你的 `WEB_DIRECTORY` 中创建 `docs` 文件夹\n2.  添加以节点名称命名的 Markdown 文件（您的节点名称是用于注册节点的 `NODE_CLASS_MAPPINGS` 字典中的字典键）：\n    *   `WEB_DIRECTORY/docs/NodeName.md` - 默认文档\n    *   `WEB_DIRECTORY/docs/NodeName/en.md` - 英文文档\n    *   `WEB_DIRECTORY/docs/NodeName/zh.md` - 中文文档\n    *   根据需要添加其他语言版本（例如 `fr.md`、`de.md` 等）\n\n系统将根据用户的语言设置自动加载相应的文档，如果没有本地化版本，则回退到 `NodeName.md`。\n\n## 支持的 Markdown 功能\n\n*   标准 Markdown 语法（标题、列表、代码块等）\n*   使用 Markdown 语法的图片：`![替代文本](image.png)`\n*   具有特定属性的 HTML 媒体元素：\n    *   `<video>` 和 `<source>` 标签\n    *   允许的属性：`controls`、`autoplay`、`loop`、`muted`、`preload`、`poster`\n\n## 示例结构\n\n```\nmy-custom-node/\n├── __init__.py\n├── web/              # WEB_DIRECTORY\n│   ├── js/\n│   │   └── my-node.js\n│   └── docs/\n│       ├── MyNode.md           # 默认文档\n│       └── MyNode/\n│           ├── en.md           # 英文版本\n│           └── zh.md           # 中文版本\n```\n\n## 示例 Markdown 文件\n\n```\n# 我的自定义节点\n\n此节点使用高级算法处理图像。\n\n## 参数\n\n- **image**: 要处理的输入图像\n- **strength**: 处理强度 (0.0 - 1.0)\n\n## 用法\n\n![使用示例](example.png)\n\n<video controls loop muted>\n  <source src=\"demo.mp4\" type=\"video/mp4\">\n</video>\n```\n\n在此页面\n\n*   [使用 Markdown 创建节点文档](#%E4%BD%BF%E7%94%A8-markdown-%E5%88%9B%E5%BB%BA%E8%8A%82%E7%82%B9%E6%96%87%E6%A1%A3)\n*   [设置](#%E8%AE%BE%E7%BD%AE)\n*   [支持的 Markdown 功能](#%E6%94%AF%E6%8C%81%E7%9A%84-markdown-%E5%8A%9F%E8%83%BD)\n*   [示例结构](#%E7%A4%BA%E4%BE%8B%E7%BB%93%E6%9E%84)\n*   [示例 Markdown 文件](#%E7%A4%BA%E4%BE%8B-markdown-%E6%96%87%E4%BB%B6)"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/comms_overview",
  "markdown": "# 服务器概览 - ComfyUI\n\n## 概览\n\nComfy 服务器构建于 [aiohttp 框架](https://docs.aiohttp.org/) 基础之上，该框架则依赖于 [asyncio](https://pypi.org/project/asyncio/) 库。 服务器向客户端发送消息时，会通过其 `send_sync` 方法（该服务器是 `server.py` 文件中定义的 `PromptServer` 类的一个实例）以 `socket` 消息的形式进行。这些消息由注册在 `api.js` 文件中的 `socket` 事件监听器负责处理。更多详情请参阅[消息传递](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_messages)。 客户端向服务器发送消息时，则通过 `api.js` 文件中定义的 `api.fetchApi()` 方法进行，这些请求由服务器端设定的 HTTP 路由负责处理。更多详情请参阅[路由机制](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_routes)部分。 python3 .github/scripts/validate-links.py\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/publishing",
  "markdown": "# 发布节点 - ComfyUI\n\n## 设置注册表账户\n\n按照以下步骤设置注册表账户并发布您的第一个节点。\n\n### 观看教程\n\n### 创建发布者\n\n发布者是一个可以向注册表(registry)发布自定义节点的身份。每个自定义节点都需要在 pyproject.toml 文件 中包含发布者标识符。 访问 [Comfy Registry](https://registry.comfy.org/)，创建一个发布者账户。您的发布者 ID 是全球唯一的，并且之后不能更改，因为它用于您的自定义节点的 URL 中。 您的发布者 ID 可以在个人资料页面上 `@` 符号后面找到。 ![Hero Dark](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/publisherid.png)\n\n### 创建用于发布的 API 密钥\n\n访问[这里](https://registry.comfy.org/nodes)并点击你想要为其创建 API 密钥的发布者。这将用于通过 CLI 发布自定义节点。 ![为特定发布者创建密钥](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-1.png) 为 API 密钥命名并将其安全保存。如果密钥丢失了它，请重新创建一个新的密钥。 ![创建 API 密钥](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-2.png)\n\n### 添加元数据\n\n这个命令将会生成下面这样的元数据：\n\n```\n# pyproject.toml\n[project]\nname = \"\" # Unique identifier for your node. Immutable after creation.\ndescription = \"\"\nversion = \"1.0.0\" # Custom Node version. Must be semantically versioned.\nlicense = { file = \"LICENSE.txt\" }\ndependencies  = [] # Filled in from requirements.txt\n\n[project.urls]\nRepository = \"https://github.com/...\"\n\n[tool.comfy]\nPublisherId = \"\" # TODO (fill in Publisher ID from Comfy Registry Website).\nDisplayName = \"\" # Display name for the Custom Node. Can be changed later.\nIcon = \"https://example.com/icon.png\" # SVG, PNG, JPG or GIF (MAX. 800x400px)\n```\n\n将此文件添加到您的仓库中。查看[规范](https://docs.comfy.org/zh-CN/registry/specifications)以获取有关 pyproject.toml 文件的更多信息。\n\n## 发布到注册表(registry)\n\n### 选项 1: Comfy CLI\n\n运行下面的命令手动将您的节点发布到注册表。\n\n会被提示要求输入 API 密钥。\n\n```\nAPI Key for publisher '<publisher id>': ****************************************************\n\n...Version 1.0.0 Published. \nSee it here: https://registry.comfy.org/publisherId/your-node\n```\n\n### 选项 2: Github Actions\n\n通过 Github Actions 自动发布您的节点。"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/overview",
  "markdown": "# 概述 - ComfyUI\n\n## 简介\n\n注册表（Registry）是一个自定义节点的公共集合。开发者可以发布、版本控制、弃用和跟踪与其自定义节点相关的指标。ComfyUI 用户可以从注册表中发现、安装和评价自定义节点。\n\n## 为什么使用 Registry？\n\n注册表通过标准化自定义节点的开发来帮助社区：   **节点版本控制：** 开发者经常发布其自定义节点的新版本，这往往会破坏依赖它们的工作流。通过使用[语义化版本控制](https://semver.org/)，用户现在可以选择安全地升级、弃用或锁定其节点版本，提前了解其操作将如何影响其工作流。工作流 JSON 将存储所使用的节点版本，因此您可以始终可靠地重现您的工作流。   **节点安全性：** 注册表将作为 [ComfyUI-manager](https://github.com/comfy-org/ComfyUI-Manager) 的后端。所有节点都将被扫描是否存在恶意行为，如自定义 pip 包、任意系统调用等。通过这些检查的节点将在 UI-manager 上其名称旁边显示验证标志（）。有关安全标准列表，请参阅[标准](https://docs.comfy.org/zh-CN/registry/standards)。   **搜索：** 在 Registry 上搜索所有节点，为您的工作流找到现有节点。\n\n## 发布节点\n\n按照[教程](https://docs.comfy.org/zh-CN/registry/publishing)开始发布您的第一个节点。\n\n## 常见问题"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/tips",
  "markdown": "# Tips - ComfyUI\n\n### \n\n[​](#recommended-development-lifecycle)\n\nRecommended Development Lifecycle"
},
{
  "url": "https://docs.comfy.org/zh-CN/specs/workflow_json",
  "markdown": "# 工作流 JSON - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyWorkflow1_0\",\n  \"definitions\": {\n    \"ComfyWorkflow1_0\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"version\": {\n          \"type\": \"number\",\n          \"const\": 1\n        },\n        \"config\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"links_ontop\": {\n                      \"type\": \"boolean\"\n                    },\n                    \"align_to_grid\": {\n                      \"type\": \"boolean\"\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"state\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"lastGroupid\": {\n              \"type\": \"number\"\n            },\n            \"lastNodeId\": {\n              \"type\": \"number\"\n            },\n            \"lastLinkId\": {\n              \"type\": \"number\"\n            },\n            \"lastRerouteId\": {\n              \"type\": \"number\"\n            }\n          },\n          \"additionalProperties\": true\n        },\n        \"groups\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"title\": {\n                \"type\": \"string\"\n              },\n              \"bounding\": {\n                \"type\": \"array\",\n                \"minItems\": 4,\n                \"maxItems\": 4,\n                \"items\": [\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"font_size\": {\n                \"type\": \"number\"\n              },\n              \"locked\": {\n                \"type\": \"boolean\"\n              }\n            },\n            \"required\": [\n              \"title\",\n              \"bounding\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"nodes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"size\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"flags\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"collapsed\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"pinned\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"allow_interaction\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"horizontal\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"skip_repeated_outputs\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"order\": {\n                \"type\": \"number\"\n              },\n              \"mode\": {\n                \"type\": \"number\"\n              },\n              \"inputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"link\": {\n                      \"type\": [\n                        \"number\",\n                        \"null\"\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"outputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"links\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        {\n                          \"type\": \"null\"\n                        }\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"properties\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"Node name for S&R\": {\n                    \"type\": \"string\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"widgets_values\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\"\n                  },\n                  {\n                    \"type\": \"object\",\n                    \"additionalProperties\": {}\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"bgcolor\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"type\",\n              \"pos\",\n              \"size\",\n              \"flags\",\n              \"order\",\n              \"mode\",\n              \"properties\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"links\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"type\": \"number\"\n              },\n              \"origin_id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"origin_slot\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"target_id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"target_slot\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"string\"\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    }\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"parentId\": {\n                \"type\": \"number\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"origin_id\",\n              \"origin_slot\",\n              \"target_id\",\n              \"target_slot\",\n              \"type\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"reroutes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"type\": \"number\"\n              },\n              \"parentId\": {\n                \"type\": \"number\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"linkIds\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"number\"\n                    }\n                  },\n                  {\n                    \"type\": \"null\"\n                  }\n                ]\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"pos\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"extra\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"ds\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"scale\": {\n                          \"type\": \"number\"\n                        },\n                        \"offset\": {\n                          \"anyOf\": [\n                            {\n                              \"type\": \"object\",\n                              \"properties\": {\n                                \"0\": {\n                                  \"type\": \"number\"\n                                },\n                                \"1\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              \"required\": [\n                                \"0\",\n                                \"1\"\n                              ],\n                              \"additionalProperties\": true\n                            },\n                            {\n                              \"type\": \"array\",\n                              \"minItems\": 2,\n                              \"maxItems\": 2,\n                              \"items\": [\n                                {\n                                  \"type\": \"number\"\n                                },\n                                {\n                                  \"type\": \"number\"\n                                }\n                              ]\n                            }\n                          ]\n                        }\n                      },\n                      \"required\": [\n                        \"scale\",\n                        \"offset\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"info\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"name\": {\n                          \"type\": \"string\"\n                        },\n                        \"author\": {\n                          \"type\": \"string\"\n                        },\n                        \"description\": {\n                          \"type\": \"string\"\n                        },\n                        \"version\": {\n                          \"type\": \"string\"\n                        },\n                        \"created\": {\n                          \"type\": \"string\"\n                        },\n                        \"modified\": {\n                          \"type\": \"string\"\n                        },\n                        \"software\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"required\": [\n                        \"name\",\n                        \"author\",\n                        \"description\",\n                        \"version\",\n                        \"created\",\n                        \"modified\",\n                        \"software\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"linkExtensions\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"parentId\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    },\n                    \"reroutes\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          },\n                          \"pos\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"0\": {\n                                    \"type\": \"number\"\n                                  },\n                                  \"1\": {\n                                    \"type\": \"number\"\n                                  }\n                                },\n                                \"required\": [\n                                  \"0\",\n                                  \"1\"\n                                ],\n                                \"additionalProperties\": true\n                              },\n                              {\n                                \"type\": \"array\",\n                                \"minItems\": 2,\n                                \"maxItems\": 2,\n                                \"items\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"number\"\n                                  }\n                                ]\n                              }\n                            ]\n                          },\n                          \"linkIds\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              {\n                                \"type\": \"null\"\n                              }\n                            ]\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"pos\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"models\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"url\": {\n                \"type\": \"string\",\n                \"format\": \"uri\"\n              },\n              \"hash\": {\n                \"type\": \"string\"\n              },\n              \"hash_type\": {\n                \"type\": \"string\"\n              },\n              \"directory\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"name\",\n              \"url\",\n              \"directory\"\n            ],\n            \"additionalProperties\": false\n          }\n        }\n      },\n      \"required\": [\n        \"version\",\n        \"state\",\n        \"nodes\"\n      ],\n      \"additionalProperties\": true\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/specs/workflow_json_0.4",
  "markdown": "# 工作流 JSON 0.4 - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyWorkflow0_4\",\n  \"definitions\": {\n    \"ComfyWorkflow0_4\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"last_node_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"last_link_id\": {\n          \"type\": \"number\"\n        },\n        \"nodes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"size\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"flags\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"collapsed\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"pinned\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"allow_interaction\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"horizontal\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"skip_repeated_outputs\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"order\": {\n                \"type\": \"number\"\n              },\n              \"mode\": {\n                \"type\": \"number\"\n              },\n              \"inputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"link\": {\n                      \"type\": [\n                        \"number\",\n                        \"null\"\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"outputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"links\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        {\n                          \"type\": \"null\"\n                        }\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"properties\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"Node name for S&R\": {\n                    \"type\": \"string\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"widgets_values\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\"\n                  },\n                  {\n                    \"type\": \"object\",\n                    \"additionalProperties\": {}\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"bgcolor\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"type\",\n              \"pos\",\n              \"size\",\n              \"flags\",\n              \"order\",\n              \"mode\",\n              \"properties\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"links\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"array\",\n            \"minItems\": 6,\n            \"maxItems\": 6,\n            \"items\": [\n              {\n                \"type\": \"number\"\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"string\"\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    }\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              }\n            ]\n          }\n        },\n        \"groups\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"title\": {\n                \"type\": \"string\"\n              },\n              \"bounding\": {\n                \"type\": \"array\",\n                \"minItems\": 4,\n                \"maxItems\": 4,\n                \"items\": [\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"font_size\": {\n                \"type\": \"number\"\n              },\n              \"locked\": {\n                \"type\": \"boolean\"\n              }\n            },\n            \"required\": [\n              \"title\",\n              \"bounding\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"config\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"links_ontop\": {\n                      \"type\": \"boolean\"\n                    },\n                    \"align_to_grid\": {\n                      \"type\": \"boolean\"\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"extra\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"ds\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"scale\": {\n                          \"type\": \"number\"\n                        },\n                        \"offset\": {\n                          \"anyOf\": [\n                            {\n                              \"type\": \"object\",\n                              \"properties\": {\n                                \"0\": {\n                                  \"type\": \"number\"\n                                },\n                                \"1\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              \"required\": [\n                                \"0\",\n                                \"1\"\n                              ],\n                              \"additionalProperties\": true\n                            },\n                            {\n                              \"type\": \"array\",\n                              \"minItems\": 2,\n                              \"maxItems\": 2,\n                              \"items\": [\n                                {\n                                  \"type\": \"number\"\n                                },\n                                {\n                                  \"type\": \"number\"\n                                }\n                              ]\n                            }\n                          ]\n                        }\n                      },\n                      \"required\": [\n                        \"scale\",\n                        \"offset\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"info\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"name\": {\n                          \"type\": \"string\"\n                        },\n                        \"author\": {\n                          \"type\": \"string\"\n                        },\n                        \"description\": {\n                          \"type\": \"string\"\n                        },\n                        \"version\": {\n                          \"type\": \"string\"\n                        },\n                        \"created\": {\n                          \"type\": \"string\"\n                        },\n                        \"modified\": {\n                          \"type\": \"string\"\n                        },\n                        \"software\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"required\": [\n                        \"name\",\n                        \"author\",\n                        \"description\",\n                        \"version\",\n                        \"created\",\n                        \"modified\",\n                        \"software\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"linkExtensions\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"parentId\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    },\n                    \"reroutes\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          },\n                          \"pos\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"0\": {\n                                    \"type\": \"number\"\n                                  },\n                                  \"1\": {\n                                    \"type\": \"number\"\n                                  }\n                                },\n                                \"required\": [\n                                  \"0\",\n                                  \"1\"\n                                ],\n                                \"additionalProperties\": true\n                              },\n                              {\n                                \"type\": \"array\",\n                                \"minItems\": 2,\n                                \"maxItems\": 2,\n                                \"items\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"number\"\n                                  }\n                                ]\n                              }\n                            ]\n                          },\n                          \"linkIds\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              {\n                                \"type\": \"null\"\n                              }\n                            ]\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"pos\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"version\": {\n          \"type\": \"number\"\n        },\n        \"models\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"url\": {\n                \"type\": \"string\",\n                \"format\": \"uri\"\n              },\n              \"hash\": {\n                \"type\": \"string\"\n              },\n              \"hash_type\": {\n                \"type\": \"string\"\n              },\n              \"directory\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"name\",\n              \"url\",\n              \"directory\"\n            ],\n            \"additionalProperties\": false\n          }\n        }\n      },\n      \"required\": [\n        \"last_node_id\",\n        \"last_link_id\",\n        \"nodes\",\n        \"links\",\n        \"version\"\n      ],\n      \"additionalProperties\": true\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/execution_model_inversion_guide",
  "markdown": "# 执行模型反转指南 - ComfyUI\n\n[PR #2666](https://github.com/comfyanonymous/ComfyUI/pull/2666) 将执行模型从原先的“后端到前端”递归方式，转变为“前端到后端”的拓扑排序方式。尽管多数自定义节点预计仍能照常工作，本指南旨在帮助自定义节点开发者识别那些_可能_因此变更而出现问题的情况。\n\n## 不兼容变更\n\n### Monkey Patching\n\n任何曾对执行模型进行 `Monkey Patching` （猴子补丁）的代码，在新模型下很可能失效。值得注意的是，此 `PR` 带来的执行性能已超越多数主流 `Monkey Patching` 方案，因此许多此类补丁已无必要。\n\n### 可选输入验证\n\n在此 `PR` 更新前，系统仅对那些完全通过一连串 `\\\"required\\\"` （必需）输入连接到输出节点的节点进行验证。如果您的自定义节点以往仅通过 `\\\"optional\\\"` （可选）输入连接，那么之前可能并未发现其验证失败的情况。\n\n以下列出了一些可能导致验证失败的情形及建议解决方案：\n\n*   为了配置自定义小部件（widget），在不适合进行比较的类型（如字典）上使用了保留的[附加参数](https://docs.comfy.org/zh-CN/custom-nodes/backend/datatypes#%E9%99%84%E5%8A%A0%E5%8F%82%E6%95%B0)（例如 `min` 和 `max`）。\n    *   将所用的附加参数更改为非保留关键字，例如 `uiMin` 和 `uiMax`。_（推荐方案）_\n        \n        ```\n        @classmethod\n        def INPUT_TYPES(cls):\n            return {\n                \"required\": {\n                    \"my_size\": (\"VEC2\", {\"uiMin\": 0.0, \"uiMax\": 1.0}),\n                }\n            }\n        ```\n        \n    *   为该输入定义一个自定义的 [VALIDATE\\_INPUTS](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#validate-inputs) 函数，从而跳过对其的验证。_（快速方案）_\n        \n        ```\n        @classmethod\n        def VALIDATE_INPUTS(cls, my_size):\n            return True\n        ```\n        \n*   使用了复合类型（例如 `CUSTOM_A,CUSTOM_B`）\n    *   （作为输出时）定义并使用类似 `MakeSmartType` 的包装器 [见于此 PR 的单元测试](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R2)\n        \n        ```\n        class MyCustomNode:\n        \n            @classmethod\n            def INPUT_TYPES(cls):\n                return {\n                    \"required\": {\n                        \"input\": (MakeSmartType(\"FOO,BAR\"), {}),\n                    }\n                }\n        \n            RETURN_TYPES = (MakeSmartType(\"FOO,BAR\"),)\n        \n            # ...\n        ```\n        \n    *   （作为输入时）定义一个自定义的 [VALIDATE\\_INPUTS](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#validate-inputs) 函数，使其接受 `input_types` 参数，从而跳过类型验证。\n        \n        ```\n        @classmethod\n        def VALIDATE_INPUTS(cls, input_types):\n            return True\n        ```\n        \n    *   （输入输出均适用，且便捷）定义并使用 `@VariantSupport` 装饰器 [见于此 PR 的单元测试](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R15)\n        \n        ```\n        @VariantSupport\n        class MyCustomNode:\n        \n            @classmethod\n            def INPUT_TYPES(cls):\n                return {\n                    \"required\": {\n                        \"input\": (\"FOO,BAR\", {}),\n                    }\n                }\n            \n            RETURN_TYPES = (MakeSmartType(\"FOO,BAR\"),)\n        \n            # ...\n        ```\n        \n*   在图（graph）定义中将列表（例如 `[1, 2, 3]`）用作常量（例如，代表一个 `VEC3` 类型的常量输入）。此用法在旧版中需配合前端扩展。并且，此前大小恰好为 `2` 的列表本身就会导致失败——它们会被视为无效链接。\n    *   将列表包装在形如 `{ \"value\": [1, 2, 3] }` 的字典中。\n\n### 执行顺序\n\n执行顺序以往便会因节点的 `ID` 不同而变化，如今，缓存值的不同也可能导致执行顺序的改变。通常而言，除了图结构所固有的约束外，执行顺序应被视为不确定的，并可能随时调整。 切勿依赖特定的执行顺序。 _HIC SUNT DRACONES_\n\n## 新增功能\n\n### 验证更改\n\n为了缓解前述[可选输入验证](#%E5%8F%AF%E9%80%89%E8%BE%93%E5%85%A5%E9%AA%8C%E8%AF%81)变更带来的影响，`VALIDATE_INPUTS` 函数新增了若干特性。\n\n*   对于由 `VALIDATE_INPUTS` 函数接收的输入，系统将不再执行默认验证流程。\n*   `VALIDATE_INPUTS` 函数现支持接收 `**kwargs` 参数。一旦使用，节点创建者将被视为已自行处理所有输入的验证。\n*   `VALIDATE_INPUTS` 函数可以接收一个名为 `input_types` 的参数。该参数是一个字典，其中包含了每个通过链接接入的输入及其对应连接输出的类型。若定义了此参数，系统将跳过对该节点所有输入的类型验证。\n\n更多详情请参阅 [VALIDATE\\_INPUTS](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#validate-inputs) 文档。\n\n### Lazy Evaluation\n\n输入现支持 `Lazy Evaluation` （惰性求值），即可以先判断是否确实需要某个输入值，再决定是否执行其连接的上游节点及其所有依赖节点。更多信息请参见[惰性求值](https://docs.comfy.org/zh-CN/custom-nodes/backend/lazy_evaluation)。\n\n### Node Expansion\n\n在运行时，节点可以动态扩展为一个子图（subgraph）。该机制使得通过尾递归（tail-recursion）实现循环等复杂逻辑成为可能。更多信息请参见[节点扩展](https://docs.comfy.org/zh-CN/custom-nodes/backend/expansion)。\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/specs/nodedef_json_1_0",
  "markdown": "# 节点定义 JSON 1.0 - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyNodeDefV1\",\n  \"definitions\": {\n    \"ComfyNodeDefV1\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"input\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"required\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"INT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"FLOAT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"round\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"boolean\",\n                                    \"const\": false\n                                  }\n                                ]\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"BOOLEAN\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"label_on\": {\n                                \"type\": \"string\"\n                              },\n                              \"label_off\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"STRING\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"string\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"multiline\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"dynamicPrompts\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultVal\": {\n                                \"type\": \"string\"\n                              },\n                              \"placeholder\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": [\n                            \"string\",\n                            \"number\"\n                          ]\n                        }\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"COMBO\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            \"optional\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"INT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"FLOAT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"round\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"boolean\",\n                                    \"const\": false\n                                  }\n                                ]\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"BOOLEAN\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"label_on\": {\n                                \"type\": \"string\"\n                              },\n                              \"label_off\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"STRING\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"string\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"multiline\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"dynamicPrompts\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultVal\": {\n                                \"type\": \"string\"\n                              },\n                              \"placeholder\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": [\n                            \"string\",\n                            \"number\"\n                          ]\n                        }\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"COMBO\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            \"hidden\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {}\n            }\n          },\n          \"additionalProperties\": false\n        },\n        \"output\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"anyOf\": [\n              {\n                \"type\": \"string\"\n              },\n              {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": [\n                    \"string\",\n                    \"number\"\n                  ]\n                }\n              }\n            ]\n          }\n        },\n        \"output_is_list\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"output_name\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"output_tooltips\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"display_name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"category\": {\n          \"type\": \"string\"\n        },\n        \"output_node\": {\n          \"type\": \"boolean\"\n        },\n        \"python_module\": {\n          \"type\": \"string\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\"\n        },\n        \"experimental\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"display_name\",\n        \"description\",\n        \"category\",\n        \"output_node\",\n        \"python_module\"\n      ],\n      \"additionalProperties\": false\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/standards",
  "markdown": "# 标准 - ComfyUI\n\n## 基本标准\n\n### 1\\. 社区价值\n\n自定义节点必须为 ComfyUI 社区提供有价值的功能 避免：\n\n*   过度自我宣传\n*   冒充或误导行为\n*   恶意行为\n*   自我宣传仅允许在指定的设置菜单部分内\n*   顶部和侧边菜单应仅包含实用功能\n\n### 2\\. 节点兼容性\n\n不要干扰其他自定义节点的操作（安装、更新、删除）\n\n*   对于其他自定义节点的依赖：\n    *   使用依赖功能时显示清晰的警告\n    *   提供展示所需节点的示例工作流\n\n### 3\\. 法律合规性\n\n必须遵守所有适用的法律和法规\n\n### 5\\. 质量要求\n\n节点必须功能完整、文档完善且积极维护。\n\n### 6\\. 分叉指南\n\n分叉的节点必须：\n\n*   与原始节点名称有明显区别\n*   在功能或代码上提供显著差异\n\n以下是发布自定义节点到注册表必须满足的标准。\n\n## 安全标准\n\n自定义节点应该是安全的。我们将开始与违反这些标准的自定义节点合作进行重写。如果有一些应该由核心暴露的主要功能，请在 [rfcs 仓库](https://github.com/comfy-org/rfcs) 中提出请求。\n\n### eval/exec 调用\n\n#### 政策\n\n由于安全考虑，禁止在自定义节点中使用 `eval` 和 `exec` 函数。\n\n#### 原因\n\n这些函数可以启用任意代码执行，在处理用户输入时创建潜在的远程代码执行（RCE）漏洞。包含将用户输入传递给 `eval` 或 `exec` 的节点的工作流可能被利用进行各种网络攻击，包括：\n\n*   键盘记录\n*   勒索软件\n*   其他恶意代码执行\n\n### 用于 pip install 的 subprocess\n\n#### 政策\n\n不允许通过 subprocess 调用进行运行时包安装。\n\n#### 原因\n\n*   ComfyUI manager 将与 ComfyUI 一起发布，并允许用户安装依赖项\n*   集中式依赖管理提高了安全性和用户体验\n*   有助于防止潜在的供应链攻击\n*   消除了多次重新加载 ComfyUI 的需要\n\n### 代码混淆\n\n#### 政策\n\n禁止在自定义节点中进行代码混淆。\n\n#### 原因\n\n混淆的代码：\n\n*   无法审查，很可能具有恶意性"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/google/gemini",
  "markdown": "# Google Gemini API 节点 ComfyUI 官方示例\n\nGoogle Gemini 是 Google 推出的一款强大的 AI 模型，支持对话、文本生成等多种功能。目前 ComfyUI 已集成 Google Gemini API，你可以直接在 ComfyUI 中使用相关节点来完成对话功能。 本篇指南中，我们将引导你完成对应对话功能。\n\n### 1\\. 工作流文件下载\n\n请下载下面的 Json 文件并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/google/api_google_gemini.json)\n\n### 2\\. 按步骤完成工作流的运行\n\n![OpenAI Chat Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg)\n\n你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Load Image` 节点中加载你需要 AI 的解读图片\n2.  (可选) 如果需要你可以修改`Google Gemini` 中的提示词，从而让 AI 来执行特定任务\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行对话。\n4.  等待 API 返回结果后，你可在 `Preview Any` 节点中查看对应 AI 返回的内容。\n\n### 3\\. 补充说明\n\n*   目前文件输入节点 `Gemini Input Files` 需要先将文件上传至`ComfyUI/input/` 目录下， 此节点正在改进，我们会在更新后修改模板\n*   工作流中提供了使用 `Batch Images` 来输入的示例，如果你有多张图片需要 AI 解读，可参考步骤图在使用右键来将对应的节点模式设置为 `总是（always）` 来启用"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/cicd",
  "markdown": "# 自定义节点 CI/CD - ComfyUI\n\n## 简介\n\n在对自定义节点进行更改时，在 Comfy 或其他自定义节点中出现问题并不罕见。在每种操作系统和不同的 Pytorch 配置上进行测试通常是不现实的。\n\n### 使用 Github Actions 运行 Comfy 工作流\n\n[Comfy-Action](https://github.com/Comfy-Org/comfy-action) 允许您在 Github Actions 上运行 Comfy workflow.json 文件。它支持下载模型、自定义节点，并可在 Linux/Mac/Windows 上运行。\n\n### 结果\n\n输出文件会上传到 [CI/CD 仪表板](https://comfyci.org/)，可以在提交新更改或发布自定义节点的新版本之前作为最后一步查看。 ![ComfyCI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyci.png)"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/manager",
  "markdown": "# 发布到 Manager - ComfyUI\n\n### 使用 ComfyUI Manager\n\n要让你的自定义节点通过 **ComfyUI Manager** 提供，你需要将其保存为 git 仓库（通常在 `github.com`）， 然后在 **ComfyUI Manager** 的 git 仓库提交一个 Pull Request，在其中编辑 `custom-node-list.json` 以添加你的节点。 [详细说明](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#how-to-register-your-custom-node-into-comfyui-manager)。 当用户安装节点时，**ComfyUI Manager** 会：\n\n### ComfyUI Manager 文件\n\n如上所述，**ComfyUI Manager** 会使用一些文件和脚本来管理自定义节点的生命周期。这些都是可选的。\n\n*   `requirements.txt` - 如上所述的 Python 依赖\n*   `install.py`, `uninstall.py` - 安装或卸载自定义节点时执行\n*   `disable.py`, `enable.py` - 禁用或重新启用自定义节点时执行\n*   `node_list.json` - 仅当自定义节点的 NODE\\_CLASS\\_MAPPINGS 模式不符合常规时才需要。\n\n官方详情请参见 [ComfyUI Manager 指南](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#custom-node-support-guide)。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/lifecycle",
  "markdown": "# 生命周期 - ComfyUI\n\n## Comfy 如何加载自定义节点\n\n当 Comfy 启动时，它会扫描 `custom_nodes` 目录下的 Python 模块，并尝试加载它们。 如果模块导出了 `NODE_CLASS_MAPPINGS`，它就会被视为自定义节点。\n\n### **init**.py\n\n当 Comfy 尝试导入模块时，会执行 `__init__.py` 文件。要让模块被识别为包含自定义节点定义，必须导出 `NODE_CLASS_MAPPINGS`。如果导出了（并且导入过程中没有出错），模块中定义的节点就会在 Comfy 中可用。如果你的代码有错误，Comfy 会继续运行，但会报告该模块加载失败。所以请检查 Python 控制台！ 一个非常简单的 `__init__.py` 文件如下所示：\n\n```\nfrom .python_file import MyCustomNode\nNODE_CLASS_MAPPINGS = { \"My Custom Node\" : MyCustomNode }\n__all__ = [\"NODE_CLASS_MAPPINGS\"]\n```\n\n#### NODE\\_CLASS\\_MAPPINGS\n\n`NODE_CLASS_MAPPINGS` 必须是一个 `dict`，将自定义节点的唯一名称（在整个 Comfy 安装中唯一）映射到对应的节点类。\n\n#### NODE\\_DISPLAY\\_NAME\\_MAPPINGS\n\n`__init__.py` 还可以导出 `NODE_DISPLAY_NAME_MAPPINGS`，它将同样的唯一名称映射为节点的显示名称。 如果没有提供 `NODE_DISPLAY_NAME_MAPPINGS`，Comfy 会使用唯一名称作为显示名称。\n\n#### WEB\\_DIRECTORY\n\n如果你需要部署客户端代码，还需要导出 JavaScript 文件所在路径（相对于模块的路径）。通常将这些文件放在自定义节点的 `js` 子目录下。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview",
  "markdown": "# 属性 - ComfyUI\n\n### 简单示例\n\n下面是“反转图片节点”的代码，概述了自定义节点开发中的关键概念。\n\n```\nclass InvertImageNode:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": { \"image_in\" : (\"IMAGE\", {}) },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"image_out\",)\n    CATEGORY = \"examples\"\n    FUNCTION = \"invert\"\n\n    def invert(self, image_in):\n        image_out = 1 - image_in\n        return (image_out,)\n```\n\n### 主要属性\n\n每个自定义节点都是一个 Python 类，具有以下关键属性：\n\n#### INPUT\\_TYPES\n\n顾名思义，`INPUT_TYPES` 定义了节点的输入。该方法返回一个 `dict`，**必须**包含 `required` 键，也**可以**包含 `optional` 和/或 `hidden` 键。`required` 和 `optional` 输入的唯一区别在于，`optional` 输入可以不连接。  \n关于 `hidden` 输入的更多信息，参见 [隐藏输入](https://docs.comfy.org/zh-CN/custom-nodes/backend/more_on_inputs#hidden-inputs)。 每个键的值又是一个 `dict`，其中的键值对指定输入的名称和类型。类型由一个 `tuple` 定义，第一个元素是数据类型，第二个元素是包含附加参数的 `dict`。 这里我们只有一个必需输入，名为 `image_in`，类型为 `IMAGE`，没有额外参数。 注意，与接下来几个属性不同，`INPUT_TYPES` 是一个 `@classmethod`。这样做的目的是让下拉小部件中的选项（比如要加载的 checkpoint 名称）可以在运行时由 Comfy 动态计算。我们稍后会详细介绍这一点。\n\n#### RETURN\\_TYPES\n\n一个由 `str` 组成的 `tuple`，定义了节点返回的数据类型。如果节点没有输出，也必须提供 `RETURN_TYPES = ()`。\n\n#### RETURN\\_NAMES\n\n用于标记输出的名称。此项为可选；如果省略，名称将直接使用 `RETURN_TYPES` 的小写形式。\n\n#### CATEGORY\n\n节点在 ComfyUI **添加节点** 菜单中的分类。可以用路径指定子菜单，例如 `examples/trivial`。\n\n#### FUNCTION\n\n节点执行时应调用的 Python 函数名。 该函数以命名参数的方式被调用。所有 `required`（和 `hidden`）输入都会包含在内；`optional` 输入只有在连接时才会包含，因此你应在函数定义中为它们提供默认值（或用 `**kwargs` 捕获）。 该函数返回一个与 `RETURN_TYPES` 对应的元组。即使没有返回内容，也必须返回元组（`return ()`）。同样，如果只有一个输出，记得加上逗号 `return (image_out,)`！\n\n### 执行控制扩展\n\nComfy 的一个很棒的特性是它会缓存输出，并且只会执行那些结果可能与上次运行不同的节点。这可以极大地加快许多工作流的速度。 本质上，这通过识别哪些节点会产生输出（比如 Image Preview 和 Save Image 节点，这些节点总是会被执行），然后反向追踪哪些节点提供了自上次运行以来可能已更改的数据。 自定义节点有两个可选特性可以协助这一过程。\n\n#### OUTPUT\\_NODE\n\n默认情况下，节点不会被视为输出节点。设置 `OUTPUT_NODE = True` 可以指定该节点为输出节点。\n\n#### IS\\_CHANGED\n\n默认情况下，如果节点的任何输入或小部件发生变化，Comfy 会认为该节点已更改。这通常是正确的，但在某些情况下你可能需要重写此行为，例如节点使用了随机数（且未指定种子——此时最好提供一个种子输入，以便用户可以控制可复现性并避免不必要的执行）、加载了可能已在外部更改的输入，或有时会忽略某些输入（因此不需要仅因这些输入变化而执行）。\n\n`IS_CHANGED` 接收与主函数（由 `FUNCTION` 指定）相同的参数，并可以返回任意 Python 对象。该对象会与上次运行时返回的对象进行比较，如果 `is_changed != is_changed_old`，则认为节点已更改（相关代码在 `execution.py` 中）。 由于 `True == True`，如果节点返回 `True` 表示已更改，实际上会被认为未更改！如果不是为了兼容现有节点，这一行为本可以在 Comfy 代码中修正。 如果你希望节点始终被认为已更改（不推荐，因为这会阻止 Comfy 优化执行流程），可以 `return float(\"NaN\")`。这会返回一个 `NaN`，它与任何值都不相等，甚至与另一个 `NaN` 也不相等。 一个实际检查变化的好例子是内置的 LoadImage 节点的代码，它会加载图片并返回哈希值：\n\n```\n    @classmethod\n    def IS_CHANGED(s, image):\n        image_path = folder_paths.get_annotated_filepath(image)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n```\n\n### 其他属性\n\n还有三个属性可以用来修改 Comfy 对节点的默认处理方式。\n\n#### INPUT\\_IS\\_LIST, OUTPUT\\_IS\\_LIST\n\n用于控制数据的顺序处理，详见[后文](https://docs.comfy.org/zh-CN/custom-nodes/backend/lists)。\n\n### VALIDATE\\_INPUTS\n\n如果定义了类方法 `VALIDATE_INPUTS`，则在工作流开始执行前会被调用。  \n`VALIDATE_INPUTS` 如果输入有效应返回 `True`，否则返回一个描述错误的字符串（这会阻止执行）。\n\n#### 常量校验\n\n`VALIDATE_INPUTS` 只会收到其签名中请求的输入（即 `inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args` 返回的参数）。通过这种方式接收的输入不会经过默认校验规则。例如，在下面的代码片段中，前端会使用 `foo` 输入指定的 `min` 和 `max`，但后端不会强制校验。\n\n```\nclass CustomNode:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": { \"foo\" : (\"INT\", {\"min\": 0, \"max\": 10}) },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, foo):\n        # YOLO，啥都行！\n        return True\n```\n\n此外，如果该函数接收 `**kwargs`，则会收到所有可用输入，并且所有这些输入都将跳过校验，就像显式指定一样。\n\n#### 类型校验\n\n如果 `VALIDATE_INPUTS` 方法接收一个名为 `input_types` 的参数，则会传入一个字典，键为每个连接到其他节点输出的输入名，值为该输出的类型。 当存在此参数时，所有输入类型的默认校验都会被跳过。下面是一个利用前端允许指定多种类型的例子：\n\n```\nclass AddNumbers:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"input1\" : (\"INT,FLOAT\", {\"min\": 0, \"max\": 1000}),\n                \"input2\" : (\"INT,FLOAT\", {\"min\": 0, \"max\": 1000})\n            },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, input_types):\n        # input1 和 input2 的 min/max 仍然会被校验，因为\n        # 我们没有将 `input1` 或 `input2` 作为参数接收\n        if input_types[\"input1\"] not in (\"INT\", \"FLOAT\"):\n            return \"input1 必须是 INT 或 FLOAT 类型\"\n        if input_types[\"input2\"] not in (\"INT\", \"FLOAT\"):\n            return \"input2 必须是 INT 或 FLOAT 类型\"\n        return True\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/specifications",
  "markdown": "# pyproject.toml - ComfyUI\n\n## 规范\n\n`pyproject.toml` 文件包含两个主要的 ComfyUI 自定义节点部分：`[project]` 和 `[tool.comfy]`。以下是每个部分的规范。\n\n## \\[project\\] 部分\n\n### name（必需）\n\n节点 ID 唯一标识自定义节点，并将用于注册表中的 URL。用户可以通过引用此名称来安装节点：\n\n```\ncomfy node install <node-id>\n```\n\n**要求:**\n\n*   必须小于 100 个字符\n*   只能包含字母、数字、连字符、下划线和句点\n*   不能有连续的特殊字符\n*   不能以数字或特殊字符开头\n*   不区分大小写比较\n\n**最佳实践:**\n\n*   使用简短、描述性的名称\n*   不要在名称中包含 “ComfyUI”\n*   使其易于记忆和输入\n\n**Examples:**\n\n```\nname = \"image-processor\"      # ✅ Good: Simple and clear\nname = \"super-resolution\"     # ✅ Good: Describes functionality\nname = \"ComfyUI-enhancer\"    # ❌ Bad: Includes ComfyUI\nname = \"123-tool\"            # ❌ Bad: Starts with number\n```\n\n更多详细信息，请参阅[官方 Python 文档](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#name)。\n\n### version（必需）\n\n使用 [语义化版本控制](https://semver.org/) 并包含三个数字的版本号 X.Y.Z：\n\n*   X（**MAJOR**）：重大更改\n*   Y（**MINOR**）：新功能（向后兼容）\n*   Z (**PATCH**): Bug fixes\n\n**Examples:**\n\n```\nversion = \"1.0.0\"    # 初始版本\nversion = \"1.1.0\"    # 添加新功能\nversion = \"1.1.1\"    # 修复错误\nversion = \"2.0.0\"    # 重大更改\n```\n\n### license（可选）\n\n指定自定义节点的许可证。可以以两种方式指定：\n\n1.  **文件引用：**\n\n```\nlicense = { file = \"LICENSE\" }     # ✅ 指向 LICENSE 文件\nlicense = { file = \"LICENSE.txt\" } # ✅ 指向 LICENSE.txt 文件\nlicense = \"LICENSE\"                # ❌ 格式错误\n```\n\n2.  **许可证名称：**\n\n```\nlicense = { text = \"MIT License\" }  # ✅ 正确格式\nlicense = { text = \"Apache-2.0\" }   # ✅ 正确格式\nlicense = \"MIT LICENSE\"             # ❌ 格式错误\n```\n\n常见许可证：[MIT](https://opensource.org/license/mit), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html), [Apache](https://www.apache.org/licenses/LICENSE-2.0)\n\n### description（推荐）\n\n自定义节点的简要描述。\n\n```\ndescription = \"A super resolution node for enhancing image quality\"\n```\n\n### repository (必需)\n\n相关资源的链接：\n\n```\n[project.urls]\nRepository = \"https://github.com/username/repository\"\n```\n\n### urls（推荐）\n\n相关资源的链接：\n\n```\n[project.urls]\nDocumentation = \"https://github.com/username/repository/wiki\"\n\"Bug Tracker\" = \"https://github.com/username/repository/issues\"\n```\n\n### requires-python（推荐）\n\n指定自定义节点支持的 Python 版本：\n\n```\nrequires-python = \">=3.8\"        # Python 3.8 或更高版本\nrequires-python = \">=3.8,<3.11\"  # Python 3.8 到 3.11 之间（不包括 3.11）\n```\n\n### 前端版本兼容性（可选）\n\n如果你的节点对 ComfyUI 前端版本有特定要求，你可以使用 `comfyui-frontend-package` 依赖项来指定。该包发布在 [PyPI](https://pypi.org/project/comfyui-frontend-package/) 上。 在以下情况下使用此字段：\n\n*   你的自定义节点使用了特定版本中引入的前端 API\n*   你发现了你的节点与某些前端版本之间的不兼容性\n*   你的节点需要仅在较新前端版本中可用的特定 UI 功能\n\n```\n[project]\ndependencies = [\n    \"comfyui-frontend-package>=1.20.0\"       # 需要前端 1.20.0 或更新版本\n    \"comfyui-frontend-package<=1.21.6\"       # 限制前端版本最高到 1.21.6\n    \"comfyui-frontend-package>=1.19,<1.22\"   # 适用于前端 1.19 到 1.21.x\n    \"comfyui-frontend-package~=1.20.0\"       # 兼容 1.20.x 但不包括 1.21.0\n    \"comfyui-frontend-package!=1.21.3\"       # 适用于任何版本，除了 1.21.3\n]\n```\n\n### classifiers（推荐）\n\n使用分类器指定操作系统的兼容性和GPU加速器。这个信息用于帮助用户找到适合他们系统的节点。\n\n```\n[project]\nclassifiers = [\n    # 适用于所有操作系统的节点\n    \"Operating System :: OS Independent\",\n\n    # 或者对于特定操作系统的节点，指定支持的系统：\n    \"Operating System :: Microsoft :: Windows\",  # Windows specific\n    \"Operating System :: POSIX :: Linux\",  # Linux specific\n    \"Operating System :: MacOS\",  # macOS specific\n    \n    # GPU 加速器支持\n    \"Environment :: GPU :: NVIDIA CUDA\",    # NVIDIA CUDA 支持\n    \"Environment :: GPU :: AMD ROCm\",       # AMD ROCm 支持\n    \"Environment :: GPU :: Intel Arc\",      # Intel Arc 支持\n    \"Environment :: NPU :: Huawei Ascend\",  # 华为昇腾支持\n    \"Environment :: GPU :: Apple Metal\",    # Apple Metal 支持\n]\n```\n\n### PublisherId（必需）\n\n你的唯一发布者标识符，通常与您的 GitHub 用户名匹配。 **Examples:**\n\n```\nPublisherId = \"john-doe\"        # ✅ 匹配 GitHub 用户名\nPublisherId = \"image-wizard\"    # ✅ 唯一标识符\n```\n\n### DisplayName（可选）\n\n你的自定义节点的用户友好名称。\n\n```\nDisplayName = \"Super Resolution Node\"\n```\n\n### Icon（可选）\n\n你的自定义节点的图标 URL，将在 ComfyUI Registry 和 ComfyUI-Manager 中显示。 **要求：**\n\n*   文件类型：SVG, PNG, JPG, 或 GIF\n*   最大分辨率：400px × 400px\n*   长宽比应该是正方形\n\n```\nIcon = \"https://raw.githubusercontent.com/username/repo/main/icon.png\"\n```\n\nURL 指向一个较大的横幅图像，将在 ComfyUI Registry 和 ComfyUI-Manager 中显示。 **要求：**\n\n*   文件类型：SVG, PNG, JPG, 或 GIF\n*   长宽比：21:9\n\n```\nBanner = \"https://raw.githubusercontent.com/username/repo/main/banner.png\"\n```\n\n### requires-comfyui（可选）\n\n指定你的节点兼容的 ComfyUI 版本。这有助于用户确保他们安装了正确版本的 ComfyUI。 **支持的操作符：** `<`, `>`, `<=`, `>=`, `~=`, `<>`, `!=` 和范围\n\n```\nrequires-comfyui = \">=1.0.0\"        # ComfyUI 1.0.0 或更高版本\nrequires-comfyui = \">=1.0.0,<2.0.0\"  # ComfyUI 1.0.0 到 2.0.0 之间（不包括 2.0.0）\nrequires-comfyui = \"~=1.0.0\"         # 兼容版本：1.0.0 或更新版本，但不包括 2.0.0\nrequires-comfyui = \"!=1.2.3\"         # 任何版本，除了 1.2.3\nrequires-comfyui = \">0.1.3,<1.0.0\"   # 大于 0.1.3 且小于 1.0.0\n```\n\n### includes（可选）\n\n指定是否强制包含某些特定文件夹。对于一些情况，例如在 frontend 项目中的自定义节点，最终打包输出的文件夹可能会被包含在 .gitignore 中。在这种情况下，我们需要强制包含它以用于注册表使用。\n\n## 完整示例\n\n```\n[project]\nname = \"super-resolution-node\"\nversion = \"1.0.0\"\ndescription = \"Enhance image quality using advanced super resolution techniques\"\nlicense = { file = \"LICENSE\" }\nrequires-python = \">=3.8\"\ndependencies = [\n    \"comfyui-frontend-package<=1.21.6\"  # 前端版本兼容性\n]\nclassifiers = [\n    \"Operating System :: OS Independent\"  # 适用于所有操作系统\n]\ndynamic = [\"dependencies\"]\n\n[tool.setuptools.dynamic]\ndependencies = {file = [\"requirements.txt\"]}\n\n[project.urls]\nRepository = \"https://github.com/username/super-resolution-node\"\nDocumentation = \"https://github.com/username/super-resolution-node/wiki\"\n\"Bug Tracker\" = \"https://github.com/username/super-resolution-node/issues\"\n\n[tool.comfy]\nPublisherId = \"image-wizard\"\nDisplayName = \"Super Resolution Node\"\nIcon = \"https://raw.githubusercontent.com/username/super-resolution-node/main/icon.png\"\nBanner = \"https://raw.githubusercontent.com/username/super-resolution-node/main/banner.png\"\nrequires-comfyui = \">=1.0.0\"  # ComfyUI 版本兼容性\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/specs/nodedef_json",
  "markdown": "# 节点定义 JSON - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyNodeDefV2\",\n  \"definitions\": {\n    \"ComfyNodeDefV2\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"inputs\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"anyOf\": [\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": \"number\"\n                        }\n                      }\n                    ]\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"min\": {\n                    \"type\": \"number\"\n                  },\n                  \"max\": {\n                    \"type\": \"number\"\n                  },\n                  \"step\": {\n                    \"type\": \"number\"\n                  },\n                  \"display\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"slider\",\n                      \"number\",\n                      \"knob\"\n                    ]\n                  },\n                  \"control_after_generate\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"INT\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": \"number\"\n                        }\n                      }\n                    ]\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"min\": {\n                    \"type\": \"number\"\n                  },\n                  \"max\": {\n                    \"type\": \"number\"\n                  },\n                  \"step\": {\n                    \"type\": \"number\"\n                  },\n                  \"display\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"slider\",\n                      \"number\",\n                      \"knob\"\n                    ]\n                  },\n                  \"round\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"boolean\",\n                        \"const\": false\n                      }\n                    ]\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"FLOAT\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"label_on\": {\n                    \"type\": \"string\"\n                  },\n                  \"label_off\": {\n                    \"type\": \"string\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"BOOLEAN\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"type\": \"string\"\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"multiline\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"dynamicPrompts\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"defaultVal\": {\n                    \"type\": \"string\"\n                  },\n                  \"placeholder\": {\n                    \"type\": \"string\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"STRING\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {},\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"control_after_generate\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"image_upload\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"image_folder\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"input\",\n                      \"output\",\n                      \"temp\"\n                    ]\n                  },\n                  \"allow_batch\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"video_upload\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"remote\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"route\": {\n                        \"anyOf\": [\n                          {\n                            \"type\": \"string\",\n                            \"format\": \"uri\"\n                          },\n                          {\n                            \"type\": \"string\",\n                            \"pattern\": \"^\\\\/\"\n                          }\n                        ]\n                      },\n                      \"refresh\": {\n                        \"anyOf\": [\n                          {\n                            \"type\": \"number\",\n                            \"minimum\": -9007199254740991,\n                            \"maximum\": 9007199254740991\n                          },\n                          {\n                            \"type\": \"number\",\n                            \"maximum\": 9007199254740991,\n                            \"minimum\": -9007199254740991\n                          }\n                        ]\n                      },\n                      \"response_key\": {\n                        \"type\": \"string\"\n                      },\n                      \"query_params\": {\n                        \"type\": \"object\",\n                        \"additionalProperties\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"refresh_button\": {\n                        \"type\": \"boolean\"\n                      },\n                      \"control_after_refresh\": {\n                        \"type\": \"string\",\n                        \"enum\": [\n                          \"first\",\n                          \"last\"\n                        ]\n                      },\n                      \"timeout\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0\n                      },\n                      \"max_retries\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0\n                      }\n                    },\n                    \"required\": [\n                      \"route\"\n                    ],\n                    \"additionalProperties\": false\n                  },\n                  \"options\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": [\n                        \"string\",\n                        \"number\"\n                      ]\n                    }\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"COMBO\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {},\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              }\n            ]\n          }\n        },\n        \"outputs\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"index\": {\n                \"type\": \"number\"\n              },\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"is_list\": {\n                \"type\": \"boolean\"\n              },\n              \"options\": {\n                \"type\": \"array\"\n              },\n              \"tooltip\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"index\",\n              \"name\",\n              \"type\",\n              \"is_list\"\n            ],\n            \"additionalProperties\": false\n          }\n        },\n        \"hidden\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {}\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"display_name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"category\": {\n          \"type\": \"string\"\n        },\n        \"output_node\": {\n          \"type\": \"boolean\"\n        },\n        \"python_module\": {\n          \"type\": \"string\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\"\n        },\n        \"experimental\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"inputs\",\n        \"outputs\",\n        \"name\",\n        \"display_name\",\n        \"description\",\n        \"category\",\n        \"output_node\",\n        \"python_module\"\n      ],\n      \"additionalProperties\": false\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/lists",
  "markdown": "# 数据列表 - ComfyUI\n\n## 长度为一的处理\n\n在内部，Comfy 服务器将从一个节点流向下一个节点的数据表示为 Python `list`，通常长度为 1，类型为相关的数据类型。 在正常操作中，当一个节点返回输出时，输出 `tuple` 中的每个元素都会被单独包裹在一个长度为 1 的列表中；然后当下一个节点被调用时，数据会被解包并传递给主函数。\n\n## 列表处理\n\n在某些情况下，单个工作流会处理多个数据实例，此时内部数据将是包含多个数据实例的列表。 例如，逐个处理一系列图像以避免 VRAM 不足，或处理不同尺寸的图像。 默认情况下，Comfy 会按顺序处理列表中的值：\n\n*   如果输入是不同长度的 `list`，较短的会通过重复最后一个值进行填充\n*   主方法会针对输入列表中的每个值调用一次\n*   输出也是 `list`，每个输出的长度与最长的输入相同\n\n相关代码可在 `execution.py` 的 `map_node_over_list` 方法中找到。 然而，由于 Comfy 会将节点输出包裹为长度为 1 的 `list`，如果自定义节点返回的 `tuple` 中包含一个 `list`，该 `list` 会被包裹并作为单个数据处理。 为了告诉 Comfy 返回的列表不应被包裹，而是作为一系列数据进行顺序处理，节点应提供一个类属性 `OUTPUT_IS_LIST`，它是一个与 `RETURN_TYPES` 长度相同的 `tuple[bool]`，用于指定哪些输出应如此处理。 节点也可以重写默认的输入行为，在一次调用中接收整个列表。这可以通过设置类属性 `INPUT_IS_LIST` 为 `True` 实现。 以下是内置节点的一个（带注释的）示例——`ImageRebatch` 接收一个或多个图像批次（作为列表接收，因为 `INPUT_IS_LIST = True`），并将它们重新分批为所需大小的批次。\n\n```\n\nclass ImageRebatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"images\": (\"IMAGE\",),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}) }}\n    RETURN_TYPES = (\"IMAGE\",)\n    INPUT_IS_LIST = True\n    OUTPUT_IS_LIST = (True, )\n    FUNCTION = \"rebatch\"\n    CATEGORY = \"image/batch\"\n\n    def rebatch(self, images, batch_size):\n        batch_size = batch_size[0]    # 所有输入都是列表，所以 batch_size 是 list[int]\n\n        output_list = []\n        all_images = []\n        for img in images:                    # 每个 img 是一个图像批次\n            for i in range(img.shape[0]):     # 每个 i 是一张单独的图像\n                all_images.append(img[i:i+1])\n\n        for i in range(0, len(all_images), batch_size): # 按 batch_size 分块，每块组成一个新批次\n            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))  # 如果图像批次宽高不同会报错！\n\n        return (output_list,)\n```\n\n#### INPUT\\_IS\\_LIST"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/snippets",
  "markdown": "# 带注释的示例 - ComfyUI\n\n不断增长的示例代码片段集合……\n\n## 图像与蒙版\n\n### 加载图像\n\n将图像加载为批量大小为1（基于 `nodes.py` 中的 `LoadImage` 源代码）\n\n```\ni = Image.open(image_path)\ni = ImageOps.exif_transpose(i)\nif i.mode == 'I':\n    i = i.point(lambda i: i * (1 / 255))\nimage = i.convert(\"RGB\")\nimage = np.array(image).astype(np.float32) / 255.0\nimage = torch.from_numpy(image)[None,]\n```\n\n### 保存图像批量\n\n保存一批图像（基于 `nodes.py` 中的 `SaveImage` 源代码）\n\n```\nfor (batch_number, image) in enumerate(images):\n    i = 255. * image.cpu().numpy()\n    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n    filepath = # some path that takes the batch number into account\n    img.save(filepath)\n```\n\n### 反转蒙版\n\n反转蒙版是一个简单的过程。由于蒙版已被归一化到 \\[0,1\\] 区间：\n\n### 将蒙版转换为图像形状\n\n```\n# 我们需要 [B,H,W,C]，其中 C = 1\nif len(mask.shape)==2: # 当前为 [H,W]，插入 B 和 C 作为第1维\n    mask = mask[None,:,:,None]\nelif len(mask.shape)==3 and mask.shape[2]==1: # 当前为 [H,W,C]\n    mask = mask[None,:,:,:]\nelif len(mask.shape)==3:                      # 当前为 [B,H,W]\n    mask = mask[:,:,:,None]\n```\n\n### 将蒙版用作透明层\n\n当用于修复或分割等任务时，蒙版的值最终会被四舍五入为最接近的整数，使其为二值——0表示要忽略的区域，1表示要处理的区域。但在蒙版传递到这些节点之前，这一步不会发生。这种灵活性允许你像在数码摄影中那样，将蒙版用作透明层：\n\n```\n# 将蒙版反转回原始透明层\nmask = 1.0 - mask\n\n# 扩展 `C`（通道）维度\nmask = mask.unsqueeze(-1)\n\n# 沿 `C` 维拼接（cat）\nrgba_image = torch.cat((rgb_image, mask), dim=-1)\n```\n\n## 噪声\n\n### 创建噪声变体\n\n以下是一个创建混合两个噪声源的噪声对象的示例。通过调整 `weight2`，可以用来生成轻微不同的噪声变体。\n\n```\nclass Noise_MixedNoise:\n    def __init__(self, nosie1, noise2, weight2):\n        self.noise1  = noise1\n        self.noise2  = noise2\n        self.weight2 = weight2\n\n    @property\n    def seed(self): return self.noise1.seed\n\n    def generate_noise(self, input_latent:torch.Tensor) -> torch.Tensor:\n        noise1 = self.noise1.generate_noise(input_latent)\n        noise2 = self.noise2.generate_noise(input_latent)\n        return noise1 * (1.0-self.weight2) + noise2 * (self.weight2)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN",
  "markdown": "# ComfyUI 官方文档 - ComfyUI\n\n## 关于 ComfyUI\n\n由 [comfyanonymous](https://github.com/comfyanonymous) 和其他[贡献者](https://github.com/comfyanonymous/ComfyUI/graphs/contributors)开发。\n\n*   **ComfyUI** 是一个基于节点的生成式 AI 界面和推理引擎\n*   用户可以通过节点组合各种 AI 模型和操作，实现高度可定制和可控的内容生成\n*   ComfyUI 完全开源，可以在本地设备上运行"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/tensors",
  "markdown": "# 使用 torch.Tensor - ComfyUI\n\n## pytorch、张量与 torch.Tensor\n\nComfy 的所有核心数值计算都是由 [pytorch](https://pytorch.org/) 完成的。如果你的自定义节点需要深入 stable diffusion 的底层，你就需要熟悉这个库，这远超本简介的范围。 不过，许多自定义节点都需要操作图像、潜变量和蒙版，这些在内部都表示为 `torch.Tensor`，因此你可能需要收藏 [torch.Tensor 的官方文档](https://pytorch.org/docs/stable/tensors.html)。\n\n### 什么是张量？\n\n`torch.Tensor` 表示张量，张量是向量或矩阵在任意维度上的数学泛化。张量的 _秩_（rank）是它的维度数量（所以向量秩为 1，矩阵秩为 2）；它的 _形状_（shape）描述了每个维度的大小。 因此，一个 RGB 图像（高为 H，宽为 W）可以被看作是三组数组（每个颜色通道一组），每组大小为 H x W，可以表示为形状为 `[H,W,3]` 的张量。在 Comfy 中，图像几乎总是以批量（batch）形式出现（即使批量中只有一张图）。`torch` 总是将批量维放在第一位，所以 Comfy 的图像形状为 `[B,H,W,3]`，通常写作 `[B,H,W,C]`，其中 C 代表通道数（Channels）。\n\n### squeeze、unsqueeze 与 reshape\n\n如果张量的某个维度大小为 1（称为折叠维度），那么去掉这个维度后的张量与原张量等价（比如只有一张图片的批量其实就是一张图片）。去除这种折叠维度称为 squeeze，插入一个这样的维度称为 unsqueeze。\n\n将同样的数据以不同的形状表示称为 reshape。通常你需要了解底层数据结构，因此请谨慎操作！\n\n### 重要符号说明\n\n`torch.Tensor` 支持大多数 Python 的切片符号、迭代和其他常见的类列表操作。张量还有一个 `.shape` 属性，返回其大小，类型为 `torch.Size`（它是 `tuple` 的子类，可以当作元组使用）。 还有一些你经常会见到的重要符号（其中几个在标准 Python 里不常见，但在处理张量时很常用）：\n\n*   `torch.Tensor` 支持在切片符号中使用 `None`，表示插入一个大小为 1 的新维度。\n*   `:` 在切片张量时常用，表示”保留整个维度”。就像 Python 里的 `a[start:end]`，但省略了起止点。\n*   `...` 表示”未指定数量的所有维度”。所以 `a[0, ...]` 会提取批量中的第一个元素，无论有多少维度。\n*   在需要传递形状的函数中，形状通常以 `tuple` 形式传递，其中某个维度可以用 `-1`，表示该维度的大小由数据总量自动推算。\n\n```\n>>> a = torch.Tensor((1,2))\n>>> a.shape\ntorch.Size([2])\n>>> a[:,None].shape \ntorch.Size([2, 1])\n>>> a.reshape((1,-1)).shape\ntorch.Size([1, 2])\n```\n\n### 元素级操作\n\n许多 `torch.Tensor` 的二元操作（包括 ’+’, ’-’, ’\\*’, ’/’ 和 ’==‘）都是元素级的（即对每个元素独立操作）。操作数必须是形状相同的两个张量，或一个张量和一个标量。所以：\n\n```\n>>> import torch\n>>> a = torch.Tensor((1,2))\n>>> b = torch.Tensor((3,2))\n>>> a*b\ntensor([3., 4.])\n>>> a/b\ntensor([0.3333, 1.0000])\n>>> a==b\ntensor([False,  True])\n>>> a==1\ntensor([ True, False])\n>>> c = torch.Tensor((3,2,1)) \n>>> a==c\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n```\n\n### 张量的布尔值\n\n你可能熟悉 Python 列表的真值：非空列表为 `True`，`None` 或 `[]` 为 `False`。而 `torch.Tensor`（只要有多个元素）没有定义的真值。你需要用 `.all()` 或 `.any()` 来合并元素级的真值：\n\n```\n>>> a = torch.Tensor((1,2))\n>>> print(\"yes\" if a else \"no\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n>>> a.all()\ntensor(False)\n>>> a.any()\ntensor(True)\n```\n\n这也意味着你需要用 `if a is not None:` 而不是 `if a:` 来判断一个张量变量是否已被赋值。"
},
{
  "url": "https://docs.comfy.org/zh-CN/get_started/first_generation",
  "markdown": "# 开始 ComfyUI 的 AI 绘图之旅\n\n本篇的主要目的是带你初步了解 ComfyUI 熟悉 ComfyUI 的一些基础操作，并引导你首次的图片生成\n\n1.  加载示例工作流\n    *   从 ComfyUI 加载`Workflows template`中的`Text to Image`工作流\n    *   使用带有`metadata` 的图片中加载工作流\n2.  指导你完成模型\n    *   自动安装模型\n    *   手动安装模型\n    *   使用 **ComfyUI Manager** 的模型管理功能安装模型\n3.  进行一次文本到图片的生成\n\n## 关于文生图的说明\n\n**文生图（Text to Image）**，是 AI 绘图的基础，通过输入文本描述来生成对应的图片，是 AI 绘图最常用的功能之一，你可以理解成你把你的**绘图要求(正向提示词、负向提示词)**告诉一个**画家(绘图模型)**，画家会根据你的要求，画出你想要的内容，由于本篇教程主要是为了引导你开始 ComfyUI 的使用，对于文生图的详细说明，我们将在[文生图](https://docs.comfy.org/zh-CN/tutorials/basic/image-to-image)章节进行详细讲解\n\n### 1\\. 启动 ComfyUI\n\n请确定你已经按照安装部分的指南完成了 ComfyUI 的启动，并可以成功打开 ComfyUI 的页面 ![ComfyUI界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/comfyui-boot-screen.jpg) 如果你还未安装 ComfyUI 请根据你的设备情况选择一个合适的版本进行安装\n\n### 2\\. 加载默认文生图工作流\n\n正常情况下，打开 ComfyUI 后是会自动加载默认的文生图工作流的, 不过你仍旧可以尝试以下不同方式加载工作流来熟悉 ComfyUI 的一些基础操作\n\n ![ComfyUI 界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1.jpg) 请对照图片中序号所对应的顺序进行操作\n\n1.  点击 ComfyUI 界面右下角的**Fit View**按钮，防止已加载工作流是在视图外导致不可见\n2.  点击侧边栏的**文件夹图标（workflows）**\n3.  点击 工作流（Workflows）面板顶部的**浏览工作流示例（Browse example workflows）** 按钮\n\n下图继续![加载工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg)\n\n4.  选择默认的第一个工作流 **Image Generation** 以加载图标\n\n或者你也可以从`workflow`菜单中选择**Browse workflow templates** 浏览工作流模板 ![ComfyUI 菜单 - 浏览工作流模板](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg) \n\n### 3\\. 安装绘图模型\n\n通常在 ComfyUI 的初始安装中，并不会包含任何的绘图模型，但是模型是我们运行图片生成必不可少的部分。 在你完成第二步，工作流的加载后，如果你的电脑上没有安装[v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) 这个模型文件时，一般会出现下图的提示 ![模型缺失](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg) 你可以直接选择点击 `Download` 按钮，让 ComfyUI 自动完成对应的模型的下载，但由于在有些地区不能够顺利访问对应模型的下载源，所以在这个步骤中，我将说明几种不同的模型安装方法。 无论使用哪种方法，模型都会被保存到 `<你的 ComfyUI 安装位置>/ComfyUI/models/` 文件夹下，你可以在你的电脑上尝试找到这个文件夹位置，你可以看到许多文件夹比如 `checkpoints`、`embeddings`、`vae`、`lora`、`upscale_model` 等，这些都是不同的模型保存的文件夹，通常以文件夹名称区分，ComfyUI 在启动时会检测这些文件夹下的模型文件，以及`extra_model_paths.yaml` 文件中配置的文件路径 ![ComfyUI 模型文件夹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg) 被检测到的不同的文件夹里的模型将可以在 ComfyUI 的不同 **模型加载节点** 里使用，下面让我们开始了解不同模型的安装方式：\n\n在你点击 **Download** 按钮后，ComfyUI 将会执行下载,根据你使用的版本不同，将会执行不同的行为\n\n桌面版将自动完成模型的下载并保存到 `<你的 ComfyUI 安装位置>/ComfyUI/models/checkpoints` 目录下 你可以等待安装完成或者在侧边栏的模型面板里查看安装进度![模型下载进度](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg)如果一切顺利，模型应该可以自动下载到本地，如果长时间未下载成功，请尝试其它安装方法\n\n### 4\\. 加载模型，并进行第一次图片生成\n\n在完成了对应的绘图模型安装后，请参考下图步骤加载对应的模型，并进行第一次图片的生成 ![图片生成](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg) 请对应图片序号，完成下面操作\n\n1.  请在 **Load Checkpoint** 节点使用箭头或者点击文本区域确保 **v1-5-pruned-emaonly-fp16.safetensors** 被选中，且左右切换箭头不会出现 **null** 的文本\n2.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl + enter(回车)` 来执行图片生成\n\n等待对应流程执行完成后，你应该可以在界面的 **保存图像(Save Image)** 节点中看到对应的图片结果，可以在上面右键保存到本地 ![ComfyUI 首次图片生成结果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg) 对于文生图的详细说明，下面的指南中会有详细的说明和指导\n\n[\n\n## ComfyUI 文生图工作流示例说明\n\n点击这里查看文生图工作流的详细说明\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)\n\n## 故障排除\n\n### 模型加载问题\n\n如果 `Load Checkpoint` 节点没有任何模型可以选择，或者显示为 **null**，请先确认你的模型安装位置正确，或者尝试 **刷新** 或者 **重启 ComfyUI** 使得对应文件夹下的模型可以被检测到"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/dall-e-3",
  "markdown": "# OpenAI DALL·E 3 节点 - ComfyUI\n\nOpenAI DALL·E 3 是 ComfyUI API 节点系列中的一员，它允许用户通过 OpenAI 的 **DALL·E 3** 模型生成图像。此节点支持文本到图像的生成功能。 ![OpenAI DALL·E 2 节点截图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-dall-e-3.jpg)\n\n## 节点概述\n\nDALL·E 3 是 OpenAI 的最新图像生成模型，能够根据文本提示创建详细且高质量的图像。通过 ComfyUI 中的这个节点，您可以直接访问 DALL·E 3 的生成能力，无需离开 ComfyUI 界面。 **OpenAI DALL·E 2** 节点通过 OpenAI 的图像生成 API 同步生成图像。它接收文本提示并返回符合描述的图像。\n\n## 参数详解\n\n### 必需参数\n\n| 参数名 | 类型  | 描述  |\n| --- | --- | --- |\n| prompt | 文本  | 用于生成图像的文本提示。支持多行输入，可以详细描述您想要生成的图像内容。 |\n\n### widget 参数\n\n| 参数名 | 类型  | 可选值 | 默认值 | 描述  |\n| --- | --- | --- | --- | --- |\n| seed | 整数  | 0-2147483647 | 0   | 用于控制生成结果的随机种子 |\n| quality | 选项  | standard, hd | standard | 图像质量设置。“hd”选项生成更高质量的图像，但可能需要更多计算资源 |\n| style | 选项  | natural, vivid | natural | 图像风格。“vivid”倾向于生成超真实和戏剧性的图像，“natural”则产生更自然、不那么夸张的图像 |\n| size | 选项  | 1024x1024, 1024x1792, 1792x1024 | 1024x1024 | 生成图像的尺寸。可以选择方形或不同方向的矩形图像 |\n\n## 使用示例\n\n你可以下载下面的图片，并拖入 ComfyUI 以加载对应的工作流 ![ComfyUI openai-dall-e-3工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png) 由于对应工作流非常简单，你也可以直接在 ComfyUI 中添加 **OpenAI DALL·E 3** 节点，并输入您想要生成的图像描述，然后运行工作流即可 ![ComfyUI openai-dall-e-3 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg)\n\n1.  在 ComfyUI 中添加 **OpenAI DALL·E 3** 节点\n2.  在提示文本框中输入您想要生成的图像描述\n3.  根据需要调整可选参数（质量、风格、尺寸等）\n4.  运行工作流程生成图像\n\n## 常见问题"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/workflow",
  "markdown": "# 工作流 - ComfyUI\n\n## 节点图\n\nComfyUI 是一个用于构建和运行生成内容的 _**工作流**_ 的环境。在这个上下文中，工作流被定义为一组称为 _**节点**_ 的程序对象，它们相互连接，形成一个网络。这个网络也被称为 _**图**_。 ComfyUI 工作流可以生成任何类型的媒体：图像、视频、音频、AI 模型、AI 代理等。\n\n## 示例工作流\n\n要开始，请尝试一些 [官方工作流](https://comfyanonymous.github.io/ComfyUI_examples)。这些工作流仅使用 ComfyUI 安装中包含的核心节点。一个蓬勃发展的开发者社区创建了丰富的 [生态系统](https://registry.comfy.org/) 的自定义节点，以扩展 ComfyUI 的功能。\n\n### 简单示例\n\n![简单工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/simple_workflow.jpeg)\n\n## 可视化编程\n\n像 ComfyUI 这样的基于节点的计算机程序提供了一种传统菜单和按钮驱动应用程序无法实现的强大灵活性。ComfyUI 节点图不受传统计算机应用程序提供的工具的限制。它是一个高级的 _**可视化编程环境**_，允许用户设计复杂的系统，而无需编写程序代码或理解高级数学。 许多其他计算机应用程序也使用相同的节点图范式。示例包括合成应用程序 Nuke、3D 程序 Maya 和 Blender、实时图形引擎 Unreal，以及交互媒体创作程序 Max。\n\n### 更复杂的示例\n\n![复杂工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/complex_workflow.jpeg)\n\n## 过程框架\n\n另一个用于描述基于节点的应用程序的术语是 _**过程框架**_。过程意味着生成：某种过程或算法被用来生成内容，例如 3D 模型或音乐作品。 ComfyUI 是所有这些东西：一个节点图、一个可视化编程环境和一个过程框架。使 ComfyUI 不同（并且令人惊叹！）的是，它的开放结构允许我们生成任何类型的媒体资产，例如图片、电影、声音、3D 模型、AI 模型等。 在 ComfyUI 的上下文中，_**工作流**_ 这个术语是节点网络或图的同义词。它对应于 3D 或多媒体程序中的 _**场景图**_：特定磁盘文件中所有节点的网络。3D 程序称之为 _**场景文件**_。视频编辑、合成和多媒体程序通常称之为 _**项目文件**_。\n\n## 保存工作流\n\nComfyUI 工作流会自动保存在任何生成图像的元数据中，允许用户打开并使用生成图像的图形。工作流也可以存储在遵循 JSON 数据格式的人类可读文本文件中。这对于不支持元数据的媒体格式是必要的。以 JSON 文件格式存储的 ComfyUI 工作流非常小，便于版本控制、归档和共享图形，而不依赖于任何生成的媒体。"
},
{
  "url": "https://docs.comfy.org/zh-CN/changelog",
  "markdown": "# 更新日志 - ComfyUI\n\n**高级采样与训练基础设施改进**本版本为AI研究人员和工作流程创建者引入了采样算法、训练功能和节点功能的重大增强：\n\n## 新的采样和生成功能\n\n*   **SA-Solver采样器**：新的重构SA-Solver采样算法，为复杂生成工作流提供增强的数值稳定性和质量\n*   **实验性CFGNorm节点**：高级无分类器引导标准化，用于改进生成质量和风格一致性的控制\n*   **嵌套双CFG支持**：为DualCFGGuider节点添加嵌套风格配置，提供更复杂的引导控制模式\n*   **SamplingPercentToSigma节点**：用于从采样百分比精确计算sigma的新实用节点，提高工作流程灵活性\n\n## 增强的训练功能\n\n*   **多图像-描述数据集支持**：LoRA训练节点现在可以同时处理多个图像-描述数据集，简化训练工作流程\n*   **更好的训练循环实现**：优化的训练算法，在模型微调过程中改善收敛性和稳定性\n*   **增强的错误检测**：为LoRA操作添加模型检测错误提示，在出现问题时提供更清晰的反馈\n\n## 平台和性能改进\n\n*   **异步节点支持**：完全支持异步节点函数，优化早期执行，改善I/O密集型操作的工作流程性能\n*   **Chroma灵活性**：在Chroma中取消硬编码的patch\\_size参数，允许更好地适应不同的模型配置\n*   **LTXV VAE解码器**：切换到改进的默认填充模式，提高LTXV模型的图像质量\n*   **Safetensors内存管理**：为mmap问题添加解决方案，提高加载大型模型文件时的可靠性\n\n## API和集成增强\n\n*   **自定义提示ID**：API现在允许指定提示ID，以便更好地跟踪和管理工作流程\n*   **Kling API优化**：增加轮询超时时间，防止视频生成工作流程中的用户超时\n*   **历史令牌清理**：从历史项目中删除敏感令牌以提高安全性\n*   **Python 3.9兼容性**：修复兼容性问题，确保更广泛的平台支持\n\n## 错误修复和稳定性\n\n*   **MaskComposite修复**：解决目标蒙版具有2个维度时的错误，提高蒙版工作流程可靠性\n*   **Fresca输入/输出**：修正Fresca模型工作流程的输入和输出处理\n*   **引用错误修复**：解决Gemini节点实现中的错误引用问题\n*   **行结束标准化**：自动检测和删除Windows行结束符，确保跨平台一致性\n\n## 开发者体验\n\n*   **警告系统**：添加torch导入错误警告，以捕获常见配置问题\n*   **模板更新**：多个模板版本更新（0.1.36、0.1.37、0.1.39），改进自定义节点开发\n*   **文档**：增强便携式配置中fast\\_fp16\\_accumulation的文档\n\n这些改进使ComfyUI在生产工作流程中更加稳健，同时引入了对高级AI研究和创意应用必不可少的强大新采样技术和训练功能。\n\n**高级采样和模型控制增强**此版本在采样算法和模型控制系统方面提供了重大改进，特别有利于高级AI研究人员和工作流创建者：\n\n## 新采样功能\n\n*   **TCFG节点**：增强的分类器无关引导控制，为您的工作流提供更细致的生成控制\n*   **ER-SDE采样器**：从VE迁移到VP算法，配备新的采样器节点，为复杂生成任务提供更好的数值稳定性\n*   **跳层引导（SLG）**：用于推理期间精确层级控制的替代实现，完美适用于高级模型导向工作流\n\n## 增强的开发工具\n\n*   **自定义节点管理**：新的`--whitelist-custom-nodes`参数与`--disable-all-custom-nodes`配对，提供精确的开发控制\n*   **性能优化**：双CFG节点现在在CFG为1.0时自动优化，减少计算开销\n*   **GitHub Actions集成**：自动化发布webhook通知让开发者及时了解新更新\n\n## 图像处理改进\n\n*   **新变换节点**：添加了ImageRotate和ImageFlip节点，增强图像操作工作流\n*   **ImageColorToMask修复**：修正了掩码值返回，提供更准确的基于颜色的掩码操作\n*   **3D模型支持**：上传3D模型到自定义子文件夹，为复杂项目提供更好的组织\n\n## 引导和条件增强\n\n*   **PerpNeg引导器**：更新了改进的前后CFG处理以及性能优化\n*   **潜在条件修复**：解决了多步骤工作流中索引 > 0 的条件问题\n*   **去噪步骤**：为多个采样器添加去噪步骤支持，获得更清洁的输出\n\n## 平台稳定性\n\n*   **PyTorch兼容性**：修复了PyTorch nightly构建的连续内存问题\n*   **FP8回退**：当FP8操作遇到异常时自动回退到常规操作\n*   **音频处理**：移除了已弃用的torchaudio.save函数依赖并修复警告\n\n## 模型集成\n\n*   **Moonvalley节点**：为Moonvalley模型工作流添加原生支持\n*   **调度器重新排序**：简单调度器现在默认优先，提供更好的用户体验\n*   **模板更新**：多个模板版本更新（0.1.31-0.1.35），改进自定义节点开发\n\n## 安全性和安全保护\n\n*   **安全加载**：在不安全加载文件时添加警告，文档说明检查点文件默认安全加载\n*   **文件验证**：增强检查点加载安全措施，确保工作流安全执行\n\n这些改进使ComfyUI在生产工作流中更加稳健，同时为使用高级采样技术和模型控制系统的AI艺术家扩展了创作可能性。\n\n**增强模型支持与工作流可靠性**本次发布在模型兼容性和工作流稳定性方面带来了重大改进：\n\n*   **扩展模型文档**：为 Flux Kontext 和 Omnigen 2 模型添加了全面的支持文档，让创作者更容易将这些强大的模型集成到他们的工作流中\n*   **VAE 编码改进**：移除了 VAE 编码过程中不必要的随机噪声注入，使工作流运行的输出更加一致和可预测\n*   **内存管理修复**：解决了专门影响 Kontext 模型使用的关键内存估算错误，防止内存不足错误并提高工作流稳定性\n\n这些变更提升了高级模型工作流的可靠性，同时保持了 ComfyUI 为从事前沿生成模型工作的 AI 艺术家和研究人员提供的灵活性。\n\n**主要模型支持新增**\n\n*   **Cosmos Predict2 支持**：全面实现文本到图像（2B 和 14B 模型）和图像到视频生成工作流，扩展视频创作功能\n*   **增强的 Flux 兼容性**：Chroma Text Encoder 现在能与常规 Flux 模型无缝协作，提升文本条件质量\n*   **LoRA 训练集成**：使用权重适配器方案的全新原生 LoRA 训练节点，支持在 ComfyUI 工作流中直接进行模型微调\n\n**性能和硬件优化**\n\n*   **AMD GPU 增强**：在 GFX1201 和其他兼容的 AMD GPU 上启用 FP8 操作和 PyTorch 注意力机制，加速推理\n*   **Apple Silicon 修复**：解决了 Apple 设备上长期存在的 FP16 注意力问题，提升 Mac 用户的稳定性\n*   **Flux 模型稳定性**：解决了特定 Flux 模型在 FP16 精度下生成黑色图像的问题\n\n**高级采样改进**\n\n*   **Rectified Flow (RF) 采样器**：新增支持 RF 的 SEEDS 和多步 DPM++ SDE 采样器，为前沿模型提供更多采样选项\n*   **ModelSamplingContinuousEDM**：新增 cosmos\\_rflow 选项，增强对 Cosmos 模型的采样控制\n*   **内存优化**：改进了支持无限分辨率的 Cosmos 模型的内存估算\n\n**开发者和集成功能**\n\n*   **SQLite 数据库支持**：增强自定义节点和工作流存储的数据管理功能\n*   **PyProject.toml 集成**：从 pyproject 文件自动注册 web 文件夹和配置设置\n*   **前端灵活性**：支持语义化版本后缀和预发布前端版本，适用于自定义部署\n*   **分词器增强**：通过 tokenizer\\_data 配置 min\\_length 设置，优化文本处理\n\n**使用体验改进**\n\n*   **Kontext 宽高比修复**：解决了仅限小组件的限制，现在在所有连接模式下都能正常工作\n*   **SaveLora 一致性**：统一所有保存节点的文件名格式，优化文件组织\n*   **Python 版本警告**：为过时的 Python 安装添加警报，防止兼容性问题\n*   **WebcamCapture 修复**：修正了 IS\\_CHANGED 签名，确保实时输入工作流的可靠性\n\n此版本显著扩展了 ComfyUI 的模型生态系统支持，同时提供了关键的稳定性改进和跨平台硬件兼容性增强。\n\n本次发布为 ComfyUI 创作者带来了强大的新工作流实用工具和性能优化：\n\n## 新的工作流工具\n\n*   **ImageStitch 节点**：在工作流中无缝拼接多个图像 - 非常适合创建对比网格或复合输出\n*   **GetImageSize 节点**：提取图像尺寸并支持批处理，对于动态调整大小的工作流至关重要\n*   **Regex Replace 节点**：高级文本处理功能，适用于提示词工程和字符串处理工作流\n\n## 增强的模型兼容性\n\n*   **改进的张量处理**：简化的列表处理使复杂的多模型工作流更加可靠\n*   **BFL API 优化**：完善了对 Kontext \\[pro\\] 和 \\[max\\] 模型的支持，提供更清晰的节点界面\n*   **性能提升**：在色度处理中使用融合乘加运算，加快生成速度\n\n## 开发者体验改进\n\n*   **自定义节点支持**：添加 pyproject.toml 支持，改善自定义节点依赖管理\n*   **帮助菜单集成**：在节点库侧边栏中新增帮助系统，加快节点发现速度\n*   **API 文档**：增强 API 节点文档，支持工作流自动化\n\n## 前端和 UI 增强\n\n*   **前端更新至 v1.21.7**：多项稳定性修复和性能改进\n*   **自定义 API 基础支持**：改进了自定义部署配置的子路径处理\n*   **安全加固**：修复 XSS 漏洞，确保工作流分享更安全\n\n## 错误修复和稳定性\n\n*   **Pillow 兼容性**：更新了已弃用的 API 调用，保持与最新图像处理库的兼容性\n*   **ROCm 支持**：改进了 AMD GPU 用户的版本检测\n*   **模板更新**：增强了自定义节点开发的项目模板\n\n这些更新强化了 ComfyUI 处理复杂 AI 工作流的基础，同时通过改进的文档和辅助工具让平台对新用户更加友好。"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/gpt-image-1",
  "markdown": "# OpenAI GPT-Image-1 节点 - ComfyUI\n\n![OpenAI GPT-Image-1 节点截图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-gpt-image-1.jpg) OpenAI GPT-Image-1 是 ComfyUI API 节点系列中的一员，它允许用户通过 OpenAI 的 **GPT-Image-1** 模型生成图像。这是与 ChatGPT 4o 图像生成相同的模型。 这个节点支持:\n\n*   文本到图像的生成\n*   图像编辑功能（通过蒙版进行修复绘制）\n\n## 节点概述\n\n**OpenAI GPT-Image-1** 节点通过 OpenAI 的图像生成 API 同步生成图像。它接收文本提示并返回符合描述的图像。GPT-Image-1 是目前 OpenAI 最先进的图像生成模型，能够创建高度详细和逼真的图像。\n\n## 参数说明\n\n### 必填参数\n\n| 参数名 | 类型  | 说明  |\n| --- | --- | --- |\n| `prompt` | 文本  | 描述您想要生成的图像内容的文本提示 |\n\n### Widget 参数\n\n| 参数名 | 类型  | 选项  | 默认值 | 说明  |\n| --- | --- | --- | --- | --- |\n| `seed` | 整数  | 0-2147483647 | 0   | 用于控制生成结果的随机种子 |\n| `quality` | 选项  | low, medium, high | low | 图像质量设置，影响成本和生成时间 |\n| `background` | 选项  | opaque, transparent | opaque | 返回的图像是否带有背景 |\n| `size` | 选项  | auto, 1024x1024, 1024x1536, 1536x1024 | auto | 生成图像的尺寸 |\n| `n` | 整数  | 1-8 | 1   | 生成的图像数量 |\n\n### 可选参数\n\n| 参数名 | 类型  | 选项  | 默认值 | 说明  |\n| --- | --- | --- | --- | --- |\n| `image` | 图像  | 任何图像输入 | 无   | 可选的参考图像，用于图像编辑 |\n| `mask` | 蒙版  | 蒙版输入 | 无   | 可选的蒙版，用于局部重绘（白色区域将被替换） |\n\n## 使用示例\n\n### 文生图像（Text to Image）示例\n\n下面的图片包含了一个简单的文生图像工作流，请下载对应的图像，并拖入 ComfyUI 以加载对应的工作流 ![ComfyUI openai-gpt-image-1工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/text2image.png) 对应的工作流非常简单： ![ComfyUI openai-gpt-image-1 工作流示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg) 你只需要加载 `OpenAI GPT-Image-1` 节点，在 `prompt` 节点中输入你想要生成的图像的描述，连接一个 `保存图像（Save Image）` 节点，然后运行工作流即可。\n\n### 图生图（Image to Image）示例\n\n下面的图片包含了一个简单的图生图工作流，请下载对应的图像，并拖入 ComfyUI 以加载对应的工作流 ![ComfyUI openai-gpt-image-1工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/image2image.png) 我们将使用下面的图片作为输入： ![ComfyUI openai-gpt-image-1 工作流 input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/input.webp) 这个工作流中，我们使用 `OpenAI GPT-Image-1` 节点生成图像，并使用 `加载图像（Load Image）` 节点加载输入的图像，然后连接到 `OpenAI GPT-Image-1` 节点的 `image` 输入中。 ![ComfyUI openai-gpt-image-1 工作流示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg)\n\n### 多张图片输入示例\n\n请下载下面的图片并拖入 ComfyUI 来加载对应的工作流 ![多张图片输入示例](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/multiple_image_input.png) 使用下面的帽子作为额外的输入图片 ![帽子](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/hat.webp) 对应工作流如下图所示： ![多张图片输入示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png) 使用了`Batch Images` 节点来将多张图像加载到 `OpenAI GPT-Image-1` 节点 中\n\n### 局部重绘（Inpainting）工作流\n\nGPT-Image-1 也支持图像编辑功能，允许您使用蒙版指定要替换的区域，下面是一个简单的局部重绘工作流示例： 下载下面的图片，并拖入 ComfyUI 以加载对应的工作流，我们将继续使用 图生图工作流部分的输入图片。 ![ComfyUI openai-gpt-image-1工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/inpaint.png) 对应工作流入图所示 ![ComfyUI openai-gpt-image-1 工作流示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg) 与图生图工作流相比，我们在`Load Image`中通过右键菜单使用 蒙版编辑器（MaskEditor） 并绘制蒙版，然后连接到 `OpenAI GPT-Image-1` 节点的 `mask` 输入中，来完成对应工作流。 **注意事项**\n\n*   蒙版和图像必须大小相同\n*   当输入大尺寸图片时，节点会自动将图像缩小到合适的尺寸\n\n## 常见问题"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/dall-e-2",
  "markdown": "# OpenAI DALL·E 2 节点 - ComfyUI\n\n![OpenAI DALL·E 2 节点截图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-dall-e-2.jpg) OpenAI DALL·E 2 是 ComfyUI API 节点系列中的一员，它允许用户通过 OpenAI 的 **DALL·E 2** 模型生成图像。 这个节点支持:\n\n*   文本到图像的生成\n*   图像编辑功能（通过蒙版进行修复绘制）\n\n## 节点概述\n\n**OpenAI DALL·E 2** 节点通过 OpenAI 的图像生成 API 同步生成图像。它接收文本提示并返回符合描述的图像。\n\n## 参数说明\n\n### 必填参数\n\n| 参数名 | 说明  |\n| --- | --- |\n| `prompt` | 文本提示，描述你想要生成的图像内容 |\n\n### Widget 参数\n\n| 参数名 | 说明  | 选项/范围 | 默认值 |\n| --- | --- | --- | --- |\n| `seed` | 生成图像的种子值（目前在后端未实现） | 0 到 2^31-1 | 0   |\n| `size` | 输出图像的尺寸 | ”256x256”, “512x512”, “1024x1024\" | \"1024x1024” |\n| `n` | 生成的图像数量 | 1 到 8 | 1   |\n\n### 可选参数\n\n| 参数名 | 说明  | 选项/范围 | 默认值 |\n| --- | --- | --- | --- |\n| `image` | 可选的参考图像，用于图像编辑 | 任何图像输入 | 无   |\n| `mask` | 可选的蒙版，用于局部重绘 | 蒙版输入 | 无   |\n\n## 使用方法\n\n## 工作流示例\n\n目前该API 节点支持两种工作流，分别是：\n\n*   文生图像（Text to Image）\n*   局部重绘（Inpainting）\n\n### 文生图像（Text to Image）示例\n\n下面的图片包含了一个简单的文生图像工作流，请下载对应的图像，并拖入 ComfyUI 以加载对应的工作流 ![ComfyUI openai-dall-e-2工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/text2image.png) 对应的示例非常简单 ![ComfyUI openai-dall-e-2 工作流示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg) 你只需要在加载 `OpenAI DALL·E 2` 节点后，在 `prompt` 节点中输入你想要生成的图像的描述，并连接一个 `保存图像（Save Image）` 节点，然后运行工作流即可\n\n### 局部重绘（Inpainting）工作流\n\nDALL·E 2 支持图像编辑功能，允许您使用蒙版指定要替换的区域，下面是一个简单的局部重绘工作流示例：\n\n#### 1\\. 工作流文件下载\n\n下载下面的图片，并拖入 ComfyUI 以加载对应的工作流 ![ComfyUI openai-dall-e-2工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/inpainting.png) 我们将使用下面的图片作为输入： ![ComfyUI openai-dall-e-2 工作流 input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/input.jpg) \n\n#### 2\\. 工作流文件使用说明\n\n![ComfyUI openai-dall-e-2 工作流示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg) 由于此工作流较为简单，如果你想要自己手动实现对应的工作流，可以按照下面的步骤完成对应的工作流\n\n1.  使用`加载图像（Load Image）`节点加载图像\n2.  在加载图像节点中右键，选择 `遮罩编辑器（MaskEditor）`\n3.  在遮罩编辑器中，使用画笔绘制你想要重绘的区域\n4.  在**OpenAI DALL·E 2** 节点 `image` 输入中连接加载的图像\n5.  **OpenAI DALL·E 2** 节点 `mask` 输入中连接蒙版\n6.  编辑 `prompt` 节点的提示词\n7.  运行工作流\n\n**注意事项**\n\n*   如果您想使用图像编辑功能，必须同时提供图像和蒙版（缺一不可）\n*   蒙版和图像必须大小相同\n*   当输入大尺寸图片时，节点会自动将图像缩小到合适的尺寸\n*   API 返回的 URL 是短期有效的，请确保及时保存需要的结果\n*   每次生成都会消耗积分，根据图像大小和数量收费\n\n## 常见问题"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/more_on_inputs",
  "markdown": "# 隐藏与灵活输入 - ComfyUI\n\n## 隐藏输入\n\n除了在客户端创建对应输入或小部件的 `required`（必需）和 `optional`（可选）输入外，还有三种 `hidden`（隐藏）输入选项，允许自定义节点从服务器请求特定信息。 这些选项通过在 `INPUT_TYPES` 的 `dict` 中返回 `hidden` 字段来访问，其签名为 `dict[str,str]`，可包含 `PROMPT`、`EXTRA_PNGINFO` 或 `UNIQUE_ID` 中的一个或多个。\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": {...},\n        \"optional\": {...},\n        \"hidden\": {\n            \"unique_id\": \"UNIQUE_ID\",\n            \"prompt\": \"PROMPT\", \n            \"extra_pnginfo\": \"EXTRA_PNGINFO\",\n        }\n    }\n```\n\n### UNIQUE\\_ID\n\n`UNIQUE_ID` 是节点的唯一标识符，与客户端节点的 `id` 属性相同。它通常用于客户端与服务器的通信（参见 [消息](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_messages#%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E8%8A%82%E7%82%B9-id-node-id)）。\n\n### PROMPT\n\n`PROMPT` 是客户端发送到服务器的完整提示（prompt）。详见 [prompt 对象](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_objects_and_hijacking#prompt)。\n\n`EXTRA_PNGINFO` 是一个字典，会被复制到任何保存的 `.png` 文件的元数据中。自定义节点可以将额外信息存储在该字典中用于保存（或作为与下游节点通信的一种方式）。\n\n### DYNPROMPT\n\n`DYNPROMPT` 是 `comfy_execution.graph.DynamicPrompt` 的一个实例。它与 `PROMPT` 不同，`DYNPROMPT` 可能会在执行过程中因 [节点扩展](https://docs.comfy.org/zh-CN/custom-nodes/backend/expansion) 而发生变化。\n\n## 灵活输入\n\n### 自定义数据类型\n\n如果你希望在自定义节点之间传递数据，定义自定义数据类型会很有帮助。这几乎只需要为数据类型选择一个唯一的大写字符串名称，例如 `CHEESE`。 然后你可以在节点的 `INPUT_TYPES` 和 `RETURN_TYPES` 中使用 `CHEESE`，Comfy 客户端只允许 `CHEESE` 输出连接到 `CHEESE` 输入。`CHEESE` 可以是任意 Python 对象。 需要注意的一点是，由于 Comfy 客户端并不了解 `CHEESE`，你需要（除非为 `CHEESE` 定义了自定义小部件，这属于进阶话题）强制它作为输入而不是小部件。这可以通过输入选项字典中的 `forceInput` 选项实现：\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": { \"my_cheese\": (\"CHEESE\", {\"forceInput\":True}) }\n    }\n```\n\n### 通配输入\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": { \"anything\": (\"*\",{})},\n    }\n\n@classmethod\ndef VALIDATE_INPUTS(s, input_types):\n    return True\n```\n\n前端允许使用 `*` 表示该输入可以连接到任意来源。由于后端并未正式支持此功能，你可以通过在 `VALIDATE_INPUTS` 函数中接受名为 `input_types` 的参数来跳过类型校验。（详见 [VALIDATE\\_INPUTS](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#validate-inputs) 了解更多信息。） 节点需要自行处理传入的数据。\n\n### 动态创建的输入\n\n如果输入是在客户端动态创建的，则无法在 Python 源码中定义。为了访问这些数据，我们需要一个 `optional` 字典，允许 Comfy 传递任意名称的数据。由于 Comfy 服务器\n\n```\nclass ContainsAnyDict(dict):\n    def __contains__(self, key):\n        return True\n...\n\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": {},\n        \"optional\": ContainsAnyDict()\n    }\n...\n\ndef main_method(self, **kwargs):\n    # 动态创建的输入数据会在 kwargs 字典中\n\n```\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/properties",
  "markdown": "# 属性 - ComfyUI\n\n## 节点是属性的容器\n\n节点通常具有 _**属性**_。也称为 _**参数**_ 或 _**特性**_，节点属性是可以更改的变量。一些属性可以通过用户手动调整，使用称为 _**小部件**_ 的数据输入字段。其他属性可以通过连接到属性 _**输入插槽**_ 或端口的其他节点自动驱动。通常，属性可以在小部件和输入之间转换，从而允许用户手动或自动控制属性值。 属性可以采取多种形式，并包含多种不同类型的信息。例如，**加载检查点** 节点具有一个属性：生成模型检查点文件的文件路径。**KSampler** 节点具有多个属性，例如采样 **步骤**、**CFG** 比例、**采样器名称** 等等。 ![节点属性](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_properties.png)\n\n## 数据类型\n\n信息可以以多种不同形式出现，称为 _**数据类型**_。例如，字母数字文本称为 _**字符串**_，整数称为 _**整数**_，带小数点的数字称为 _**浮点数**_ 或 _**浮点**_。新的数据类型总是被添加到 ComfyUI 中。 ComfyUI 是用 Python 脚本语言编写的，该语言对数据类型非常宽容。相比之下，ComfyUI 环境是非常 _**强类型**_ 的。这意味着不同的数据类型不能混合。例如，我们不能将图像输出连接到整数输入。这对用户来说是一个巨大的好处，指导他们正确构建工作流程并防止程序错误。"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/rodin/model-generation",
  "markdown": "# Rodin API 节点模型生成 ComfyUI 官方示例\n\nHyper3D Rodin (hyper3d.ai) 是一个专注于通过人工智能快速生成高质量、可直接用于生产环境的3D模型和材质的平台。 目前 ComfyUI 已原生集成了对应 Rodin 模型生成 API ，现在你可以在 ComfyUI 中便捷地使用相关节点来进行模型生成 目前 ComfyUI 的 API 节点中已经支持 Rodin 以下模型生成能力：\n\n*   单视角模型生成\n*   多视角模型生成\n*   多种不同精度的模型生成\n\n## 单视角模型生成工作流\n\n### 1\\. 工作流文件下载\n\n下载下面的文件，并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/rodin_image_to_model.json)\n\n下载下面的图片作为输入图片 ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/doll.jpg)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Rodin Image to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Load Image` 节点中加载提供的输入图片\n2.  (可选)在 `Rodin 3D Generate - Regular Generate` 调整对应参数\n    *   polygon\\_count: 可以设置不同的面数, 越大模型越平滑越精细\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行模型的生成，工作流完成后对应的模型会自动保存至 `ComfyUI/output/Rodin` 目录下\n4.  在 `Preview 3D` 节点中点击展开菜单\n5.  选择`Export` 可以直接将对应模型导出\n\n## 多视角模型生成工作流\n\n对应的 `Rodin 3D Generate - Regular Generate` 最多允许5张图像输入\n\n### 1\\. 工作流文件下载\n\n你可以将单视角部分的工作流修改为多视角工作流，或者直接下载下面的工作流文件 下载下面的文件，并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/api_rodin_multiview_to_model.json)\n\n下载下面的图片作为输入图片 ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/front.jpg) ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/back.jpg) ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/left.jpg)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Rodin Image to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Load Image` 节点中加载提供的输入图片\n2.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行模型的生成，工作流完成后对应的模型会自动保存至 `ComfyUI/output/Rodin` 目录下\n3.  在 `Preview 3D` 节点中点击展开菜单\n4.  选择`Export` 可以直接将对应模型导出\n\n## 其它相关节点\n\n目前在 ComfyUI 中， Rodin 提供了不同类型的模型生成节点，由于对应输入条件与本文介绍的工作流相同，你可以按需启用，另外在对应模板中，我们提供了对应的节点，你也可以按需修改对应节点模式来启用 ![Rodin 其它相关节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/other_nodes.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/runway/video-generation",
  "markdown": "# Runway API 节点 视频生成 ComfyUI 官方示例\n\nRunway 是一家专注于生成式 AI 的科技公司，提供强大的视频生成功能。目前 ComfyUI 已集成 Runway API，你可以直接在 ComfyUI 中使用相关节点进行视频生成。 目前 ComfyUI 中原生集成了 Runway 的以下视频生成模型：\n\n*   Runway Gen3a turbo\n*   Runway Gen4 turbo\n*   Runway First Last Frame to video\n\n## Gen3a turbo 图生视频工作流\n\n### 1\\. 工作流文件下载\n\n下面的视频的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/runway_image_to_video_gen3a_turbo.json)\n\n下载下面的图片作为输入图片 ![ComfyUI Runway gen3a turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/steampunk.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Runway gen3a turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Load Image` 节点中加载提供的输入图片\n2.  在 `Runway Gen3a turbo` 节点中设置 `prompt` 描述视频内容，修改 `duration` 参数来设置视频时长, 修改 `ratio` 参数来设置视频宽高比\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频的生成。\n4.  等待 API 返回结果后，你可在 `Save Video` 节点中查看生成的视频（右键菜单可以保存），对应的视频也会被保存至 `ComfyUI/output/` 目录下。\n\n## Gen4 turbo 图生视频工作流\n\n### 1\\. 工作流文件下载\n\n下面的视频的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/runway_gen4_turo_image_to_video.json)\n\n下载下面的图片作为输入图片 ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/godfather.jpg)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Runway gen4 turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Load Image` 节点中加载提供的输入图片\n2.  在 `Runway Gen4 turbo` 节点中设置 `prompt` 描述视频内容，修改 `duration` 参数来设置视频时长, 修改 `ratio` 参数来设置视频宽高比\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频的生成。\n4.  等待 API 返回结果后，你可在 `Save Video` 节点中查看生成的视频（右键菜单可以保存），对应的视频也会被保存至 `ComfyUI/output/` 目录下。\n\n## 首尾帧视频生成工作流\n\n### 1\\. 工作流文件下载\n\n下面的视频的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/runway_first_last_frame.json)\n\n下载下面的图片作为输入图片 ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/first.jpg) ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/last.jpg)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Runway gen4 turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Load Image` 节点中加载起始帧\n2.  在 `Load Image` 节点中加载结束帧\n3.  在 `Runway First-Last-Frame to Video` 节点中设置 `prompt` 描述视频内容，修改 `duration` 参数来设置视频时长, 修改 `ratio` 参数来设置视频宽高比\n4.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频的生成。\n5.  等待 API 返回结果后，你可在 `Save Video` 节点中查看生成的视频（右键菜单可以保存），对应的视频也会被保存至 `ComfyUI/output/` 目录下。"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/models",
  "markdown": "# 模型 - ComfyUI\n\n## 模型是必不可少的\n\n模型是媒体生成工作流程的核心组件。通过组合和混合，它们能够实现多样化的创意效果。 _**模型**_ 一词有多种含义。在这里，它指的是一种数据文件，包含节点图执行任务所需的信息。具体来说，它是一种数据结构，用于表示某种功能。作为动词，建模意味着对某物进行表示或提供示例。 在ComfyUI中，模型数据文件的典型示例是AI _**扩散模型**_。它是一组庞大的数据集，表示文本与图像之间的复杂关系，从而实现文字与图片的相互转换。其他用于图像生成的常见模型示例包括多模态视觉和语言模型，如CLIP，以及图像放大模型，如RealESRGAN。\n\n## 模型文件\n\n模型文件是生成媒体制作的必需品。没有模型文件，工作流程将无法进行。ComfyUI安装包中不包含模型文件，但它通常可以自动下载并安装缺失的模型文件。许多模型可以通过**ComfyUI管理器**窗口下载和安装。模型还可以在以下网站获取：[huggingface.co](https://huggingface.co/)、[civitai.green](https://civitai.green/)和[github.com](https://github.com/)。\n\n### 在ComfyUI中使用模型\n\n1.  下载并将其放置在ComfyUI程序目录中\n    1.  在**models**文件夹中，您会找到各种类型模型的子文件夹，例如**checkpoints**\n    2.  **ComfyUI管理器**帮助自动化搜索、下载和安装的过程\n    3.  如果ComfyUI正在运行，请重启它\n2.  在您的工作流程中，创建适合模型类型的节点，例如**Load Checkpoints**、**Load LoRA**、**Load VAE**\n3.  在加载节点中，选择您希望使用的模型\n4.  将加载节点连接到工作流程中的其他节点\n\n## 添加外部模型路径\n\n如果你想要在 `ComfyUI/models` 之外管理你的模型文件，可能出于以下原因:\n\n*   你有多个 ComfyUI 实例，你想要让这些实例共享模型文件，从而减少磁盘占用\n*   你有多个不同的类型的 GUI 程序，如：WebUI, 你想要他们共用模型文件\n*   模型文件无法被识别或读取到\n\n我们提供了通过 `extra_model_paths.yaml` 配置文件来添加额外模型搜索路径的方法。\n\n### 不同 ComfyUI 版本配置文件位置\n\n对于[便携版](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)和[手动安装](https://docs.comfy.org/zh-CN/installation/manual_install)的 ComfyUI版本，你可以在 ComfyUI 的根目录下找到 `extra_model_paths.yaml.example` 的示例文件\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\n复制并重命名为 `extra_model_paths.yaml` 来使用, 并保持在 ComfyUI 的根目录下, 路径应该是 `ComfyUI/extra_model_paths.yaml`你也可以在 [这里](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 找到配置示例文件\n\n### 配置示例\n\n比如，你需要额外让 ComfyUI 识别的模型文件位于下面的文件夹:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 loras/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\n那么你可以进行如下的配置来让 ComfyUI 识别到你设备上的模型路径\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\n或者使用\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\n或者你也可以参考默认的 [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 来配置，保存之后， 需要 **重启 ComfyUI** 才能生效。 下面是完整的原始的配置配置示例:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### 添加额外自定义节点路径\n\n除了添加外部模型之外，你同样可以添加不在 ComfyUI 默认路径下的自定义节点路径\n\n下面是一个简单的配置示例（Mac 系统），请根据你的实际情况进行修改，并新增到对应的配置文件中，保存后需要 **重启 ComfyUI** 才能生效:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n### 文件大小\n\n相对于图像文件，模型文件可能非常大。一个典型的未压缩图像可能需要几MB的磁盘存储。生成的AI模型可能大到数万倍，单个模型可达数十GB。它们占用大量磁盘空间，并且在网络上传输需要很长时间。\n\n## 模型训练和优化\n\n生成式AI模型是通过在非常大的数据集上训练机器学习程序创建的，例如图像和文本描述的配对。AI模型并不明确存储训练数据，而是存储数据中隐含的相关性。 像Stability AI和Black Forest Labs这样的组织和公司发布“基础”模型，这些模型携带大量通用信息。这些是通用的生成AI模型。通常，基础模型需要进行_**优化**_，以获得高质量的生成输出。一个专门的社区致力于优化基础模型。新的优化模型产生更好的输出，提供新的或不同的功能，并/或使用更少的资源。优化后的模型通常可以在计算能力和/或内存较少的系统上运行。\n\n## 辅助模型\n\n模型功能可以通过辅助模型进行扩展。例如，艺术指导文本到图像的工作流程以实现特定结果，单靠扩散模型可能会很困难或不可能。额外的模型可以在工作流程图中优化扩散模型，以产生所需的结果。示例包括**LoRA**（低秩适应），一个针对特定主题训练的小模型；**ControlNet**，一个使用引导图像帮助控制构图的模型；以及**Inpainting**，一个允许某些扩散模型在现有图像中生成新内容的模型。 ![辅助模型](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_auxiliary-model.png)"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/dependencies",
  "markdown": "# 依赖关系 - ComfyUI\n\n## 工作流文件依赖于其他文件\n\n我们经常可以从社区里获取到各种各样的工作流文件，但往往在运行之后发现工作流无法直接运行，这是因为一个工作流文件除了工作流自身之外还要依赖于其它文件，比如媒体资产的输入、模型、自定义节点、相关的 Python 依赖等等。 ComfyUI 的工作流只有在所有相关依赖条件都被满足的情况下才能正常运行。 ComfyUI 工作流的运行依赖主要分为以下几类：\n\n*   资产（媒体文件包括音频、视频、图像等等输入）\n*   自定义节点\n*   Python 依赖\n*   模型（如：Stable Diffusion 模型等）\n\n## 资产\n\nAI 模型是一个 _**资产**_ 的例子。在媒体制作中，资产是提供输入数据的某种媒体文件。例如，视频编辑程序处理存储在磁盘上的电影文件。编辑程序的项目文件保存了这些电影文件资产的链接，允许非破坏性编辑，而不改变原始电影文件。 ComfyUI 的工作方式也是如此。工作流只有在找到并加载所有必需的资产时才能运行。生成性 AI 模型、图像、电影和声音是工作流可能依赖的一些资产示例。因此，这些被称为 _**依赖资产**_ 或 _**资产依赖关系**_。\n\n## 自定义节点\n\n自定义节点是 ComfyUI 的一个重要组成部分，\n\n## Python 依赖\n\nComfyUI 是一个基于 Python 的项目，我们构建了一个独立的 Python 运行环境，来运行 ComfyUI，所有的相关依赖都会被安装在在这个独立的 Python 运行环境中。\n\n### ComfyUI 的依赖\n\n你可以在 ComfyUI 的 [requirements.txt](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) 文件中查看 ComfyUI 当前的依赖\n\n```\ncomfyui-frontend-package==1.14.5\ntorch\ntorchsde\ntorchvision\ntorchaudio\nnumpy>=1.25.0\neinops\ntransformers>=4.28.1\ntokenizers>=0.13.3\nsentencepiece\nsafetensors>=0.4.2\naiohttp>=3.11.8\nyarl>=1.18.0\npyyaml\nPillow\nscipy\ntqdm\npsutil\n\n#non essential dependencies:\nkornia>=0.7.1\nspandrel\nsoundfile\nav\n```\n\n随着 ComfyUI 的发展，我们可能也会调整相应的依赖，比如添加新的依赖，或者删除一些不再需要的依赖。 所以如果你是使用 Git 来更新 ComfyUI 那么你需要在拉取最新的更新之后在对应的环境下使用\n\n```\npip install -r requirements.txt\n```\n\n从而来安装 ComfyUI 最新的依赖以保证 ComfyUI 的正常运行，你也可以通过修改特定包的依赖版本来实现部分依赖的升级或者降级 另外 ComfyUI 的前端 [ComfyUI\\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend) 目前是作为一个独立的项目来进行维护，我们会在对应版本稳定之后更新对应的 `comfyui-frontend-package` 依赖版本，如果你需要切换对应的前端版本，你可以在[这里](https://pypi.org/project/comfyui-frontend-package/#history)查看对应的版本信息。\n\n### 自定义节点的依赖\n\n感谢 ComfyUI 社区众多作者的努力，使得我们可以通过使用不同的自定义节点（Custom Nodes）来扩展 ComfyUI 的功能，实现令人赞叹的创意。 通常，每个自定义节点都会有一个独立的依赖，并且每个自定义节点都会有一个独立的 `requirements.txt` 文件。 如果你使用 [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) 来安装自定义节点，那么通常 ComfyUI Manager 会自动帮你安装对应的依赖。 当然也有一些需要你手动进行依赖安装的情况，目前所有的自定义节点都将被安装至 `ComfyUI/custom_nodes` 目录下， 你需要在你的 ComfyUI Python 环境中进入到对应插件的目录然后执行 `pip install -r requirements.txt` 来安装对应的依赖。 如果是 [Windows 便携版](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)，你可以在便携版的`ComfyUI_windows_portable`目录下使用\n\n```\npython_embeded\\python.exe -m pip install -r ComfyUI\\custom_nodes\\<custom_node_name>\\requirements.txt\n```\n\n来安装对应节点的依赖。\n\n### 依赖冲突\n\n依赖冲突是我们在使用 ComfyUI 的时候经常会遇到的问题，你可能会发现在安装或者更新好某个自定义节点后，之前安装的一些自定义节点在 ComfyUI 的节点库中查找不到了，或者出现了错误弹窗，其中一个可能的原因就是依赖冲突。 依赖冲突的原因可能有很多，比如：\n\n1.  自定义节点的版本锁定\n\n部分插件在开发时会固定依赖库的精确版本（如`open_clip_torch==2.26.1`），而其他插件可能要求更高版本（如`open_clip_torch>=2.29.0`），导致版本无法同时满足。 **解决方法**：你可以试着把对应的固定版本依赖改为范围约束比如 `open_clip_torch>=2.26.1`，然后重新执行依赖的安装来解决这些问题。\n\n2.  环境污染\n\n在执行自定义节点依赖安装的过程中，可能会覆盖其它插件已经安装的库的版本，比如多个插件依赖 `PyTorch` 但要求的是不同的 CUDA 版本，后安装的插件会破坏原有的环境。 **解决方法**：\n\n*   你可以试着手动在 python 虚拟环境中手动安装特定版本的依赖，来解决这类问题。\n*   或者为不同的插件创建不同的 python 虚拟环境，来解决这类问题。\n*   试着逐个安装插件，在安装完每个插件后，重新启动 ComfyUI 来观察是否会出现依赖冲突。\n\n3.  自定义节点依赖版本与 ComfyUI 依赖版本不兼容\n\n这类依赖冲突的问题可能较难解决，你可能需要通过升降级 ComfyUI 或者更改自定义节点的依赖版本，来解决这类问题。 **解决方法**：这类依赖冲突的问题可能较难解决，你可能需要通过升降级 ComfyUI 或者更改自定义节点的依赖版本，来解决这类问题。\n\n## 模型\n\n模型是 ComfyUI 的一个重要资产依赖，基本上各类的自定义节点和工作流都围绕着特定的模型展开，比如 stable diffusion 系列、Flux 系列、Ltxv 等等。 这些模型是我们使用 ComfyUI 进行创作的重要基础，所以我们在使用 ComfyUI 的时候需要确保我们使用的模型是正常可用的，通常，我们对应的模型都在 `ComfyUI/models/` 目录的对应目录进行保存，当然你也可以通过修改模板创建一个 [extra\\_model\\_paths.yaml](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 来使得额外的模型路径被 ComfyUI 识别。 这样就可以实现多个 ComfyUI 实例共享同一个模型库，从而减少磁盘的占用。"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/custom-nodes",
  "markdown": "# 自定义节点 - ComfyUI\n\n## 关于自定义节点\n\n当你在安装了 ComfyUI 后，你将会发现 ComfyUI 中已经包含了许多节点，这些原生的节点称为 **Comfy Core** 节点，这些节点都是由 ComfyUI 官方维护的。 此外还有许多来自 ComfyUI 社区的众多作者带来的各种各样的 [**自定义节点**](https://registry.comfy.org/)，这些自定义节点为 ComfyUI 带来了诸多的扩展功能，大大扩展了 ComfyUI 的功能和能力边界。 在本指南中，我们将介绍如何自定义节点相关的一些操作，包括安装、更新、禁用、卸载、依赖安装等。 每个人都可以开发自己的自定义的扩展功能到 ComfyUI 中，并分享给其他人使用，你可以在[这里](https://registry.comfy.org/)找到许多来自社区的自定义节点，如果你想要开发自己的自定义节点请访问下面的部分开始：\n\n[](https://docs.comfy.org/zh-CN/custom-nodes/overview)\n\n## 自定义节点管理\n\n在这个部分我们将讲解：\n\n*   安装自定义节点\n*   安装节点依赖\n*   自定义节点版本控制\n*   卸载自定义节点\n*   临时禁用自定义节点\n*   处理自定义节点依赖冲突\n\n### 1\\. 安装自定义节点\n\n目前 ComfyUI 支持通过多种方式安装自定义节点，包括：\n\n*   \\[通过 ComfyUI Manager 安装（推荐）\\](#通过 ComfyUI Manager 安装)\n*   通过 Git 安装\n*   手动安装\n\n我们推荐通过 **ComfyUI Manager** 来安装自定义节点，这是一个在 ComfyUI 自定义节点生态中具有非常重要意义的一个工具，它使得自定义节点管理（如搜索、安装、更新、禁用和卸载）变得简单，你只需要在 ComfyUI Manager 中搜索你想要安装的节点，然后点击安装即可。 但由于目前所有的自定义节点都是存储在 GitHub 上，所以在本篇针对于某些无法正常访问 GitHub 的的地区，我们在本篇撰写了详尽的不同的自定义节点的安装方式。 另外由于我们推荐使用 **ComfyUI Manager** 进行对应的插件管理，我们推荐使用这一工具来进行插件的管理，你可以在[这里](https://github.com/Comfy-Org/ComfyUI-Manager)找到它的源码。 所以在本篇文档中，我们将会使用安装 ComfyUI Manager 作为自定义节点安装示例，并在本篇的相关介绍部分补充如何使用它来进行节点的管理。\n\n### 2\\. 安装节点依赖\n\n自定义节点都需要进行相关的依赖的安装，比如对于 ComfyUI-Manager 来说，你可以访问[requirements.txt](https://github.com/Comfy-Org/ComfyUI-Manager/blob/main/requirements.txt) 文件来查看对应的依赖包的要求, 在之前的步骤中，我们仅仅是把对应的自定义节点代码克隆到了本地，并没有安装对应的依赖，所以接下来我们需要安装对应的依赖。\n\n在关于[依赖关系](https://docs.comfy.org/zh-CN/development/core-concepts/dependencies)章节中，我们介绍了 ComfyUI 中依赖关系的相关内容，ComfyUI 是一个基于 **Python** 的项目，我们构建了一个用于运行 ComfyUI 的独立 **Python** 运行环境，所有的相关依赖都需要被安装在在这个独立的 **Python** 运行环境中。 如果你直接在系统级的终端运行 `pip install -r requirements.txt`，对应的依赖可能会被安装到了系统级的 **Python** 环境中，会导致对应的自定义节点在 ComfyUI 的环境中依赖还是丢失的，导致对应自定义节点无法正常运行。 所以接下来我们需要使用 ComfyUI 的独立 Python 运行环境来完成对应的依赖安装。 依据不同的 ComfyUI 版本我们将使用不同的方式来进行对应的依赖安装，\n\n对于 ComfyUI 便携版（Portable）来说，它使用的是一个嵌入式的 Python ，对应 Python 位于 `\\ComfyUI_windows_portable\\python_embeded` 目录下, 我们需要使用对应的 Python 来完成对应的依赖安装。首先先在便携版的目录下启动 terminal 或者启动 terminal 后使用 `cd` 命令进入到 `\\ComfyUI_windows_portable\\` 目录下![启动 terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/custom_nodes/open_terminal.jpg)确保对应终端的目录为 `\\ComfyUI_windows_portable\\` 目录下，如下图为 `D:\\ComfyUI_windows_portable\\`![终端](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/custom_nodes/terminal.jpg)然后使用 `python_embeded\\python.exe` 来完成对应的依赖安装。\n\n```\npython_embeded\\python.exe -m pip install -r ComfyUI\\custom_nodes\\ComfyUI-Manager\\requirements.txt\n```\n\n当然你可以把对应的 ComfyUI-Manager 替换为你实际安装的自定义节点名称，但需要确保对应节点目录下的确存在 `requirements.txt` 文件。\n\n### 自定义节点版本控制\n\n自定义节点的版本控制，实际上是基于 Git 的版本控制来进行的，你可以通过 Git 来进行对应的节点版本管理，但实际上在 ComfyUI Manager 中已经很好地集成了这一版本管理功能，非常感谢 [@Dr.Lt.Data](https://github.com/ltdrdata) 为我们带来如此便捷的工具。 在这个部分我们依旧会为你讲解这两种不同插件版本管理的方法，但如果你是使用 zip 压缩包进行手动安装的，那么对应的 git 版本历史信息会丢失，会导致你无法进行对应的版本管理。\n\n### 卸载自定义节点\n\n待更新\n\n### 临时禁用自定义节点\n\n待更新\n\n### 自定义节点依赖冲突\n\n待更新\n\n## ComfyUI Manager\n\n![ComfyUI 管理器界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png) 目前在 [Desktop 版本](https://docs.comfy.org/zh-CN/installation/desktop/windows) 中已默认包含该工具,而在[便携（Portable）版](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)中，你需要参考本文档中[安装管理器](#%E5%AE%89%E8%A3%85%E8%87%AA%E5%AE%9A%E4%B9%89%E8%8A%82%E7%82%B9)章节中的说明进行安装。\n\n### 安装管理器\n\n如果您正在运行 ComfyUI 服务器应用程序，则需要安装管理器。如果 ComfyUI 正在运行，请在继续之前将其关闭。 第一步是安装 Git，这是一个用于软件版本控制的命令行应用程序。Git 将从 [github.com](https://github.com/) 下载 ComfyUI 管理器。从 [git-scm.com](https://git-scm.com/) 下载并安装 Git。 安装 Git 后，导航到 ComfyUI 服务器程序目录，进入标记为 **custom\\_nodes** 的文件夹。打开命令窗口或终端。确保命令行显示当前目录路径为 **custom\\_nodes**。输入以下命令。这将下载管理器。从技术上讲，这被称为 _克隆 Git 仓库_。\n\n### 检测缺失的节点\n\n在安装管理器后，你可以在管理器中检测到缺失的节点。 ![ComfyUI 管理器界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png)\n\n## 开发一个自定义节点\n\n如果你具有一定的开发能力，请从下面的文档开始以了解如何开始开发一个自定义节点。\n\n[](https://docs.comfy.org/zh-CN/custom-nodes/overview)\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/nodes",
  "markdown": "# 节点 - ComfyUI\n\n在 ComfyUI 中，节点是我们执行任务的单元，他们是构建好的一个个独立的模块，无论是 **Comfy Core** 还是 **自定义节点** ，每个节点都是一个独立的模块，有着自己独特的功能，节点之间通过连线连接，我们可以像搭乐高积木一样搭建起来复杂的功能。 可以说，不同的节点组合构建出了 ComfyUI 的无限可能。 ![Comfy Core K-Sampler 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg) 例如在 K-Sampler 节点中，你可以看到它有多个输入和输出，也同时包含多个参数设置，这些参数决定了节点执行的逻辑，它的背后是对应编写好的 Python 逻辑，从而可以让你不用去接触代码就可以实现对应的功能。\n\n## 节点的的不同状态\n\n![节点状态](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/status.jpg) 在 ComfyUI 中，节点有多种状态，下面是一些常见的节点状态：\n\n1.  **正常(Normal)状态**： 正常状态\n2.  **运行(Running)状态**： 运行中状态，通常在你开始运行工作流后，正在执行的节点会显示这个状态\n3.  **错误(Error)状态**： 节点错误，通常在运行工作流后，如果对应的节点输入存在问题，导致了错误会显示这个状态，并用红色标识对应出错的输入节点，你需要解决对应出错的输入来保证工作流正常运行\n4.  **丢失(Missing)状态**： 这个状态通常在你导入了一些工作流后会出现，存在两种可能\n    *   ComfyCore 原生节点丢失： 这通常是因为 ComfyUI 的版本更新了，而你当前使用的 ComfyUI 版本较旧，你需要更新 ComfyUI 来解决这个问题\n    *   自定义节点丢失： 工作流中是用了第三方作者开发的自定义节点，而你的本地的 ComfyUI 版本没有安装对应的自定义节点，你可以使用 [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) 来查找丢失的自定义节点\n\n## 节点之间的连接\n\n在 ComfyUI 中，节点通过[连线](https://docs.comfy.org/zh-CN/development/core-concepts/links)连接，从而让相同的数据类型在不同的处理单元之间进行流转处理,从而获得最终的结果。 ![ComfyUI 节点连线](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/inpaint.jpg) 每个节点都会接收一些输入内容，然后经过模块处理将他们转换为对应的输出，不同的节点链接之间，必须符合数据类型规定的要求，在 ComfyUI 中，我们使用不同的颜色来区分节点的数据类型,下面是一些基础的数据类型。 ![ComfyUI 节点数据类型](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/data_type.jpg)\n\n| 数据类型 | 颜色  |\n| --- | --- |\n| 扩散模型 | 薰衣草色 |\n| CLIP 模型 | 黄色  |\n| VAE 模型 | 玫瑰色 |\n| 条件化 | 橙色  |\n| 潜在图像 | 粉色  |\n| 像素图像 | 蓝色  |\n| 蒙版  | 绿色  |\n| 数字 (整数或浮点数) | 浅绿色 |\n| 网格（Mesh） | 亮绿色 |\n\n随着 ComfyUI 的迭代，我们可能会拓展更多的数据类型，以符合更多场景的需求。\n\n### 节点之间的连接和取消连接\n\n![ComfyUI 节点连接](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/link_nodes.gif) **连接**： 在上一个节点的输出点中拖拽到下一个节点相同颜色的输入中，即可连接 **取消连接**： 在被输入的端点，点击后鼠标左键拖拽输入，即可取消连接，或者通过连线的中点菜单来取消连接。\n\n## 节点的外观\n\n![节点外观](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/node.jpg) 我们为提供了多种样式设置，你可以根据你的需求来设置节点的外观:\n\n*   修改样式\n*   双击节点标题修改节点名称\n*   通过上下文菜单将节点输入在 input 和 组件（widget）之间进行切换\n*   通过节点右下角来缩放节点尺寸\n\n### 节点标签 Badges\n\n![节点标签](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/badge.jpg) 我们提供了多个节点标签（Badges）的显示功能，比如：\n\n*   节点ID\n*   节点来源\n\n目前 **Comfy Core 节点** 采用小狐狸的图标来展示，自定义节点则采用其名称，这样你可以快速了解到对应节点是来自哪个节点包。 你可以在菜单中设置对应的显示： ![标签设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/badge_setting.jpg)\n\n## 节点上下文菜单\n\n节点的上下文菜单主要分为两种\n\n*   针对节点本身的上下文菜单\n*   针对输入 / 输出的上下文菜单\n\n### 节点的上下文菜单\n\n通过在节点上点击鼠标右键，你可以展开对应的节点上下文菜单，下面是对应的菜单截图： ![节点上下文菜单](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/context_menus_1.jpg) 在节点的右键上下文菜单中你可以\n\n*   调整节点的颜色样式\n*   修改标题\n*   克隆、复制、删除节点\n*   设置节点的模式\n\n在这个菜单中，除了外观相关的设置比较重要的是下面的菜单操作\n\n*   **模式（Mode）**： 设置节点的模式，Always、Never、绕过（Bypass）\n*   **切换节点输入的控件（Widget）和 输入模式**： 切换节点输入的控件（Widget）和 输入模式\n\n#### 模式（Mode）\n\n对于模式，你可能注意到目前我们提供了：Always、Never、On Event、On Trigger 四种模式，但实际上只有 **Always** 和 **Never** 是有效的，**On Event** 和 **On Trigger** 实际上是无效的，目前我们尚未完全实现这个功能，另外你可以把 **绕过（Bypass）** 也理解为一种模式，下面是对于几种可用模式的解释\n\n*   **Always**： 节点默认模式，当节点首次运行或者自上一次执行后，对应输入有变化对应节点都会执行\n*   **Never**： 节点在任何情况下都不会执行，就像节点被删除了，后续节点无法读取接收到任何来自它的数据\n*   **绕过（Bypass）**： 节点在任何情况下都不会执行，但是后续的节点仍旧可以试着获取到未经这个节点的处理的数据\n\n下面是对于 `Never` 和 `Bypass` 模式的对比： ![Never 和 Bypass 模式](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/never_vs_bypass.jpg) 在这个对比的例子中，你可以看到，两个工作流都是同时应用了两个 LoRA 模型，差异在于其中一个`Load LoRA` 节点被设置为 `Never` 模式而另一个被设置为`Bypass` 模式。\n\n*   被设置为 `Never` 模式的节点，后续的节点由于接收不到任何的输入数据而出现了报错\n*   被设置为 `Bypass` 模式的节点，后续的节点仍旧可以获取到未经这个节点处理的数据，从而加载了第一个`Load LoRA` 节点的输出数据，所以后续的工作流依旧可以正常运行\n\n#### 切换节点输入的控件（Widget）和 输入模式\n\n在有些情况下，我们需要使用来自其它节点的输出结果作为输入，此时我们就可以通过切换节点输入的控件（Widget）和 输入模式来实现。 下面是一个非常简单的例子： ![切换控件和输入模式](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/switch_widget.jpg) 通过将 K-Sampler 的 Seed 从输入控件（Widget）切换为输入模式，从而统一多个节点的 seed ，实现多个采样间的变量统一。 对比第一个节点和后续的两个节点，你可以看到后两个节点的 seed 是输入模式了，同样你还可以把它再转换回控件模式： ![转换控件和输入模式](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/convert_input.jpg)\n\n### 输入 / 输出的上下文菜单\n\n这里上下文菜单主要和对应输入输出的数据类型相关 ![节点输入输出上下文菜单](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/context_menus_2.jpg) 在拖动节点的输入 / 输出的时候，当有连线出现，但你未连接到其它节点的输入或输出的时候，此时释放鼠标则会弹出针对输入 / 输出的上下文菜单，用于快速添加相关类型的节点。 你可以在设置中调整对应的节点建议的数量 ![节点建议数量](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/node_suggestions.jpg)\n\n## 节点选择工具箱\n\n**节点选择工具箱（Selection tool box）** 是一个为节点提供快速操作的一个浮层工具，当你选中一个节点的时候，它会悬浮在选中的节点上方，通过这个节点你可以：\n\n*   修改节点的颜色\n*   快速设置节点为 Bypass 模式(在运行时候不执行)\n*   固定节点\n*   删除节点\n\n当然，这些功能在对应节点的右键菜单中也可以找到，节点选择工具箱只是提供了一个快捷操作，如果你想要关闭这个功能，可以在设置中关闭。 ![关闭节点选择工具箱](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/setting_selection_toolbox.jpg)\n\n## 节点组\n\n在 ComfyUI 中，你可以将一个工具流的部分，同时选用，再使用右键菜单将它们合并成一个节点组，使得对应的部分可以成为一个可复用的模块，从而在你的 ComfyUI 中进行重复调用\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/audio/ace-step/ace-step-v1",
  "markdown": "# ComfyUI ACE-Step 原生示例 - ComfyUI\n\nACE-Step是由中国团队阶跃星辰（StepFun）与ACE Studio联合开发的​​开源音乐生成基础大模型​​，旨在为音乐创作者提供高效、灵活且高质量的音乐生成与编辑工具。 该模型采用[Apache-2.0](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-license)许可证发布，可免费商用。 ACE-Step 作为一个强大的音乐生成基座，提供了丰富的扩展能力。通过 LoRA、ControlNet 等微调技术，开发者可以根据实际需求对模型进行定制化训练。 无论是音频编辑、歌声合成、伴奏制作、声音克隆还是风格转换等应用场景，ACE-Step 都能提供稳定可靠的技术支持。 这种灵活的架构设计大大简化了音乐 AI 应用的开发流程，让更多创作者能够快速将 AI 技术应用到音乐创作中。 目前 ACE-Step 已经发布相关的训练代码，包括 LoRA 模型训练等，对应 ControlNet 的训练代码也将在未来陆续发布，你可以访问他们的[Github](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-roadmap) 来了解更多详情。\n\n### 1\\. 工作流及相关模型下载\n\n点击下面的按钮下载对应的工作流文件，拖入 ComfyUI 中即可加载对应的工作流信息，对应工作流已包含模型下载信息。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_t2m.json)\n\n你也可以手动下载[ace\\_step\\_v1\\_3.5b.safetensors](https://huggingface.co/Comfy-Org/ACE-Step_ComfyUI_repackaged/blob/main/all_in_one/ace_step_v1_3.5b.safetensors) 后保存到 `ComfyUI/models/checkpoints` 文件夹下\n\n### 2\\. 按步骤完成工作流的运行\n\n![步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg)\n\n1.  确保 `Load Checkpoints` 节点加载了 `ace_step_v1_3.5b.safetensors` 模型\n2.  （可选）在 `EmptyAceStepLatentAudio` 节点上你可以设置生成音乐的时长\n3.  （可选）在 `LatentOperationTonemapReinhard` 节点，你可以调整 `multiplier` 来调整人声的音量大小（数字越大，人声音量越明显）\n4.  （可选）在 `TextEncodeAceStepAudio` 的 `tags` 输入对应的音乐风格等等\n5.  （可选）在 `TextEncodeAceStepAudio` 的 `lyrics` 中输入对应的歌词，如果你不知道该输入哪些歌词\n6.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行音频的生成。\n7.  工作流完成后，你可在 `Save Audio` 节点中查看生成的音频，你可以点击播放试听，对应的音频也会被保存至 `ComfyUI/output/audio` （由`Save Audio`节点决定子目录名称）。\n\n## ACE-Step ComfyUI 音频到音频工作流\n\n你可以像图生图工作流一样，输入一段音乐，使用下面的工作流来达到重新对音乐采样生成，同样，你也可以通过控制 `Ksampler` 的 `denoise` 来调整和原始音频的区别程度。 通过这样的流程，可以实现对音乐的重新编辑，来达到你想要的效果。\n\n### 1\\. 工作流文件下载\n\n点击下面的按钮下载对应的工作流文件，拖入 ComfyUI 中即可加载对应的工作流信息\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_m2m_editing.json)\n\n下载下面的音频作为输入音频\n\n[\n\n下载示例音频文件用于输入\n\n](https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/audio/ace-step/input.mp3)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ACE-Step 步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg)\n\n1.  确保 `Load Checkpoints` 节点加载了 `ace_step_v1_3.5b.safetensors` 模型\n2.  在 `LoadAudio` 节点中上传提供的音频文件\n3.  （可选）在 `TextEncodeAceStepAudio` 的 `tags` 和 `lyrics` 中输入对应的音乐风格歌词等，提供歌词对于音频编辑来说非常重要\n4.  （可选）修改 `Ksampler` 节点的 `denoise` 参数，来调整采样过程中添加的噪声来调整与原始音频的相似程度，（越小与原始音频越相似，如果设置为 `1.00`则可以近似认为没有音频输入）\n5.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行音频的生成。\n6.  工作流完成后，你可在 `Save Audio` 节点中查看生成的音频，你可以点击播放试听，对应的音频也会被保存至 `ComfyUI/output/audio` （由`Save Audio`节点决定子目录名称）。\n\n### 3\\. 工作流补充说明\n\n1.  在 `TextEncodeAceStepAudio` 的 `tags` 中示例工作流中，将原本男声的 `tags` 修改为 `female voice` 来生成女声的音频\n2.  在 `TextEncodeAceStepAudio` 的 `lyrics` 中示例工作流中，中对原本的歌词进行了调整修改，具体编辑你可以参考 ACE-Step 项目页面中的示例来了解如何完成修改\n\n## ACE-Step 提示词指南\n\nACE 的提示词目前使用的有两个，一个是 `tags` 一个是 `lyrics`。\n\n*   `tags`： 主要用来描述音乐的风格、场景等, 和我们平常其它生成的 prompt 类似，主要描述音频整体的风格和要求，使用英文逗号分隔\n*   `lyrics`： 主要用来描述歌词，支持歌词结构标签，如 \\[verse\\]（主歌）、\\[chorus\\]（副歌）和 \\[bridge\\]（过渡段）来区分歌词的不同部分，也可以在纯音乐情况下输入乐器名称\n\n对应的 `tags` 和 `lyrics` 在 [ACE-Step 模型主页](https://ace-step.github.io/) 中可以找到丰富的示例,你可以参考对应示例来尝试对应的提示词，本文档的提示词指南基于项目做了一些整理，以便让你能够快速尝试组合，来达到最想要的效果\n\n### tags标签(prompt)\n\n#### 主流音乐风格\n\n使用简短标签组合，来生成特定风格的音乐\n\n*   electronic（电子音乐）\n*   rock（摇滚）\n*   pop（流行）\n*   funk（放克）\n*   soul（灵魂乐）\n*   cyberpunk（赛博朋克）\n*   Acid jazz（酸爵士）\n*   electro（电子）\n*   em（电子音乐）\n*   soft electric drums（软电鼓）\n*   melodic（旋律）\n\n#### 场景类型\n\n结合具体使用场景和氛围，生成符合对应氛围的音乐\n\n*   background music for parties（派对背景音乐）\n*   radio broadcasts（电台广播音乐）\n*   workout playlists（健身播放列表音乐）\n\n#### 乐器元素\n\n*   saxophone,\n*   azz（萨克斯风、爵士）\n*   piano, violin（钢琴、小提琴）\n\n#### 人声类型\n\n*   female voice（女声）\n*   male voice（男声）\n*   clean vocals（纯净人声）\n\n#### 专业用语\n\n使用音乐中常用的一些专业的用词，来精准控制音乐效果\n\n*   110 bpm（每分钟节拍数为110）\n*   fast tempo（快节奏）\n*   slow tempo（慢节奏）\n*   loops（循环片段）\n*   fills（填充音）\n*   acoustic guitar（木吉他）\n*   electric bass（电贝斯）\n\n### 歌词（lyrics）\n\n#### 歌词结构标签\n\n*   \\[intro\\] (前奏)\n*   \\[verse\\] (主歌)\n*   \\[pre-chorus\\] (导歌)\n*   \\[chorus\\] (副歌/合唱)\n*   \\[bridge\\] (过渡段/桥段)\n*   \\[outro\\] (尾声)\n*   \\[hook\\] (钩子/主题旋律)\n*   \\[refrain\\] (重复段落)\n*   \\[interlude\\] (间奏)\n*   \\[breakdown\\] (分解段)\n*   \\[ad-lib\\] (即兴段落)\n\n#### 多语言支持\n\n*   ACE-Step V1 是支持多语言的，实际使用的时候 ACE-Step 会获取到对应的不同语言转换后的英文字母，然后进行音乐生成。\n*   在 ComfyUI 中我们并没有完全实现全部多语言到英文字母的转换，目前仅实现了[日语平假名和片假名字符](https://github.com/comfyanonymous/ComfyUI/commit/5d3cc85e13833aeb6ef9242cdae243083e30c6fc) 所以如果你需要使用多语言来进行相关的音乐生成，你需要首先将对应的语言转换成英文字母，然后在对应 `lyrics` 开头输入对应语言代码的缩写，比如中文`[zh]` 韩语 `[ko]` 等\n\n比如：\n\n```\n[verse]\n\n[zh]wo3zou3guo4shen1ye4de5jie1dao4\n[zh]leng3feng1chui1luan4si1nian4de5piao4liang4wai4tao4\n[zh]ni3de5wei1xiao4xiang4xing1guang1hen3xuan4yao4\n[zh]zhao4liang4le5wo3gu1du2de5mei3fen1mei3miao3\n\n[chorus]\n\n[verse]​\n[ko]hamkke si-kkeuleo-un sesang-ui sodong-eul pihae​\n[ko]honja ogsang-eseo dalbich-ui eolyeompus-ileul balaboda​\n[ko]niga salang-eun lideum-i ganghan eum-ag gatdago malhaess-eo​\n[ko]han ta han tamada ma-eum-ui ondoga eolmana heojeonhanji ijge hae\n\n[bridge]\n[es]cantar mi anhelo por ti sin ocultar\n[es]como poesía y pintura, lleno de anhelo indescifrable\n[es]tu sombra es tan terca como el viento, inborrable\n[es]persiguiéndote en vuelo, brilla como cruzar una mar de nubes\n\n[chorus]\n[fr]que tu sois le vent qui souffle sur ma main\n[fr]un contact chaud comme la douce pluie printanière\n[fr]que tu sois le vent qui s'entoure de mon corps\n[fr]un amour profond qui ne s'éloignera jamais\n```\n\n目前 ACE-Step 支持了 19 种语言，但下面十种语言的支持会更好一些：\n\n*   English\n*   Chinese: \\[zh\\]\n*   Russian: \\[ru\\]\n*   Spanish: \\[es\\]\n*   Japanese: \\[ja\\]\n*   German: \\[de\\]\n*   French: \\[fr\\]\n*   Portuguese: \\[pt\\]\n*   Italian: \\[it\\]\n*   Korean: \\[ko\\]\n\n## ACE-Step 相关资源\n\n*   [项目主页](https://ace-step.github.io/)\n*   [Hugging Face 模型](https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B)\n*   [GitHub 仓库](https://github.com/ace-step/ACE-Step)\n*   [训练脚本](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-train)"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/links",
  "markdown": "# 连线 - ComfyUI\n\n## 连线连接节点\n\n在 ComfyUI 的术语中，节点之间的线条或曲线称为 _**连线**_。这些连线可以以多种方式显示，例如曲线、直角线、直线或直接完全隐藏。 ![连线样式](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/link_styles.jpg) 你可以在 **设置菜单** —> **画面（Lite Graph）** —> **画面（Grap）** —> **连线渲染模式(Link Render Mode)** 进行连线样式的修改。 ![Canvas Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/link/render_mode.jpg) 也可以在 **Canvas Menu** 中临时隐藏连线。 ![Canvas Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/canvas_menu.jpg) 根据情况，可能需要查看所有链接。特别是在学习、分享或仅仅理解工作流时，连线之间的可见性可以让其它用户理解不同节点之间的相互作用。但对于那些不打算被更改的打包工作流，你也可以隐藏连线从而获得一个干净简洁的布局。\n\n### 重新路由节点\n\n通常，当我们节点过多时，对应的连接线难免被遮挡或者出现交叉，这时理解工作流作用就变得十分困难，如果你想要对应连线保持清晰，则可以采用一个 **重新路由（reroute）** 节点来手动调整连线。 ![ComfyUI 重新路由节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/reroute.jpg) 同时我们也在努力迭代，我们也已经完善 litegraph 原生的重路由功能，我们更建议在未来使用这个功能进行连线的重新组织。 ![ComfyUI 重新路由节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/native_reroute.jpg)\n\n## 颜色编码\n\n节点属性的数据类型通过输入/输出端口和链接连接线的颜色编码来表示。我们总是可以通过颜色判断哪些输入和输出可以相互连接。端口只能连接到相同颜色的其他端口来保证对应数据类型的匹配。 目前常见数据类型： ![ComfyUI 节点数据类型](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/data_type.jpg)\n\n| 数据类型 | 颜色  |\n| --- | --- |\n| 扩散模型 | 薰衣草色 |\n| CLIP 模型 | 黄色  |\n| VAE 模型 | 玫瑰色 |\n| 条件化 | 橙色  |\n| 潜在图像 | 粉色  |\n| 像素图像 | 蓝色  |\n| 蒙版  | 绿色  |\n| 数字（整数或浮点数） | 浅绿色 |\n| 网格（Mesh） | 亮绿色 |\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/lora",
  "markdown": "# ComfyUI LoRA 使用示例 - ComfyUI\n\n**LoRA 模型​（Low-Rank Adaptation）** 是一种用于微调大型生成模型（如 Stable Diffusion）的高效技术。 它通过在预训练模型的基础上引入可训练的低秩矩阵，仅调整部分参数，而非重新训练整个模型，从而以较低的计算成本实现特定任务的优化，相对于类似 SD1.5 这样的大模型，LoRA 模型更小，更容易训练。 ![LoRA 模型与基础模型对比](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/compare.png) 上面的图片对比了同样参数下 [dreamshaper\\_8](https://civitai.com/models/4384?modelVersionId=128713) 直接生成和使用 [blindbox\\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA 模型生成的图片对比，我们可以看到通过使用 LoRA 模型，可以在不调整基础模型的情况下，生成更符合我们需求的图片。 我们将演示如何使用 LoRA 的示例。所有 LoRA 变体：Lycoris, loha, lokr, locon, 等… 都是以这种方式使用。 在本示例中，我们将完成以下内容来学习[ComfyUI](https://github.com/comfyanonymous/ComfyUI) 中加载并使用 LoRA 模型，将涉及以下内容：\n\n1.  安装 LoRA 模型\n2.  使用 LoRA 模型生成图片\n3.  `Load LoRA` 节点的简单介绍\n\n## 相关模型安装\n\n请下载 [dreamshaper\\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) 并保存至 `ComfyUI/models/checkpoints` 目录 请下载 [blindbox\\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) 并保存至 `ComfyUI/models/loras` 目录\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - LoRA](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/lora.png) \n\n## 按步骤完成工作流的运行\n\n请参照下图步骤，来确保对应的工作流可以正常运行 ![ComfyUI 工作流 - LoRA 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/flow_diagram.png)\n\n1.  确保`Load Checkpoint` 加载了 `dreamshaper_8.safetensors`\n2.  确保`Load LoRA` 加载了 `blindbox_V1Mix.safetensors`\n3.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n\n## Load LoRA 节点介绍\n\n![Load LoRA 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_lora.jpg) 位于`ComfyUI\\models\\loras` 的模型会被 ComfyUI 检测到，并在这个节点中加载\n\n### 输入类型\n\n| 参数名称 | 作用  |\n| --- | --- |\n| `model` | 连接基础模型 |\n| `clip` | 连接 CLIP 模型 |\n| `lora_name` | 选择要加载使用的 LoRA 模型 |\n| `strength_model` | 影响 LoRA 对 模型权重（model）的影响程度，数值越大 LoRA 风格越强 |\n| `strength_clip` | 影响 LoRA 对 CLIP 词嵌入（clip）的影响程度 |\n\n### 输出类型\n\n| 参数名称 | 作用  |\n| --- | --- |\n| `model` | 输出应用了 LoRA 调整的模型 |\n| `clip` | 输出应用了 LoRA 调整的 CLIP 模型 |\n\n该节点支持链式连接，可以将多个`Load LoRA` 节点串联来应用多个 LoRA 模型，具体请参考[ComfyUI 应用多个 LoRA 示例](https://docs.comfy.org/zh-CN/tutorials/basic/multiple-loras) ![LoRA 节点链式连接](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)\n\n## 开始你的尝试\n\n1.  试着修改提示词，或者调整 `Load LoRA` 节点的不同参数，比如 `strength_model` ，来观察生成图片的变化，熟悉对应节点。\n2.  访问 [CivitAI](https://civitai.com/models) 网站，下载其它风格的 LoRA 模型，尝试使用。"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/inpaint",
  "markdown": "# ComfyUI 局部重绘工作流 - ComfyUI\n\n本篇将引导了解 AI 绘图中，局部重绘的概念，并在 ComfyUI 中完成局部重绘工作流生成，我们将接触以下内容：\n\n*   使用局部重绘工作流完成画面的修改\n*   了解并使用 ComfyUI 中遮罩编辑器\n*   了解相关节点 VAE Encoder (for Inpainting)\n\n## 关于局部重绘\n\n在 AI 图像生成过程中，我们常会遇到生成的画面整体较为满意，但是画面中存在一些不希望出现或者错误的元素，但是重新生成可能会生成另外一张完全不同的图片，所以这时候利用局部重绘来修复这部分的元素就非常有必要了。 这就像让 **画家(AI 绘图模型)** 画了一幅画，但是总是会有稍微有 **局部区域需要调整**，我们需要向画家说明**需要调整的区域(遮罩)**，然后让画家会根据我们的要求进行 **重新绘制(重绘)**。 局部重绘的场景包括：\n\n*   **瑕疵修复：** 消除照片中多余物体、错误的AI生成的画面的肢体等\n*   **细节优化：** 精准调整局部元素（如修改服装纹理、调整面部表情）\n*   等其它场景\n\n### 模型及相关素材准备\n\n#### 1\\. 模型安装\n\n下载下面的模型文件，并保存到`ComfyUI/models/checkpoints`目录下\n\n*   [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)\n\n#### 2\\. 局部重绘素材\n\n请下载下面的图片，我们将在这个示例中使用这个图片作为输入使用 ![ComfyUI局部重绘输入图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/input.png)\n\n#### 3\\. 局部重绘工作流\n\n下面这张图的 metadata 包含的对应的json工作流，请将其下载后 **拖入** ComfyUI 界面或者使用菜单 **工作流(Workflow)** —> **打开工作流(Open,快捷键 `Ctrl + O`)** 来加载这个局部重绘工作流 ![ComfyUI局部重绘工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)\n\n### ComfyUI 局部重绘工作流示例讲解\n\n![ComfyUI 局部重绘工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_workflow.png) 请参照图片序号对照下面的提示完下操作：\n\n1.  请确保已经加载了你所下载使用的模型\n2.  请在在 `Load Image` 节点中加载局部重绘的素材\n3.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成\n\n![ComfyUI局部重绘工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png) 此外我们在这里可以对比一下，下图是使用[v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) 模型来进行 inpainting 的结果。 ![ComfyUI 局部重绘工作流 - SD1.5](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png) 你会发现 [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) 模型生成的结果局部重绘的效果更好过渡更自然。 这因为这个模型是专为 inpainting 设计的模型，它可以帮助我们更好地控制生成区域，从而获得更好的局部重绘效果。 记得我们一直用的比喻吗？不同的模型就像能力不同的画家一样，但每个画家都有自己能力的上限，选择合适的模型可以让你的生成效果更好。 你可以进行下面的尝试来让画面达到你想要的效果:\n\n1.  修改正向 、负向提示词，使用更具体的描述\n2.  尝试多次运行，让 `KSampler` 使用不同的种子，从而带来不同的生成效果\n3.  在了解本篇遮罩编辑器使用的部分后，对于生成的结果再次进行重绘以获得满意的结果。\n\n接下来我们将简单了解如何使用 **遮罩编辑器(Mask Editor)** ，因为之前提供的输入图片中是已经包含了`alpha`透明通道（也就是我们希望在绘图过程中进行编辑的区域），所以并不需要你手动绘制，但在日常使用中我们会更经常使用 **遮罩编辑器(Mask Editor)** 来绘制 蒙版(Mask)\n\n### 使用遮罩编辑器(Mask Editor) 绘制蒙版\n\n首先在上一步工作流中的`Save Image` 节点上右键，你可以在右键菜单中看到`复制(Clipspace)` 选项，点击后会复制当前图片到剪贴板 ![ComfyUI 局部重绘 - 复制图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png) 然后在 **加载图像(Load Image)** 节点上右键，你可以在右键菜单中看到`Paste(Clipspace)` 选项，点击后会从剪贴板中粘贴图片 ![ComfyUI 局部重绘 - 粘贴图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png) 然后在 **加载图像(Load Image)** 节点上右键，你可以在右键菜单中看到`在遮罩编辑器中打开(Open in MaskEditor)` 选项，点击后会打开遮罩编辑器 ![打开遮罩编辑器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg) ![遮罩编辑器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint-maskeditor.gif)\n\n1.  你可以右侧编辑相关参数，比如调整画笔大小、透明度等等\n2.  绘制错误区域可以使用橡皮檫来擦除\n3.  绘制完成后点击 `Save` 按钮保存蒙版\n\n这样绘制完成的内容就会作为 遮罩(Mask) 输入到 VAE Encoder (for Inpainting) 节点中一起进行编码 然后试着调整提示词，再次进行生成，直到你可以完成满意的生成结果。\n\n## 局部重绘制相关节点\n\n通过[文生图](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)、[图生图](https://docs.comfy.org/zh-CN/tutorials/basic/image-to-image) 和本篇的工作流对比，我想你应该可以看到这几个工作流主要的差异都在于 VAE 部分这部分的条件输入, 在这个工作流中我们使用到的是 **VAE 内部编码器** 节点，这个节点是专门用于局部重绘的节点，它可以帮助我们更好地控制生成区域，从而获得更好的生成效果。 ![VAE Encoder (for Inpainting) 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg) **输入类型**\n\n| 参数名称 | 作用  |\n| --- | --- |\n| `pixels` | 需要编码到潜空间的输入图像。 |\n| `vae` | 用于将图片从像素空间编码到潜在空间的 VAE 模型。 |\n| `mask` | 图片遮罩，用来具体指明哪个区域需要进行修改。 |\n| `grow_mask_by` | 在原有的遮罩基础上，向外扩展的像素值，保证在遮罩区域外围有一定的过度区域，避免重绘区域与原图存在生硬的过渡。 |\n\n**输出类型**\n\n| 参数名称 | 作用  |\n| --- | --- |\n| `latent` | 经过 VAE 编码后的潜空间图像。 |"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/overview",
  "markdown": "# ComfyUI 界面概览 - ComfyUI\n\n可视化界面是目前绝大数用户使用 ComfyUI 来调用 [ComfyUI Server](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_overview) 进行相应媒体资源生成的方式，它提供了一个可供用户操作和组织工作流的可视化界面，用于组织和调试工作流，并生成令人惊叹的作品。 在本篇我们将粗略介绍 ComfyUI 的界面以及各个部分的功能，在后续的章节中我们将详细介绍各个部分的功能和使用方法。 通常，当你当你启动 ComfyUI 后你可以看到下面这样的一个界面： ![ComfyUI 基础界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui_new_interface.jpg) 如果你是较为早期的用户，你应该还见过之前的这样的菜单界面： ![ComfyUI 旧界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui_old_interface.jpg) 这两个菜单界面可以通过设置进行切换，但随着 ComfyUI 的功能日益强大和复杂，我们建议你使用新版的菜单界面来获得更好的使用体验。 目前 ComfyUI前端是一个独立的项目，作为一个独立的 ComfyUI 依赖进行进行发布和更新维护，如果你想要参与贡献，可以 fork 这个[仓库](https://github.com/Comfy-Org/ComfyUI_frontend)，并进行 pull request。\n\n## 本地化支持\n\n目前 ComfyUI 支持：包括英文、中文、俄罗斯语、法语、日文、韩文。 如果你需要切换界面语言到你需要的语言可以点击 **设置齿轮图标** 然后在 `Comfy` —> `Locale` 中选择你需要的语言。 ![ComfyUI 本地化支持](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/locale.jpg)\n\n## 新版菜单界面\n\n### 界面分区（Workspace）\n\n下面是主要的 ComfyUI 的界面分区以及各部分的简要介绍。 ![ComfyUI 工作区](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/comfyui-new-interface-main.png) 目前 ComfyUI 的界面除开主要的工作流界面，主要分为以下几个部分：\n\n1.  菜单栏：提供工作流、编辑、帮助菜单，工作流执行、ComfyUI Manager入口等等\n2.  侧边栏面板切换按钮：用于切换工作流历史队列、节点库、模型库、本地用户工作流浏览等\n3.  切换主题按钮： 在 ComfyUI 默认的暗色主题和亮色主题之间进行快速切换\n4.  设置：点击后可打开设置按钮\n5.  画布菜单： 提供了ComfyUI 画布的视图放大、缩小、自适应操作等\n\n### 菜单栏功能\n\n![ComfyUI 工作区](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/comfyui-new-interface-menu-bar.png) 上图是顶部菜单栏的对应功能，包含的常见的功能，我们会在具体的功能使用部分再详细介绍对应的功能\n\n### 侧边栏面板按钮\n\n![ComfyUI 侧边栏面板](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/side-panel.png) 在目前的 ComfyUI 中，我们提供了四个侧边面板包含了以下功能：\n\n1.  工作流历史队列(Queue)： 所有 ComfyUI 执行媒体内容生成的队列信息\n2.  节点库(Node Library)： 所有 ComfyUI 中的节点包括`Comfy Core` 和你安装的自定义节点都会可以在这里进行查找\n3.  模型库(Model Library)： 你本地的`ComfyUI/models` 目录下的模型可以在这里被查找到\n4.  本地用户工作流(Workflows)： 你本地保存的工作流可以在这里被查找到\n\n## 旧版菜单\n\n目前 ComfyUI 默认启用新版界面，如果你更偏好使用旧版界面，可以点击 **设置齿轮图标** 然后在 `Comfy` —> `菜单(Menu)` 将 `使用新菜单(Use new menu)` 设置为 `disabled` 即可切换到旧版本的菜单。\n\n旧版本菜单界面功能标注说明如下： ![ComfyUI 旧版菜单](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/comfyui-old-menu.png)"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/maskeditor",
  "markdown": "# 遮罩编辑器 - 在 ComfyUI 中创建和编辑遮罩\n\n遮罩编辑器是 ComfyUI 中一个非常实用的功能，它可以帮助用户在图像中创建和编辑遮罩，而不需要在其它应用程序中进行操作。 遮罩编辑器目前通过 `Load image` 节点来触发，当你上传图像后，可以在节点上右键通过菜单 `Open in MaskEditor` 来打开遮罩编辑器。 ![ComfyUI 遮罩编辑器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/maskeditor/maskeditor.jpg) ![ComfyUI 遮罩编辑器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/maskeditor/maskeditor_ui.jpg) 然后你就可以通过鼠标在图像上点击来创建和编辑遮罩了。\n\n## 演示视频\n\n你的浏览器不支持 video 标签。"
},
{
  "url": "https://docs.comfy.org/zh-CN/troubleshooting/model-issues",
  "markdown": "# 如何排查和解决 ComfyUI 中模型相关的问题 - ComfyUI\n\n## 模型架构不匹配\n\n**症状：** 生成过程中出现张量维度错误，特别是在 VAE 解码阶段 **常见错误消息：**\n\n*   `Given groups=1, weight of size [64, 4, 3, 3], expected input[1, 16, 128, 128] to have 4 channels, but got 16 channels instead`\n*   `Given groups=1, weight of size [4, 4, 1, 1], expected input[1, 16, 144, 112] to have 4 channels, but got 16 channels instead`\n*   `Given groups=1, weight of size [320, 4, 3, 3], expected input[2, 16, 192, 128] to have 4 channels, but got 16 channels instead`\n*   `The size of tensor a (49) must match the size of tensor b (16) at non-singleton dimension 1`\n*   `Tensors must have same number of dimensions: got 2 and 3`\n*   `mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)`\n\n**根本原因：** 将来自不同架构系列的模型混合使用\n\n### 解决方案\n\n1.  **验证模型系列兼容性：**\n    *   **Flux 模型**使用 16 通道潜在空间，配合双文本编码器调节（CLIP-L + T5-XXL）\n    *   **SD1.5 模型**使用 4 通道潜在空间，配合单个 CLIP ViT-L/14 文本编码器\n    *   **SDXL 模型**使用 4 通道潜在空间，配合双文本编码器（CLIP ViT-L/14 + OpenCLIP ViT-bigG/14）\n    *   **SD3 模型**使用 16 通道潜在空间，配合三重文本编码器调节（CLIP-L + OpenCLIP bigG + T5-XXL）\n    *   **ControlNet 模型**必须与基础检查点的架构匹配（SD1.5 ControlNet 仅适用于 SD1.5 检查点，SDXL ControlNet 仅适用于 SDXL 检查点，等等）\n2.  **常见不匹配场景和修复：** **Flux + 错误的 VAE：**\n    \n    ```\n    问题：将 taesd 或 sdxl_vae.safetensors 与 Flux 检查点一起使用\n    修复：使用来自 Hugging Face Flux 发布的 ae.safetensors（Flux VAE）\n    ```\n    \n    **Flux + 不正确的 CLIP 配置：**\n    \n    ```\n    问题：在 DualClipLoader 的两个 CLIP 插槽中都使用 t5xxl_fp8_e4m3fn.safetensors\n    修复：在一个插槽中使用 t5xxl_fp8_e4m3fn.safetensors，在另一个插槽中使用 clip_l.safetensors\n    ```\n    \n    **ControlNet 架构不匹配：**\n    \n    ```\n    问题：SD1.5 ControlNet 与 SDXL 检查点（或反之）\n    错误：\"mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)\"\n    修复：使用为您的检查点架构设计的 ControlNet 模型\n         - SD1.5 检查点需要 SD1.5 ControlNet\n         - SDXL 检查点需要 SDXL ControlNet\n    ```\n    \n3.  **快速诊断：**\n    \n    ```\n    # 检查错误是否发生在 VAE 解码阶段\n    # 寻找 \"expected input[X, Y, Z] to have N channels, but got M channels\"\n    # Y 值表示通道数：4 = SD 模型，16 = Flux 模型\n    ```\n    \n4.  **预防策略：**\n    *   将所有工作流模型保持在同一架构系列内\n    *   从同一来源/发布下载完整的模型包\n    *   使用 ComfyUI 管理器的模型兼容性指示器\n    *   在自定义之前使用默认示例测试工作流\n\n## 缺少模型错误\n\n**错误消息：**\n\n```\nPrompt execution failed\nPrompt outputs failed validation:\nCheckpointLoaderSimple:\n- Value not in list: ckpt_name: 'model-name.safetensors' not in []\n```\n\n### 解决方案\n\n1.  **下载所需模型：**\n    *   使用 ComfyUI 管理器自动下载模型\n    *   验证模型在正确的子文件夹中\n2.  **检查模型路径：**\n    *   **检查点**：`models/checkpoints/`\n    *   **VAE**：`models/vae/`\n    *   **LoRA**：`models/loras/`\n    *   **ControlNet**：`models/controlnet/`\n    *   **嵌入**：`models/embeddings/`\n3.  **在 UI 之间共享模型或使用自定义路径：**\n    *   参见 [ComfyUI 模型共享或自定义模型文件夹存储位置配置](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows#2-comfyui-%E6%A8%A1%E5%9E%8B%E5%85%B1%E4%BA%AB%E6%88%96%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E5%A4%B9%E5%AD%98%E5%82%A8%E4%BD%8D%E7%BD%AE%E9%85%8D%E7%BD%AE) 获取详细说明\n    *   编辑 `extra_model_paths.yaml` 文件添加自定义模型目录\n\n### 模型搜索路径配置\n\n如果您的模型在自定义位置，请参见详细的 [ComfyUI 模型共享或自定义模型文件夹存储位置配置](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows#2-comfyui-%E6%A8%A1%E5%9E%8B%E5%85%B1%E4%BA%AB%E6%88%96%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E5%A4%B9%E5%AD%98%E5%82%A8%E4%BD%8D%E7%BD%AE%E9%85%8D%E7%BD%AE) 指南来配置 ComfyUI 找到它们。\n\n## 模型加载错误\n\n**错误消息：** “Error while deserializing header”\n\n### 解决方案\n\n1.  **重新下载模型** - 下载过程中文件可能已损坏\n2.  **检查可用磁盘空间** - 确保有足够的空间用于模型加载（模型可能 2-15GB+）\n3.  **检查文件权限** - 确保 ComfyUI 可以读取模型文件\n4.  **使用不同模型测试** - 验证问题是模型特定的还是系统范围的\n\n## 模型性能问题\n\n### 模型加载缓慢\n\n**症状：** 切换模型或开始生成时长时间延迟 **解决方案：**\n\n1.  **将模型保持在显存中：**\n    \n    ```\n    python main.py --highvram\n    ```\n    \n2.  **使用更快的存储：**\n    *   如果使用 HDD，将模型移至 SSD\n    *   使用 NVMe SSD 获得最佳性能\n3.  **调整缓存设置：**\n    \n    ```\n    python main.py --cache-classic       # 使用旧式（积极）缓存\n    ```\n    \n\n### 大型模型的内存问题\n\n**“RuntimeError: CUDA out of memory”：**\n\n```\n# 渐进式内存减少\npython main.py --lowvram          # 首先尝试\npython main.py --novram           # 如果 lowvram 不够\npython main.py --cpu              # 最后手段\n```\n\n**模型特定的内存优化：**\n\n```\n# 强制更低精度\npython main.py --force-fp16\n\n# 减少注意力内存使用\npython main.py --use-pytorch-cross-attention\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/troubleshooting/overview",
  "markdown": "# 如何排查和解决 ComfyUI 中出现的错误 - ComfyUI\n\n[\n\n## 自定义节点故障排除指南\n\n查看如何排查自定义节点导致的问题。\n\n\n\n](https://docs.comfy.org/zh-CN/troubleshooting/custom-node-issues)\n\n## 常见问题与快速修复\n\n在深入详细故障排除之前，请尝试这些常见解决方案：\n\n### ComfyUI 无法启动\n\n**症状：** 应用程序在启动时崩溃、黑屏或无法加载 **快速修复：**\n\n1.  **检查系统要求** - 确保您的系统符合[最低要求](https://docs.comfy.org/zh-CN/installation/system_requirements)\n2.  **更新 GPU 驱动程序** - 从 NVIDIA/AMD/Intel 下载最新驱动程序\n\n### 生成失败或产生错误\n\n**症状：** “Prompt execution failed”（提示执行失败）对话框，带有”Show report”（显示报告）按钮，工作流停止执行 **快速修复：**\n\n1.  **点击”Show report”** - 阅读详细错误消息以识别具体问题\n2.  **检查是否是自定义节点问题** - [遵循我们的自定义节点故障排除指南](https://docs.comfy.org/zh-CN/troubleshooting/custom-node-issues)\n3.  **验证模型文件** - 查看[模型文档](https://docs.comfy.org/zh-CN/development/core-concepts/models)了解模型设置\n4.  **检查显存使用情况** - 关闭其他使用 GPU 内存的应用程序\n\n### 性能缓慢\n\n**症状：** 生成时间非常慢、系统冻结、内存不足错误 **快速修复：**\n\n1.  **降低分辨率/批次大小** - 减少图像大小或图像数量\n2.  **使用内存优化标志** - 请参见下方性能优化部分\n3.  **关闭不必要的应用程序** - 释放 RAM 和显存\n4.  **检查 CPU/GPU 使用率** - 使用任务管理器识别瓶颈\n\n**性能优化命令：** 对于低显存系统：\n\n```\n# 低显存模式（将模型分成多个部分）\npython main.py --lowvram\n\n# 当 --lowvram 不够用时的更低显存模式\npython main.py --novram\n\n# CPU 模式（非常慢但适用于任何硬件）\npython main.py --cpu\n```\n\n提高性能：\n\n```\n# 禁用预览（节省显存和处理）\npython main.py --preview-method none\n\n# 将模型保持在显存中（更快但使用更多显存）\npython main.py --highvram\n\n# 强制 FP16 精度（更快，使用更少显存）\npython main.py --force-fp16\n\n# 使用优化的注意力机制\npython main.py --use-pytorch-cross-attention\npython main.py --use-flash-attention\n\n# 异步权重卸载\npython main.py --async-offload\n```\n\n内存管理：\n\n```\n# 为操作系统保留特定显存量（以 GB 为单位）\npython main.py --reserve-vram 2\n\n# 禁用智能内存管理\npython main.py --disable-smart-memory\n\n# 使用不同的缓存策略\npython main.py --cache-none  # 更少的内存使用\npython main.py --cache-lru 10  # 缓存 10 个结果\n```\n\n## 安装过程中出现的问题\n\n### 桌面应用问题\n\n有关全面的桌面安装故障排除，请参见[桌面安装指南](https://docs.comfy.org/zh-CN/installation/desktop/windows)。\n\n*   **无法安装**：以管理员身份运行安装程序\n*   **缺少依赖项**：安装 [Visual C++ 可再发行组件](https://aka.ms/vs/17/release/vc_redist.x64.exe)\n*   **启动时崩溃**：检查 Windows 事件查看器以获取错误详细信息\n\n### 手动安装问题\n\n**Python 版本冲突：**\n\n```\n# 检查 Python 版本（需要 3.9+，推荐 3.12）\npython --version\n\n# 使用虚拟环境（推荐）\npython -m venv comfyui_env\nsource comfyui_env/bin/activate  # Linux/Mac\ncomfyui_env\\Scripts\\activate     # Windows\n```\n\n**包安装失败：**\n\n```\n# 首先更新 pip\npython -m pip install --upgrade pip\n\n# 安装依赖项\npip install -r requirements.txt\n\n# 对于 NVIDIA GPU（CUDA 12.8）\npip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128\n\n# 对于 AMD GPU（仅限 Linux - ROCm 6.3）\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3\n```\n\n### Linux 特定问题\n\n**LD\\_LIBRARY\\_PATH 错误：** 常见错误：\n\n*   “libcuda.so.1: cannot open shared object file”\n*   “libnccl.so: cannot open shared object file”\n*   “ImportError: libnvinfer.so.X: cannot open shared object file”\n\n**解决方案：**\n\n1.  **现代 PyTorch 安装（最常见）：**\n\n```\n# 对于带有 NVIDIA 包的虚拟环境\nexport LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# 对于 conda 环境\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# 或自动查找您的 Python site-packages\nPYTHON_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# 您可能还需要其他 NVIDIA 库\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cuda_runtime/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cublas/lib:$LD_LIBRARY_PATH\n```\n\n2.  **查找你拥有的库：**\n\n```\n# 检查已安装的 NVIDIA 包\npython -c \"import site; import os; nvidia_path=os.path.join(site.getsitepackages()[0], 'nvidia'); print('NVIDIA libs:', [d for d in os.listdir(nvidia_path) if os.path.isdir(os.path.join(nvidia_path, d))] if os.path.exists(nvidia_path) else 'Not found')\"\n\n# 查找 PyTorch 需要的缺失库\npython -c \"import torch; print(torch.__file__)\"\nldd $(python -c \"import torch; print(torch.__file__.replace('__init__.py', 'lib/libtorch_cuda.so'))\")\n```\n\n3.  **为你的环境永久设置：**\n\n```\n# 对于虚拟环境，添加到激活脚本\necho 'export LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> $VIRTUAL_ENV/bin/activate\n\n# 对于 conda 环境\nconda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# 对于全局 bashrc（根据需要调整 Python 版本）\necho 'export LD_LIBRARY_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> ~/.bashrc\n```\n\n4.  **替代方案：使用 ldconfig：**\n\n```\n# 检查当前库缓存\nldconfig -p | grep cuda\nldconfig -p | grep nccl\n\n# 如果缺失，添加库路径（需要 root 权限）\nsudo echo \"/usr/local/cuda/lib64\" > /etc/ld.so.conf.d/cuda.conf\nsudo ldconfig\n```\n\n5.  **调试库加载：**\n\n```\n# 详细库加载以查看缺失的内容\nLD_DEBUG=libs python main.py 2>&1 | grep \"looking for\"\n\n# 检查 PyTorch CUDA 可用性\npython -c \"import torch; print('CUDA available:', torch.cuda.is_available()); print('CUDA version:', torch.version.cuda)\"\n```\n\n## 模型相关问题\n\n有关综合模型故障排除，包括架构不匹配、缺少模型和加载错误，请参见专门的[模型问题](https://docs.comfy.org/zh-CN/troubleshooting/model-issues)页面。\n\n## 网络和 API 问题\n\n### API 节点不工作\n\n**症状：** API 调用失败、超时错误、配额超出 **解决方案：**\n\n1.  **检查 API 密钥有效性** - 在[用户设置](https://docs.comfy.org/zh-CN/interface/user)中验证密钥\n2.  **检查账户积分** - 确保有足够的 [API 积分](https://docs.comfy.org/zh-CN/interface/credits)\n3.  **验证互联网连接** - 使用其他在线服务进行测试\n4.  **检查服务状态** - 提供商可能正在经历停机\n\n### 连接问题\n\n**症状：** “无法连接到服务器”、超时错误 **解决方案：**\n\n1.  **检查防火墙设置** - 允许 ComfyUI 通过防火墙\n2.  **尝试不同端口** - 默认是 8188，尝试 8189 或 8190\n3.  **临时禁用 VPN** - VPN 可能阻止连接\n4.  **检查代理设置** - 如果不需要，禁用代理\n\n## 硬件特定问题\n\n### NVIDIA GPU 问题\n\n**CUDA 错误、GPU 未检测到：**\n\n```\n# 检查 CUDA 安装\nnvidia-smi\n\n# 验证 PyTorch CUDA 支持\npython -c \"import torch; print(torch.cuda.is_available())\"\n\n# 重新安装带 CUDA 的 PyTorch\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n```\n\n### AMD GPU 问题\n\n**ROCm 支持、性能问题：**\n\n```\n# 安装 ROCm 版本的 PyTorch\npip install torch torchvision --index-url https://download.pytorch.org/whl/rocm5.7\n```\n\n### Apple Silicon (M1/M2/M3) 问题\n\n**MPS 后端错误：**\n\n```\n# 检查 MPS 可用性\npython -c \"import torch; print(torch.backends.mps.is_available())\"\n\n# 如果 MPS 导致问题，强制使用 CPU\npython main.py --force-fp16 --cpu\n```\n\n## 获取帮助和报告错误\n\n### 报告错误之前\n\n1.  **检查是否是已知问题：**\n    *   搜索 [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n    *   检查 [ComfyUI 论坛](https://forum.comfy.org/)\n    *   查看 [Discord 讨论](https://discord.com/invite/comfyorg)\n2.  **尝试基本故障排除：**\n    *   使用[默认工作流](https://docs.comfy.org/zh-CN/get_started/first_generation)进行测试\n    *   禁用所有自定义节点（参见[自定义节点故障排除](https://docs.comfy.org/zh-CN/troubleshooting/custom-node-issues)）\n    *   检查控制台/终端中的错误消息\n\n### 如何有效报告错误\n\n#### 对于 ComfyUI 核心问题\n\n**问题提交：** [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n\n#### 对于桌面应用问题\n\n**问题提交：** [桌面 GitHub Issues](https://github.com/Comfy-Org/desktop/issues)\n\n#### 对于前端问题\n\n**问题提交：** [前端 GitHub Issues](https://github.com/Comfy-Org/ComfyUI_frontend/issues)\n\n#### 对于自定义节点问题\n\n**问题提交：** 请到对应的自定义节点仓库中提交问题\n\n### 在 issue 中你需要提供的信息\n\n报告任何问题时，请包括以下内容：\n\n## 社区资源\n\n*   **官方论坛：** [forum.comfy.org](https://forum.comfy.org/)\n*   **Discord：** [ComfyUI Discord 服务器](https://discord.com/invite/comfyorg)\n*   **Reddit：** [r/comfyui](https://reddit.com/r/comfyui)\n*   **YouTube：** [ComfyUI 教程](https://www.youtube.com/@comfyorg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/troubleshooting/custom-node-issues",
  "markdown": "# 如何解决和排查 ComfyUI 中自定义节点导致的问题 - ComfyUI\n\n关于自定义节点问题排查，本篇文档的总体思路如下：\n\n## 如何禁用所有的自定义节点？\n\n从设置菜单中启动禁用自定义节点的 ComfyUI 桌面版 ![设置菜单-禁用自定义节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/troubleshooting/desktop-diable-custom-node.jpg) 或手动运行服务器：\n\n```\ncd path/to/your/comfyui\npython main.py --disable-all-custom-nodes\n```\n\n**结果：**\n\n*   ✅ **问题消失**：自定义节点导致问题 → 继续步骤 2\n*   ❌ **问题仍然存在**：不是自定义节点问题 → [报告问题](#%E6%8A%A5%E5%91%8A%E9%97%AE%E9%A2%98)\n\n## 二分法\n\n在本篇中我们将会介绍使用二分搜索来进行自定义节点问题排查的思路，也就是一次排查一半的自定义节点，直到定位到导致问题的自定义节点 具体思路清参考下面的流程图，即每次启用所有未启用节点的一半，看看对应的问题是否出现，直到定位到对应的自定义节点是哪个\n\n## 两种排查方法\n\n在本篇文档中，我们将排查的自定义节点分为两类 ![自定义节点类型](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/troubleshooting/custom_node_type.jpg)\n\n*   A:包含前端扩展的自定义节点\n*   B: 常规节点\n\n首先让我们先了解不同类型的自定义节点可能导致的问题和原因\n\n对于自定义节点我们又特别需要对包含前端扩展的自定义节点进行优先排查，这类节点导致的问题是最多的, 他们主要的冲突是与 ComfyUI 前端版本更新产生的冲突。常见的一些问题有:\n\n*   工作流无法执行\n*   节点预览图丢失\n*   UI元素错位\n*   无法进入 ComfyUI 前端\n*   UI 完全损坏或显示空白屏幕\n*   无法和 ComfyUI 后端正常通信\n*   节点之间的连线无法正常工作\n*   等等\n\n常见这类节点导致的原因是因为：\n\n*   我们在对前端进行更新过程中进行了一些修改调整，这些自定义节点没有及时更新\n*   用户常常在更新过程中只是更新了 ComfyUI 并没有对自定义节点进行同步升级，虽然作者进行了更新，但是用户并没有在使用最新的兼容版本\n*   作者停止了维护，导致对应的自定义节点扩展无法和前端相互兼容\n\n## 使用二分法进行排查\n\n这两种上面两种不同的自定义节点问题里，自定义节点前端扩展和 ComfyUI 的冲突较为常见，我们会优先排查这类节点，后续的整体的问题排查思路如下\n\n### 1.排查自定义节点的前端扩展\n\n使用这种方法，你不用多次重启 ComfyUI 仅需要在每次启用 / 禁用自定义节点的前端扩展后重载 ComfyUI 即可, 而且你的排查范围也只是在有前端扩展的节点里，会大大缩小节点排查范围\n\n### 2\\. 通用的自定义节点排查方法\n\n## 修复自定义节点问题\n\n一旦你识别出有问题的自定义节点：\n\n### 选项 1:更新节点\n\n1.  检查 ComfyUI 管理器中是否有可用更新\n2.  更新节点并再次测试\n\n### 选项 2:替换节点\n\n1.  寻找具有类似功能的替代自定义节点\n2.  查看 [ComfyUI 注册表](https://registry.comfy.org/) 寻找替代方案\n\n### 选项 3:报告问题\n\n联系自定义节点开发者：\n\n1.  找到节点的 GitHub 仓库\n2.  创建问题并包含：\n    *   你的 ComfyUI 版本\n    *   错误消息/日志\n    *   重现步骤\n    *   你的操作系统\n\n### 选项 4:移除节点\n\n如果没有修复可用且你不需要该功能：\n\n1.  从 `custom_nodes/` 中移除有问题的节点\n2.  重启 ComfyUI\n\n## 报告非自定义节点导致的问题\n\n如果问题不是由自定义节点引起的，请参考通过[故障排除概述](https://docs.comfy.org/zh-CN/troubleshooting/overview)了解其他常见问题。\n\n### 自定义节点特定问题\n\n联系自定义节点开发者：\n\n*   找到节点的 GitHub 仓库\n*   创建问题并包含你的 ComfyUI 版本、错误消息、重现步骤和操作系统\n*   查看节点文档了解已知问题\n\n### ComfyUI 核心问题\n\n*   **GitHub**：[ComfyUI Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n*   **论坛**：[官方 ComfyUI 论坛](https://forum.comfy.org/)\n\n### 桌面应用问题\n\n*   **GitHub**：[ComfyUI 桌面问题](https://github.com/Comfy-Org/desktop/issues)\n\n### 前端问题\n\n*   **GitHub**：[ComfyUI 前端问题](https://github.com/Comfy-Org/ComfyUI_frontend/issues)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/controlnet",
  "markdown": "# ComfyUI ControlNet 使用示例 - ComfyUI\n\n在 AI 图像生成过程中，要精确控制图像生成并不是一键容易的事情，通常需要通过许多次的图像生成才可能生成满意的图像，但随着 **ControlNet** 的出现，这个问题得到了很好的解决。 ControlNet 是一种基于扩散模型（如 Stable Diffusion）的条件控制生成模型，最早由[Lvmin Zhang](https://lllyasviel.github.io/)与 Maneesh Agrawala 等人于 2023 年提出[Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543) ControlNet 模型通过引入多模态输入条件（如边缘检测图、深度图、姿势关键点等），显著提升了图像生成的可控性和细节还原能力。 使得我们可以进一步开始控制图像的风格、细节、人物姿势、画面结构等等，这些限定条件让图像生成变得更加可控，在绘图过程中也可以同时使用多个 ControlNet 模型，以达到更好的效果。 在没有 ControlNet 之前，我们每次只能让模型生成图像，直到生成我们满意的图像，充满了随机性。 ![ComfyUI 随机种子生成的图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/generated_with_random_seed.jpg) 但随着 ControlNet 的出现，我们可以通过引入额外的条件，来控制图像的生成，比如我们可以使用一张简单的涂鸦，来控制图像的生成，就可以生成差不多类似的图片。 ![ComfyUI 涂鸦控制图像生成](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/scribble_example.jpg) 在本示例中，我们将引导你完成在 [ComfyUI](https://github.com/comfyanonymous/ComfyUI) 中 ControlNet 模型的安装与使用, 并完成一个涂鸦控制图像生成的示例。 ![ComfyUI ControlNet 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)\n\n不同类型的 ControlNet 模型，通常需要使用不同类型的参考图： ![参考图](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/main/examples/CNAuxBanner.jpg?raw=true)\n\n> 图源：[ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n由于目前 **Comfy Core** 节点中，不包含所有类型的 **预处理器** 类型，但在本文档的实际示例中，我们都将提供已经经过处理后的图片， 但在实际使用过程中，你可能需要借助一些自定义节点来对图片进行预处理，以满足不同 ControlNet 模型的需求，下面是一些相关的插件\n\n*   [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n*   [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n## ComfyUI ControlNet 工作流示例讲解\n\n### 1\\. ControlNet 工作流素材\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)\n\n请下载下面的图片，我们将会将它作为输入 ![ComfyUI 涂鸦图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_input.png)\n\n### 2\\. 手动模型安装\n\n*   [dreamCreationVirtual3DECommerce\\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n*   [control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── dreamCreationVirtual3DECommerce_v10.safetensors\n│   ├── vae/\n│   │   └── vae-ft-mse-840000-ema-pruned.safetensors\n│   └── controlnet/\n│       └── control_v11p_sd15_scribble_fp16.safetensors\n```\n\n### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI 工作流 - ControlNet 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_scribble.png)\n\n1.  确保`Load Checkpoint`可以加载 **dreamCreationVirtual3DECommerce\\_v10.safetensors**\n2.  确保`Load VAE`可以加载 **vae-ft-mse-840000-ema-pruned.safetensors**\n3.  在`Load Image`中点击`Upload` 上传之前提供的输入图片\n4.  确保`Load ControlNet`可以加载 **control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors**\n5.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n\n## 相关节点讲解\n\n### Load ControlNet 节点讲解\n\n![load controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_controlnet_model.jpg) 位于`ComfyUI\\models\\controlnet` 的模型会被 ComfyUI 检测到，并在这个节点中识别并加载\n\n### Apply ControlNet 节点讲解\n\n![apply controlnet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg) 这个节点接受 `load controlnet` 加载的 ControlNet 模型，并根据输入的图片，生成对应的控制条件。 **输入类型**\n\n| 参数名称 | 作用  |\n| --- | --- |\n| `positive` | 正向条件 |\n| `negative` | 负向条件 |\n| `control_net` | 要应用的controlNet模型 |\n| `image` | 用于 controlNet 应用参考的预处理器处理图片 |\n| `vae` | Vae模型输入 |\n| `strength` | 应用 ControlNet 的强度，越大则 ControlNet 对生成图像的影响越大 |\n| `start_percent` | 确定开始应用controlNet的百分比，比如取值0.2，意味着ControlNet的引导将在扩散过程完成20%时开始影响图像生成 |\n| `end_percent` | 确定结束应用controlNet的百分比，比如取值0.8，意味着ControlNet的引导将在扩散过程完成80%时停止影响图像生成 |\n\n**输出类型**\n\n| 参数名称 | 作用  |\n| --- | --- |\n| `positive` | 应用了 ControlNet 处理后的正向条件数据 |\n| `negative` | 应用了 ControlNet 处理后的负向条件数据 |\n\n你可以使用链式链接来应用多个 ControlNet 模型，如下图所示，你也可以参考 [混合 ControlNet 模型](https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets) 部分的指南来了解更多关于混合 ControlNet 模型的使用 ![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg) \n\n## 开始你的尝试\n\n1.  试着制作类似的涂鸦图片，甚至自己手绘，并使用 ControlNet 模型生成图像，体验 ControlNet 带来的乐趣\n2.  调整 Apply ControlNet 节点的 `Control Strength` 参数，来控制 ControlNet 模型对生成图像的影响\n3.  访问 [ControlNet-v1-1\\_fp16\\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) 仓库下载其它类型的 ControlNet 模型，并尝试使用它们生成图像"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/depth-controlnet",
  "markdown": "# ComfyUI Depth ControlNet 使用示例 - ComfyUI\n\n深度图(Depth Map)是一种特殊的图像，它通过灰度值表示场景中各个物体与观察者或相机的距离。在深度图中，灰度值与距离成反比：​越亮的区域（接近白色）表示距离越近，​越暗的区域（接近黑色）表示距离越远。 ![Depth 图像](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png) Depth ControlNet 是专门训练用于理解和利用深度图信息的 ControlNet 模型。它能够帮助 AI 正确解读空间关系，使生成的图像符合深度图指定的空间结构，从而实现对三维空间布局的精确控制。\n\n### 深度图结合 ControlNet 应用场景\n\n深度图在多种场景中都有比较多的应用：\n\n1.  **人像场景**：控制人物与背景的空间关系，避免面部等关键部位畸变\n2.  **风景场景**：控制近景、中景、远景的层次关系\n3.  **建筑场景**：控制建筑物的空间结构和透视关系\n4.  **产品展示**：控制产品与背景的分离度和空间位置\n\n本篇示例中，我们将使用深度图生成建筑可视化的场景生成。\n\n## ComfyUI ControlNet 工作流示例讲解\n\n### 1\\. ControlNet 工作流素材\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![Depth 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_controlnet.png)\n\n请下载下面的图片，我们将会将它作为输入。 ![Depth 图像](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)\n\n### 2\\. 模型安装\n\n*   [architecturerealmix\\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [control\\_v11f1p\\_sd15\\_depth\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── architecturerealmix_v11.safetensors\n│   └── controlnet/\n│       └── control_v11f1p_sd15_depth_fp16.safetensors\n```\n\n### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI 工作流 - Depth ControlNet 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth.jpg)\n\n1.  确保`Load Checkpoint`可以加载 **architecturerealmix\\_v11.safetensors**\n2.  确保`Load ControlNet`可以加载 **control\\_v11f1p\\_sd15\\_depth\\_fp16.safetensors**\n3.  在`Load Image`中点击`Upload` 上传之前提供的 Depth 图像\n4.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n\n## 混合深度控制与其他技术\n\n根据不同创作需求，可以将深度图 ControlNet 与其它类型的 ControlNet 混合使用来达到更好的效果：\n\n1.  **Depth + Lineart**：保持空间关系的同时强化轮廓，适用于建筑、产品、角色设计\n2.  **Depth + Pose**：控制人物姿态的同时维持正确的空间关系，适用于人物场景\n\n关于多个 ControlNet 混合使用，可以参考 [混合 ControlNet](https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets) 示例。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_overview",
  "markdown": "# Javascript 扩展 - ComfyUI\n\n## 扩展 Comfy 客户端\n\nComfy 可以通过扩展机制进行修改。要添加一个扩展，你需要：\n\n*   从你的 Python 模块中导出 `WEB_DIRECTORY`，\n*   将一个或多个 `.js` 文件放入该目录，\n*   使用 `app.registerExtension` 注册你的扩展。\n\n下面是这三个步骤。了解如何添加扩展后，可以查阅可用的[钩子](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_hooks)以让你的代码被调用， 也可以了解你可能需要的各种 [Comfy 对象](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_objects_and_hijacking)， 或者直接跳转到一些[示例代码片段](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_examples)。\n\n### 导出 `WEB_DIRECTORY`\n\n可以通过在你的自定义节点目录下创建一个子目录（通常叫 `js`），并导出 `WEB_DIRECTORY` 来扩展 Comfy 网页客户端——所以你的 `__init__.py` 应该包含如下内容：\n\n```\nWEB_DIRECTORY = \"./js\"\n__all__ = [\"NODE_CLASS_MAPPINGS\", \"NODE_DISPLAY_NAME_MAPPINGS\", \"WEB_DIRECTORY\"]\n```\n\n### 包含 `.js` 文件\n\n只有 `.js` 文件会被添加到网页。其他资源（如 `.css` 文件）可以通过 `extensions/custom_node_subfolder/the_file.css` 访问，并可通过代码动态添加。\n\n### 注册扩展\n\n扩展的基本结构是导入主 Comfy `app` 对象，并调用 `app.registerExtension`， 传入一个包含唯一 `name` 和一个或多个由 Comfy 钩子调用的函数的字典。 一个完整、简单且“烦人”的扩展示例如下：\n\n```\nimport { app } from \"../../scripts/app.js\";\napp.registerExtension({ \n\tname: \"a.unique.name.for.a.useless.extension\",\n\tasync setup() { \n\t\talert(\"Setup complete!\")\n\t},\n})\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_hooks",
  "markdown": "# Comfy 钩子（Hooks） - ComfyUI\n\n## 扩展钩子\n\n在 Comfy 执行的不同阶段，应用会调用 `#invokeExtensionsAsync` 或 `#invokeExtensions`，并传入钩子的名称。 这些方法会在所有已注册的扩展上调用同名方法（如果存在），比如上面例子中的 `setup`。 Comfy 提供了多种钩子，供自定义扩展代码使用，以修改客户端行为。\n\n下面介绍了一些最重要的钩子。由于 Comfy 仍在积极开发中，新的钩子会不时加入，因此可以在 `app.js` 中搜索 `#invokeExtensions` 以查找所有可用钩子。 另请参阅[钩子的调用顺序](#call-sequences)。\n\n### 常用钩子\n\n从 `beforeRegisterNodeDef` 开始，大多数扩展都会用到它，通常也是唯一需要的钩子。\n\n#### beforeRegisterNodeDef()\n\n对每种节点类型（即 `AddNode` 菜单中可用的节点列表）调用一次，用于修改节点的行为。\n\n```\nasync beforeRegisterNodeDef(nodeType, nodeData, app) \n```\n\n传入的 `nodeType` 参数本质上是该类型所有节点的模板，因此对 `nodeType.prototype` 的修改会应用到所有该类型节点上。`nodeData` 封装了 Python 代码中定义的节点相关信息，如类别、输入和输出。`app` 是主 Comfy app 对象的引用（你应该已经导入了它！）\n\n常见做法是检查 `nodeType.ComfyClass`，它保存了该节点对应的 Python 类名，以判断是否需要修改该节点。通常这意味着只修改你自己添加的自定义节点，但有时也可能需要修改其他节点（或其他自定义节点也可能修改你的节点！），此时要注意兼容性。\n\n在 `beforeRegisterNodeDef` 中非常常见的做法是”劫持”已有方法：\n\n```\nasync beforeRegisterNodeDef(nodeType, nodeData, app) {\n\tif (nodeType.comfyClass==\"MyNodeClass\") { \n\t\tconst onConnectionsChange = nodeType.prototype.onConnectionsChange;\n\t\tnodeType.prototype.onConnectionsChange = function (side,slot,connect,link_info,output) {     \n\t\t\tconst r = onConnectionsChange?.apply(this, arguments);   \n\t\t\tconsole.log(\"Someone changed my connection!\");\n\t\t\treturn r;\n\t\t}\n\t}\n}\n```\n\n这种做法是先保存原型上的原方法，然后替换为新方法。新方法会调用原方法（`?.apply` 保证即使没有原方法也不会出错），然后执行额外操作。根据你的代码逻辑，可能需要在新方法的其他位置调用 `apply`，甚至有条件地调用。 以这种方式劫持方法时，建议查看核心 comfy 代码（断点调试很有用），以确保方法签名一致。\n\n#### nodeCreated()\n\n当某个节点实例被创建时调用（就在 `nodeType` 上的 `ComfyNode()` 构造函数结束时）。在这个钩子里你可以修改节点的具体实例。\n\n#### init()\n\n当 Comfy 网页被加载（或重新加载）时调用。调用时机是在图对象已创建，但还未注册或创建任何节点之前。可以用来劫持 app 或 graph（`LiteGraph` 对象）的方法，从而修改核心 Comfy 行为。详见[Comfy 对象与劫持](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_objects_and_hijacking)。\n\n#### setup()\n\n在启动流程结束时调用。适合添加事件监听器（无论是 Comfy 事件还是 DOM 事件），或添加全局菜单，相关内容在其他地方有详细介绍。\n\n### 调用顺序\n\n以下顺序是通过在 Comfy `app.js` 文件中插入日志代码获得的。你也可以用类似方法帮助理解执行流程。\n\n```\n/* 截至目前大约在第 220 行： */\n\t#invokeExtensions(method, ...args) {\n\t\tconsole.log(`invokeExtensions      ${method}`) // 此行为新增\n\t\t// ...\n\t}\n/* 截至目前大约在第 250 行： */\n\tasync #invokeExtensionsAsync(method, ...args) {\n\t\tconsole.log(`invokeExtensionsAsync ${method}`) // 此行为新增\n\t\t// ...\n\t}\n```\n\n#### 网页加载时\n\n```\ninvokeExtensionsAsync init\ninvokeExtensionsAsync addCustomNodeDefs\ninvokeExtensionsAsync getCustomWidgets\ninvokeExtensionsAsync beforeRegisterNodeDef    [多次重复]\ninvokeExtensionsAsync registerCustomNodes\ninvokeExtensionsAsync beforeConfigureGraph\ninvokeExtensionsAsync nodeCreated\ninvokeExtensions      loadedGraphNode\ninvokeExtensionsAsync afterConfigureGraph\ninvokeExtensionsAsync setup\n```\n\n#### 加载工作流\n\n```\ninvokeExtensionsAsync beforeConfigureGraph\ninvokeExtensionsAsync beforeRegisterNodeDef   [0、1 或多次]\ninvokeExtensionsAsync nodeCreated             [多次重复]\ninvokeExtensions      loadedGraphNode         [多次重复]\ninvokeExtensionsAsync afterConfigureGraph\n```\n\n#### 添加新节点\n\n```\ninvokeExtensionsAsync nodeCreated\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-controlnet",
  "markdown": "# ComfyUI Flux.1 ControlNet 示例 - ComfyUI\n\n ![Flux.1 Canny Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-canny-controlnet.png) ![Flux.1 Depth Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-depth-controlnet.png)\n\nFLUX.1 Canny 和 Depth 是由 [Black Forest Labs](https://blackforestlabs.ai/) 推出的 ​[FLUX.1 Tools 套件](https://blackforestlabs.ai/flux-1-tools/) 中的两个强大模型。这套工具旨在为 FLUX.1 添加控制和引导能力，使用户能够修改和重新创建真实或生成的图像。 **FLUX.1-Depth-dev** 和 **FLUX.1-Canny-dev** 都是 12B 参数的 Rectified Flow Transformer 模型，能够基于文本描述生成图像，同时保持与输入图像的一致性。其中 Depth 版本通过深度图提取技术来维持源图像的空间结构，而 Canny 版本则利用边缘检测技术来保持源图像的结构特征，使得用户可以根据不同需求选择合适的控制方式。 这两个模型都具有以下特点：\n\n*   顶级的输出质量和细节表现\n*   出色的提示遵循能力，同时保持源图像的结构布局\n*   使用引导蒸馏技术训练，提高效率\n*   开放权重供社区研究使用\n*   提供 API 接口（pro 版）和开源权重（dev 版）\n\n此外，Black Forest Labs 还提供了从完整模型中提取的 **FLUX.1-Depth-dev-lora** 和 **FLUX.1-Canny-dev-lora** 适配器版本，它们可以应用于 FLUX.1 \\[dev\\] 基础模型，以较小的文件体积提供类似的功能，特别适合资源受限的环境。 本文将以分别以完整版本的 **FLUX.1-Canny-dev** 和 **FLUX.1-Depth-dev-lora** 为例，完成ComfyUI 中 Flux ControlNet 的工作流示例。\n\n## FLUX.1-Canny-dev 完整版工作流\n\n### 1\\. 工作流及相关素材\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev.png) 请下载下面的图片，我们将使用它来作为输入图片 ![ComfyUI Flux.1 Canny Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev-input.png)\n\n### 2\\. 手动模型下载\n\n完整模型列表：\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) （请确保你已经同意了对应 repo 的协议）\n\n文件保存位置：\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── t5xxl_fp16.safetensors\n│   ├── vae/\n│   │   └── ae.safetensors\n│   └── diffusion_models/\n│       └── flux1-canny-dev.safetensors\n```\n\n### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI Flux.1 Canny Controlnet 步骤流程](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg)\n\n1.  确保在`Load VAE`中加载了`ae.safetensors`\n2.  确保在`Load Diffusion Model`加载了`flux1-canny-dev.safetensors`\n3.  确保在`DualCLIPLoader`中下面的模型已加载：\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n4.  在`Load Image`节点中上传了文档中提供的输入图片\n5.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n### 4\\. 开始你的尝试\n\n尝试使用[FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) 模型完成 Depth 版本的工作流 你可以使用下面的图片作为输入 ![ComfyUI 室内深度图](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png) 或者借助下面自定义节点中完成图像预处理:\n\n*   [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n*   [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n## FLUX.1-Depth-dev-lora 工作流\n\nLoRA 版本的工作流是在完整版本的基础上，添加了 LoRA 模型,相对于[完整版本的 Flux 工作流](https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-text-to-image),增加了对应 LoRA 模型的加载使用节点。\n\n### 1\\. 工作流及相关素材\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora.png) 请下载下面的图片，我们将使用它来作为输入图片 ![ComfyUI Flux.1 Depth Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora-input.png)\n\n### 2\\. 手动模型下载\n\n完整模型列表：\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)\n*   [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)\n\n文件保存位置：\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── t5xxl_fp16.safetensors\n│   ├── vae/\n│   │   └── ae.safetensors\n│   ├── diffusion_models/\n│   │   └── flux1-dev.safetensors\n│   └── loras/\n│       └── flux1-depth-dev-lora.safetensors\n```\n\n### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI Flux.1 Depth Controlnet 步骤流程](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg)\n\n1.  确保在`Load Diffusion Model`加载了`flux1-dev.safetensors`\n2.  确保在`LoraLoaderModelOnly`中加载了`flux1-depth-dev-lora.safetensors`\n3.  确保在`DualCLIPLoader`中下面的模型已加载：\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n4.  在`Load Image`节点中上传了文档中提供的输入图片\n5.  确保在`Load VAE`中加载了`ae.safetensors`\n6.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n### 4\\. 开始你的尝试\n\n尝试使用[FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) 模型完成 Canny 版本的工作流 借助 [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) 或者 [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) 完成图像预处理\n\n## 社区版本 Flux Controlnets\n\nXLab 和 InstantX + Shakker Labs 已经为 Flux 发布了 Controlnet。 **InstantX:**\n\n*   [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)\n*   [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)\n*   [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)\n\n**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections) 将这些文件放在 `ComfyUI/models/controlnet` 目录下。 你可以访问[Flux Controlnet 示例](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png)来获取对应工作流图片，并使用[这里](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png)的图片作为输入图片。"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_objects_and_hijacking",
  "markdown": "# Comfy 对象 - ComfyUI\n\n## LiteGraph\n\nComfy UI 构建于 [LiteGraph](https://github.com/jagenjo/litegraph.js) 之上。 Comfy 的许多功能都由 LiteGraph 提供，因此如果你要开发更复杂的节点，建议克隆该仓库并查阅文档，文档位于 `doc/index.html`。\n\n## ComfyApp\n\n`app` 对象（始终可通过 `import { app } from \"../../scripts/app.js\";` 获取）代表在浏览器中运行的 Comfy 应用，包含许多有用的属性和函数，部分如下所示。\n\n### 属性\n\n`app` 的重要属性包括（非完整列表）：\n\n| 属性  | 内容  |\n| --- | --- |\n| `canvas` | 一个 LGraphCanvas 对象，代表当前用户界面。包含如 `node_over` 和 `selected_nodes` 等有用属性。 |\n| `canvasEl` | DOM `<canvas>` 元素 |\n| `graph` | 指向当前图的 LGraph 对象的引用 |\n| `runningNodeId` | 执行期间，当前正在执行的节点 |\n| `ui` | 可访问部分 UI 元素，如队列、菜单和对话框 |\n\n`canvas`（用于图形元素）和 `graph`（用于逻辑连接）大概率是你最常用的。\n\n### 函数\n\n同样，函数也有很多。以下是一些重要的：\n\n| 函数  | 说明  |\n| --- | --- |\n| graphToPrompt | 将图转换为可发送到 Python 服务器的 prompt |\n| loadGraphData | 加载一个图 |\n| queuePrompt | 将 prompt 提交到队列 |\n| registerExtension | 你已经见过——用于添加扩展 |\n\n## LGraph\n\n`LGraph` 对象是 LiteGraph 框架的一部分，代表当前图的逻辑状态（节点和连线）。 如果你想操作图，LiteGraph 文档（克隆 `https://github.com/jagenjo/litegraph.js` 后在 `doc/index.html`）描述了你需要的函数。 你可以用 `graph` 获取节点和连线的详细信息，例如：\n\n```\nconst ComfyNode_object_for_my_node = app.graph._nodes_by_id(my_node_id) \nComfyNode_object_for_my_node.inputs.forEach(input => {\n    const link_id = input.link;\n    if (link_id) {\n        const LLink_object = app.graph.links[link_id]\n        const id_of_upstream_node = LLink_object.origin_id\n        // 等等\n    }\n});\n```\n\n## LLink\n\n`LLink` 对象可通过 `graph.links` 访问，代表图中一条从节点 `link.origin_id` 的输出槽 `link.origin_slot` 到节点 `link.target_id` 的输入槽 `link.target_slot` 的连线。它还有一个字符串类型 `link.type` 和 `link.id`。 `LLink` 是在 `LGraphNode`（`ComfyNode` 是其子类）的 `connect` 方法中创建的。\n\n## ComfyNode\n\n`ComfyNode` 是 `LGraphNode` 的子类，因此 LiteGraph 文档对于通用操作也很有帮助。不过，Comfy 对 LiteGraph 的核心行为做了大量扩展，也没有用到 LiteGraph 的全部功能。\n\n`ComfyNode` 对象代表当前工作流中的一个节点。它有许多重要属性和大量可用或可劫持的函数，用于修改行为。 为了更全面地了解节点对象，你可以在扩展中插入如下代码，并在 `console.log` 处打断点。创建新节点时即可用调试器查看节点。\n\n```\nasync nodeCreated(node) {\n    console.log(\"nodeCreated\")\n}\n```\n\n### 属性\n\n| 属性  | 内容  |\n| --- | --- |\n| `bgcolor` | 节点的背景色，默认 undefined |\n| `comfyClass` | 节点对应的 Python 类 |\n| `flags` | 包含节点状态相关标志的字典。特别是 `flags.collapsed` 表示节点是否折叠。 |\n| `graph` | 指向 LGraph 对象的引用 |\n| `id` | 唯一 id |\n| `input_type` | 输入类型列表（如 “STRING”、“MODEL”、“CLIP” 等）。通常与 Python 的 INPUT\\_TYPES 匹配 |\n| `inputs` | 输入列表（见下文） |\n| `mode` | 通常为 0，静音为 2，旁路为 4。1 和 3 未被 Comfy 使用 |\n| `order` | 节点的执行顺序。由 `LGraph.computeExecutionOrder()` 在提交 prompt 时设置 |\n| `pos` | 节点在画布上的 \\[x,y\\] 位置 |\n| `properties` | 包含 “Node name for S&R” 的字典，由 LiteGraph 使用 |\n| `properties_info` | `properties` 中条目的类型和默认值 |\n| `size` | 节点在画布上的宽高 |\n| `title` | 显示标题 |\n| `type` | 节点类的唯一名称（来自 Python） |\n| `widgets` | 小部件列表（见下文） |\n| `widgets_values` | 小部件的当前值列表 |\n\n### 函数\n\n函数非常多（上次统计有 85 个）。以下是部分常用函数。 大多数函数未被 Comfy 修改，仍为 LiteGraph 核心代码。\n\n#### 输入、输出、小部件\n\n| 函数  | 说明  |\n| --- | --- |\n| Inputs / Outputs | 大多数有同名的输出方法：s/In/Out/ |\n| `addInput` | 创建新输入，需指定名称和类型 |\n| `addInputs` | `addInput` 的数组版本 |\n| `findInputSlot` | 通过输入名查找槽索引 |\n| `findInputSlotByType` | 按类型查找输入。可选参数优先或仅使用空闲槽 |\n| `removeInput` | 按槽索引移除输入 |\n| `getInputNode` | 获取连接到该输入的节点。输出等价方法为 `getOutputNodes`，返回列表 |\n| `getInputLink` | 获取连接到该输入的 LLink。无输出等价方法 |\n| Widgets |     |\n| `addWidget` | 添加标准 Comfy 小部件 |\n| `addCustomWidget` | 添加自定义小部件（在 `getComfyWidgets` 钩子中定义） |\n| `addDOMWidget` | 添加由 DOM 元素定义的小部件 |\n| `convertWidgetToInput` | 如果 `isConvertableWidget` 允许，将小部件转为输入（见 `widgetInputs.js`） |\n\n#### 连接\n\n| 函数  | 说明  |\n| --- | --- |\n| `connect` | 将本节点输出连接到其他节点输入 |\n| `connectByType` | 按类型将输出连接到其他节点——连接到第一个可用的匹配槽 |\n| `connectByTypeOutput` | 按类型将输入连接到其他节点输出 |\n| `disconnectInput` | 移除输入（按名称或索引）上的所有连线 |\n| `disconnectOutput` | 断开输出与指定节点输入的连接 |\n| `onConnectionChange` | 每个节点都会调用。`side==1` 表示是本节点的输入 |\n| `onConnectInput` | 在建立连接前调用。如果返回 `false`，则拒绝连接 |\n\n#### 显示\n\n| 函数  | 说明  |\n| --- | --- |\n| `setDirtyCanvas` | 指定前景（节点）和/或背景（连线和图像）需要重绘 |\n| `onDrawBackground` | 用 `CanvasRenderingContext2D` 对象绘制背景。Comfy 用于渲染图像 |\n| `onDrawForeground` | 用 `CanvasRenderingContext2D` 对象绘制节点 |\n| `getTitle` | 要显示的标题 |\n| `collapse` | 切换节点折叠状态 |\n\n#### 其他\n\n| 函数  | 说明  |\n| --- | --- |\n| `changeMode` | 用于设置节点为旁路（`mode == 4`）或非旁路（`mode == 0`） |\n\n## 输入与小部件\n\n输入和小部件是向节点输入数据的两种方式。一般来说，小部件可以转为输入，但并非所有输入都能转为小部件（许多数据类型无法通过 UI 元素输入）。 `node.inputs` 是当前所有输入的列表（节点左侧的彩色圆点），包含 `.name`、`.type` 和 `.link`（指向 `app.graph.links` 中的 LLink）。 如果输入是已转换的小部件，还会在 `.widget` 中保存对该小部件（现已失效）的引用。 `node.widgets` 是所有小部件的列表，无论是否已转为输入。小部件有：\n\n| 属性/函数 | 说明  |\n| --- | --- |\n| `callback` | 小部件值变化时调用的函数 |\n| `last_y` | 小部件在节点中的垂直位置 |\n| `name` | 小部件名称（节点内唯一） |\n| `options` | Python 代码中指定的选项（如默认值、最小值、最大值） |\n| `type` | 小部件类型名称（见下文），小写 |\n| `value` | 当前小部件值。此属性有 get/set 方法 |\n\n### 小部件类型\n\n`app.widgets` 是当前已注册小部件类型的字典，键为类型名的大写。 Comfy 内置小部件类型包括直观的 `BOOLEAN`、`INT`、`FLOAT`， 还有 `STRING`（分单行和多行）、 `COMBO`（下拉列表选择）、`IMAGEUPLOAD`（用于加载图片节点）。 可通过在扩展中提供 `getCustomWidgets` 方法添加自定义小部件类型。\n\n### 关联小部件\n\n小部件也可以关联——如内置的 `seed` 和 `control_after_generate`。 关联小部件的 `.type = 'base_widget_type:base_widget_name'`；如 `control_after_generate` 可能有类型 `int:seed`。\n\n## Prompt\n\n当你在 Comfy 中点击”Queue Prompt”按钮时，会调用 `app.graphToPrompt()` 方法，将当前图转换为可发送到服务器的 prompt。 `app.graphToPrompt` 返回一个对象（下称 `prompt`），包含 `output` 和 `workflow` 两个属性。\n\n### output\n\n`prompt.output` 将图中每个节点的 `node_id` 映射为一个对象，包含两个属性：\n\n*   `prompt.output[node_id].class_type`，自定义节点类的唯一名称（在 Python 代码中定义）\n*   `prompt.output[node_id].inputs`，包含每个输入（或小部件）的值，是一个从输入名到以下内容的映射：\n    *   如果是小部件，则为选中的值\n    *   如果有连线，则为一个数组，内容为（`upstream_node_id`, `upstream_node_output_slot`）\n    *   如果是已转为输入但未连接的小部件，则为 undefined\n    *   其他未连接的输入不会出现在 `.inputs` 中\n\n### workflow\n\n`prompt.workflow` 包含以下属性：\n\n*   `config` - 额外配置项字典（默认空）\n*   `extra` - 包含工作流额外信息的字典。默认有：\n    *   `extra.ds` - 描述当前图视图（`scale` 和 `offset`）\n*   `groups` - 工作流中的所有分组\n*   `last_link_id` - 最后添加的连线 id\n*   `last_node_id` - 最后添加的节点 id\n*   `links` - 图中所有连线的列表。每项为五个整数和一个字符串的数组：\n    *   (`link_id`, `upstream_node_id`, `upstream_node_output_slot`, `downstream_node_id`, `downstream_node_input_slot`, `data type`)\n*   `nodes` - 图中所有节点的列表。每项为节点部分属性的映射，见[上文](#comfynode)\n    *   包含属性：`flags`、`id`、`inputs`、`mode`、`order`、`pos`、`properties`、`size`、`type`、`widgets_values`\n    *   另外，除非节点没有输出，还会有 `outputs` 属性，为该节点所有输出的列表，每项包含：\n        *   `name` - 输出名称\n        *   `type` - 输出数据类型\n        *   `links` - 从该输出出发的所有连线的 `link_id` 列表（无连接时为空数组或 null）\n        *   `shape` - 绘制输出时的形状（默认 3，表示圆点）\n        *   `slot_index` - 输出的槽编号\n*   `version` - LiteGraph 版本号（当前为 `0.4`）"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_toast",
  "markdown": "# Toast API - ComfyUI\n\nToast API 提供了一种向用户显示非阻塞通知消息的方式。这对于在不打断工作流的情况下提供反馈非常有用。\n\n## 基本用法\n\n### 简单 Toast\n\n```\n// 显示一个简单的信息提示\napp.extensionManager.toast.add({\n  severity: \"info\",\n  summary: \"信息\",\n  detail: \"操作已成功完成\",\n  life: 3000\n});\n```\n\n### Toast 类型\n\n```\n// 成功提示\napp.extensionManager.toast.add({\n  severity: \"success\",\n  summary: \"成功\",\n  detail: \"数据保存成功\",\n  life: 3000\n});\n\n// 警告提示\napp.extensionManager.toast.add({\n  severity: \"warn\",\n  summary: \"警告\",\n  detail: \"此操作可能导致问题\",\n  life: 5000\n});\n\n// 错误提示\napp.extensionManager.toast.add({\n  severity: \"error\",\n  summary: \"错误\",\n  detail: \"请求处理失败\",\n  life: 5000\n});\n```\n\n### Alert 辅助方法\n\n```\n// 快捷方式创建警告提示\napp.extensionManager.toast.addAlert(\"这是一条重要消息\");\n```\n\n### Toast 消息\n\n```\napp.extensionManager.toast.add({\n  severity?: \"success\" | \"info\" | \"warn\" | \"error\" | \"secondary\" | \"contrast\", // 消息严重级别（默认：\"info\"）\n  summary?: string,         // Toast 的简短标题\n  detail?: any,             // 详细消息内容\n  closable?: boolean,       // 用户是否可以关闭该提示（默认：true）\n  life?: number,            // 自动关闭前的持续时间（毫秒）\n  group?: string,           // 用于管理相关 Toast 的分组标识\n  styleClass?: any,         // 消息的样式类\n  contentStyleClass?: any   // 内容的样式类\n});\n```\n\n### Alert 辅助方法\n\n```\napp.extensionManager.toast.addAlert(message: string);\n```\n\n### 其他方法\n\n```\n// 移除指定的 toast\napp.extensionManager.toast.remove(toastMessage);\n\n// 移除所有 toast\napp.extensionManager.toast.removeAll();\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_settings",
  "markdown": "# 设置 - ComfyUI\n\n你可以为 ComfyUI 提供一个设置对象，这些设置会显示在用户打开 ComfyUI 设置面板时。\n\n## 基本操作\n\n### 添加一个设置项\n\n```\nimport { app } from \"../../scripts/app.js\";\n\napp.registerExtension({\n    name: \"My Extension\",\n    settings: [\n        {\n            id: \"example.boolean\",\n            name: \"示例布尔设置\",\n            type: \"boolean\",\n            defaultValue: false,\n        },\n    ],\n});\n```\n\n`id` 必须在所有扩展中唯一，并将用于获取设置值。 如果你没有[指定分类](#categories)，那么 `id` 会通过 `.` 分割来决定它在设置面板中的显示位置。\n\n*   如果你的 `id` 不包含 `.`，它会显示在”其他”分类下，并以你的 `id` 作为分组标题。\n*   如果你的 `id` 至少包含一个 `.`，最左边的部分会作为设置分类，第二部分作为分组标题，后续部分会被忽略。\n\n### 读取设置项\n\n```\nimport { app } from \"../../scripts/app.js\";\n\nif (app.extensionManager.setting.get('example.boolean')) {\n    console.log(\"设置已启用。\");\n} else {\n    console.log(\"设置已禁用。\");\n}\n```\n\n### 响应设置变化\n\n当用户在设置面板中更改设置时，`onChange()` 事件处理器会被立即调用。 每次页面加载、扩展注册时也会调用。\n\n```\n{\n    id: \"example.boolean\",\n    name: \"示例布尔设置\",\n    type: \"boolean\",\n    defaultValue: false,\n    onChange: (newVal, oldVal) => {\n        console.log(`设置从 ${oldVal} 变为 ${newVal}`);\n    },\n}\n```\n\n### 写入设置项\n\n```\nimport { app } from \"../../scripts/app.js\";\n\ntry {\n    await app.extensionManager.setting.set(\"example.boolean\", true);\n} catch (error) {\n    console.error(`更改设置时出错: ${error}`);\n}\n```\n\n### 额外配置\n\n设置类型基于 [PrimeVue](https://primevue.org/) 组件。 在 `attrs` 字段中添加 PrimeVue 文档中描述的属性即可为 ComfyUI 设置项配置更多参数。 例如，下面为数字输入框添加了增减按钮：\n\n```\n{\n    id: \"example.number\",\n    name: \"示例数字设置\",\n    type: \"number\",\n    defaultValue: 0,\n    attrs: {\n        showButtons: true,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`设置从 ${oldVal} 变为 ${newVal}`);\n    },\n}\n```\n\n## 类型\n\n### 布尔值（Boolean）\n\n显示一个开关。 基于 [ToggleSwitch PrimeVue 组件](https://primevue.org/toggleswitch/)。\n\n```\n{\n    id: \"example.boolean\",\n    name: \"示例布尔设置\",\n    type: \"boolean\",\n    defaultValue: false,\n    onChange: (newVal, oldVal) => {\n        console.log(`设置从 ${oldVal} 变为 ${newVal}`);\n    },\n}\n```\n\n### 文本（Text）\n\n自由文本输入。 基于 [InputText PrimeVue 组件](https://primevue.org/inputtext/)。\n\n```\n{\n    id: \"example.text\",\n    name: \"示例文本设置\",\n    type: \"text\",\n    defaultValue: \"Foo\",\n    onChange: (newVal, oldVal) => {\n        console.log(`设置从 ${oldVal} 变为 ${newVal}`);\n    },\n}\n```\n\n### 数字（Number）\n\n用于输入数字。 如需允许小数位，请将 `maxFractionDigits` 属性设置为大于 0 的数字。 基于 [InputNumber PrimeVue 组件](https://primevue.org/inputnumber/)。\n\n```\n{\n    id: \"example.number\",\n    name: \"示例数字设置\",\n    type: \"number\",\n    defaultValue: 42,\n    attrs: {\n        showButtons: true,\n        maxFractionDigits: 1,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`设置从 ${oldVal} 变为 ${newVal}`);\n    },\n}\n```\n\n### 滑块（Slider）\n\n允许用户直接输入数字或通过滑块选择。 基于 [Slider PrimeVue 组件](https://primevue.org/slider/)。不支持区间。\n\n```\n{\n    id: \"example.slider\",\n    name: \"示例滑块设置\",\n    type: \"slider\",\n    attrs: {\n        min: -10,\n        max: 10,\n        step: 0.5,\n    },\n    defaultValue: 0,\n    onChange: (newVal, oldVal) => {\n        console.log(`设置从 ${oldVal} 变为 ${newVal}`);\n    },\n}\n```\n\n### 下拉选择（Combo）\n\n允许用户从下拉列表中选择。 你可以用纯字符串或带 `text` 和 `value` 字段的对象提供选项。如果只提供字符串，则会同时作为显示和实际值。 通过 `editable: true` 属性允许用户输入自定义内容，通过 `filter: true` 属性允许搜索。 基于 [Select PrimeVue 组件](https://primevue.org/select/)。不支持分组。\n\n```\n{\n    id: \"example.combo\",\n    name: \"示例下拉设置\",\n    type: \"combo\",\n    defaultValue: \"first\",\n    options: [\n        { text: \"我的第一个选项\", value: \"first\" },\n        \"我的第二个选项\",\n    ],\n    attrs: {\n        editable: true,\n        filter: true,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`设置从 ${oldVal} 变为 ${newVal}`);\n    },\n}\n```\n\n### 颜色（Color）\n\n允许用户通过颜色选择器选择颜色或输入十六进制颜色值。 注意格式必须为六位十六进制，不支持三位简写。 基于 [ColorPicker PrimeVue 组件](https://primevue.org/colorpicker/)。\n\n```\n{\n    id: \"example.color\",\n    name: \"示例颜色设置\",\n    type: \"color\",\n    defaultValue: \"ff0000\",\n    onChange: (newVal, oldVal) => {\n        console.log(`设置从 ${oldVal} 变为 ${newVal}`);\n    },\n}\n```\n\n### 图片（Image）\n\n允许用户上传图片。 设置会以 [data URL](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data) 格式保存。 基于 [FileUpload PrimeVue 组件](https://primevue.org/fileupload/)。\n\n```\n{\n    id: \"example.image\",\n    name: \"示例图片设置\",\n    type: \"image\",\n    onChange: (newVal, oldVal) => {\n        console.log(`设置从 ${oldVal} 变为 ${newVal}`);\n    },\n}\n```\n\n### 隐藏（Hidden）\n\n隐藏设置不会显示在设置面板，但你可以在代码中读写它们。\n\n```\n{\n    id: \"example.hidden\",\n    name: \"示例隐藏设置\",\n    type: \"hidden\",\n}\n```\n\n## 其他\n\n### 分类（Categories）\n\n你可以通过 `category` 字段单独指定设置的分类。 这样可以在不更改 `id` 的情况下调整分类和命名，不会丢失用户已设置的值。\n\n```\n{\n    id: \"example.boolean\",\n    name: \"示例布尔设置\",\n    type: \"boolean\",\n    defaultValue: false,\n    category: [\"分类名称\", \"分组标题\", \"设置标签\"],\n}\n```\n\n### 工具提示（Tooltips）\n\n你可以通过 `tooltip` 字段添加额外的上下文帮助。这会在字段名后显示一个小的 ℹ︎ 图标，用户悬停时会显示帮助文本。\n\n```\n{\n    id: \"example.boolean\",\n    name: \"示例布尔设置\",\n    type: \"boolean\",\n    defaultValue: false,\n    tooltip: \"这是一些有用的提示信息\",\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/image/cosmos/cosmos-predict2-t2i",
  "markdown": "# Cosmos Predict2 文生图 ComfyUI 官方示例\n\nCosmos-Predict2 是由 NVIDIA 推出的新一代物理世界基础模型，专为物理 AI 场景下的高质量视觉生成与预测任务设计。 该模型具备极高的物理准确性、环境交互性和细节还原能力，能够真实模拟复杂的物理现象与动态场景。 Cosmos-Predict2 支持文本到图像（Text2Image）和视频到世界（Video2World）等多种生成方式，广泛应用于工业仿真、自动驾驶、城市规划、科学研究等领域，是推动智能视觉与物理世界深度融合的重要基础工具。 GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2) huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959) 本篇指南将引导你完成在 ComfyUI 中 **文生图** 工作流程。 对于视频生成部分，请参考下面的部分\n\n[\n\n使用 Cosmos-Predict2 的进行视频生成\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/video/cosmos/cosmos-predict2-video2world)\n\n## Cosmos Predict2 Video2World 工作流\n\n对于 2B 版本，在实际运行时，需要 10GB 的显存\n\n### 1.下载工作流文件\n\n![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/cosmos/predict2/cosmos_predict2_2B_t2i.png)\n\n### 2.手动模型安装\n\n**Diffusion model**\n\n*   [cosmos\\_predict2\\_2B\\_t2i.safetensors](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged/resolve/main/cosmos_predict2_2B_t2i.safetensors)\n\n其它权重请访问 [Cosmos\\_Predict2\\_repackaged](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged) 进行下载 **Text encoder** [oldt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/resolve/main/text_encoders/oldt5_xxl_fp8_e4m3fn_scaled.safetensors) **VAE** [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors) 文件保存位置\n\n```\n📂 ComfyUI/\n├──📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └─── cosmos_predict2_2B_t2i.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── oldt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── 📂 vae/\n│       └──  wan_2.1_vae.safetensors\n```\n\n### 3\\. 按步骤完成工作流运行\n\n![工作流使用步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/cosmos/cosmos_predict2_2B_t2i_step_guide.jpg) 请参照图片序号进行逐步确认，来保证对应工作流的顺利运行\n\n1.  确保 `Load Diffusion Model` 节点加载了 `cosmos_predict2_2B_t2i.safetensors`\n2.  确保 `Load CLIP` 节点加载了 `oldt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`\n4.  在 `EmptySD3LatentImage` 设置图片的尺寸\n5.  在 `ClipTextEncode` 节点中修改提示词\n6.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行文生图\n7.  生成完成后对应的图片会自动保存到 `ComfyUI/output/` 目录下，你也可以在 `save image` 节点中预览或者调整保存位置"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/image/hidream/hidream-e1",
  "markdown": "# ComfyUI 原生版本 HiDream-E1, E1.1 工作流示例\n\n![HiDream-E1 演示](https://raw.githubusercontent.com/HiDream-ai/HiDream-E1/refs/heads/main/assets/demo.jpg) HiDream-E1 是智象未来(HiDream-ai) 正式开源的交互式图像编辑大模型，是基于 HiDream-I1 构建的图像编辑模型。 可以使用自然语言来实现对图像的编辑，该模型基于 [MIT 许可证](https://github.com/HiDream-ai/HiDream-E1?tab=MIT-1-ov-file) 发布，支持用于个人项目、科学研究以及商用。 通过与此前发布的 [hidream-i1](https://docs.comfy.org/zh-CN/tutorials/image/hidream/hidream-i1)的共同组合，实现了 **从图像生成到编辑的** 创作能力。\n\n| 名称  | 更新时间 | 推理步数 | 分辨率 | HuggingFace 仓库 |\n| --- | --- | --- | --- | --- |\n| HiDream-E1-Full | 2025-4-28 | 28  | 768x768 | 🤗 [HiDream-E1-Full](https://huggingface.co/HiDream-ai/HiDream-E1-Full) |\n| HiDream-E1.1 | 2025-7-16 | 28  | 动态（1百万像素） | 🤗 [HiDream-E1.1](https://huggingface.co/HiDream-ai/HiDream-E1-1) |\n\n[HiDream E1 - Github](https://github.com/HiDream-ai/HiDream-E1)\n\n本篇指南涉及的所有模型你都可以在[这里](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files)找到, E1, E1.1 除了 Diffusion model 之外都是用相同的模型 我们在对应的工作流文件中也以包含了对应的模型信息，你可以选择手动下载模型保存，或者在加载工作流后按工作流提示进行下载，推荐使用 E1.1 这个模型的运行对显存占用要求极高，具体显存占用请参考对应部分的说明 **Diffusion Model** 你不用同时下载这两个模型，由于 E1.1 是基于 E1 的迭代版本，在实际测试中它的质量和效果较 E1 都有较大提升\n\n*   [hidream\\_e1\\_1\\_bf16.safetensors(推荐)](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_1_bf16.safetensors) 34.2GB\n*   [hidream\\_e1\\_full\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_full_bf16.safetensors) 34.2GB\n\n**Text Encoder**：\n\n*   [clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors) 236.12MB\n*   [clip\\_g\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors) 1.29GB\n*   [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) 4.8GB\n*   [llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors) 8.46GB\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) 319.77MB\n\n> 这个是 Flux 的 VAE 模型，如果你之前使用过 Flux 的工作流，你可能已经下载了这个文件。\n\n文件保存位置\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 text_encoders/\n│   │   ├─── clip_l_hidream.safetensors\n│   │   ├─── clip_g_hidream.safetensors\n│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors\n│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors\n│   └── 📂 vae/\n│   │   └── ae.safetensors\n│   └── 📂 diffusion_models/\n│       ├── hidream_e1_1_bf16.safetensors\n│       └── hidream_e1_full_bf16.safetensors\n```\n\n## HiDream E1.1 ComfyUI 原生工作流示例\n\nE1.1 是于 2025年7月16日更新迭代的版本, 这个版本支持动态一百万分辨率，在工作流中使用了 `Scale Image to Total Pixels` 节点来将输入图片动态调整为 1百万像素\n\n### 1\\. HiDream E1.1 工作流及相关素材\n\n下载下面的图片并拖入 ComfyUI 已加载对应工作流及模型 ![HiDream E1.1 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/hidream_e1_1.png) 下载下面的图片作为输入 ![HiDream E1.1 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/input.webp) \n\n### 2\\. 按步骤完成 HiDream-e1 工作流运行\n\n![hidream_e1_1_guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/hidream/hidream-e1-1-guide.jpg) 按步骤完成工作流的运行\n\n1.  确保`Load Diffusion Model` 节点加载了 `hidream_e1_1_bf16.safetensors` 模型\n2.  确保`QuadrupleCLIPLoader` 中四个对应的 text encoder 被正确加载\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  确保`Load VAE` 节点中使用的是 `ae.safetensors` 文件\n4.  在 `Load Image` 节点中加载提供的输入或你需要的图片\n5.  在`Empty Text Encoder(Positive)` 节点中输入 **想要对图片进行的修改**\n6.  在`Empty Text Encoder(Negative)` 节点中输入 **不想要在画面中出现的内容**\n7.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片生成\n\n### 3\\. 工作流补充说明\n\n*   由于 HiDream E1.1 支持的是动态总像素为一百万像素输入，所以工作流使用了 `Scale Image to Total Pixels` 来将所有输入图片进行处理转化，这可能会导致比例尺寸相对于输入图片会有所变化\n*   使用 fp16 版本的模型，在实际测试过程中，在 A100 40GB 和 4090D 24GB 时使用完整版本时会 Out of memory，所以工作流默认设置了使用 `fp8_e4m3fn_fast` 来进行推理\n\n## HiDream E1 ComfyUI 原生 工作流示例\n\nE1 是于 2025 年 4 月 28 日发布的，这个模型只支持 768\\*768 的分辨率\n\n### 1\\. HiDream-e1 工作流及相关素材\n\n#### 1.1 下载工作流文件\n\n请下载下面的图片并拖入 ComfyUI 中，工作流已包含模型下载信息，加载后将会提示你进行对应的模型下载。 ![ComfyUI 原生 HiDream-e1 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/hidream_e1_full.png)\n\n#### 1.2 下载输入图片\n\n请下载下面的图片，我们将用于输入 ![ComfyUI 原生 HiDream-e1 工作流 输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/input.webp)\n\n### 2\\. 按步骤完成 HiDream-e1 工作流运行\n\n![hidream_e1_full_step_guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg) 按步骤完成工作流的运行\n\n1.  确保`Load Diffusion Model` 节点加载了 `hidream_e1_full_bf16.safetensors` 模型\n2.  确保`QuadrupleCLIPLoader` 中四个对应的 text encoder 被正确加载\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  确保`Load VAE` 节点中使用的是 `ae.safetensors` 文件\n4.  在 `Load Image` 节点中加载我们之前下载的输入图片\n5.  （重要）在`Empty Text Encoder(Positive)` 节点中输入 **想要修改的画面的提示词**\n6.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片生成\n\n### ComfyUI HiDream-e1 工作流补充说明\n\n*   可能需要修改多次提示词或者进行多次的生成才能得到较好的结果\n*   这个模型在改变图片风格上比较难保持一致性，需要尽可能完善提示词\n*   由于模型支持的是 768\\*768 的分辨率，在实际测试中调整过其它尺寸，在其它尺寸下图像表现能力不佳，甚至差异较大"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_sidebar_tabs",
  "markdown": "# 侧边栏标签页 - ComfyUI\n\n侧边栏标签页 API 允许扩展为 ComfyUI 界面的侧边栏添加自定义标签页。这对于添加需要持续可见性和快速访问的功能非常有用。\n\n## 基本用法\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"customSidebar\",\n  icon: \"pi pi-compass\",\n  title: \"自定义标签页\",\n  tooltip: \"我的自定义侧边栏标签页\",\n  type: \"custom\",\n  render: (el) => {\n    el.innerHTML = '<div>这是我的自定义侧边栏内容</div>';\n  }\n});\n```\n\n## 标签页配置\n\n每个标签页需要以下属性：\n\n```\n{\n  id: string,              // 标签页的唯一标识符\n  icon: string,            // 标签按钮的图标类名\n  title: string,           // 标签页标题文本\n  tooltip?: string,        // 悬停时的提示文本（可选）\n  type: string,            // 标签页类型（通常为 \"custom\"）\n  render: (element) => void // 用于填充标签页内容的函数\n}\n```\n\n`render` 函数会接收一个 DOM 元素，你应在其中插入标签页的内容。\n\n## 图标选项\n\n侧边栏标签页图标可使用多种图标集：\n\n*   PrimeVue 图标：`pi pi-[icon-name]`（如 `pi pi-home`）\n*   Material Design 图标：`mdi mdi-[icon-name]`（如 `mdi mdi-robot`）\n*   Font Awesome 图标：`fa-[style] fa-[icon-name]`（如 `fa-solid fa-star`）\n\n使用这些图标前请确保已加载相应的图标库。\n\n## 有状态标签页示例\n\n你可以创建带有状态的标签页：\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"statefulTab\",\n  icon: \"pi pi-list\",\n  title: \"笔记\",\n  type: \"custom\",\n  render: (el) => {\n    // 创建元素\n    const container = document.createElement('div');\n    container.style.padding = '10px';\n    \n    const notepad = document.createElement('textarea');\n    notepad.style.width = '100%';\n    notepad.style.height = '200px';\n    notepad.style.marginBottom = '10px';\n    \n    // 加载已保存内容（如有）\n    const savedContent = localStorage.getItem('comfyui-notes');\n    if (savedContent) {\n      notepad.value = savedContent;\n    }\n    \n    // 自动保存内容\n    notepad.addEventListener('input', () => {\n      localStorage.setItem('comfyui-notes', notepad.value);\n    });\n    \n    // 组装 UI\n    container.appendChild(notepad);\n    el.appendChild(container);\n  }\n});\n```\n\n## 使用 React 组件\n\n你可以在侧边栏标签页中挂载 React 组件：\n\n```\n// 在你的扩展中引入 React 依赖\nimport React from \"react\";\nimport ReactDOM from \"react-dom/client\";\n\n// 注册带有 React 内容的侧边栏标签页\napp.extensionManager.registerSidebarTab({\n  id: \"reactSidebar\",\n  icon: \"mdi mdi-react\",\n  title: \"React 标签页\",\n  type: \"custom\",\n  render: (el) => {\n    const container = document.createElement(\"div\");\n    container.id = \"react-sidebar-container\";\n    el.appendChild(container);\n    \n    // 定义一个简单的 React 组件\n    function SidebarContent() {\n      const [count, setCount] = React.useState(0);\n      \n      return (\n        <div style={{ padding: \"10px\" }}>\n          <h3>React 侧边栏</h3>\n          <p>计数：{count}</p>\n          <button onClick={() => setCount(count + 1)}>\n            递增\n          </button>\n        </div>\n      );\n    }\n    \n    // 挂载 React 组件\n    ReactDOM.createRoot(container).render(\n      <React.StrictMode>\n        <SidebarContent />\n      </React.StrictMode>\n    );\n  }\n});\n```\n\n如需查看将 React 应用集成为侧边栏标签页的真实案例，请参考 [ComfyUI-Copilot 项目（GitHub）](https://github.com/AIDC-AI/ComfyUI-Copilot)。\n\n## 动态内容更新\n\n你可以根据图变化动态更新侧边栏内容：\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"dynamicSidebar\",\n  icon: \"pi pi-chart-line\",\n  title: \"统计信息\",\n  type: \"custom\",\n  render: (el) => {\n    const container = document.createElement('div');\n    container.style.padding = '10px';\n    el.appendChild(container);\n    \n    // 更新统计信息的函数\n    function updateStats() {\n      const stats = {\n        nodes: app.graph._nodes.length,\n        connections: Object.keys(app.graph.links).length\n      };\n      \n      container.innerHTML = `\n        <h3>工作流统计</h3>\n        <ul>\n          <li>节点数：${stats.nodes}</li>\n          <li>连接数：${stats.connections}</li>\n        </ul>\n      `;\n    }\n    \n    // 初始更新\n    updateStats();\n    \n    // 监听图变化\n    const api = app.api;\n    api.addEventListener(\"graphChanged\", updateStats);\n    \n    // 标签页销毁时清理监听器\n    return () => {\n      api.removeEventListener(\"graphChanged\", updateStats);\n    };\n  }\n});\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/3d",
  "markdown": "# ComfyUI 3D 设置 - ComfyUI\n\n这部分的设置主要用于控制 ComfyUI 中 3D 相关组件的初始化设置，包括相机、光照、场景等，在创建新的3D组件时，会根据这些设置进行初始化，在创建后，这些设置仍然可以单独调整。\n\n## 相机\n\n### 摄像机类型\n\n*   **选项**:\n    *   `perspective` (透视)\n    *   `orthographic` (正交)\n*   **功能**: 控制创建新的3D组件时，默认的相机是透视还是正交。这个默认设置仍然可以在创建后在节点的画布中单独调整\n\n![摄像机类型](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/camera_type.jpg)\n\n## 光照\n\n这部分菜单用于设置 3D 相关组件的光照设置的预设, 对应的设置在 ComfyUI 的 3D 设置中同样可以进行修改 ![光照](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/light.jpg)\n\n### 光照调整步长\n\n*   **默认值**: 0.5\n*   **功能**: 控制在3D场景中调整光照强度时的步长。较小的步长值可以实现更精细的光照调整，较大的值则会使每次调整的变化更加明显\n\n### 光照强度下限\n\n*   **默认值**: 1\n*   **功能**: 设置3D场景允许的最小光照强度值。此项定义在调整任何3D控件照明时可设定的最低亮度\n\n### 最大光照强度\n\n*   **默认值**: 10\n*   **功能**: 设置3D场景允许的最大光照强度值。此项定义了在调整任何3D控件照明时可设定的最高亮度上限\n\n### 初始光照强度\n\n*   **默认值**: 3\n*   **功能**: 设置3D场景中灯光的默认亮度级别。该数值决定新建3D控件时灯光照亮物体的强度，但每个控件在创建后都可以单独调整\n\n## 场景\n\n这个设置允许你使用设置默认的 3D 节点的偏好设置\n\n### 初始背景颜色\n\n*   **作用**: 控制3D场景的默认背景颜色。此设置决定新建3D组件时的背景外观，但每个组件在创建后都可以单独调整\n*   **默认值**: `282828` (深灰色)\n\n修改背景颜色，同样可以在画布中进行调整 ![初始背景颜色](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/background_color.jpg) \n\n### 显示预览\n\n*   **作用**: 控制创建新的3D组件时是否默认显示预览屏幕。此默认设置在创建后仍可为每个组件单独切换\n*   **默认值**: true (开启)\n\n![预览](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/hide_preview.jpg)\n\n### 显示网格\n\n*   **作用**: 控制创建新的3D组件时是否默认显示网格。此默认设置在创建后仍可为每个组件单独切换\n*   **默认值**: true (开启)\n\n![隐藏网格](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/hide_grid.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/ltxv",
  "markdown": "# LTX-Video - ComfyUI\n\n## 快速入门\n\n[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) 是 Lightricks 开发的高效视频生成模型。 使用该模型的关键是提供详细的长描述提示词。 请下载 [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true) 文件并放入 `ComfyUI/models/checkpoints` 目录。 若尚未下载 [t5xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true) 文件，请将其放入 `ComfyUI/models/text_encoders` 目录。\n\n如果在加载下面的工作流文件时，你发现存在节点缺失，可能是因为以下情况：\n\n1.  你使用的 ComfyUI 版本不是最新的开发（nightly）版本。\n2.  你使用的 ComfyUI 版本是稳定（release）版本或桌面版（desktop）版本（不包含最新的功能更新）。\n3.  你使用的 ComfyUI 版本是最新的 commit 版本，但在启动过程中部分节点导入失败了。\n\n请先确保你已经成功更新 ComfyUI 到最新的开发（nightly）版本, 请查看：[如何更新 ComfyUI](https://docs.comfy.org/zh-CN/installation/update_comfyui) 部分了解如何更新 ComfyUI。\n\n## 多帧控制\n\n通过系列图像控制视频生成。可下载输入图像：[起始帧](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) 和 [结束帧](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png)。 ![LTX-Video 多帧控制工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/workflow.webp)\n\n## 图生视频\n\n通过首帧图像控制视频生成：[示例首帧](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png)。 ![LTX-Video 图生视频工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/workflow.webp)\n\n## 文生视频\n\n![LTX-Video 文生视频工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/t2v.webp)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/depth-t2i-adapter",
  "markdown": "# ComfyUI Depth T2I Adapter 使用示例\n\n[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) 是由 ​[腾讯ARC实验室](https://github.com/TencentARC) 开发的轻量级适配器，用于增强文本到图像生成模型（如Stable Diffusion）的结构、颜色和风格控制能力。 它通过外部条件（如边缘检测图、深度图、草图或颜色参考图）与模型内部特征对齐，实现高精度控制，无需修改原模型结构。其参数仅约77M（体积约300MB），推理速度比 [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly) 快约3倍，支持多条件组合（如草图+颜色网格）。应用场景包括线稿转图像、色彩风格迁移、多元素场景生成等。\n\n### T2I Adapter 与 ControlNet 的对比\n\n虽然功能相似，但两者在实现和应用上有明显区别：\n\n1.  **轻量级设计**：T2I Adapter 参数量更少，占用内存更小\n2.  **推理速度**：T2I Adapter 通常比 ControlNet 快约3倍\n3.  **控制精度**：ControlNet 在某些场景下控制更精确，而 T2I Adapter 更适合轻量级控制\n4.  **多条件组合**：T2I Adapter 在多条件组合时资源占用优势更明显\n\n### T2I Adapter 主要类型\n\nT2I Adapter 提供多种类型以控制不同方面：\n\n*   **深度 (Depth)**：控制图像的空间结构和深度关系\n*   **线稿 (Canny/Sketch)**：控制图像的边缘和线条\n*   **关键点 (Keypose)**：控制人物姿态和动作\n*   **分割 (Seg)**：通过语义分割控制场景布局\n*   **颜色 (Color)**：控制图像的整体配色方案\n\n在 ComfyUI 中，使用 T2I Adapter 与 [ControlNet](https://docs.comfy.org/zh-CN/tutorials/controlnet/controlnet) 的界面和工作流相似。在本篇示例中，我们将以深度 T2I Adapter 控制室内场景为例，展示其使用方法。 ![ComfyUI Depth T2I Adapter 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)\n\n## 深度 T2I Adapter 应用价值\n\n深度图（Depth Map）在图像生成中有多种重要应用：\n\n1.  **空间布局控制**：准确描述三维空间结构，适用于室内设计、建筑可视化\n2.  **物体定位**：控制场景中物体的相对位置和大小，适用于产品展示、场景构建\n3.  **透视关系**：维持合理的透视和比例，适用于风景、城市场景生成\n4.  **光影布局**：基于深度信息的自然光影分布，增强真实感\n\n我们将以室内设计为例，展示深度 T2I Adapter 的使用方法，但这些技巧也适用于其他应用场景。\n\n## ComfyUI Depth T2I Adapter工作流示例讲解\n\n### 1\\. Depth T2I Adapter 工作流素材\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - Depth T2I Adapter](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)\n\n请下载下面的图片，我们将会将它作为输入 ![ComfyUI 室内深度图](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)\n\n### 2\\. 模型安装\n\n*   [interiordesignsuperm\\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [t2iadapter\\_depth\\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── interiordesignsuperm_v2.safetensors\n│   └── controlnet/\n│       └── t2iadapter_depth_sd15v2.pth\n```\n\n### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI 工作流 - Depth T2I Adapter 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg)\n\n1.  确保`Load Checkpoint`可以加载 **interiordesignsuperm\\_v2.safetensors**\n2.  确保`Load ControlNet`可以加载 **t2iadapter\\_depth\\_sd15v2.pth**\n3.  在`Load Image`中点击`Upload` 上传之前提供的输入图片\n4.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n\n## T2I Adapter 通用使用技巧\n\n### 输入图像质量优化\n\n无论应用场景如何，高质量的输入图像都是成功使用 T2I Adapter 的关键：\n\n1.  **对比度适中**：控制图像（如深度图、线稿）应有明确的对比，但不要过度极端\n2.  **清晰的边界**：确保主要结构和元素边界在控制图像中清晰可辨\n3.  **噪点控制**：尽量避免控制图像中有过多噪点，特别是深度图和线稿\n4.  **合理的布局**：控制图像应当具有合理的空间布局和元素分布\n\n## T2I Adapter 的使用特点\n\nT2I Adapter 的一大优势是可以轻松组合多个条件，实现复杂的控制效果：\n\n1.  **深度 + 边缘**：控制空间布局的同时保持结构边缘清晰，适用于建筑、室内设计\n2.  **线稿 + 颜色**：控制形状的同时指定配色方案，适用于角色设计、插画\n3.  **姿态 + 分割**：控制人物动作的同时定义场景区域，适用于复杂叙事场景\n\nT2I Adapter 之间的混合，或与其他控制方法（如ControlNet、区域提示词等）的组合，可以进一步扩展创作可能性。要实现混合，只需按照与 [混合 ControlNet](https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets) 相同的方式，通过链式连接多个 `Apply ControlNet` 节点即可。\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/3d/hunyuan3D-2",
  "markdown": "# ComfyUI Hunyuan3D-2 示例 - ComfyUI\n\n## 混元3D 2.0 简介\n\n ![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif) ![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif) [混元3D 2.0](https://github.com/Tencent/Hunyuan3D-2) 是腾讯推出的开源 3D 资产生成模型，可以通过文本、图像或草图生成带有高分辨率纹理贴图的高保真 3D 模型。 混元3D 2.0采用两阶段生成，首先采用生成无纹理的几何模型，再合成高分辨率的纹理贴图，有效分离了形状和纹理生成的复杂性，下面是混元3D 2.0的两个核心组件:\n\n1.  **几何生成模型（Hunyuan3D-DiT）**：基于流扩散的Transformer架构，生成无纹理的几何模型，可精准匹配输入条件。\n2.  **纹理生成模型（Hunyuan3D-Paint）**：结合几何条件和多视图扩散技术，为模型添加高分辨率纹理，支持PBR材质。\n\n**主要优势**\n\n*   **高精度生成**：几何结构锐利，纹理色彩丰富，支持PBR材质生成，实现接近真实的光影效果。\n*   **多样化使用方式**：提供代码调用、Blender插件、Gradio应用及官网在线体验，适合不同用户需求。\n*   **轻量化与兼容性**：Hunyuan3D-2mini模型仅需5GB显存，标准版本形状生成需6GB显存，完整流程（形状+纹理）仅需12GB显存。\n\n近期（2025 年 3 月 18 日），混元3D 2.0 还提供多视角形状生成模型（Hunyuan3D-2mv），支持从不同视角输入生成更精细的几何结构。 在本示例中包含三个工作流：\n\n*   使用 Hunyuan3D-2mv 配合多个视图输入生成3D模型\n*   使用 Hunyuan3D-2mv-turbo 配合多个视图输入生成3D模型\n*   使用 Hunyuan3D-2 配合单个视图输入生成3D模型\n\nHunyuan3D-2mv 工作流中，我们将使用多视角的图片来生成3D模型，另外多个视角的图片在这个工作流中并不是必须的，你可以只输入 `front` 视角的图片来生成3D模型。\n\n### 1\\. 工作流\n\n请下载下面的图片，并拖入 ComfyUI 以加载工作流, ![Hunyuan3D-2mv workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/hunyuan-3d-multiview-elf.webp) 下载下面的图片，同时我们将使用这些图片作为图片输入\n\n![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/front.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/left.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/back.png)\n\n### 2\\. 手动安装模型\n\n下载下面的模型，并保存到对应的 ComfyUI 文件夹\n\n*   hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) 下载后可重命名为 `hunyuan3d-dit-v2-mv.safetensors`\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── hunyuan3d-dit-v2-mv.safetensors  // 重命名后的文件\n```\n\n### 3\\. 按步骤运行工作流\n\n![ComfyUI hunyuan3d_2mv](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg)\n\n1.  确保 Image Only Checkpoint Loader(img2vid model) 加载了我们下载并重命名的 `hunyuan3d-dit-v2-mv.safetensors` 模型\n2.  在 `Load Image` 节点的各个视角中加载了对应视角的图片\n3.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n如果你需要增加更多的视角，请确保 `Hunyuan3Dv2ConditioningMultiView` 节点中加载了其它视角的图片，并确保在 `Load Image` 节点中加载了对应视角的图片。\n\n## 使用 Hunyuan3D-2mv-turbo 工作流\n\nHunyuan3D-2mv-turbo 工作流中，我们将使用 Hunyuan3D-2mv-turbo 模型来生成3D模型，这个模型是 Hunyuan3D-2mv 的分步蒸馏（Step Distillation）版本，可以更快地生成3D模型，在这个版本的工作流中我们设置 `cfg` 为 1.0 并添加 `flux guidance` 节点来控制 `distilled cfg` 的生成。\n\n### 1\\. 工作流\n\n请下载下面的图片，并拖入 ComfyUI 以加载工作流, ![Hunyuan3D-2mv-turbo workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/hunyuan-3d-turbo.webp) 我们将使用下面的图片作为多视角的输入\n\n![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/front.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/right.png)\n\n### 2\\. 手动安装模型\n\n下载下面的模型，并保存到对应的 ComfyUI 文件夹\n\n*   hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) 下载后可重命名为 `hunyuan3d-dit-v2-mv-turbo.safetensors`\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── hunyuan3d-dit-v2-mv-turbo.safetensors  // 重命名后的文件\n```\n\n### 3\\. 按步骤运行工作流\n\n![ComfyUI hunyuan3d_2mv_turbo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg)\n\n1.  确保 `Image Only Checkpoint Loader(img2vid model)` 节点加载了我们重命名后的 `hunyuan3d-dit-v2-mv-turbo.safetensors` 模型\n2.  在 `Load Image` 节点的各个视角中加载了对应视角的图片\n3.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n## 使用 Hunyuan3D-2 单视图工作流\n\nHunyuan3D-2 工作流中，我们将使用 Hunyuan3D-2 模型来生成3D模型，这个模型不是一个多视角的模型，在这个工作流中，我们使用`Hunyuan3Dv2Conditioning` 节点替换掉 `Hunyuan3Dv2ConditioningMultiView` 节点。\n\n### 1\\. 工作流\n\n请下载下面的图片，并拖入 ComfyUI 以加载工作流 ![Hunyuan3D-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d-non-multiview-train.webp) 同时我们将使用这张图片作为图片输入 ![ComfyUI Hunyuan 3D 2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan_3d_v2_non_multiview_train.png)\n\n### 2\\. 手动安装模型\n\n下载下面的模型，并保存到对应的 ComfyUI 文件夹\n\n*   hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) 下载后可重命名为 `hunyuan3d-dit-v2.safetensors`\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── hunyuan3d-dit-v2.safetensors  // 重命名后的文件\n```\n\n### 3\\. 按步骤运行工作流\n\n![ComfyUI hunyuan3d_2](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg)\n\n1.  确保 `Image Only Checkpoint Loader(img2vid model)` 节点加载了我们重命名后的 `hunyuan3d-dit-v2.safetensors` 模型\n2.  在 `Load Image` 节点中加载了对应视角的图片\n3.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n## 社区资源\n\n下面是 Hunyuan3D-2 的相关的 ComfyUI 社区资源\n\n*   [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)\n*   [Kijai/Hunyuan3D-2\\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)\n*   [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)\n\n## 混元3D 2.0 开源模型系列\n\n目前混元3D 2.0 开源了多个模型，覆盖了完整的3D生成流程，你可以访问 [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) 了解更多。 **Hunyuan3D-2mini 系列**\n\n| 模型  | 描述  | 日期  | 参数  | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-mini | Mini 图像到形状模型 | 2025-03-18 | 0.6B | [前往](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini) |\n\n**Hunyuan3D-2mv 系列**\n\n| 模型  | 描述  | 日期  | 参数  | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-mv-Fast | 指导蒸馏版本，可以将 DIT 推理时间减半 | 2025-03-18 | 1.1B | [前往](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast) |\n| Hunyuan3D-DiT-v2-mv | 多视角图像到形状模型，适合需要用多个角度理解场景的 3D 创作 | 2025-03-18 | 1.1B | [前往](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv) |\n\n**Hunyuan3D-2 系列**\n\n| 模型  | 描述  | 日期  | 参数  | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-0-Fast | 指导蒸馏模型 | 2025-02-03 | 1.1B | [前往](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast) |\n| Hunyuan3D-DiT-v2-0 | 图像到形状模型 | 2025-01-21 | 1.1B | [前往](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0) |\n| Hunyuan3D-Paint-v2-0 | 纹理生成模型 | 2025-01-21 | 1.3B | [前往](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0) |\n| Hunyuan3D-Delight-v2-0 | 图像去光影模型 | 2025-01-21 | 1.3B | [前往](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0) |\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-control",
  "markdown": "# ComfyUI Wan2.1 Fun Control 视频示例\n\n**Wan2.1-Fun-Control** 是阿里团队推出的开源视频生成与控制项目，通过引入创新性的控制代码（Control Codes）机制，结合深度学习和多模态条件输入，能够生成高质量且符合预设控制条件的视频。该项目专注于通过多模态控制条件实现对生成视频内容的精准引导。 目前 Fun Control 模型支持多种控制条件，包括 **Canny（线稿）**、**Depth（深度）**、**OpenPose（人体姿势）**、**MLSD（几何边缘）** 等，同时支持使用 **轨迹控制**。 模型还支持多分辨率视频预测，分辨率可选 512、768 和 1024，帧率为每秒 16 帧，最长可生成 81 帧（约 5 秒）的视频。 模型版本方面：\n\n*   **1.3B** 轻量版：适合本地部署和快速推理，**对显存要求较低**\n*   **14B** 高性能版：模型体积达 32GB+，效果更优但 **需高显存支持**\n\n下面是相关代码仓库的示例\n\n*   [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)\n*   [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)\n*   代码仓库：[VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)\n\n**目前 ComfyUI 已原生支持了 Wan2.1 Fun Control 模型** ，在开始本篇教程前，请更新你的 ComfyUI 保证你的版本在[这个提交](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82)版本之后 在本篇指南中我们将提供两个工作流：\n\n*   仅使用原生的 Comfy Core 节点的工作流\n*   使用自定义节点的工作流\n\n## 相关模型安装\n\n这些模型你仅需要安装一次，另外在对应的工作流图片中也包含了模型下载信息，你可以选择你喜欢的方式下载模型。 下面的模型你可以在 [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) 和 [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) 找到 点击对应链接进行下载，如果你之前使用过 Wan 相关的工作流，那么你仅需要下载 **Diffusino models** **Diffusion models** 选择 1.3B 或 14B, 14B 的文件体积更大（32GB）但是对于运行显存要求也较高，\n\n*   [wan2.1\\_fun\\_control\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)\n*   [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true)： 建议下载后重命名为 `Wan2.1-Fun-14B-Control.safetensors`\n\n**Text encoders** 选择下面两个模型中的一个，fp16 精度体积较大对性能要求高\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\n文件保存位置\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └── wan2.1_fun_control_1.3B_bf16.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── 📂 vae/\n│   │   └── wan_2.1_vae.safetensors\n│   └── 📂 clip_vision/\n│       └──  clip_vision_h.safetensors                 \n```\n\n## ComfyUI 原生工作流\n\n在此工作流中，我们使用转换成 WebP 格式的视频，这是因为目前`Load Image` 节点还不支持 mp4 格式的视频，另外我们使用 Canny Edge 来对原始的视频进行图像的预处理, 由于经常有用户在安装自定义节点过程中遇到安装失败和环境的问题，所以这一版本的工作流完全使用原生节点来实现，来优先保证体验。 感谢我们强大的 ComfyUI 作者们，他们带来了功能丰富的相关节点，如果你需要直接查看相关版本直接查看[使用自定义节点的工作流](#%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E8%8A%82%E7%82%B9%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81)\n\n### 1\\. 工作流相关文件下载\n\n#### 1.1 工作流文件\n\n下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流 ![Wan2.1 Fun Control 原生工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_native.webp)\n\n#### 1.2 输入图片及视频下载\n\n请下载下面的图片及视频，我们将作为输入。 ![输入参考图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_remix.png) ![输入参考视频](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_video.webp)\n\n### 2\\. 按步骤完成工作流\n\n![Wan2.1 Fun Control 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_native_flow_diagram.png)\n\n1.  确保 `Load Diffusion Model` 节点加载了 `wan2.1_fun_control_1.3B_bf16.safetensors`\n2.  确保 `Load CLIP` 节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`\n4.  确保 `Load CLIP Vision` 节点加载了 `clip_vision_h.safetensors`\n5.  在 `Load Image` 节点（已被重命名为`Start_image`） 上传起始帧\n6.  在第二个 `Load Image` 节点上传用于控制视频。注意： 目前这个节点还不支持 mp4 只能使用 Webp 视频\n7.  （可选）修改 Prompt 使用中英文都可以\n8.  （可选）在 `WanFunControlToVideo` 修改对应视频的尺寸，不要使用过大的尺寸\n9.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n\n### 3\\. 使用说明\n\n*   由于我们需要和控制视频一致的帧数输入到 `WanFunControlToVideo` 节点，如果对应的帧数数值大于实际的控制视频帧数，将会导致多余的帧不符合控制条件的画面出现，这个问题我们将在[使用自定义节点的工作流](#%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E8%8A%82%E7%82%B9%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81)中解决\n*   使用类似 [ComfyUI-comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) 来实现更丰富的控制\n\n## 使用自定义节点的工作流\n\n我们将需要安装下面两个自定义节点：\n\n*   [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n*   [ComfyUI-comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n你可以使用 [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) 安装缺失节点的功能或者参照对应自定义节点包的安装说明来完成对应节点的安装\n\n### 1\\. 工作流相关文件下载\n\n#### 1.1 工作流文件\n\n下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流 ![工作流文件](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.webp)\n\n#### 1.2 输入图片及视频下载\n\n请下载下面的图片及视频，我们将会用于输入 ![输入参考图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-robot's_eye.png) \n\n### 2\\. 按步骤完成工作流\n\n![Wan2.1 Fun Control 使用自定义节点的工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png)\n\n> 模型部分基本是一致的，如果你已经体验过仅使用原生节点的工作流，你可以直接上传对应的图片然后运行即可\n\n1.  确保 `Load Diffusion Model` 节点加载了 `wan2.1_fun_control_1.3B_bf16.safetensors`\n2.  确保 `Load CLIP` 节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`\n4.  确保 `Load CLIP Vision` 节点加载了 `clip_vision_h.safetensors`\n5.  在 `Load Image` 节点上传起始帧\n6.  在 `Load Video(Upload)` 自定义节点上传 mp4 格式视频，请注意对应工作流有对默认的 `frame_load_cap`进行了调整\n7.  `DWPose Estimator` 处针对当前图像仅使用了 `detect_face` 的选项\n8.  （可选）修改 Prompt 使用中英文都可以\n9.  （可选）在 `WanFunControlToVideo` 修改对应视频的尺寸，不要使用过大的尺寸\n10.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n\n### 3\\. 工作流说明\n\n感谢 ComfyUI 社区作者带来的自定义节点包\n\n*   在这个示例中使用了 `Load Video(Upload)` 来实现对 mp4 视频的支持\n*   `Load Video(Upload)` 中获取到的 `video_info` 我们得以对输出的视频保持同样的 `fps`\n*   你可以替换 `DWPose Estimator` 为 `ComfyUI-comfyui_controlnet_aux` 节点包中的其它预处理器\n\n## 使用技巧\n\n![Apply Multi Control Videos](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/apply_multi_control_videos.jpg)\n\n*   一个有用的技巧是，你可以结合多种图像预处理技术，然后使用 `Image Blend` 节点来实现同时应用多种控制方法的目的。\n*   你可以使用 `ComfyUI-VideoHelperSuite` 的 `Video Combine` 节点来实现将对应视频存储为 mp4 格式\n*   我们使用 `SaveAnimatedWEBP` 是因为我们目前并不支持在 **mp4** 中嵌入工作流信息, 而且有些自定义节点可能没有考虑工作流嵌入，为了在视频中保存工作流，所以我们选择 `SaveAnimatedWEBP` 节点。\n*   不要设置过大的画面尺寸，这可能导致采样过程非常耗时，可以试着先生成小尺寸的图片然后再进行采样放大\n*   发挥你的想象力，在这个工作流基础上加上一些文生图或者其它类型的工作流，实现直接从文本到视频生成风格转换\n*   在 `WanFunControlToVideo` 节点中，`control_video` 不是必须的，所以有时候你可以不使用控制视频，先生成特别小尺寸的视频比如 320x320，然后使用再把它们作为控制视频输入来获得确定的结果\n\n## 其它 Wan2.1 Fun Control 或者视频相关自定义节点\n\n*   [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n*   [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-inp",
  "markdown": "# ComfyUI Wan2.1 Fun InP 视频示例\n\nWan-Fun InP 是阿里巴巴推出的开源视频生成模型，属于 ​​Wan2.1-Fun​​ 系列的一部分，专注于通过图像生成视频并实现首尾帧控制。 **核心功能**：\n\n*   首尾帧控制：支持输入首帧和尾帧图像，生成中间过渡视频，提升视频连贯性与创意自由度。相比早期社区版本，阿里官方模型的生成效果更稳定且质量显著提升。\n*   多分辨率支持：支持生成512×512、768×768、1024×1024等分辨率的视频，适配不同场景需求。\n\n**模型版本方面**：\n\n*   1.3B 轻量版：适合本地部署和快速推理，对显存要求较低\n*   14B 高性能版：模型体积达 32GB+，效果更优但需高显存支持\n\n下面是相关模型权重和代码仓库：\n\n*   [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)\n*   [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)\n*   代码仓库：[VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)\n\n## Wan2.1 Fun Control 工作流\n\n下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流 ![工作流文件](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_inp/wan2.1_fun_inp.webp)\n\n### 1\\. 工作流文件下载\n\n### 2\\. 手动模型安装\n\n如果对应的自动模型下载无效，请手动进行模型下载，并保存到对应的文件夹 下面的模型你可以在 [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) 和 [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) 找到 **Diffusion models** 选择 1.3B 或 14B, 14B 的文件体积更大（32GB）但是对于运行显存要求也较高，\n\n*   [wan2.1\\_fun\\_inp\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)\n*   [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true)： 建议下载后重命名为 `Wan2.1-Fun-14B-InP.safetensors`\n\n**Text encoders** 选择下面两个模型中的一个，fp16 精度体积较大对性能要求高\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\n文件保存位置\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └── wan2.1_fun_inp_1.3B_bf16.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── 📂 vae/\n│   │   └── wan_2.1_vae.safetensors\n│   └── 📂 clip_vision/\n│       └──  clip_vision_h.safetensors                 \n```\n\n### 3\\. 按步骤完成工作流\n\n![ComfyUI Wan2.1 Fun Control 视频生成工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_inp_flow_diagram.png)\n\n1.  确保 `Load Diffusion Model` 节点加载了 `wan2.1_fun_inp_1.3B_bf16.safetensors`\n2.  确保 `Load CLIP` 节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`\n4.  确保 `Load CLIP Vision` 节点加载了 `clip_vision_h.safetensors`\n5.  在 `Load Image` 节点（已被重命名为`Start_image`） 上传起始帧\n6.  在第二个 `Load Image` 节点上传用于控制视频。注意： 目前这个节点还不支持 mp4 只能使用 Webp 视频\n7.  （可选）修改 Prompt 使用中英文都可以\n8.  （可选）在 `WanFunInpaintToVideo` 修改对应视频的尺寸，不要使用过大的尺寸\n9.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n\n### 4\\. 工作流说明\n\n*   在体验 Wan Fun InP 时，你可能需要频繁修改提示词，从而来确保对应画面的过渡的准确性\n\n## 其它 Wan2.1 Fun Inp 或者视频相关自定义节点\n\n*   [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n*   [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n*   [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_topbar_menu",
  "markdown": "# 顶部菜单栏 - ComfyUI\n\n顶部菜单栏 API 允许扩展为 ComfyUI 的顶部菜单栏添加自定义菜单项。这对于提供高级功能或不常用命令的访问非常有用。\n\n## 基本用法\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  // 定义命令\n  commands: [\n    { \n      id: \"myCommand\", \n      label: \"我的命令\", \n      function: () => { alert(\"命令已执行！\"); } \n    }\n  ],\n  // 将命令添加到菜单\n  menuCommands: [\n    { \n      path: [\"扩展\", \"我的扩展\"], \n      commands: [\"myCommand\"] \n    }\n  ]\n});\n```\n\n命令定义方式与 [命令与快捷键绑定 API](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_commands_keybindings) 相同。详细定义命令请参见该页面。\n\n## 命令配置\n\n每个命令都需要 `id`、`label` 和 `function`：\n\n```\n{\n  id: string,              // 命令的唯一标识符\n  label: string,           // 命令显示名称\n  function: () => void     // 命令被触发时执行的函数\n}\n```\n\n## 菜单配置\n\n`menuCommands` 数组定义了命令在菜单结构中的位置：\n\n```\n{\n  path: string[],          // 表示菜单层级的数组\n  commands: string[]       // 要添加到该位置的命令 ID 数组\n}\n```\n\n`path` 数组指定菜单层级。例如，`[\"文件\", \"导出\"]` 会将命令添加到”文件”菜单下的”导出”子菜单。\n\n## 菜单示例\n\n### 添加到已有菜单\n\n```\napp.registerExtension({\n  name: \"MenuExamples\",\n  commands: [\n    { \n      id: \"saveAsImage\", \n      label: \"另存为图片\", \n      function: () => { \n        // 保存画布为图片的代码\n      } \n    },\n    { \n      id: \"exportWorkflow\", \n      label: \"导出工作流\", \n      function: () => { \n        // 导出工作流的代码\n      } \n    }\n  ],\n  menuCommands: [\n    // 添加到文件菜单\n    { \n      path: [\"文件\"], \n      commands: [\"saveAsImage\", \"exportWorkflow\"] \n    }\n  ]\n});\n```\n\n### 创建子菜单结构\n\n```\napp.registerExtension({\n  name: \"SubmenuExample\",\n  commands: [\n    { \n      id: \"option1\", \n      label: \"选项 1\", \n      function: () => { console.log(\"选项 1\"); } \n    },\n    { \n      id: \"option2\", \n      label: \"选项 2\", \n      function: () => { console.log(\"选项 2\"); } \n    },\n    { \n      id: \"suboption1\", \n      label: \"子选项 1\", \n      function: () => { console.log(\"子选项 1\"); } \n    }\n  ],\n  menuCommands: [\n    // 创建嵌套菜单结构\n    { \n      path: [\"扩展\", \"我的工具\"], \n      commands: [\"option1\", \"option2\"] \n    },\n    { \n      path: [\"扩展\", \"我的工具\", \"高级\"], \n      commands: [\"suboption1\"] \n    }\n  ]\n});\n```\n\n### 多个菜单位置\n\n你可以将同一个命令添加到多个菜单位置：\n\n```\napp.registerExtension({\n  name: \"MultiLocationExample\",\n  commands: [\n    { \n      id: \"helpCommand\", \n      label: \"获取帮助\", \n      function: () => { window.open(\"https://docs.example.com\", \"_blank\"); } \n    }\n  ],\n  menuCommands: [\n    // 添加到帮助菜单\n    { \n      path: [\"帮助\"], \n      commands: [\"helpCommand\"] \n    },\n    // 也添加到扩展菜单\n    { \n      path: [\"扩展\"], \n      commands: [\"helpCommand\"] \n    }\n  ]\n});\n```\n\n命令可以与其他 ComfyUI API（如设置）配合使用。关于设置 API 的更多信息，请参见 [设置 API](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_settings) 文档。\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/vace",
  "markdown": "# ComfyUI Wan2.1 VACE 视频示例 - ComfyUI\n\nVACE 14B 是阿里通义万相团队推出的开源视频编辑统一模型。该模型通过整合多任务能力、支持高分辨率处理及灵活的多模态输入机制，显著提升了视频创作的效率与质量。 该模型基于 [Apache-2.0](https://github.com/ali-vilab/VACE?tab=Apache-2.0-1-ov-file) 协议开源，可用于个人商业用途。 以下是其核心特性与技术亮点的综合分析：\n\n*   多模态输入:支持文本、图像、视频、遮罩、控制信号等多种输入形式\n*   统一架构:单一模型支持多种任务,可自由组合功能\n*   动作迁移:基于参考视频生成连贯动作\n*   局部替换:通过遮罩替换视频中的特定区域\n*   视频扩展:补全动作或扩展背景\n*   背景替换:保留主体更换环境背景\n\n目前 VACE 发布了 1.3B 和 14B 两个版本，14B 版本相比 1.3B 版本，支持 720P 分辨率输出,画面细节和稳定性更好。\n\n| 模型  | 480P | 720P |\n| --- | --- | --- |\n| [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B) | ✅   | ❌   |\n| [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B) | ✅   | ✅   |\n\n相关模型权重和代码仓库：\n\n*   [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B)\n*   [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B)\n*   [Github](https://github.com/ali-vilab/VACE)\n*   [VACE 项目主页](https://ali-vilab.github.io/VACE-Page/)\n\n## 模型下载及在工作流中的加载\n\n由于本篇文档中设计的几个工作流都使用同一套工作流模板，所以我们可以先完成模型下载及加载的信息介绍，然后通过 Bypass 不同的节点来启用/ 禁用不同的输入来实现不同的工作流。 在具体示例中对应的工作流信息中已经嵌入了模型下载信息，所以你也可以在下载具体示例的工作流时来完成模型下载。\n\n### 模型下载\n\n**diffusion\\_models** [wan2.1\\_vace\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_14B_fp16.safetensors) [wan2.1\\_vace\\_1.3B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_1.3B_fp16.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n从**Text encoders** 选择一个版本进行下载\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n文件保存位置\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └─── wan2.1_vace_14B_fp16.safetensors # 或 wan2.1_vace_1.3B_fp16.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors # 或 umt5_xxl_fp16.safetensors\n│   └── 📂 vae/\n│       └──  wan_2.1_vae.safetensors\n```\n\n### 模型加载\n\n由于在本篇指南中，我们所使用的模型是一致的，工作流也相同，只是 Bypass 了部分的节点来启用/ 禁用不同的输入，请参考下面的图片确保在对应不同的工作流中，对应的模型都已正确加载 ![Wan2.1 VACE 模型加载](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-model-loading.jpg)\n\n1.  确保 `Load Diffusion Model` 节点加载了 `wan2.1_vace_14B_fp16.safetensors`\n2.  确保 `Load CLIP` 节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 或者 `umt5_xxl_fp16.safetensors`\n3.  确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`\n\n### 如何取消节点的 Bypass 状态\n\n当一个节点被设置为 Bypass 状态时，通过该节点的数据将不受节点的影响，直接输出，下面是如何取消节点的 Bypass 状态的三种方法 我们通常在不需要一些节点时设置节点的 Bypass 状态，而不用将它们从节点中删除改变工作流。 ![取消 Bypass](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/nodes/cancel-bypass.jpg)\n\n1.  选中节点后，在选择工具箱中点击标识部分的箭头，即可快速切换节点的 Bypass 状态\n2.  选中节点后，鼠标右键点击节点，选择 `模式(Mode)` -> `总是(Always)` 切换到 Always 模式\n3.  选中节点后，鼠标右键点击节点，选择 `绕过(Bypass)` 选项，切换 Bypass 状态\n\n## VACE 文生视频工作流\n\n### 1\\. 工作流下载\n\n下载下面视频，并拖入 ComfyUI 中，以加载对应的工作流\n\n### 2\\. 按步骤完成工作流的运行\n\n![图像](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg) 请参照图片序号进行逐步确认，来保证对应工作流的顺利运行\n\n1.  在 `CLIP Text Encode (Positive Prompt)` 节点中输入正向提示词\n2.  在 `CLIP Text Encode (Negative Prompt)` 节点中输入负向提示词\n3.  在 `WanVaceToVideo` 设置对应图像的尺寸（首次运行建议设置 640\\*640 的分辨率），帧数（视频的时长）\n4.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n5.  生成完成后对应的视频会自动保存到 `ComfyUI/output/video` 目录下（子文件夹位置取决于 `save video` 节点设置）\n\n## VACE 图生视频工作流\n\n你可以继续使用上面的工作流文件，只需要将 **Load reference image** 的 `Load image` 节点的 Bypass 取消，并输入对应的图片，你也可以使用下面的图片，在这个文件里，我们已经完成了对应的参数设置。\n\n### 1\\. 工作流下载\n\n下载下面的视频，并拖入 ComfyUI 中，以加载对应的工作流\n\n请下载下面图片作为输入图片 ![vace-i2v-input](https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/video/wan/vace/i2v/input.jpg)\n\n### 2\\. 按步骤完成工作流的运行\n\n![工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg) 请参照图片序号进行逐步确认，来保证对应工作流的顺利运行\n\n1.  在 `Load image` 节点中输入对应的图片\n2.  你可以像文生图工作流一样完成来进行提示词的修改和编辑\n3.  在 `WanVaceToVideo` 设置对应图像的尺寸（首次运行建议设置 640\\*640 的分辨率），帧数（视频的时长）\n4.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n5.  生成完成后对应的视频会自动保存到 `ComfyUI/output/video` 目录下（子文件夹位置取决于 `save video` 节点设置）\n\n### 3\\. 工作流补充说明\n\nVACE 还支持在一张图像中输入多个参考图像，来生成对应的视频，你可以在 VACE 的项目页中看到相关的[示例](https://ali-vilab.github.io/VACE-Page/)\n\n## VACE 视频到视频工作流\n\n### 1\\. 工作流下载\n\n下载下面的视频并拖入 ComfyUI 中，以加载对应的工作流\n\n我们将使用下面的素材作为输入:\n\n1.  用于参考图像的输入图片 ![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/vace/v2v/input.jpg) \n2.  下面的视频已经经过预处理，我们将用于控制视频的生成\n\n3.  下面的视频是原始视频，你可以下载下面的素材来使用类似 [comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) 这样的预处理节点来对图像进行预处理\n\n### 2\\. 按步骤完成工作流的运行\n\n![工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg) 请参照图片序号进行逐步确认，来保证对应工作流的顺利运行\n\n1.  在 `Load reference image` 中的 `Load Image` 节点输入参考图片\n2.  在 `Load control video` 中的 `Load Video` 节点输入控制视频，由于提供的视频是经过预处理的，所以你不需要进行额外的处理\n3.  如果你需要自己针对原始视频进行预处理，可以修改 `Image preprocessing` 分组，或者使用 `comfyui_controlnet_aux` 节点来完成对应的节点预处理\n4.  修改提示词\n5.  在 `WanVaceToVideo` 设置对应图像的尺寸（首次运行建议设置 640\\*640 的分辨率），帧数（视频的时长）\n6.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n7.  生成完成后对应的视频会自动保存到 `ComfyUI/output/video` 目录下（子文件夹位置取决于 `save video` 节点设置）\n\n## VACE 视频扩展工作流\n\n\\[待更新\\]\n\n## VACE 首尾帧视频生成\n\n\\[待更新\\] 要保证首尾帧生效，需要满足：\n\n*   对应视频 `length` 设置需要满足 `length-1` 后能够被 `4` 整除\n*   对应的 `Batch_size` 设置需要满足 `Batch_size = length - 2`\n\n## 相关节点文档\n\n请查阅下面的文档了解相关的节点\n\n[\n\n## WanVaceToVideo 节点文档\n\nWanVaceToVideo 节点文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/conditioning/video-models/wan-vace-to-video)[\n\n## TrimVideoLatent 节点文档\n\nComfyUI TrimVideoLatent 节点文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/latent/video/trim-video-latent)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image",
  "markdown": "# ComfyUI 文生图工作流 - ComfyUI\n\n本篇目的主要带你初步了解 ComfyUI 的文生图的工作流，并初步了解一些 ComfyUI 相关节点的功能和使用。 在本篇文档中我们将完成以下内容：\n\n*   完成一次文生图工作流\n*   简单了解扩散模型原理\n*   了解工作流中的节点的功能和作用\n*   初步了解 SD1.5 模型\n\n我们将会先进行文生图工作流的运行，然后进行相关内容的讲解，请按你的需要选择对应部分开始。\n\n## 关于文生图\n\n**文生图(Text to Image)** ，是 AI 绘图中的基础流程，通过输入文本描述来生成对应的图片，它的核心是 **扩散模型**。 在文生图过程中我们需要以下条件：\n\n*   **画家：** 绘图模型\n*   **画布：** 潜在空间\n*   \\*\\*对画面的要求（提示词）：\\*\\*提示词，包括正向提示词（希望在画面中出现的元素）和负向提示词（不希望在画面中出现的元素）\n\n这个文本到图片图片生成过程，可以简单理解成你把你的**绘图要求(正向提示词、负向提示词)**告诉一个**画家(绘图模型)**，画家会根据你的要求，画出你想要的内容。\n\n### 1\\. 开始开始前的准备\n\n请确保你已经在 `ComfyUI/models/checkpoints` 文件夹至少有一个 SD1.5 的模型文件，如果你还不了解如何安装模型，请参[开始 ComfyUI 的 AI 绘图之旅](https://docs.comfy.org/zh-CN/get_started/first_generation#3-%E5%AE%89%E8%A3%85%E7%BB%98%E5%9B%BE%E6%A8%A1%E5%9E%8B)章节中关于模型安装的部分说明。 你可以使用下面的这些模型：\n\n*   [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)\n*   [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)\n*   [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)\n\n### 2\\. 加载文生图工作流\n\n请下载下面的图片，并将图片拖入 ComfyUI 的界面中，或者使用菜单 **工作流（Workflows）** -> **打开（Open）** 打开这个图片以加载对应的 workflow ![ComfyUI-文生图工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/text-to-image-workflow.png) 也可以从菜单 **工作流（Workflows）** -> **浏览工作流示例（Browse example workflows）** 中选择 **Text to Image** 工作流\n\n### 3\\. 加载模型，并进行第一次图片生成\n\n在完成了对应的绘图模型安装后，请参考下图步骤加载对应的模型，并进行第一次图片的生成 ![图片生成](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg) 请对应图片序号，完成下面操作\n\n1.  请在**Load Checkpoint** 节点使用箭头或者点击文本区域确保 **v1-5-pruned-emaonly-fp16.safetensors** 被选中，且左右切换箭头不会出现**null** 的文本\n2.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成\n\n等待对应流程执行完成后，你应该可以在界面的\\*\\*保存图像（Save Image）\\*\\*节点中看到对应的图片结果，可以在上面右键保存到本地 ![ComfyUI 首次图片生成结果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)\n\n### 4\\. 开始你的尝试\n\n你可以尝试修改**CLIP Text Encoder**处的文本 ![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg) 其中连接到 KSampler 节点的`Positive`为正向提示词，连接到 KSampler 节点的`Negative`为负向提示词 下面是针对 SD1.5 模型的一些简单提示词原则\n\n*   尽量使用英文\n*   提示词之间使用英文逗号 `,` 隔开\n*   尽量使用短语而不是长句子\n*   使用更具体的描述\n*   可以使用类似 `(golden hour:1.2)` 这样的表达来提升特定关键词的权重，这样它在画面中出现的概率会更高，`1.2` 为权重，`golden hour` 为关键词\n*   可以使用类似 `masterpiece, best quality, 4k` 等关键词来提升生成质量\n\n下面是几组不同的 prompt 示例，你可以尝试使用这些 prompt 来查看生成的效果，或者使用你自己的 prompt 来尝试生成 **1\\. 二次元动漫风格** 正向提示词：\n\n```\nanime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details\n\nmasterpiece, best quality, 4k\n```\n\n负向提示词：\n\n```\nlow quality, blurry, deformed hands, extra fingers\n```\n\n**2\\. 写实风格** 正向提示词：\n\n```\n(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), \nfull body, soft cinematic lighting, (golden hour:1.2), \n(fujifilm XT4:1.1), shallow depth of field, \n(skin texture details:1.3), (film grain:1.1), \ngentle wind flow, warm color grading, (perfect facial symmetry:1.3)\n```\n\n负向提示词：\n\n```\n(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)\n```\n\n**3\\. 特定艺术家风格** 正向提示词：\n\n```\nfantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style\n```\n\n负向提示词：\n\n```\nblurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting\n```\n\n## 文生图工作原理\n\n整个文生图的过程，我们可以理解成是**扩散模型的反扩散过程**，我们下载的 [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) 是一个已经训练好的可以 **从纯高斯噪声生成目标图片的模型**，我们只需要输入我们的提示词，它就可以通随机的噪声降噪生成目标图片。\n\n我们可能需要了解下两个概念，\n\n1.  **潜在空间：** 潜在空间（Latent Space）是扩散模型中的一种抽象数据表示方式，通过把图片从像素空间转换为潜在空间，可以减少图片的存储空间，并且可以更容易的进行扩散模型的训练和减少降噪的复杂度，就像建筑师设计建筑时使用蓝图（潜在空间）来进行设计，而不是直接在建筑上进行设计（像素空间），这种方式可以保持结构特征的同时，又大幅度降低修改成本\n2.  **像素空间：** 像素空间（Pixel Space）是图片的存储空间，就是我们最终看到的图片，用于存储图片的像素值。\n\n如果你想要了解更多扩散模型相关内容，可以阅读下面的文章：\n\n*   [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)\n*   [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)\n*   [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)\n\n## ComfyUI 文生图工作流节点讲解\n\n![ComfyUI 文生图工作流讲解](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/text-image-workflow.jpg)\n\n### A. 加载模型（Load Checkpoint）节点\n\n![加载模型](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_checkpoint.jpg) 这个节点通常用于加载绘图模型, 通常 `checkpoint` 中会包含 `MODEL（UNet）`、`CLIP` 和 `VAE` 三个组件\n\n*   `MODEL（UNet）`：为对应模型的 UNet 模型, 负责扩散过程中的噪声预测和图像生成,驱动扩散过程\n*   `CLIP`：这个是文本编码器,因为模型并不能直接理解我们的文本提示词（prompt）,所以需要将我们的文本提示词（prompt）编码为向量,转换为模型可以理解的语义向量\n*   `VAE`：这个是变分自编码器,我们的扩散模型处理的是潜在空间,而我们的图片是像素空间,所以需要将图片转换为潜在空间,然后进行扩散,最后将潜在空间转换为图片\n\n### B. 空Latent图像（Empty Latent Image）节点\n\n![空Latent图像](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/empty_latent_image.jpg) 定义一个潜在空间（Latent Space）,它输出到 KSampler 节点，空Latent图像节点构建的是一个 **纯噪声的潜在空间** 它的具体的作用你可以理解为定义画布尺寸的大小，也就是我们最终生成图片的尺寸\n\n### C. CLIP文本编码器（CLIP Text Encoder）节点\n\n![CLIP文本编码器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg) 用于编码提示词，也就是输入你对画面的要求\n\n*   连接到 KSampler 节点的 `Positive` 条件输入的为正向提示词（希望在画面中出现的元素）\n*   连接到 KSampler 节点的 `Negative` 条件输入的为负向提示词（不希望在画面中出现的元素）\n\n对应的提示词被来自 `Load Checkpoint` 节点的 `CLIP` 组件编码为语义向量，然后作为条件输出到 KSampler 节点\n\n### D. K 采样器（KSampler）节点\n\n![K 采样器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg) **K 采样器** 是整个工作流的核心，整个噪声降噪的过程都在这个节点中完成，并最后输出一个潜空间图像\n\nKSampler 节点的参数说明如下\n\n| 参数名称 | 描述  | 作用  |\n| --- | --- | --- |\n| **model** | 去噪使用的扩散模型 | 决定生成图像的风格与质量 |\n| **positive** | 正向提示词条件编码 | 引导生成包含指定元素的内容 |\n| **negative** | 负向提示词条件编码 | 抑制生成不期望的内容 |\n| **latent\\_image** | 待去噪的潜在空间图像 | 作为噪声初始化的输入载体 |\n| **seed** | 噪声生成的随机种子 | 控制生成结果的随机性 |\n| **control\\_after\\_generate** | 种子生成后控制模式 | 决定多批次生成时种子的变化规律 |\n| **steps** | 去噪迭代步数 | 步数越多细节越精细但耗时增加 |\n| **cfg** | 分类器自由引导系数 | 控制提示词约束强度（过高导致过拟合） |\n| **sampler\\_name** | 采样算法名称 | 决定去噪路径的数学方法 |\n| **scheduler** | 调度器类型 | 控制噪声衰减速率与步长分配 |\n| **denoise** | 降噪强度系数 | 控制添加到潜在空间的噪声强度，0.0保留原始输入特征，1.0为完全的噪声 |\n\n在 KSampler 节点中，潜在空间使用 `seed` 作为初始化参数构建随机的噪声,语义向量 `Positive` 和 `Negative` 会作为条件输入到扩散模型中 然后根据 `steps` 参数指定的去噪步数，进行去噪，每次去噪会根据 `denoise` 参数指定的降噪强度系数，对潜在空间进行降噪，并生成新的潜在空间图像\n\n### E. VAE 解码（VAE Decode）节点\n\n![VAE 解码](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/vae_decode.jpg) 将 **K 采样器(KSampler)** 输出的潜在空间图像转换为像素空间图像\n\n### F. 保存图像（Save Image）节点\n\n![保存图像](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/save_image.jpg) 预览并保存从潜空间解码的图像，并保存到本地`ComfyUI/output`文件夹下\n\n## SD1.5 模型简介\n\n**SD1.5(Stable Diffusion 1.5)** 是一个由[Stability AI](https://stability.ai/)开发的AI绘图模型，Stable Diffusion系列的基础版本，基于 **512×512** 分辨率图片训练，所以其对 **512×512** 分辨率图片生成支持较好，体积约为4GB，可以在\\*\\*消费级显卡（如6GB显存）\\*\\*上流畅运行。目前 SD1.5 的相关周边生态非常丰富，它支持广泛的插件（如ControlNet、LoRA）和优化工具。 作为AI绘画领域的里程碑模型，SD1.5凭借其开源特性、轻量架构和丰富生态，至今仍是最佳入门选择。尽管后续推出了SDXL/SD3等升级版本，但其在消费级硬件上的性价比仍无可替代。\n\n### 基础信息\n\n*   **发布时间**：2022年10月\n*   **核心架构**：基于Latent Diffusion Model (LDM)\n*   **训练数据**：LAION-Aesthetics v2.5数据集（约5.9亿步训练）\n*   **开源特性**：完全开源模型/代码/训练数据\n\n### 优缺点\n\n模型优势：\n\n*   轻量化：体积小，仅 4GB 左右，在消费级显卡上流畅运行\n*   使用门槛低：支持广泛的插件和优化工具\n*   生态成熟：支持广泛的插件和优化工具\n*   生成速度快：在消费级显卡上流畅运行\n\n模型局限：\n\n*   细节处理：手部/复杂光影易畸变\n*   分辨率限制：直接生成1024x1024质量下降\n*   提示词依赖：需精确英文描述控制效果"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/image-to-image",
  "markdown": "# ComfyUI 图生图工作流 - ComfyUI\n\n## 什么是图生图\n\n图生图（Image to Image）是 ComfyUI 中的一种工作流，它允许用户将一张图像作为输入，并生成一张新的图像。 图生图可以使用在以下场景中：\n\n*   原始图像风格的转换，如把写实照片转为艺术风格\n*   将线稿图像转换为写实图像\n*   图像的修复\n*   老照片着色\n*   … 等其它场景\n\n用一个比喻来讲解的话，大概是这样： 你需要画家根据你的参考图片，画出符合你要求特定效果的作品。 如果你仔细比对本篇教程和[文生图](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)教程，你会发现图生图的流程和文生图的流程非常相似，只是多了个输入的参考图片作为输入条件，也就是在文生图中，我们是让画家（绘图模型）根据我们的提示词生成自由发挥，而在图生图中，我们是让画家（绘图模型）根据我们的参考图片和提示词生成图片。\n\n### 1\\. 模型安装\n\n请确保你已经在 `ComfyUI/models/checkpoints` 文件夹至少有一个 SD1.5 的模型文件，如果你还不了解如何安装模型，请参[开始 ComfyUI 的 AI 绘图之旅](https://docs.comfy.org/zh-CN/get_started/first_generation#3-%E5%AE%89%E8%A3%85%E7%BB%98%E5%9B%BE%E6%A8%A1%E5%9E%8B)章节中关于模型安装的部分说明。 你可以使用下面的这些模型：\n\n*   [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)\n*   [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)\n*   [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)\n\n### 2\\. 图生图工作流相关文件\n\n保存并下载下面的图片到本地，然后 **拖拽或使用 ComfyUI 打开** 它，就会加载对应的工作流 ![图生图工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image_to_image.png) 或在 ComfyUI 的 **workflow template** 中加载 **image to image** 工作流 ![ComfyUI 工作流模板 - 图生图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/image-to-image-01-template.jpg) 下载下面的图片作为使用示例，我们会在后面的步骤中使用它 ![图片示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/input.jpeg) \n\n### 3\\. 开始图生图工作流\n\n![ComfyUI 图生图工作流 - 步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image-to-image-02-guide.jpg) 在加载图生图工作流后，请对照图片，按照序号完成以下操作，完成示例工作流的生成\n\n1.  在 **Load Checkpoint** 节点中加载好你本地的绘图模型\n2.  在 **Load Image** 节点点击 `upload` 按钮，上传准备步骤中提供的图片\n3.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成\n\n## 开始你自己的尝试\n\n1.  试着修改 **KSampler** 节点中的 `denoise` 参数，逐渐从 1 到 0 变化，观察生成图片的变化\n2.  更换你自己的提示词和参考图片，生成属于你自己的图片效果\n\n## 图生图工作流核心要点\n\n图生图工作流的核心在于在于 `KSampler` 节点中的 `denoise` 参数要是 **小于 1** 如果你调整过 `denoise` 参数，进行生成后会发现：\n\n*   `denoise` 越小，生成图片和参考图片的差异就会越小，\n*   `denoise` 越大，生成图片和参考图片的差异就会越大。\n\n因为 `denoise` 决定了对应图片转换为潜空间图像后，向潜在空间图像添加的噪声强度，如果 `denoise` 为 1，对应潜空间图像就会变成一个完全随机的噪声，那这样就和`empty latent image`节点生成的潜在空间一样了，就会丢失参考图片的所有特征。 对应原理可以参考[文生图](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)教程中的原理讲解。"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/overview",
  "markdown": "# API Nodes - ComfyUI\n\nAPI Nodes 是 ComfyUI 新增的调用闭源模型的方式，通过 API 调用，这将为 ComfyUI 用户提供访问外部最先进 AI 模型的能力，而无需复杂的 API 密钥设置。\n\nAPI Nodes 是一组特殊的节点，它们能够连接到外部 API 服务，让您直接在 ComfyUI 工作流中使用闭源或第三方托管的 AI 模型。这些节点设计用于无缝集成外部模型的功能，同时保持 ComfyUI 核心的开源特性。 目前支持的模型包括：\n\n*   **Black Forest Labs**: Flux 1.1\\[pro\\] Ultra, Flux .1\\[pro\\], Flux .1 Kontext Pro, Flux .1 Kontext Max\n*   **Google**: Veo2, Gemini 2.5 Pro, Gemini 2.5 Flash\n*   **Ideogram**: V3, V2, V1\n*   **Kling**: 2.0, 1.6, 1.5 & Various Effects\n*   **Luma**: Photon, Ray2, Ray1.6\n*   **MiniMax**: Text-to-Video, Image-to-Video\n*   **OpenAI**: o1, o1-pro, o3, gpt-4o, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, DALL·E 2, DALL·E 3, GPT-Image-1\n*   **PixVerse**: V4 & Effects\n*   **Pika**: 2.2\n*   **Recraft**: V3, V2 & Various Tools\n*   **Rodin**: 3D Generation\n*   **Stability AI**: Stable Image Ultra, Stable Diffusion 3.5 Large, Image Upscale\n*   **Tripo**: v1-4, v2.0, v2.5\n\n## 使用 API Nodes 的前提要求\n\n要使用 API Nodes 节点，需要有以下节点要求\n\n### 1\\. ComfyUI 版本要求\n\n请更新你的 ComfyUI 到最新版本，由于我们后期可能会新增更多的 API 支持, 相应的节点也会进行更新, 所以请保持你的 ComfyUI 处于最新版本。\n\n### 2\\. 账号及账户余额要求\n\n需要当前已经在 ComfyUI 中登录了 [Comfy账号](https://docs.comfy.org/zh-CN/interface/user)，并且账户[积分](https://docs.comfy.org/zh-CN/interface/credits)大于 0 在 `设置` -> `用户` 中进行登录： ![ComfyUI 用户界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/user.jpg) 并在 `设置` -> `积分` 中购买积分： ![ComfyUI 积分界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/menu-credits.jpg) 请参考对应的账号及积分部分的文档来确保这一要求：\n\n*   [Comfy账号](https://docs.comfy.org/zh-CN/interface/user): 在设置菜单中找到`用户`部分，进行登录\n*   [积分](https://docs.comfy.org/zh-CN/interface/credits): 登录后设置界面会出现积分菜单，您可以在`设置` → `积分`中购买积分，我们使用预付费，不会有意外的费用\n\n### 3\\. 网络环境要求\n\n*   本地网络仅允许 `127.0.0.1` 或者 `localhost` 访问可以直接使用登录功能\n*   如果是局域网或者非白名单网站访问请使用 API Key 登录，请参考[使用 API Key 进行登录](https://docs.comfy.org/zh-CN/interface/user#%E4%BD%BF%E7%94%A8-api-key-%E8%BF%9B%E8%A1%8C%E7%99%BB%E5%BD%95)\n*   能够正常访问我们的 API 服务（在某些地区可能需要使用代理服务）\n*   要求在 `https` 环境下访问，保证请求的安全性\n\n### 4\\. 使用对应节点\n\n**添加到工作流**：将 API 节点添加到您的工作流中，就像使用其他节点一样 **运行**：设置好参数后运行工作流 ![API Nodes](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/sidebar.jpg)\n\n## 使用 API Key 在非白名单网站登录 ComfyUI 账户来使用 API Nodes\n\n目前我们设置有白名单来限制可以登录ComfyUI 账户的网站，如果需要在一些非白名单网站登录 ComfyUI 账户，请参考账号管理相关的部分了解如何使用 API Key 来进行登录，在这种情况下不需要对应的网站在我们的白名单中。\n\n[\n\n## 账户管理\n\n查看如何使用 ComfyUI API Key 登录\n\n\n\n](https://docs.comfy.org/zh-CN/interface/user#%E4%BD%BF%E7%94%A8-api-key-%E8%BF%9B%E8%A1%8C%E7%99%BB%E5%BD%95)\n\n![Select Comfy API Key Login](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user/user-login-api-1.jpg)\n\n## 使用 ComfyUI API Key 集成来调用付费模型 API 节点\n\n目前我们支持通过 ComfyUI API Key 集成来访问我们的服务来调用付费模型 API 节点，请参考 API Key 集成章节了解如何使用 API Key 集成来调用付费模型 API 节点\n\n[\n\n## API Key 集成\n\n请参考 API Key 集成章节了解如何使用 API Key 集成来调用付费模型 API 节点\n\n\n\n](https://docs.comfy.org/zh-CN/development/comfyui-server/api-key-integration)\n\n## API Nodes 的优势\n\nAPI Nodes 为 ComfyUI 用户提供了几个重要优势：\n\n*   **访问闭源模型**：使用最先进的 AI 模型，无需自行部署\n*   **无缝集成**：API 节点与其他 ComfyUI 节点完全兼容，可以组合创建复杂工作流\n*   **简化的体验**：无需管理 API 密钥或处理复杂的 API 请求\n*   **可控的成本**：预付费系统确保您完全控制支出，没有意外费用\n\n## 计费方式\n\n[\n\n## API 节点计费\n\n请参考定价页面了解对应的 API 定价\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/pricing)\n\n![选择 Comfy API Key 登录](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/user/user-login-api-1.jpg)\n\n## 关于开源和选择加入\n\n重要的是要注意，**API Nodes 是完全可选的**。ComfyUI 将始终保持完全开源，并对本地用户免费。API 节点设计为”选择加入”功能，为那些想要访问外部 SOTA（最先进）模型的用户提供便利。\n\n## 如何使用 API Nodes\n\nAPI Nodes 的一个强大应用是将外部模型的输出与本地节点结合。例如：\n\n*   使用 [GPT-Image-1](https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/gpt-image-1) 生成基础图像，然后通过本地 `WanImageToVideo` 节点转换为视频\n*   结合外部生成的图像与本地的上采样或风格转换节点\n*   创建混合工作流，充分利用闭源和开源模型的优势\n\n这种灵活性使 ComfyUI 成为真正的通用生成式 AI 入口，将各种不同的 AI 功能整合到一个统一的工作流中，带来了更多可能性\n\n## 常见问题"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/faq",
  "markdown": "# API Nodes 场景问题 - ComfyUI\n\n为什么我找不到 API 节点？\n\n请更新你的 ComfyUI 到最新版本（最新的 commit,或者最新的[桌面版](https://www.comfy.org/download)），由于我们后期可能会新增更多的 API 支持, 相应的节点也会进行更新, 所以请保持你的 ComfyUI 处于最新版本。\n\n请注意需要区分 nightly 版本和 release 版本，有些情况下 `nightly` 版本(也就是最新的代码 commit 提交)才会包含最新的节点，因为 release 版本可能不会及时更新。由于我们仍在快速地迭代中，所以当你无法找到对应节点时请确保你使用的是最新的版本。\n\n为什么我无法使用 / 登录 API Nodes 节点？\n\nAPI 访问需要你当前的请求是基于安全的网络环境，目前对 API 访问的网络环境要求如下:\n\n*   本地网络仅允许 `127.0.0.1` 或者 `localhost` 访问, 这可能意味着，你无法在局域网环境下使用带有`--listen` 参数启动的 ComfyUI 服务中中使用 API Nodes 节点\n*   能够正常访问我们的 API 服务（在某些地区可能需要使用代理服务）\n*   你的账号没有足够的[积分](https://docs.comfy.org/zh-CN/interface/credits)\n\n为什么登录了还是无法使用或在使用时还会继续要求我登录？\n\n*   目前仅支持 `127.0.0.1` 或者 `localhost` 访问,\n*   确保你的账户有足够余额\n\nAPI Nodes 节点可以免费使用吗？\n\nAPI Nodes 节点由于需要通过 API 调用闭源模型，所以需要使用积分，不支持免费使用\n\n要如何购买积分？\n\n请参考下面的文档：\n\n1.  [Comfy账号](https://docs.comfy.org/zh-CN/interface/user): 在设置菜单中找到`用户`部分，进行登录\n2.  [积分](https://docs.comfy.org/zh-CN/interface/credits): 登录后设置界面会出现积分菜单，您可以在`设置` → `积分`中购买积分，我们使用预付费，不会有意外的费用\n3.  通过 Stripe 完成付款\n4.  查看积分是否更新，如果没有试着重启或者刷新页面\n\n未用完的积分支持退款吗？\n\n目前我们不支持对积分进行退款。 如果你觉得是因为技术问题出现了错误而存在未使用的余额，请[联系支持](mailto:support@comfy.org)\n\n积分可以出现负数吗？\n\n不允许积分出现负数，所以在对应 API 调用前，请确保你有足够的积分。\n\n我可以在哪里查看使用量和花费？\n\n请在登录后访问[积分](https://docs.comfy.org/zh-CN/interface/credits) 菜单，查看相应的积分。\n\n支持使用自己的 API Key 吗？\n\n目前 API Nodes 节点仍在测试阶段，目前暂不支持，我们已经把这个功能纳入考虑中了。。\n\n积分会过期吗？\n\n不你的积分不会过期。\n\n积分可以转让或者共享吗？\n\n不，你的积分不能转让给其他用户，也只限制于当前登录账户使用，但是我们并不限制登录设备的数量\n\n我可以在不同设备间使用同一个账户吗？\n\n我们不限制登录的设备数量，你可以在你想要的任何地方使用你的账号？\n\n我该如何请求删除我的账户或信息？"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/pricing",
  "markdown": "# 计费 - ComfyUI\n\nBFLFlux 1.1 \\[pro\\] Ultra ImageNA$0.06BFLFlux.1 Canny Control ImageNA$0.05BFLFlux.1 Depth Control ImageNA$0.05BFLFlux.1 Expand ImageNA$0.05BFLFlux.1 Fill ImageNA$0.05BFLFlux.1 Kontext \\[max\\] ImageNA$0.08BFLFlux.1 Kontext \\[pro\\] ImageNA$0.04BFLFlux.1 Kontext \\[pro\\] ImageNA$0.05KlingKling Image Generationkling-v1-5, 1, image to image$0.028KlingKling Image Generationkling-v1-5, 1, text to image$0.014KlingKling Image Generationkling-v1, 1, image to image$0.0035KlingKling Image Generationkling-v1, 1, text to image$0.0035KlingKling Image Generationkling-v2, 1, text to image$0.014KlingKling Virtual Try OnNA$0.07KlingKling, Text to Video (Camera Control)NA$0.49KlingKling Dual Character Video, Effectskling-v1-5, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1-5, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1-5, std, 5$0.28KlingKling Dual Character Video, Effectskling-v1-5, std, 10$0.56KlingKling Dual Character Video, Effectskling-v1-6, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1-6, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1-6, std, 5$0.28KlingKling Dual Character Video, Effectskling-v1-6, std, 10$0.56KlingKling Dual Character Video, Effectskling-v1, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1, std, 5$0.14KlingKling Dual Character Video, Effectskling-v1, std, 10$0.28KlingKling Image to Videokling-v1-5, pro, 5$0.49KlingKling Image to Videokling-v1-5, pro, 10$0.98KlingKling Image to Videokling-v1-5, std, 5$0.28KlingKling Image to Videokling-v1-5, std, 10$0.56KlingKling Image to Videokling-v1-6, pro, 5$0.49KlingKling Image to Videokling-v1-6, pro, 10$0.98KlingKling Image to Videokling-v1-6, std, 5$0.28KlingKling Image to Videokling-v1-6, std, 10$0.56KlingKling Image to Videokling-v1, pro, 5$0.49KlingKling Image to Videokling-v1, pro, 10$0.98KlingKling Image to Videokling-v1, std, 5$0.14KlingKling Image to Videokling-v1, std, 10$0.28KlingKling Image to Videokling-v2-maser, pro, 5s$1.4KlingKling Image to Videokling-v2-maser, pro, 10s$2.8KlingKling Image to Videokling-v2-maser, std, 5s$1.4KlingKling Image to Videokling-v2-maser, std, 10s$2.8KlingKling Lip Sync Video with, Audio5s$0.07KlingKling Lip Sync Video with, Audio10s$0.14KlingKling Lip Sync Video with Text5s$0.07KlingKling Lip Sync Video with Text10s$0.14KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1$0.49KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1-5$0.49KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1-6$0.49KlingKling Start-End Frame to Videopro mode / 10s duration / kling-v1-5$0.98KlingKling Start-End Frame to Videopro mode / 10s duration / kling-v1-6$0.98KlingKling Start-End Frame to Videostandard mode / 5s duration / kling-v1$0.14KlingKling Text to Videopro mode / 5s duration / kling-v1$0.49KlingKling Text to Videopro mode / 5s duration / kling-v2-master$1.4KlingKling Text to Videopro mode / 10s duration / kling-v1$0.98KlingKling Text to Videopro mode / 10s duration / kling-v2-master$2.8KlingKling Text to Videostandard mode / 5s duration / kling-v1$0.14KlingKling Text to Videostandard mode / 5s duration / kling-v1-6$0.28KlingKling Text to Videostandard mode / 5s duration / kling-v2-master$1.4KlingKling Text to Videostandard mode / 10s duration / kling-v1$0.28KlingKling Text to Videostandard mode / 10s duration / kling-v1-6$0.56KlingKling Text to Videostandard mode / 10s duration / kling-v2-master$2.8KlingKling Video Effectsdizzydizzy/bloombloom, 5$0.49KlingKling Video Effectsfuzzyfuzzy/squish/expansion, 5$0.28KlingKling Video ExtendNA$0.28LumaLuma, Text to Imagephoto-flash-1$0.0019LumaLuma, Text to Imagephoto-flash-1$0.0019LumaLuma Image to Imagephoton-1$0.0073LumaLuma Image to Imagephoton-1$0.0073LumaLuma Image to Videoray-1-6, 4k, 5s$3.19LumaLuma Image to Videoray-1-6, 4k, 9s$5.73LumaLuma Image to Videoray-1-6, 540p, 5s$0.2LumaLuma Image to Videoray-1-6, 540p, 9s$0.36LumaLuma Image to Videoray-1-6, 720p, 5s$0.35LumaLuma Image to Videoray-1-6, 720p, 9s$0.64LumaLuma Image to Videoray-1-6, 1080p, 5s$0.8LumaLuma Image to Videoray-1-6, 1080p, 9s$1.43LumaLuma Image to Videoray-2, 4k, 5s$6.37LumaLuma Image to Videoray-2, 4k, 9s$11.47LumaLuma Image to Videoray-2, 540p, 5s$0.4LumaLuma Image to Videoray-2, 540p, 9s$0.72LumaLuma Image to Videoray-2, 720p, 5s$0.71LumaLuma Image to Videoray-2, 720p, 9s$1.27LumaLuma Image to Videoray-2, 1080p, 5s$1.59LumaLuma Image to Videoray-2, 1080p, 9s$2.87LumaLuma Image to Videoray-flash-2, 4k, 5s$2.19LumaLuma Image to Videoray-flash-2, 4k, 9s$3.94LumaLuma Image to Videoray-flash-2, 540p, 5s$0.14LumaLuma Image to Videoray-flash-2, 540p, 9s$0.25LumaLuma Image to Videoray-flash-2, 720p, 5s$0.24LumaLuma Image to Videoray-flash-2, 720p, 9s$0.44LumaLuma Image to Videoray-flash-2, 1080p, 5s$0.55LumaLuma Image to Videoray-flash-2, 1080p, 9s$0.99LumaLuma Text-to-videoray-1-6, 4k, 5s$3.19LumaLuma Text-to-videoray-1-6, 4k, 9s$5.73LumaLuma Text-to-videoray-1-6, 540p, 5s$0.2LumaLuma Text-to-videoray-1-6, 540p, 9s$0.36LumaLuma Text-to-videoray-1-6, 720p, 5s$0.35LumaLuma Text-to-videoray-1-6, 720p, 9s$0.64LumaLuma Text-to-videoray-1-6, 1080p, 5s$0.8LumaLuma Text-to-videoray-1-6, 1080p, 9s$1.43LumaLuma Text-to-videoray-2, 4k, 5s$6.37LumaLuma Text-to-videoray-2, 4k, 9s$11.47LumaLuma Text-to-videoray-2, 540p, 5s$0.4LumaLuma Text-to-videoray-2, 540p, 9s$0.72LumaLuma Text-to-videoray-2, 720p, 5s$0.71LumaLuma Text-to-videoray-2, 720p, 9s$1.27LumaLuma Text-to-videoray-2, 1080p, 5s$1.59LumaLuma Text-to-videoray-2, 1080p, 9s$2.87LumaLuma Text-to-videoray-flash-2, 4k, 5s$2.19LumaLuma Text-to-videoray-flash-2, 4k, 9s$3.94LumaLuma Text-to-videoray-flash-2, 540p, 5s$0.14LumaLuma Text-to-videoray-flash-2, 540p, 9s$0.25LumaLuma Text-to-videoray-flash-2, 720p, 5s$0.24LumaLuma Text-to-videoray-flash-2, 720p, 9s$0.44LumaLuma Text-to-videoray-flash-2, 1080p, 5s$0.55LumaLuma Text-to-videoray-flash-2, 1080p, 9s$0.99GoogleGoogle Veo2 Video Generation5$2.5GoogleGoogle Veo2 Video Generation8$4GoogleGoogle Geminigemini-2.5-flash-preview-04-171.25每百万输入tokens+1.25 每百万输入tokens + 10 每百万输出 tokens (< 200K tokens)GoogleGoogle Geminigemini-2.5-pro-preview-05-060.16每百万输入tokens+0.16 每百万输入tokens + 0.6 每百万输出 tokens + $1每百万输入音频 tokens (< 200K tokens)MinimaxMinimax, Text to Video6s clip$0.43MinimaxMinimax Hailuo-02768P 6s$0.28MinimaxMinimax Hailuo-02768P 10s$0.56MinimaxMinimax Hailuo-021080P 6s$0.49MinimaxMinimax Image to Video6s clip$0.43RecraftRecraft, Creative Upscale ImageNA$0.25RecraftRecraft, Crisp Upscale ImageNA$0.004RecraftRecraft, Image Inpainting1$0.04RecraftRecraft, Image to Image1$0.04RecraftRecraft, Remove BackgroundNA$0.01RecraftRecraft, Replace Background1$0.04RecraftRecraft, Text to Image1$0.04RecraftRecraft, Text to Vector1$0.08RecraftRecraft, Vectorize ImageNA$0.01IdeogramIdeogram, V11, false$0.06IdeogramIdeogram, V11, true$0.02IdeogramIdeogram V21, false$0.08IdeogramIdeogram V21, true$0.05IdeogramIdeogram V31, Balanced$0.06IdeogramIdeogram V31, Quality$0.09IdeogramIdeogram V31, Turbo$0.03RunwayRuway, Text to ImageNA$0.08RunwayRunway, First-Last-Frame to Video5s$0.25RunwayRunway, First-Last-Frame to Video10s$0.5RunwayRunway Image to Video (Gen3a, Turbo)5s$0.25RunwayRunway Image to Video (Gen3a, Turbo)10s$0.5RunwayRunway Image to Video (Gen4, Turbo)5s$0.25RunwayRunway Image to Video (Gen4, Turbo)10s$0.5OpenAIGPT-Image-1 - Actualinput image tokens10/1Mtokens+,inputtexttokens10 / 1M tokens +, input text tokens5 / 1M tokens +,output tokens$40 / 1M tokensOpenAIGPT-Image-1 (近似价格)high, 1024x1024$0.167OpenAIGPT-Image-1 (近似价格)high, 1024x1536$0.25OpenAIGPT-Image-1 (近似价格)high, 1536x1024$0.25OpenAIGPT-Image-1 (近似价格)low, 1024x1024$0.011OpenAIGPT-Image-1 (近似价格)low, 1024x1536$0.016OpenAIGPT-Image-1 (近似价格)low, 1536x1024$0.016OpenAIGPT-Image-1 (近似价格)medium, 1024x1024$0.042OpenAIGPT-Image-1 (近似价格)medium, 1024x1536$0.063OpenAIGPT-Image-1 (近似价格)medium, 1536x1024$0.063OpenAIImage Generation (DALL·E 2)size = 512 \\* 512$0.018OpenAIImage Generation (DALL·E 2)size = 1024 \\* 1024$0.02OpenAIImage Generation (DALL·E 2)size 256 \\* 256$0.016OpenAIImage Generation (DALL·E 3 HD)size = 1024 \\* 1024, hd$0.08OpenAIImage Generation (DALL·E 3 HD)size = 1024 \\* 1792, hd$0.12OpenAIImage Generation (DALL·E 3 HD)size = 1792 \\* 1024, hd$0.12OpenAIImage Generation (DALL·E 3 Std)size = 1024 \\* 1024,std$0.04OpenAIImage Generation (DALL·E 3 Std)size = 1024 \\* 1792, std$0.08OpenAIImage Generation (DALL·E 3 Std)size = 1792 \\* 1024, std$0.08PixversePixVerse, Text to Video360p fast 5s$0.9PixversePixVerse, Text to Video360p normal 5s$0.45PixversePixVerse, Text to Video360p normal 8s$0.9PixversePixVerse, Text to Video540p fast 5s$0.9PixversePixVerse, Text to Video540p normal 5s$0.45PixversePixVerse, Text to Video540p normal 8s$0.9PixversePixVerse, Text to Video720p fast 5s$1.2PixversePixVerse, Text to Video720p normal 5s$0.6PixversePixVerse, Text to Video720p normal 8s$1.2PixversePixVerse, Text to Video1080p normal 5s$1.2PixversePixVerse, Transition Video360p fast 5s$0.9PixversePixVerse, Transition Video360p normal 5s$0.45PixversePixVerse, Transition Video360p normal 8s$0.9PixversePixVerse, Transition Video540p fast 5s$0.9PixversePixVerse, Transition Video540p normal 5s$0.45PixversePixVerse, Transition Video540p normal 8s$0.9PixversePixVerse, Transition Video720p fast 5s$1.2PixversePixVerse, Transition Video720p normal 5s$0.6PixversePixVerse, Transition Video720p normal 8s$1.2PixversePixVerse, Transition Video1080p normal 5s$1.2PixversePixVerse,Image to Video360p fast 5s$0.9PixversePixVerse,Image to Video360p normal 5s$0.45PixversePixVerse,Image to Video360p normal 8s$0.9PixversePixVerse,Image to Video540p fast 5s$0.9PixversePixVerse,Image to Video540p normal 5s$0.45PixversePixVerse,Image to Video540p normal 8s$0.9PixversePixVerse,Image to Video720p fast 5s$1.2PixversePixVerse,Image to Video720p normal 5s$0.6PixversePixVerse,Image to Video720p normal 8s$1.2PixversePixVerse,Image to Video1080p normal 5s$1.2PikaPika, Scenes (Video Image Composition)720p, 5s$0.3PikaPika, Scenes (Video Image Composition)720p, 10s$0.4PikaPika, Scenes (Video Image Composition)1080p, 5s$0.5PikaPika, Scenes (Video Image Composition)1080p, 10s$1.5PikaPika, Start and End Frame to Video720p, 5s$0.2PikaPika, Start and End Frame to Video720p, 10s$0.25PikaPika, Start and End Frame to Video1080p, 5s$0.3PikaPika, Start and End Frame to Video1080p, 10s$1PikaPika, Text to Video720p, 5s$0.2PikaPika, Text to Video720p, 10s$0.6PikaPika, Text to Video1080p, 5s$0.45PikaPika, Text to Video1080p, 10s$1PikaPika,Image to Video720p, 5s$0.2PikaPika,Image to Video720p, 10s$0.6PikaPika,Image to Video1080p, 5s$0.45PikaPika,Image to Video1080p, 10s$1PikaPika Swaps, (Video Object Replacement)NA$0.3PikaPikadditios, (Video Object Insertion)NA$0.3PikaPikaffects, (Video Effects)NA$0.45MoonvalleyImage to video - 5sNA$1.5MoonvalleyText to video - 5sNA$1.5MoonvalleyVideo to video - 5sNA$2.25RodinRodin 3D, Generate - Regular GenerateNA$0.4RodinRodin 3D Generate - Detail, GenerateNA$0.4RodinRodin 3D Generate - Sketch, GenerateNA$0.4RodinRodin 3D Generate - Smooth, GenerateNA$0.4TripoTripo:, Text to Modelany style, false, any quality, false$0.15TripoTripo:, Text to Modelany style, false, any quality, true$0.2TripoTripo:, Text to Modelany style, true, detailed, false$0.35TripoTripo:, Text to Modelany style, true, detailed, true$0.4TripoTripo:, Text to Modelany style, true, standard, false$0.25TripoTripo:, Text to Modelany style, true, standard, true$0.3TripoTripo:, Text to Modelnone, false, any, quality, false$0.1TripoTripo:, Text to Modelnone, false, any quality, true$0.15TripoTripo:, Text to Modelnone, true, detailed, false$0.3TripoTripo:, Text to Modelnone, true, detailed, true$0.35TripoTripo:, Text to Modelnone, true, standard, false$0.2TripoTripo:, Text to Modelnone, true, standard, true$0.25TripoTripo:,Image to Model / Multiview to Modelany style, false, any quality, false$0.25TripoTripo:,Image to Model / Multiview to Modelany style, false, any quality, true$0.3TripoTripo:,Image to Model / Multiview to Modelany style, true, detailed, false$0.45TripoTripo:,Image to Model / Multiview to Modelany style, true, detailed, true$0.5TripoTripo:,Image to Model / Multiview to Modelany style, true, standard, false$0.35TripoTripo:,Image to Model / Multiview to Modelany style, true, standard, true$0.4TripoTripo:,Image to Model / Multiview to Modelnone, false, any, quality, false$0.2TripoTripo:,Image to Model / Multiview to Modelnone, false, any quality, true$0.25TripoTripo:,Image to Model / Multiview to Modelnone, true, detailed, false$0.4TripoTripo:,Image to Model / Multiview to Modelnone, true, detailed, true$0.45TripoTripo:,Image to Model / Multiview to Modelnone, true, standard, false$0.3TripoTripo:,Image to Model / Multiview to Modelnone, true, standard, true$0.35TripoTripo: Convert modelNA$0.1TripoTripo: Refine Draft modelNA$0.3TripoTripo: Retarget rigged modelNA$0.1TripoTripo: Rig modelNA$0.25TripoTripo: Texture modeldetailed$0.2TripoTripo: Texture modelstandard$0.1Stability AIStability, AI Stable Image UltraNA$0.08Stability AIStability AI Stable Diffusion, 3.5 Imagesd3.5-large$0.065Stability AIStability AI Stable Diffusion, 3.5 Imagesd3.5-medium$0.035Stability AIStability AI Upscale, ConservativeNA$0.25Stability AIStability AI Upscale CreativeNA$0.25Stability AIStability AI Upscale FastNA$0.01"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/black-forest-labs/flux-1-kontext",
  "markdown": "# ComfyUI Flux.1 Kontext Pro Image API 节点 ComfyUI 官方示例\n\nFLUX.1 Kontext 是由 Black Forest Labs 开发的一款专业的图像到图像编辑模型，专注于智能理解图像上下文并执行精确编辑。 能够在无需复杂描述的情况下实现多种编辑任务，包括对象修改、风格转换、背景替换、角色一致性编辑和文本编辑等。 Kontext 的核心优势在于其出色的上下文理解能力和角色一致性保持，即使经过多次迭代编辑，也能确保人物特征、构图布局等关键元素保持稳定。 目前，ComfyUI 中支持了 Flux.1 Kontext 的两个模型：\n\n*   **Kontext Pro** 适合编辑、合成和混音。\n*   **Kontext Max** 在排版、提示词精确度和速度方面突破极限。\n\n本篇指南，我们将通过对应的工作流来简单介绍如何使用 Flux.1 Kontext 的相关 API 节点来完成图像编辑。\n\n我们最近更新支持了多图输入工作流，使用新增的 `Image Stitch` 节点，将允许你将多张图像拼接成一张图像，并使用 Flux.1 Kontext 进行编辑。\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![ComfyUI Flux.1 Kontext Pro Image API 节点 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/multiple_image_input.png) 下载下面的图片用于输入或者使用你自己的图片： ![ComfyUI Flux.1 Kontext Pro Image API 节点 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/girl.jpg) ![ComfyUI Flux.1 Kontext Pro Image API 节点 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/dog.jpg) ![ComfyUI Flux.1 Kontext Pro Image API 节点 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/sofa.jpg)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Flux.1 Kontext Pro Image API 节点 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg) 你可参考图片中的序号来完成图工作流的运行：\n\n1.  在 `Load image` 节点中请分别上传提供的图片\n2.  在 `Flux.1 Kontext Pro Image` 修改必要的参数：\n    *   `prompt` 输入你想要编辑的图像的提示词\n    *   `aspect_ratio` 设置原图的高宽比，比例必须在 1:4 到 4:1 之间\n    *   `prompt_upsampling` 设置是否使用提示词上采样，如果开启，会自动修改提示词以获得更丰富的结果，但结果是不可重复的\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的编辑。\n4.  等待 API 返回结果后，你可在 `Save Image` 节点中查看编辑后的图像，对应的图像也会被保存至 `ComfyUI/output/` 目录下。\n\n## Flux.1 Kontext Pro Image API 节点 工作流\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![ComfyUI Flux.1 Kontext Pro Image API 节点 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image.png) 下载下面的图片用于输入或者使用你自己的图片： ![ComfyUI Flux.1 Kontext Pro Image API 节点 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image_input.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Flux.1 Kontext Pro Image API 节点 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg) 你可参考图片中的序号来完成图工作流的运行：\n\n1.  在 `Load Image` 节点中加载需要编辑的图像\n2.  在 `Flux.1 Kontext Pro Image` 修改必要的参数\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的编辑。\n4.  等待 API 返回结果后，你可在 `Save Image` 节点中查看编辑后的图像，对应的图像也会被保存至 `ComfyUI/output/` 目录下。\n\n## Flux.1 Kontext Max Image API 节点 工作流\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![ComfyUI Flux.1 Kontext Max Image API 节点 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image.png) 下载下面的图片用于输入或者使用你自己的图片进行演示： ![ComfyUI Flux.1 Kontext Max Image API 节点 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image_input.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Flux.1 Kontext Max Image API 节点 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg) 你可参考图片中的序号来完成图工作流的运行：\n\n1.  在 `Load Image` 节点中加载需要编辑的图像\n2.  在 `Flux.1 Kontext Max Image` 修改必要的参数\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的编辑。\n4.  等待 API 返回结果后，你可在 `Save Image` 节点中查看编辑后的图像，对应的图像也会被保存至 `ComfyUI/output/` 目录下。\n\n## Flux Kontext 提示词技巧\n\n### 1\\. 基础修改\n\n*   简单直接：`\"Change the car color to red\"`\n*   保持风格：`\"Change to daytime while maintaining the same style of the painting\"`\n\n### 2\\. 风格转换\n\n**原则：**\n\n*   明确命名风格：`\"Transform to Bauhaus art style\"`\n*   描述特征：`\"Transform to oil painting with visible brushstrokes, thick paint texture\"`\n*   保留构图：`\"Change to Bauhaus style while maintaining the original composition\"`\n\n### 3\\. 角色一致性\n\n**框架：**\n\n*   具体描述：`\"The woman with short black hair\"`而非`\"she\"`\n*   保留特征：`\"while maintaining the same facial features, hairstyle, and expression\"`\n*   分步修改：先改背景，再改动作\n\n### 4\\. 文本编辑\n\n*   使用引号：`\"Replace 'joy' with 'BFL'\"`\n*   保持格式：`\"Replace text while maintaining the same font style\"`\n\n## 常见问题解决\n\n### 角色变化过大\n\n❌ 错误：`\"Transform the person into a Viking\"` ✅ 正确：`\"Change the clothes to be a viking warrior while preserving facial features\"`\n\n### 构图位置改变\n\n❌ 错误：`\"Put him on a beach\"` ✅ 正确：`\"Change the background to a beach while keeping the person in the exact same position, scale, and pose\"`\n\n### 风格应用不准确\n\n❌ 错误：`\"Make it a sketch\"` ✅ 正确：`\"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\"`\n\n## 核心原则\n\n1.  **具体明确** - 使用精确描述，避免模糊词汇\n2.  **分步编辑** - 复杂修改分为多个简单步骤\n3.  **明确保留** - 说明哪些要保持不变\n4.  **动词选择** - 用”change”、“replace”而非”transform”\n\n## 最佳实践模板\n\n**对象修改：** `\"Change [object] to [new state], keep [content to preserve] unchanged\"` **风格转换：** `\"Transform to [specific style], while maintaining [composition/character/other] unchanged\"` **背景替换：** `\"Change the background to [new background], keep the subject in the exact same position and pose\"` **文本编辑：** `\"Replace '[original text]' with '[new text]', maintain the same font style\"`\n\n> **记住：** 越具体越好，Kontext 擅长理解详细指令并保持一致性。"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2FClipSetLastLayer",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/update_comfyui",
  "markdown": "# 如何更新 ComfyUI - ComfyUI\n\n尽管我们可能已经在不同版本的各部分章节，对于 ComfyUI 的更新过程都有所说明，但是为了方便用户能够更清楚的了解 ComfyUI 的更新过程，我们会在这部分对于 ComfyUI 的更新进行详细的说明。\n\nComfyUI 便携版提供了便捷的批处理脚本来完成更新操作。\n\n### 更新脚本位置\n\n在便携版安装目录下的 `update` 文件夹中，可以找到以下更新脚本：\n\n```\nComfyUI_windows_portable\n└─ 📂update\n   ├── update.py\n   ├── update_comfyui.bat                           // 更新到最新开发版本\n   ├── update_comfyui_stable.bat                    // 更新到最新稳定版本\n   └── update_comfyui_and_python_dependencies.bat   // 更新依赖（问题修复时使用）\n```\n\n## ComfyUI 的不同版本说明\n\n首先取决于你安装方式的不同，目前 ComfyUI 有以下的几种安装版本，下面的相关链接中已经包含了针对不同版本的更新说明。\n\n## 在更新 ComfyUI 时都需要更新什么内容？\n\n目前 ComfyUI 的更新主要需要确保两部分内容：\n\n1.  更新 ComfyUI 的核心代码\n2.  更新 ComfyUI 的核心依赖，包括必要的 Python 依赖和 ComfyUI 的功能依赖包。\n\n**核心代码**： 新的节点，新的模型支持，新的功能等。 **核心依赖**： 主要包括 ComfyUI 的前端功能，工作流模板，节点帮助文档等。\n\n```\ncomfyui-frontend-package   # ComfyUI 前端功能\ncomfyui-workflow-templates # ComfyUI 工作流模板  \ncomfyui-embedded-docs      # ComfyUI 节点的帮助文档\n```\n\n目前这三个核心依赖项目分别在不同的仓库中维护：\n\n*   [ComfyUI\\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend/) - 前端界面和交互功能\n*   [workflow\\_templates](https://github.com/Comfy-Org/workflow_templates) - 预置工作流模板\n*   [comfyui-embedded-docs](https://github.com/Comfy-Org/embedded-docs) - 节点帮助文档\n\n另外很有必要说明的一点是，开发版本(nightly) 和 稳定版本(release) 的区别：\n\n*   **开发版本(nightly)**：最新 commit 的代码，你可以体验到我们最新提供的一些功能，但是也有可能存在一些潜在的问题\n*   **稳定版本(release)**：是基于稳定版本构建，通常会滞后于开发版本，但是稳定性更高，我们会在相关功能发布稳定后对稳定版本进行支持\n\n目前较多用户总是在更新过程中处于 release 版本或者桌面版，但是发现需要的功能是开发版本中提供的对应版本并不存在，对于此情况请检查本地 `ComfyUI/requirements.txt` 和[nightly 版本的依赖](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt)是否一致，来确定当前是否所有依赖都是我们最新版本的功能支持。\n\n## 常见更新问题\n\n### 更新后前端、工作流模板、节点帮助文档等缺失或滞后\n\n经常有用户只是使用 `git pull` 命令来更新 ComfyUI 的代码，但**忽略了核心依赖更新**，导致出现以下问题：\n\n*   前端功能缺失或显示异常\n*   找不到新增的工作流模板\n*   节点帮助文档过时或缺失\n*   新功能没有对应的前端支持\n\n请在使用了 `git pull` 命令后，在对应的 ComfyUI 环境使用 `pip install -r requirements.txt` 命令来更新依赖。\n\n### 如何正确更新核心依赖\n\n**推荐方法**：使用 `ComfyUI_windows_portable\\update\\update_comfyui.bat` 这个批处理脚本，这个脚本会同时更新 ComfyUI 代码和所有 Python 依赖包。**手动更新依赖**： 如果你需要手动更新依赖，可以使用以下命令：\n\n```\n# 在便携版目录下打开命令行\n.\\python_embeded\\python.exe -m pip install -r ComfyUI\\requirements.txt\n```\n\n### 依赖更新故障排除\n\n如果依赖更新失败，请按以下步骤排查：\n\n### 为什么我更新后找不到新功能？\n\n这是最常见的问题之一：\n\n*   如果你使用的是**桌面版**，因为桌面版是基于稳定版本构建的，它的功能更新相对滞后\n*   请确定你使用的是**开发版本(nightly)**，而不是**稳定版本(release)**\n\n另外还需要确保在更新过程中对应的依赖已经成功更新，如果更新后仍然存在问题，请参考[依赖更新故障排除](#%E4%BE%9D%E8%B5%96%E6%9B%B4%E6%96%B0%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4)章节来排查问题。\n\n### 如何切换到开发（nightly）版本或者稳定（release）版本？\n\n不同版本的区别\n\n*   **特点**：包含最新的 commit 代码\n*   **优势**：可以第一时间体验到最新功能和改进\n*   **风险**：可能存在未发现的 bug 或不稳定因素\n*   **适合人群**：开发者、测试用户、想要体验最新功能的用户\n\n使用 `update_comfyui.bat` 而不是 `update_comfyui_stable.bat`：\n\n```\n# 开发版本（最新功能）\ndouble-click: update_comfyui.bat\n\n# 稳定版本\ndouble-click: update_comfyui_stable.bat\n```\n\n### 更新后出现错误怎么办？\n\n1.  **检查依赖**：运行 `pip install -r requirements.txt` 确保所有依赖都已更新\n2.  **检查自定义节点**：某些自定义节点可能与新版本不兼容\n3.  **回退版本**：如果问题严重，可以回退到之前的稳定版本\n\n如果出现问题，可以参考我们的问题排查页面来解决。\n\n[](https://docs.comfy.org/zh-CN/troubleshooting/overview)\n\n### 如何了解最新功能？\n\n*   **GitHub Releases**：查看 [ComfyUI Releases](https://github.com/comfyanonymous/ComfyUI/releases) 了解稳定版本更新\n*   **GitHub Commits**：查看 [最新提交](https://github.com/comfyanonymous/ComfyUI/commits/master) 了解开发进度\n*   **社区讨论**：关注我们的[博客](https://blog.comfy.org/)和[推特](https://x.com/comfyui)来了解最新动态"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-image-to-image",
  "markdown": "# Luma Image to Image API 节点 ComfyUI 官方示例\n\n[Luma Image to Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-image-to-image) 节点允许你使用Luma AI的技术根据文本提示词修改现有图像，同时保留原始图像的某些特征和结构。 本篇指南中，我们将引导你如何使用对应节点来进行图生图的工作流设置。\n\n你可查阅下面的文档了解对应节点的详细参数设置等\n\n[\n\n## Luma Image to Image 节点文档\n\nLuma Image to Image API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-image-to-image)\n\n## Luma Image to Image API 节点图生图工作流\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![Luma 图生图工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/luma_i2i.png) 请下载下面的图片，我们将用做输入图： ![Luma 图生图工作流输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/input.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![Luma 图生图工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg) 你可参考图片中的序号来完成最基础的工作流运行\n\n1.  在 `Load image` 节点中点击 **upload** 按钮上传输入图片\n2.  （可选）修改工作流的提示词\n3.  （可选）修改`image_weight` 来修改输入图片的权重（越小越接近原图）\n4.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n5.  等待 API 返回结果后，你可在`Save Image`节点中查看生成的图像, 对应的图片也会被保存至`ComfyUI/output/` 目录下\n\n### 3\\. 不同 `image_weight` 参数输入结果\n\n![权重对比](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/i2i_image_weight.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-camera",
  "markdown": "# ComfyUI Wan2.1 Fun Camera 官方原生示例\n\n**Wan2.1 Fun Camera** 是阿里团队推出的视频生成项目，专注于通过摄像机运动来控制视频生成效果。 **模型权重下载地址**：\n\n*   [14B 版本](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-14B-Control-Camera)\n*   [1.3B 版本](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-1.3B-Control-Camera)\n\n**代码仓库**：[VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun) **目前 ComfyUI 已原生支持了 Wan2.1 Fun Camera 模型**。\n\n## 相关模型安装\n\n这些模型你仅需要安装一次，另外在对应的工作流图片中也包含了模型下载信息，你可以选择你喜欢的方式下载模型。 下面的所有模型你可以在 [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) 找到 **Diffusion Models** 选择 1.3B 或 14B：\n\n*   [wan2.1\\_fun\\_camera\\_v1.1\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors)\n*   [wan2.1\\_fun\\_camera\\_v1.1\\_14B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_14B_bf16.safetensors)\n\n下面的模型，如果你使用过 Wan2.1 的相关模型，那么你应该已经有了下面的模型，如果没有，请下载下面的模型： **Text Encoders** 选择其中一个：\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)\n\n文件保存位置：\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│ ├── 📂 diffusion_models/\n│ │   ├── wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors # 1.3B 版本\n│ │   └── wan2.1_fun_camera_v1.1_14B_bf16.safetensors # 14B 版本\n│ ├── 📂 text_encoders/\n│ │   └── umt5_xxl_fp8_e4m3fn_scaled.safetensors\n│ ├── 📂 vae/\n│ │   └── wan_2.1_vae.safetensors\n│ └── 📂 clip_vision/\n│     └── clip_vision_h.safetensors\n```\n\n## ComfyUI Wan2.1 Fun Camera 1.3B 原生工作流示例\n\n### 1\\. 工作流相关文件下载\n\n#### 1.1 工作流文件\n\n下载下面的视频，并拖入 ComfyUI 中以加载对应的工作流：\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B.json)\n\n#### 1.2 输入图片下载\n\n请下载下面的图片，我们将作为起始帧： ![输入参考图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B_input.jpg)\n\n### 2\\. 按步骤完成工作流\n\n![Wan2.1 Fun Camera 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg)\n\n1.  确保加载了正确版本的模型文件：\n    *   1.3B 版本：`wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors`\n    *   14B 版本：`wan2.1_fun_camera_v1.1_14B_bf16.safetensors`\n2.  确保 `Load CLIP` 节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`\n4.  确保 `Load CLIP Vision` 节点加载了 `clip_vision_h.safetensors`\n5.  在 `Load Image` 节点上传起始帧\n6.  修改 Prompt，如果你使用了你自己的图像输入\n7.  在 `WanCameraEmbedding` 节点设置相机动作\n8.  点击 `Run` 按钮，或使用快捷键 `Ctrl(cmd) + Enter(回车)` 执行生成\n\n## ComfyUI Wan2.1 Fun Camera 14B 工作流及输入图片\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B.json)\n\n**输入图片** ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B_input.jpg) \n\n## 性能参考\n\n**1.3B 版本**：\n\n*   512×512 RTX 4090 生成 81 帧约需 72 秒\n\n**14B 版本**：\n\n*   RTX4090 24GB 显存在生成 512×512 分辨率时可能会出现显存不足, 在 A100 上运行尺寸过大时也出现过显存不足的情况\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/hunyuan-video",
  "markdown": "# ComfyUI 混元视频示例 - ComfyUI\n\n混元视频（Hunyuan Video）系列是是[腾讯](https://huggingface.co/tencent)研发并开源的，该模型以混合架构为核心，支持[文本生成视频](https://github.com/Tencent/HunyuanVideo) 和[图生成视频](https://github.com/Tencent/HunyuanVideo-I2V)，参数规模达 13B。 技术特点：\n\n*   **核心架构：** 采用类似Sora的DiT（Diffusion Transformer）架构，有效融合了文本、图像和动作信息，提高了生成视频帧之间的一致性、质量和对齐度，通过统一的全注意力机制实现多视角镜头切换，确保主体一致性。\n*   **3D VAE：** 定义的 3D VAE 将视频压缩到紧凑的潜空间，同时压缩视频，使得图生视频的生成更加高效。\n*   **卓越的图像-视频-文本对齐：** 使用 MLLM 文本编码器，在图像和视频生成中表现出色，能够更好地遵循文本指令，捕捉细节，并进行复杂推理。\n\n你可以在[混元视频](https://github.com/Tencent/HunyuanVideo) 和[混元视频-I2V](https://github.com/Tencent/HunyuanVideo-I2V) 了解到更多开源信息。 本篇指南将引导你完成在 ComfyUI 中 **文生视频** 和 **图生视频** 的视频生成。\n\n## 工作流共用模型\n\n在文生视频和图生视频的工作流中下面的这些模型是共有的，请完成下载并保存到指定目录中\n\n*   [clip\\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)\n*   [llava\\_llama3\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)\n*   [hunyuan\\_video\\_vae\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)\n\n保存位置：\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── llava_llama3_fp8_scaled.safetensors\n│   ├── vae/\n│   │   └── hunyuan_video_vae_bf16.safetensors\n```\n\n## 混元文生视频工作流\n\n混元文生视频开源于 2024 年 12 月，支持通过自然语言描述生成 5 秒的短视频，支持中英文输入。\n\n### 1\\. 文生视频相关工作流\n\n请保存下面的图片，并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - 混元文生视频](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp) \n\n### 2\\. 混元文生图模型\n\n请下载 [hunyuan\\_video\\_t2v\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) 并保存至 `ComfyUI/models/diffusion_models` 文件夹中 确保包括共用模型文件夹有以下完整的模型文件：\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors                       // 共用模型\n│   │   └── llava_llama3_fp8_scaled.safetensors      // 共用模型\n│   ├── vae/\n│   │   └── hunyuan_video_vae_bf16.safetensors       // 共用模型\n│   └── diffusion_models/\n│       └── hunyuan_video_t2v_720p_bf16.safetensors  // T2V 模型\n```\n\n### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI 混元视频 T2V 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg)\n\n1.  确保在`DualCLIPLoader`中下面的模型已加载：\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  确保在`Load Diffusion Model`加载了`hunyuan_video_t2v_720p_bf16.safetensors`\n3.  确保在`Load VAE`中加载了`hunyuan_video_vae_bf16.safetensors`\n4.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n## 混元图生视频工作流\n\n混元图生视频模型开源于2025年3月6日，基于 HunyuanVideo 框架，支持将静态图像转化为流畅的高质量视频，同时开放了 LoRA 训练代码，支持定制特殊视频效果如：头发生长、物体变形等等。 目前混元图生视频模型分为两个版本：\n\n*   v1 “concat” : 视频的运动流畅性较好，但比较少遵循图像引导\n*   v2 “replace”: 在v1 更新后的次日更新的版本，图像的引导性较好，但相对于 V1 版本似乎不那么有活力\n\nv1 “concat”\n\n![HunyuanVideo v1](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp)\n\nv2 “replace”\n\n![HunyuanVideo v2](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp)\n\n### v1 及 v2 版本共用的模型\n\n请下载下面的文件，并保存到 `ComfyUI/models/clip_vision` 目录中\n\n*   [llava\\_llama3\\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)\n\n### v1 “concat” 图生视频工作流\n\n#### 1\\. 工作流及相关素材\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - 混元图生视频v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp) 请下载下面的图片，我们将使用它作为图生视频的起始帧 ![起始帧](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png) \n\n#### 2\\. v1 版本模型\n\n*   [hunyuan\\_video\\_image\\_to\\_video\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)\n\n确保包括共用模型文件夹有以下完整的模型文件：\n\n```\nComfyUI/\n├── models/\n│   ├── clip_vision/\n│   │   └── llava_llama3_vision.safetensors                     // I2V 共用模型\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors                                  //  共用模型\n│   │   └── llava_llama3_fp8_scaled.safetensors                 //  共用模型\n│   ├── vae/\n│   │   └── hunyuan_video_vae_bf16.safetensors                  // 共用模型\n│   └── diffusion_models/\n│       └── hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 \"concat\" 版本模型\n```\n\n#### 3\\. 按步骤完成工作流\n\n![ComfyUI 混元视频I2V v1 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg)\n\n1.  确保 `DualCLIPLoader` 中下面的模型已加载：\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  确保 `Load CLIP Vision` 加载了 `llava_llama3_vision.safetensors`\n3.  请在 `Load Image Model` 加载了 `hunyuan_video_image_to_video_720p_bf16.safetensors`\n4.  确保 `Load VAE` 中加载了 `hunyuan_video_vae_bf16.safetensors`\n5.  确保 `Load Diffusion Model` 中加载了 `hunyuan_video_image_to_video_720p_bf16.safetensors`\n6.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n### v2 “replace” 图生视频工作流\n\nv2 版本的工作流与 v1 版本的工作流基本相同，你只需要下载一个 **replace** 的模型，然后在 `Load Diffusion Model` 中使用即可。\n\n#### 1\\. 工作流及相关素材\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - 混元图生视频v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp) 请下载下面的图片，我们将使用它作为图生视频的起始帧 ![起始帧](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png) \n\n#### 2\\. v2 版本模型\n\n*   [hunyuan\\_video\\_v2\\_replace\\_image\\_to\\_video\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)\n\n确保包括共用模型文件夹有以下完整的模型文件：\n\n```\nComfyUI/\n├── models/\n│   ├── clip_vision/\n│   │   └── llava_llama3_vision.safetensors                                // I2V 共用模型\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors                                             //  共用模型\n│   │   └── llava_llama3_fp8_scaled.safetensors                            //  共用模型\n│   ├── vae/\n│   │   └── hunyuan_video_vae_bf16.safetensors                             //  共用模型\n│   └── diffusion_models/\n│       └── hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 \"replace\" 版本模型\n```\n\n#### 3\\. 按步骤完成工作流\n\n![ComfyUI 混元视频I2V v2 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg)\n\n1.  确保 `DualCLIPLoader` 中下面的模型已加载：\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  确保 `Load CLIP Vision` 加载了 `llava_llama3_vision.safetensors`\n3.  请在 `Load Image Model` 加载了 `hunyuan_video_image_to_video_720p_bf16.safetensors`\n4.  确保 `Load VAE` 中加载了 `hunyuan_video_vae_bf16.safetensors`\n5.  确保 `Load Diffusion Model` 中加载了 `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`\n6.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n## 开始你的尝试\n\n下面是我们提供了一些示例图片和对应的提示词，你可以基于这些内容，进行修改，创作出属于你自己的视频。 ![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png)\n\n```\nFuturistic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/samurai.png)\n\n```\nSamurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/a_flying_car.png)\n\n```\nflying car fastly moving and flying through the city\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png)\n\n```\ncyberpunk car race in night city, dynamic, super fast, fast shot\n```\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/system_requirements",
  "markdown": "# 系统要求 - ComfyUI\n\n在本篇我们将介绍安装 ComfyUI 的系统要求, 由于 ComfyUI 的更新频繁，本篇文档未必能够及时更新，请参考[ComfyUI](https://github.com/comfyanonymous/ComfyUI)中的相关说明。 无论是哪个版本的 ComfyUI，都是运行在一个独立的 Python 环境中。\n\n### 操作系统要求\n\n目前我们支持以下操作系统：\n\n*   Windows\n*   Linux\n*   macOS（支持 Apple Silicon M1/M2）\n\n请参考[ComfyUI Windows 和 Linux 手动安装章节](https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#manual-install-windows-linux)了解详细的安装步骤。 你可以参考下面的章节来了解不同系统和版本 ComfyUI 的安装方式，在不同版本的安装中我们简单对安装的系统要求进行了说明。\n\n### Python 版本\n\n*   推荐 Python 3.12\n*   支持 Python 3.13（部分自定义节点可能不兼容）\n\n### 支持的硬件\n\n*   NVIDIA 显卡\n*   AMD 显卡\n*   Intel 显卡（包括 Arc 系列，支持 IPEX）\n*   Apple Silicon（M1/M2）\n*   Ascend NPU\n*   Cambricon MLU\n*   CPU（可用 —cpu 参数，速度较慢）\n\n请参考[ComfyUI Windows 和 Linux 手动安装章节](https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#manual-install-windows-linux)了解详细的安装步骤。\n\n### 依赖\n\n*   需安装 PyTorch（不同硬件需不同版本，详见下方）\n*   需安装 ComfyUI 的 requirements.txt 中所有依赖\n\n[\n\n## Manual Installation\n\n请参考手动安装章节了解详细的安装步骤。\n\n\n\n](https://docs.comfy.org/zh-CN/installation/manual_install)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/ideogram/ideogram-v3",
  "markdown": "# ComfyUI Ideogram 3.0 API 节点官方示例\n\nIdeogram 3.0 是由 Ideogram 发布的一款强大的文本到图像生成模型，以其卓越的照片级逼真度、精确的文本渲染和一致的风格控制而著称。 目前[Ideogram V3](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v3) 节点支持以下两种模式：\n\n*   文生图模式\n*   图像编辑模式（当同时提供了图像和遮罩输入时）\n\n你可查阅下面的文档了解对应节点的详细参数设置等\n\n*   [Ideogram V3](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v3)\n\n## Ideogram 3.0 API 节点文生图模式\n\n当你只使用 [Ideogram V3](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v3) 而不输入图像和蒙版时，节点将使用文生图模式。\n\n### 1\\. 工作流文件下载\n\n请下载下面的文件，并拖入 ComfyUI 以加载对应的工作流 ![Ideogram 3.0 ComfyUI 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/ideogram/v3/ideogram_v3_t2i.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![Ideogram 3.0 工作流执行步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg) 你可参考图片中的序号来完成最基础的工作流运行\n\n1.  在 `Ideogram V3` 节点的 `prompt` 中输入你想要生成的图像的描述\n2.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n3.  等待 API 返回结果后，你可在`Save Image`节点中查看生成的图像, 对应的图像也会被保存至`ComfyUI/output/` 目录下\n\n## Ideogram 3.0 API 节点图像编辑模式\n\n\\[待更新\\]\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2Fapi-node%2Fimage%2Fstability-ai%2Fstability-ai-stable-image-ultra",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/manual_install",
  "markdown": "# 如何在不同系统上手动安装 ComfyUI - ComfyUI\n\n对于 ComfyUI 的安装， 主要分为几个步骤\n\n1.  创建一个虚拟环境(避免污染系统级 Python 环境)\n2.  克隆 ComfyUI 代码仓库\n3.  安装依赖\n4.  启动 ComfyUI\n\n你也可以参考 [ComfyUI CLI](https://docs.comfy.org/zh-CN/comfy-cli/getting-started) 来安装 ComfyUI, 它是一个命令行工具，可以方便地安装 ComfyUI 并管理其依赖。\n\n## （可选）创建虚拟环境\n\n[Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). 这将帮助您安装 ComfyUI 所需的正确版本的 Python 和其他库。 使用 Conda 创建一个环境。\n\n```\nconda create -n comfyenv\nconda activate comfyenv\n```\n\n## 克隆代码仓库\n\n你需要保证你的系统上已经安装了 [Git](https://git-scm.com/downloads), 首先你需要打开终端（命令行）,然后克隆代码仓库。\n\n```\ngit clone git@github.com:comfyanonymous/ComfyUI.git\n```\n\n## 安装GPU 及 ComfyUI 依赖\n\n## 添加外部模型路径\n\n如果你想要在 `ComfyUI/models` 之外管理你的模型文件，可能出于以下原因:\n\n*   你有多个 ComfyUI 实例，你想要让这些实例共享模型文件，从而减少磁盘占用\n*   你有多个不同的类型的 GUI 程序，如：WebUI, 你想要他们共用模型文件\n*   模型文件无法被识别或读取到\n\n我们提供了通过 `extra_model_paths.yaml` 配置文件来添加额外模型搜索路径的方法。\n\n### 不同 ComfyUI 版本配置文件位置\n\n对于[便携版](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)和[手动安装](https://docs.comfy.org/zh-CN/installation/manual_install)的 ComfyUI版本，你可以在 ComfyUI 的根目录下找到 `extra_model_paths.yaml.example` 的示例文件\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\n复制并重命名为 `extra_model_paths.yaml` 来使用, 并保持在 ComfyUI 的根目录下, 路径应该是 `ComfyUI/extra_model_paths.yaml`你也可以在 [这里](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 找到配置示例文件\n\n### 配置示例\n\n比如，你需要额外让 ComfyUI 识别的模型文件位于下面的文件夹:\n\n```\n📁 YOUR_PATH/\n  ├── 📁models/\n  |   ├── 📁 loras/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 checkpoints/\n  |   │   └── xxxxx.safetensors\n  |   ├── 📁 vae/\n  |   │   └── xxxxx.safetensors\n  |   └── 📁 controlnet/\n  |       └── xxxxx.safetensors\n```\n\n那么你可以进行如下的配置来让 ComfyUI 识别到你设备上的模型路径\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\n或者使用\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\n或者你也可以参考默认的 [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) 来配置，保存之后， 需要 **重启 ComfyUI** 才能生效。 下面是完整的原始的配置配置示例:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### 添加额外自定义节点路径\n\n除了添加外部模型之外，你同样可以添加不在 ComfyUI 默认路径下的自定义节点路径\n\n下面是一个简单的配置示例（Mac 系统），请根据你的实际情况进行修改，并新增到对应的配置文件中，保存后需要 **重启 ComfyUI** 才能生效:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/install_custom_node",
  "markdown": "# 如何在 ComfyUI 中安装自定义节点 - ComfyUI\n\n## 什么是自定义节点？\n\n自定义节点是ComfyUI的扩展插件，能够增加新功能，如高级图像处理、机器学习微调、颜色调整等。这些节点由社区开发，可显著扩展ComfyUI的基础功能。\n\n所有的自定义节点安装都需要完成下面的两个步骤：\n\n1.  克隆节点代码到 `ComfyUI/custom_nodes` 目录\n2.  安装对应的 Python 依赖\n\n在本文中我们将介绍三种安装方法，下面是对应的优缺点对比，由于目前 [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) 还未正式加入到核心依赖中，但在未来 ComfyUI Manager 将会成为核心依赖的一部分，但本部分的指南仍旧提供了其它安装插件的指南,以便满足你的需求。\n\n| 方法  | 优点  | 缺点  |\n| --- | --- | --- |\n| **ComfyUI Manager** (推荐) | 1\\. 自动化安装  <br>2\\. 依赖处理  <br>3\\. 图形界面 | 不在 registry 中注册的节点无法通过 Manager 直接搜索到 |\n| **Git 克隆** | 可以安装不在 registry 中注册的节点 | 1\\. 需要Git知识  <br>2\\. 手动处理依赖  <br>3\\. 存在安装风险 |\n| **代码仓库 ZIP 下载** | 1\\. 无需Git  <br>2\\. 手动处理一切 | 1\\. 需要手动处理依赖  <br>2\\. 无版本控制  <br>3\\. 存在安装风险 |\n\n提示： 在安装自定义节点前，请先查看插件的 README 文件，了解插件的安装方法和使用方法，有些插件有对特定的包和环境有要求，比如对应的模型、依赖的版本、常见问题解决等\n\n## 方法一:ComfyUI Manager（推荐）\n\n## 方法二:使用 Git 进行安装手动安装\n\n适用于Manager中找不到的新节点或需要特定版本时， 需要你的系统中已经安装好了 [Git](https://git-scm.com/)\n\n## 方法三:ZIP下载安装\n\n适用于无法使用 Git 或 Manager 安装的用户"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/chat",
  "markdown": "# OpenAI Chat API 节点 ComfyUI 官方示例\n\nOpenAI 是一家专注于生成式 AI 的科技公司，提供强大的对话功能。目前 ComfyUI 已集成 OpenAI API，你可以直接在 ComfyUI 中使用相关节点来完成对话功能。 本篇指南中，我们将引导你完成对应对话功能。\n\n### 1\\. 工作流文件下载\n\n请下载下面的 Json 文件并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/openai/api_openai_chat.json)\n\n### 2\\. 按步骤完成工作流的运行\n\n![OpenAI Chat Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg)\n\n你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Load Image` 节点中加载你需要 AI 的解读图片\n2.  (可选) 如果需要你可以修改`OpenAI Chat Advanced Options` 中的设定，从而让 AI 来执行特定任务\n3.  在 `OpenAI Chat` 节点你可以修改 `Prompt` 来设置对话的提示词,或者修改 `model` 来选择不同的模型\n4.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行对话。\n5.  等待 API 返回结果后，你可在 `Preview Any` 节点中查看对应 AI 返回的内容。\n\n### 3\\. 补充说明\n\n*   目前文件输入节点 `OpenAI Chat Input Files` 需要先将文件上传至`ComfyUI/input/` 目录下， 此节点正在改进，我们会在更新后修改模板\n*   工作流中提供了使用 `Batch Images` 来输入的示例，如果你有多张图片需要 AI 解读，可参考步骤图在使用右键来将对应的节点模式设置为 `总是（always）` 来启用"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/overview",
  "markdown": "# ComfyUI 设置概览 - ComfyUI\n\n这个部分是关于 ComfyUI 前端设置菜单中详细的设置说明， 对于所有的用户设置将会自动保存到 `ComfyUI/user/default/comfy.settings.json` 文件夹 你可以使用 `ctrl + ,` 快捷键来打开设置面板， 然后点击对应的设置选项进行设置。 由于自定义节点也可以在菜单中注册对应的设置类目，在我们的官方文档说明中目前仅包含原生的设置内容，另外有部分选项设置 **仅针对 ComfyUI 桌面版** 有效，我们也在对应页面中做了注释说明。"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/stability-ai/stable-image-ultra",
  "markdown": "# Stability AI Stable Image Ultra API 节点 ComfyUI 官方示例\n\n[Stability Stable Image Ultra](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra) 节点允许你使用 Stability AI 的 Stable Image Ultra 模型，通过文本提示词或参考图像创建高质量、细节丰富的图像内容。 本篇指南中，我们将引导你如何使用对应节点来进行文生图和图生图的工作流设置。\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![Stability AI Stable Image Ultra 文生图工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_image_ultra_t2i.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![Stability AI  Stable Image Ultra 文生图步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  (可选)修改 `Stability AI Stable Image Ultra` 节点中的 `prompt` 参数，输入你想要生成的图像描述。提示词越详细，生成的图像质量往往越好。你可以使用`(词:权重)`格式来控制特定词的权重，例如：`天空是清爽的(蓝色:0.3)和(绿色:0.8)`表示天空是蓝色和绿色的，但绿色更为突出。\n2.  (可选)选择 `style_preset` 参数来控制图像的视觉风格。不同的预设风格会产生不同风格特点的图像，如”cinematic”（电影感）、“anime”（动漫风格）等。选择”None”则不应用任何特定风格。\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的生成。\n4.  等待 API 返回结果后，你可在 `Save Image` 节点中查看生成的图像，对应的图像也会被保存至 `ComfyUI/output/` 目录下。\n\n### 3\\. 补充说明\n\n*   **提示词(Prompt)**：提示词是生成过程中最重要的参数之一，详细、清晰的描述会带来更好的效果。可以包含场景、主体、颜色、光照、风格等元素。\n*   **风格预设(Style Preset)**：提供多种预设风格，如电影感、动漫风、数字艺术等，能够快速定义图像的整体风格。\n*   **负面提示词(Negative Prompt)**：用于指定不希望在生成图像中出现的元素，可以帮助避免常见问题，如额外的肢体、扭曲的面部等。\n*   **Seed 参数**：可以用于复现或微调生成结果，对于创作过程中的迭代很有帮助。\n*   当前 `Load Image` 节点为 “绕过（Bypass）” 模式，如需启用可以参考步骤图在对应节点上右键然后将“模式（Mode）”设置为“总是（Always）” 来启用输入,即可转为图生图模式\n\n## Stability AI Stable Image Ultra 图生图工作流\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![Stability Stable Image Ultra 图生图工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/stable_image_ultra_i2i.png) 下载下面的图片我们将用于输入图片 ![Stability Stable Image Ultra 图生图工作流输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/input.png) \n\n### 2\\. 按步骤完成工作流的运行\n\n![Stability Stable Image Ultra 图生图步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg) 你可参考图片中的序号来完成图生图工作流运行：\n\n1.  通过 `Load Image` 节点加载一张参考图像，该图像将作为生成的基础。\n2.  (可选)修改 `Stability Stable Image Ultra` 节点中的 `prompt` 参数，描述你希望在参考图像基础上改变或增强的元素。\n3.  (可选)调整 `image_denoise` 参数（范围0.0-1.0）来控制对原始图像的修改程度：\n    *   值越接近0.0，生成的图像越接近输入的参考图像\n    *   值越接近1.0，生成的图像越接近纯文本生成的效果\n4.  (可选)同样可以设置 `style_preset` 和其他参数来进一步控制生成效果。\n5.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的生成。\n6.  等待 API 返回结果后，你可在 `Save Image` 节点中查看生成的图像，对应的图像也会被保存至 `ComfyUI/output/` 目录下。\n\n### 3\\. 补充说明\n\n**图像去噪强度(Image Denoise)**：这个参数决定了生成过程中保留原始图像特征的程度，是图生图模式中最关键的调节参数,下图是不同的去噪强度下生成的图像效果 ![Stability Stable Image Ultra 图生图去噪强度说明](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg)\n\n*   **参考图像选择**：选择具有清晰主体和良好构图的图像通常能获得更好的结果。\n*   **提示词技巧**：在图生图模式中，提示词应该更多地关注你希望改变或增强的部分，而不需要描述已经存在于图像中的所有元素。\n\n## 相关节点详解\n\n你可查阅下面的文档了解对应节点的详细参数设置等\n\n[\n\n## Stability Stable Image Ultra 节点文档\n\nStability Stable Image Ultra API 节点说明文档\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/user",
  "markdown": "# ComfyUI 账号管理 - ComfyUI\n\n账号系统是为了支持 `API Nodes` 节点而新增的，`API Nodes`支持了对闭源模型 API 的调用，这大大扩展了 ComfyUI 的可能性，由于对应的 API 调用需要消耗 Token，所以我们增加了对应的用户系统。 当前我们支持以下几种登录方式：\n\n*   邮箱登录\n*   Google 登录\n*   Github 登录\n*   API Key 登录(非白名单网站授权使用)\n\n登录要求相关及说明，我们在本篇文档中会进行对应说明\n\n你可能至少需要使用 [ComfyUI v0.3.0](https://github.com/comfyanonymous/ComfyUI/releases/tag/v0.3.30) 版本才能使用账号系统,确保对应前端版本至少为`1.17.11`,有时候前端可能会安装失败而导致回滚到旧版本，所以请在`设置` -> `关于` 查看前端版本是否大于`1.17.11` 在部分地区可能会因为网络限制无法正常访问登录 API 导致登录超时或者失败， 在登录前请**确保你的网络环境不会被限制对应 API 的访问**，保证能够正常访问 Google 或者 Github 等网站。\n\n## 网络要求\n\n要使用API，你必须处于安全的网络环境中：\n\n*   允许从`127.0.0.1`或`localhost`访问。\n*   不支持使用`--listen`参数通过局域网访问API节点。\n*   在未启用 SSL 证书即非 `https` 开头的站点你可能无法成功登录\n*   你可能无法在不在我们白名单的站点中登录（可以通过 API Key 登录）\n*   确保你能够正常连接我们的服务（在某些地区可能需要代理来访问）。\n\n## 如何进行登录\n\n在 `设置` -> `用户` 中进行登录： ![ComfyUI 用户界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/user.jpg)\n\n## 登录方式\n\n![user-login](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/user-login.jpg) 如果是首次登录，请首先创建一个账户。\n\n## 使用 API Key 进行登录\n\n由于目前并非所有 ComfyUI 的相关部署都是在我们的域名授权白名单中，所以在通过非白名单的网站登录时，在近期（2025-05-10）的更新中我们提供了 API Key 登录，下面是使用 API Key 登录的相关步骤：\n\n## 登录后状态\n\n登录后在 CofmyUI 界面顶部菜单栏显示登录按钮，并可以通过该按钮打开对应的登录界面，并可以在设置菜单中退出对应的账号 ![user-logged](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user-logged.jpg) ![menu-user-logged](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/menu-user-logged.jpg)\n\n## 常见问题"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/comfy",
  "markdown": "# Comfy 设置 - ComfyUI\n\n## API 节点\n\n### 显示 API 节点定价徽章\n\n*   **默认值**：启用\n*   **功能**：控制是否在 API 节点上显示定价徽章，帮助用户识别 API 节点的使用成本\n\n![启用效果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/api_node_pricing_badge.jpg)\n\n> 更多关于 API 节点请参考 [API 节点](https://docs.comfy.org/zh-CN/tutorials/api-nodes/overview)\n\n## 开发者模式\n\n### 启用开发模式选项\n\n*   **默认值**：禁用\n*   **功能**：启用开发者模式选项（如API保存等）\n\n## 编辑令牌权重\n\n### Ctrl+上/下 精度\n\n*   **默认值**：0.01\n*   **功能**：当你在使用类型 CLIPTextEncode 或者文本框输入类的节点组件时，使用 Ctrl+上/下 可以快速调整权重，这个选项会改变每次调整的权重值\n\n## 区域设置（本地化）\n\n### 语言\n\n*   **选项**：中文 (Chinese)、English (英文)、日本語 (日文)、한국어 (韩文)、Русский (俄文)、Español (西班牙语)、Français (法语)\n*   **默认值**：自动检测浏览器语言\n*   **功能**：修改 ComfyUI 界面显示语言\n\n## 菜单\n\n### 使用新菜单\n\n*   **默认值**：顶部\n*   **功能**：选择菜单界面和位置，目前仅支持顶部、底部、禁用\n\n菜单界面将会显示在工作界面的顶部 ![顶部菜单](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/UseNewMenu_top.jpg) \n\n## 模型库\n\n模型库指的是 ComfyUI 侧边菜单栏中的模型管理功能，你可以通过这个功能来查看你在 `ComfyUI/models` 及额外配置的模型文件夹中的模型 ![模型库](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/sidepanel/model_library.jpg)\n\n### 模型库名称格式\n\n*   **默认值**：标题\n*   **功能**：选择在模型库树视图中显示的名称格式，目前仅支持文件名、标题\n\n### 自动加载所有模型文件夹\n\n*   **默认值**：禁用\n*   **功能**：是否在点击模型库时自动检测所有文件夹下的模型文件，启用可能导致加载延迟（需要循环遍历所有文件夹），禁用时只有点击文件夹名称才会加载对应文件夹下的文件\n\n## 节点\n\n在 ComfyUI 的迭代过程中，我们会对一些节点进行调整，也会启用一些节点，这些节点可能在未来版本中发生重大变化或被移除，但是为了保证兼容性类似弃用的节点并没有被移除，你可以通过下面的设置来启用是否显示 **实验性节点** 和 **已弃用节点**\n\n### 显示已弃用节点\n\n*   **默认值**：禁用\n*   **功能**：控制是否在搜索中显示已弃用的节点，已弃用节点在UI中默认隐藏，但在现有工作流中仍然有效\n\n![已弃用节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/depr_node.jpg)\n\n### 显示实验性节点\n\n*   **默认值**：启用\n*   **功能**：控制是否在搜索中显示实验性节点，实验性节点说一些新的功能支持，但未完全稳定，可能在未来版本中发生变化或被移除\n\n![已弃用节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/beta_node.jpg)\n\n## 节点搜索框\n\n### 节点建议数量\n\n*   **默认值**：5\n*   **功能**：用于修改相关节点上下文菜单中推荐的节点数量，数值越大显示的相关推荐节点数量越多\n\n![节点建议数量](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_suggestions.jpg)\n\n### 显示节点频率\n\n*   **默认值**：禁用\n*   **功能**：控制是否在搜索结果中显示节点使用频率\n\n![节点频率](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_frequency.png)\n\n### 显示节点ID名称\n\n*   **默认值**：禁用\n*   **功能**：控制是否在搜索结果中显示节点ID名称\n\n![节点ID名称](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_id_name.jpg)\n\n### 显示节点类别\n\n*   **默认值**：启用\n*   **功能**：控制是否在搜索结果中显示节点类别，帮助用户了解节点的分类信息\n\n### 节点预览\n\n*   **默认值**：启用\n*   **功能**：控制是否在搜索结果中显示节点预览，方便你快速预览节点\n\n### 节点搜索框\n\n*   **默认值**：默认\n*   **功能**： 选择节点搜索框的实现方式（实验性功能），如果选择 `litegraph（旧版）` 则会切换到早期的 ComfyUI 的搜索框\n\n## 节点组件\n\n### 组件控制模式\n\n*   **选项**：之前、之后\n*   **功能**：控制节点组件值的更新时机是在工作流运行之前还是滞后，比如更新 seed 种子的值\n\n### 文本框组件拼写检查\n\n*   **默认值**：禁用\n*   **功能**：控制文本域小部件是否启用拼写检查, 在文本输入时提供拼写检查功能, 这一功能是通过浏览器的 spellcheck 属性实现的\n\n## 队列\n\n### 队列历史大小\n\n*   **默认值**：100\n*   **功能**：控制侧边栏队列历史面板里记录的队列历史大小，数值越大记录的队列历史越多，数量多大时加载页面时也会占用更多内存\n\n## 执行按钮\n\n### 批处理计数限制\n\n*   **默认值**：100\n*   **功能**：设置单次点击添加到队列的最大任务数量，防止意外添加过多任务到队列\n\n## 验证和开发者设置\n\n### 验证节点定义\n\n*   **默认值**：禁用\n*   **功能**：控制是否在启动时验证所有节点定义（慢），仅推荐给节点开发者使用，当启用时系统会使用 Zod 模式对每个节点定义进行严格验证，这一功能会消耗更多内存和时间\n*   **错误处理**：验证失败的节点定义会被跳过，并在控制台输出警告信息\n\n### 校验工作流\n\n*   **默认值**：启用\n*   **功能**：确保工作流的结构和连接正确性, 如果启用，系统会调用 `useWorkflowValidation().validateWorkflow()` 对工作流数据进行验证\n*   **验证过程**：验证过程包含两个步骤：\n    *   模式验证: 使用 Zod 模式验证工作流结构\n    *   链接修复: 检查并修复节点间的连接问题\n*   **错误处理**：验证失败时会显示错误提示，但不会阻止工作流加载\n\n## 窗口\n\n### 关闭窗口时显示确认\n\n*   **默认值**：启用\n*   **功能**：当存在已修改但未保存的工作流时，控制关闭浏览器窗口或标签页时是否显示确认，防止意外关闭窗口导致未保存的工作流丢失\n\n## 工作流\n\n### 持久化工作流状态并在页面（重新）加载时恢复\n\n*   **默认值**：启用\n*   **功能**：控制是否在页面（重新）加载时恢复工作流状态，在页面刷新后保持工作流内容\n\n### 自动保存\n\n*   **默认值**：关闭\n*   **功能**：控制工作流的自动保存行为，自动保存工作流更改，避免数据丢失\n\n### 自动保存延迟（毫秒）\n\n*   **默认值**：1000\n*   **功能**：设置自动保存的延迟时间，仅在自动保存设置为”延迟后”时生效\n\n### 删除工作流时显示确认\n\n*   **默认值**：启用\n*   **功能**：控制在侧边栏删除工作流时是否显示确认对话框，防止意外删除重要工作流\n\n### 在工作流中保存和恢复视图位置及缩放\n\n*   **默认值**：启用\n*   **功能**：控制是否在工作流中保存和恢复画布位置和缩放级别，在重新打开工作流时恢复之前的视图状态\n\n### 已打开工作流位置\n\n*   **选项**：侧边栏、顶部栏、顶部栏（第二行）\n*   **默认值**：顶部栏\n*   **功能**：控制打开的工作流标签的显示位置，目前仅支持侧边栏、顶部栏、顶部栏（第二行）\n\n### 保存工作流时提示文件名\n\n*   **默认值**：启用\n*   **功能**：控制保存工作流时是否提示输入文件名，允许用户自定义工作流文件名\n\n### 保存工作流时排序节点ID\n\n*   **默认值**：禁用\n*   **功能**：决定保存工作流时是否对节点ID进行排序，使工作流文件格式更规范，便于版本控制\n\n### 显示缺失节点警告\n\n*   **默认值**：启用\n*   **功能**：控制是否显示工作流中缺失节点的警告，帮助用户识别工作流中不可用的节点\n\n### 显示缺失模型警告\n\n*   **默认值**：启用\n*   **功能**： 我们支持在工作流文件中对 widget 的值添加模型链接信息，用于加载模型文件时的提示，当启用时如果你本地没有对应的模型文件则会显示工作流中缺失模型的警告\n\n### 清除工作流时需要确认\n\n*   **默认值**：启用\n*   **功能**：控制清除工作流时是否显示确认对话框，防止意外清除工作流内容\n\n### 保存节点ID到工作流\n\n*   **默认值**：启用\n*   **功能**：控制是否在保存工作流时保存节点ID，使工作流文件格式更规范，便于版本控制"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/lite-graph",
  "markdown": "# ComfyUI 画面(LiteGraph)设置 - ComfyUI\n\nLiteGraph 是 ComfyUI 的底层图形渲染引擎，这个分类下的设置主要控制画布、节点、链接等图形界面的行为和外观。\n\n## 画布相关设置\n\n### 显示选择工具箱\n\n*   **默认值**：启用\n*   **功能**：选择工具箱是选中节点后在节点上浮动显示的快捷操作工具栏，提供了常用的快捷操作如部分运行、固定、删除、颜色修改等等\n\n![显示选择工具箱](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/selection-toolbox.jpg)\n\n### 低质量渲染缩放阈值\n\n*   **默认值**：0.6\n*   **范围**：0.1 - 1.0\n*   **功能**： 在渲染界面时，特别是当工作流特别复杂及整个画布特别大时，对应元素的前端渲染会消耗特别多的内存而造成卡顿，通过调低此阈值，可以控制元素在缩放到特定百分比时进入低质量渲染模式，从而降低内存消耗，对应不同渲染模式如下图\n\n![低质量渲染](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/render-mode.jpg)\n\n### 最大FPS\n\n*   **默认值**：0（使用屏幕刷新率）\n*   **范围**：0 - 120\n*   **功能**：限制画布的渲染帧率，0表示使用屏幕刷新率，越高的 FPS 会让画面（Canvas） 渲染越流畅，但同时也会消耗更多性能，但过小时则会有越明显的卡顿感。\n\n### 始终吸附到网格\n\n*   **默认值**：禁用\n*   **功能**：在此选项没有启用时，你可以按住 `Shift` 键来使节点边缘和网格对齐，在启用后则无需按住 `Shift` 键即可自动对齐网格\n\n### 吸附网格大小\n\n*   **范围**：1 - 500\n*   **功能**：在启用自动吸附或者按住 `Shift` 键进行节点的移动时，这个参数会决定吸附的网格大小，默认值为 10，你可以根据你的需求进行调整。\n\n### 启用快速缩放快捷键\n\n*   **默认值**：启用\n*   **功能**：启用 `Ctrl + Shift + 鼠标左键拖拽` 的快速缩放功能，提供更快速的缩放操作方式\n\n### 显示图形画布菜单\n\n*   **默认值**：启用\n*   **功能**：控制是否显示右下角的画布菜单\n\n画布菜单位于整个 ComfyUI 界面的右下角，包含了画布的缩放、临时隐藏所有连线、快速缩放工作流到适应画布等操作，如下图所示 ![显示图形画布菜单](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/canvas_menu.jpg)\n\n### 画布缩放速度\n\n*   **默认值**：1.1\n*   **范围**：1.01 - 2.5\n*   **功能**：控制画布缩放的速度，调整鼠标滚轮缩放的敏感度\n\n### 在左下角显示画布信息（fps等）\n\n*   **默认值**：启用\n*   **功能**：控制是否显示画布右键菜单，启用/禁用画布的上下文菜单\n\n![快速缩放](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/canvas-info.jpg)\n\n## 上下文菜单\n\n### 放大时缩放节点组合部件菜单（列表）\n\n*   **默认值**：启用\n*   **功能**：控制是否在放大时显示节点组合部件菜单（列表），允许用户选择节点组合部件\n\n## 画面\n\n### 连线渲染模式\n\n*   **默认值**：2（Spline样条线）\n*   **选项**：直线、线性、样条线、隐藏\n*   **功能**：设置连线的渲染样式，控制节点间连线的视觉样式\n\n![连线渲染模式](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/link-render-mode.jpg)\n\n## 组\n\n这部分的设置主要和节点组功能相关 ![节点组](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-group.png)\n\n### 双击组标题以编辑\n\n*   **默认值**：启用\n*   **功能**：控制是否可以双击节点标题进行编辑，允许用户重命名节点，图中标注为 `1` 的部分\n\n### 分组选中节点填充\n\n*   **默认值**：10\n*   **范围**：0 - 100\n*   **功能**：设置分组选中节点时的内边距，控制分组框与节点间的间距，图中标注为 `2` 箭头标注部分\n\n## 连线\n\n### 链接中点标记\n\n*   **默认值**：Circle（圆形）\n*   **选项**：无、圆形、箭头\n*   **功能**：设置链接中点的标记样式，在链接中点显示方向指示\n\n![连线渲染模式](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/link-midpoint.jpg)\n\n## 释放链接\n\n这部分的菜单目前主要控制当链接连线释放时的相关操作，目前两个相关操作为： **释放后会出现和当前输入 / 输出相关的节点推荐列表**\n\n**释放后会启动搜索框**\n\n### 链接释放动作（Shift键）\n\n*   **默认值**： 搜索框\n*   **选项**： 上下文菜单、搜索框、无操作\n*   **功能**：设置按住Shift键释放链接时的动作，按住Shift释放链接时的特殊行为\n\n### 链接释放动作（无修饰键）\n\n*   **默认值**： 上下文菜单\n*   **选项**： 上下文菜单、搜索框、无操作\n*   **功能**：设置释放链接时的默认动作，控制拖拽链接后释放时的行为\n\n## 节点\n\n### 始终收缩新节点\n\n*   **默认值**：启用\n*   **功能**：控制是否在创建新节点时自动收缩，从而让节点能够始终显示最小的尺寸，但可能会导致添加时有些文本显示会被截断，需要手动调整节点大小\n\n### 启用DOM元素裁剪（启用可能会降低性能）\n\n*   **默认值**：启用\n*   **功能**：启用DOM元素裁剪（可能影响性能），优化渲染但可能降低性能\n\n### 中键单击创建新的转接点\n\n*   **默认值**：启用\n*   **功能**：中键点击时创建新的重路由节点，快速创建用于整理连线的重路由节点\n\n### 删除节点时保留连线\n\n*   **默认值**：启用\n*   **功能**：删除中间节点时自动绕过连接，删除节点时尝试重新连接其输入输出链接\n\n### 吸附高亮节点\n\n*   **默认值**：启用\n*   **功能**：拖拽链接到节点时高亮显示节点，提供视觉反馈，显示可连接的节点,启用后效果如下图，对应链接的一侧会显示高亮的样式\n\n![吸附高亮节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/highlights-node.jpg)\n\n### 连线自动吸附到节点接口\n\n*   **默认值**：启用\n*   **功能**：拖拽链接到节点上时自动吸附到可用插槽，简化连接操作，自动找到合适的输入插槽\n\n### 启用工具提示\n\n*   **默认值**：启用\n*   **功能**：在部分节点信息中会包含一些工具提示，包含了一些参数说明等，当启用后会在鼠标悬停时显示这些工具提示，如下图\n\n![工具提示](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/tooltips.jpg)\n\n### 工具提示延迟\n\n*   **默认值**：500\n*   **功能**：控制工具提示的延迟时间，单位为毫秒，设置为0表示立即显示工具提示\n\n### 节点制作周期标签\n\n*   **默认值**：ShowAll（显示全部）\n*   **功能**：控制节点生命周期标记的显示，显示节点的状态信息\n\n### 节点ID标签\n\n*   **默认值**：None（不显示）\n*   **功能**：控制节点ID标记的显示，显示节点的唯一标识符\n\n![节点ID标签](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-id-badge.jpg)\n\n### 节点源标签\n\n*   **选项**：\n    *   None（不显示）\n    *   HideBuiltIn（隐藏内置）\n    *   ShowAll（显示全部）\n*   **功能**：控制节点源标记的显示模式，显示节点来源信息,对应的显示效果如下图，如果显示全部则会显示自定义节点和内置节点的标签，方便你判断对应的节点来源，对应小狐狸标志为 ComfyUI 内置节点\n\n![节点源标签](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-source-badge.jpg)\n\n### 双击节点标题以编辑\n\n*   **默认值**：启用\n*   **功能**：控制是否可以双击节点标题进行编辑，允许用户重命名节点\n\n## 节点组件\n\n### 浮点组件四舍五入的小数位数 \\[0 = 自动\\]\n\n*   **默认值**：0（自动）\n*   **范围**：0 - 6\n*   **功能**：设置浮点小部件四舍五入的小数位数，0表示自动，需要页面重新加载\n\n### 禁用默认浮点组件四舍五入\n\n*   **默认值**：禁用\n*   **功能**：控制是否禁用默认的浮点小部件四舍五入，需要页面重新加载，当节点后端设置了四舍五入时无法禁用\n\n### 禁用节点组件滑块\n\n*   **默认值**：禁用\n*   **功能**：控制是否禁用节点小部件中的滑块控件，强制使用文本输入而非滑块\n\n### 预览图像格式\n\n*   **默认值**：空字符串（使用原格式）\n*   **功能**：设置图像小部件中预览图像的格式，转换为轻量级格式如 webp、jpeg 等\n\n### 在图像预览下方显示宽度×高度\n\n*   **默认值**：启用\n*   **功能**：在图像预览下方显示宽度×高度信息，显示图像的尺寸信息\n\n![在图像预览下方显示宽度×高度](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/show-size.jpg)\n\n## 指针\n\n### 启用触控板手势\n\n*   **默认值**：启用\n*   **功能**：此设置为画布启用触控板模式，允许使用双指捏合缩放和拖动。\n\n### 双击间隔（最大）\n\n*   **默认值**：300\n*   **功能**：双击的两次点击之间的最大时间(毫秒)。增加此值有助于解决双击有时未被识别的问题。\n\n### 指针点击漂移延迟\n\n*   **默认值**：150\n*   **功能**：按下指针按钮后，忽略指针移动的最大时间(毫秒)。有助于防止在点击时意外移动鼠标。\n\n### 指针点击漂移（距离）\n\n*   **默认值**：6\n*   **功能**：如果指针在按住按钮时移动超过此距离，则视为拖动(而不是点击)。有助于防止在点击时意外移动鼠标\n\n## 重新路由\n\n### 重新路由样条偏移\n\n*   **默认值**：20\n*   **功能**：用于确定重路由节点两侧的曲线的平滑程度，值越大，曲线越平滑，值越小，曲线越尖锐\n\n![重新路由样条偏移](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/reroute-spline-offset.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/comfy-desktop",
  "markdown": "# ComfyUI 桌面应用通用设置 - ComfyUI\n\n## 常规\n\n**窗口样式**\n\n*   **默认值**: default\n*   **说明**: 控制应用窗口的标题栏样式\n\n**自动更新检查**\n\n*   **默认值**: 启用\n*   **说明**: 自动检查 ComfyUI 桌面版的更新，在更新可同时将会提醒你进行更新\n\n**发送匿名使用情况统计**\n\n*   **默认值**: 启用\n*   **说明**: 发送匿名使用情况统计数据，帮助改进软件 ，对应的设置修改需要重启才能生效\n\n## UV（包管理器）\n\n这个部分主要针对中国地区用户设置使用，因为 Desktop 使用的许多原始镜像都是中国境外的，所以对国内用户访问不一定友好，你可以在这里设置你自己的镜像源，以提高访问速度，保证对应包可以正常访问下载。 **Python 安装镜像**\n\n*   **默认值**: 空（使用默认源）\n*   **说明**:\n    *   管理的 Python 安装包从 Astral python-build-standalone 项目下载\n    *   可设置镜像 URL 使用不同的 Python 安装源\n    *   提供的 URL 将替换默认的 GitHub 下载地址\n    *   支持使用 file:// 协议从本地目录读取分发包\n*   **验证**: 自动检查镜像可达性\n\n**PyPI 安装镜像**\n\n*   **默认值**: 空（使用默认源）\n*   **说明**: 默认的 pip 包安装镜像源\n\n**Torch安装镜像**\n\n*   **默认值**: 空（使用默认源）\n*   **说明**: PyTorch 专用的 pip 安装镜像源"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2Fapi-node%2Fimage%2Frecraft%2Frecraft-style-digital-illustration",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/mask-editor",
  "markdown": "# ComfyUI 遮罩编辑器设置 - ComfyUI\n\n## 画笔调整\n\n### 画笔调整速度倍增器\n\n*   **功能**: 控制调整时画笔大小和硬度变化的速度\n*   **说明**: 更高的值意味着更快的变化\n\n### 将画笔调整锁定到主轴\n\n*   **功能**: 启用后，画笔调整将仅根据您移动的方向影响大小或硬度\n*   **说明**: 这个功能可以让用户更精确地控制画笔属性的调整\n\n## 新编辑器\n\n### 使用新画笔编辑器\n\n*   **功能**: 切换到新的画笔编辑器界面\n*   **说明**: 允许用户在新旧编辑器界面之间切换\n\n新版本具有更好的 UI 界面和交互，功能会更加完整 ![new](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/maskeditor/new-mask-editor.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/extension",
  "markdown": "# 扩展设置 - ComfyUI\n\n扩展设置面板是 ComfyUI 前端设置系统中的一个特殊管理面板，专门用于管理前端扩展插件的启用/禁用状态，区别于自定义节点（Custom Node），这个面板只是用于管理自定义节点注册的前端扩展，而不是禁用自定义节点。 ![extension](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/extension/extension.jpg) 这些前端扩展插件是用于增强 ComfyUI 的体验，比如提供快捷键、设置、UI 组件、菜单项等功能。 扩展状态更改后需要重新加载页面才能生效：\n\n## Extension 设置面板功能\n\n### 1\\. 扩展列表管理\n\n显示所有已注册的扩展，包括：\n\n*   扩展名称\n*   核心扩展标识（显示 “Core” 标签）\n*   启用/禁用状态\n\n### 2\\. 搜索功能\n\n提供搜索框快速查找特定扩展：\n\n### 3\\. 启用/禁用控制\n\n每个扩展都有独立的切换开关：\n\n### 4\\. 批量操作\n\n提供右键菜单进行批量操作：\n\n*   启用所有扩展\n*   禁用所有扩展\n*   禁用第三方扩展（保留核心扩展）\n\n## 注意事项\n\n*   扩展状态更改需要重新加载页面才能生效\n*   某些核心扩展无法被禁用\n*   系统会自动禁用已知有问题的扩展\n*   扩展设置会自动保存到用户配置文件中\n\n这个 Extension 设置面板本质上是一个”前端插件管理器”，让用户可以灵活控制 ComfyUI 的功能模块。"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/shortcuts",
  "markdown": "# ComfyUI 的快捷键及自定义设置 - ComfyUI\n\n目前 ComfyUI 已经支持快捷键自定义，你可以在点击 `设置（齿轮图标）` —> `快捷键` 中进行快捷键的设置。 ![ComfyUI 快捷键设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/keybinding.jpg) 在对应菜单中，你可以看到目前 ComfyUI 所有的快捷键设置，点击对应命令之前的`编辑图标`，就可以对快捷键进行自定义。\n\n| 快捷键 | 命令  |\n| --- | --- |\n| Ctrl + Enter | 执行提示词 |\n| Ctrl + Shift + Enter | 执行提示词（前端） |\n| Ctrl + Alt + Enter | 中断  |\n| Ctrl + Z / Ctrl + Y | 撤销/重做 |\n| Ctrl + S | 保存工作流 |\n| Ctrl + O | 加载工作流 |\n| Ctrl + A | 选择所有节点 |\n| Alt + C | 折叠/展开选定节点 |\n| Ctrl + M | 静音/取消静音选定节点 |\n| Ctrl + B | 忽略/取消忽略选定节点 |\n| Delete  <br>Backspace | 删除选定节点 |\n| Backspace | 清除工作流 |\n| Space | 按住并移动光标时移动画布 |\n| Ctrl + Click  <br>Shift + Click | 将点击的节点添加到选择中 |\n| Ctrl + C/Ctrl + V | 复制并粘贴选定节点（不保持与未选定节点输出的连接） |\n| Ctrl + C/Ctrl + Shift + V | 复制并粘贴选定节点（保持未选定节点输出到粘贴节点输入的连接） |\n| Shift + Drag | 同时移动多个选定节点 |\n| Ctrl + G | 添加框到选中节点 |\n| Ctrl + , | 显示设置对话框 |\n| Alt + = | 放大（画布） |\n| Alt + - | 缩小（画布） |\n| .   | 适应视图到选中节点 |\n| P   | 固定/取消固定选中项 |\n| Q   | 切换执行队列侧边栏 |\n| W   | 切换工作流侧边栏 |\n| N   | 切换节点库侧边栏 |\n| M   | 切换模型库侧边栏 |\n| Ctrl + \\` | 切换日志底部面板 |\n| F   | 切换焦点模式（全屏） |\n| R   | 刷新节点定义 |\n| 双击左键 | 快速搜索要添加的节点 |"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/server-config",
  "markdown": "# 服务器配置 - ComfyUI\n\n## 网络\n\n### 主机：要监听的IP地址\n\n*   **主机地址 (listen)**:\n*   **功能**：设置服务器绑定的IP地址。默认 `127.0.0.1` 表示只允许本地访问，如果需要局域网访问可设置为 `0.0.0.0`\n\n### 端口端口：要监听的端口\n\n**功能**：服务器监听的端口号。桌面版默认8000端口，Web版通常使用8188端口\n\n### TTLS 密钥文件：HTTPS 的 TLS 密钥文件路径\n\n**功能**：HTTPS加密所需的私钥文件路径，用于建立安全连接\n\n### TLS 证书文件：HTTPS 的 TLS 证书文件路径\n\n**功能**：HTTPS加密所需的证书文件路径，与私钥配合使用\n\n### 最大上传大小\n\n*   **最大上传大小 (max-upload-size)**:\n*   **功能**：限制单个文件上传的最大尺寸，单位为MB，默认100MB。影响图片、模型等文件的上传限制 限制单个文件上传的最大尺寸，单位为MB，默认100MB。影响图片、模型等文件的上传限制\n\n## CUDA\n\n### 要使用的 CUDA 设备索引\n\n**功能**：指定使用哪块NVIDIA显卡。0表示第一块显卡，1表示第二块，以此类推。对多GPU系统很重要\n\n### 使用 CUDA malloc 进行内存分配\n\n**功能**：控制是否使用CUDA的内存分配器。可以改善某些情况下的内存管理效率\n\n## 推理\n\n### 全局浮点精度\n\n**功能**：设置模型计算的数值精度。FP16节省显存但可能影响质量，FP32更精确但占用更多显存\n\n### UNET 精度\n\n**选项**：\n\n*   `auto`：自动选择最合适的精度\n*   `fp64`：64位浮点精度，精度最高但显存占用最大\n*   `fp32`：32位浮点精度，标准精度\n*   `fp16`：16位浮点精度，可节省显存\n*   `bf16`：16位brain浮点精度，介于fp16和fp32之间\n*   `fp8_e4m3fn`：8位浮点精度(e4m3)，显存占用最小\n*   `fp8_e5m2`：8位浮点精度(e5m2)，显存占用最小\n\n**功能**：专门控制扩散模型核心组件UNET的计算精度。更高的精度可以提供更好的图像生成质量，但会占用更多显存。较低的精度可以显著节省显存，但可能会影响生成结果的质量。\n\n### VAE 精度\n\n**选项与建议**：\n\n*   `auto`：自动选择最合适的精度，推荐8-12GB显存的用户使用\n*   `fp16`：16位浮点精度，推荐6GB及以下显存的用户使用，可节省显存但可能影响质量\n*   `fp32`：32位浮点精度，推荐16GB及以上显存且追求最佳质量的用户使用\n*   `bf16`：16位brain浮点精度，推荐支持此格式的新型显卡使用，可获得更好的性能平衡\n\n**功能**：控制变分自编码器(VAE)的计算精度，影响图像编码/解码的质量和速度。更高的精度可以提供更好的图像重建质量，但会占用更多显存。较低的精度可以节省显存，但可能会影响图像的细节还原。\n\n### 文本编码器精度\n\n**选项**：\n\n*   `auto`：自动选择最合适的精度\n*   `fp8_e4m3fn`：8位浮点精度(e4m3)，显存占用最小\n*   `fp8_e5m2`：8位浮点精度(e5m2)，显存占用最小\n*   `fp16`：16位浮点精度，可节省显存\n*   `fp32`：32位浮点精度，标准精度\n\n**功能**：控制文本提示词编码器的计算精度，影响文本理解的准确性和显存占用。更高的精度可以提供更准确的文本理解，但会占用更多显存。较低的精度可以节省显存，但可能会影响提示词的解析效果。\n\n## 内存\n\n### 强制使用 channels-last 内存格式\n\n**功能**：改变内存中数据的排列方式，可能提升某些硬件上的性能\n\n### DirectML 设备索引\n\n**功能**：在Windows上使用DirectML加速时指定设备，主要用于AMD显卡\n\n### 禁用IPEX优化\n\n**功能**：关闭Intel CPU优化，主要影响Intel处理器的性能\n\n### VRAM 管理模式\n\n**选项**：\n\n*   `auto`：自动管理显存，根据模型大小和需求自动分配显存\n*   `lowvram`：低显存模式，只使用最低限度的显存，可能会影响生成质量\n*   `normalvram`：标准显存模式，平衡显存使用和性能\n*   `highvram`：高显存模式，使用较多显存以获得更好性能\n*   `novram`：不使用显存，完全使用系统内存运行\n*   `cpu`：仅使用CPU运行，不使用显卡\n\n**功能**：控制显存的使用策略，如自动管理、低显存模式等\n\n### 保留VRAM\n\n**功能**：为操作系统和其他程序预留的显存量，防止系统卡死\n\n### 禁用智能内存管理\n\n**功能**：关闭自动内存优化，强制将模型移到系统内存以释放显存\n\n### CPU 运行 VAE\n\n**功能**：强制VAE在CPU上运行，可以节省显存但会降低处理速度\n\n## 预览\n\n### 用于潜空间预览的方法\n\n**选项**:\n\n*   `none`: 不显示预览图像,生成过程中只显示进度条\n*   `auto`: 自动选择最合适的预览方法,根据系统性能和显存情况动态调整\n*   `latent2rgb`: 直接将潜空间数据转换为RGB图像进行预览,速度较快但质量一般\n*   `taesd`: 使用轻量级的TAESD模型进行预览,在速度和质量之间取得平衡\n\n**功能**: 控制生成过程中如何预览中间结果。不同的预览方法会影响预览的质量和性能消耗。选择合适的预览方法可以在预览效果和系统资源占用之间找到平衡点。\n\n### 预览图像大小\n\n**功能**：设置预览图像的分辨率，影响预览清晰度和性能，尺寸越大，预览质量越高，但也会占用更多显存\n\n### 算法优化\n\n*   **确定性算法 (deterministic)**: 启用后使用确定性算法，相同输入会产生相同输出，但计算速度较慢\n*   **快速模式 (fast)**: 启用实验性优化，可能提升速度但可能影响生成质量\n\n## 缓存\n\n### 经典缓存系统\n\n**功能**：使用传统的缓存策略，更保守但稳定\n\n### 使用 LRU 缓存，最多缓存 N 个节点结果\n\n**功能**：使用最近最少使用(Least Recently Used)算法的缓存系统，可以缓存指定数量的节点计算结果 **说明**:\n\n*   通过设置一个具体的数字来控制最大缓存数量，如 10、50、100 等\n*   缓存可以避免重复计算相同的节点操作，提高工作流执行速度\n*   当缓存达到上限时，会自动清除最久未使用的结果\n*   缓存的结果会占用系统内存(RAM/VRAM)，数值越大占用越多\n\n**使用建议**:\n\n*   默认值为 null，表示不启用 LRU 缓存\n*   根据系统内存容量和使用需求设置合适的缓存数量\n*   对于经常重复使用相同节点配置的工作流，建议启用此功能\n*   如果系统内存充足，可以设置较大的数值以获得更好的性能提升\n\n## 注意力\n\n### 交叉注意力方法\n\n**选项**:\n\n*   `auto`: 自动选择最合适的注意力计算方法\n*   `split`: 分块计算注意力,可以节省显存但速度较慢\n*   `quad`: 使用四分注意力算法,在速度和显存使用上取得平衡\n*   `pytorch`: 使用PyTorch原生注意力计算,速度较快但显存占用大\n\n**功能**: 控制模型计算注意力时使用的具体算法。不同的算法会在生成质量、速度和显存占用之间做出不同的权衡。通常建议使用auto自动选择。\n\n*   **强制upcast-attention (force-upcast-attention)**: 强制使用高精度计算注意力，提升质量但增加显存使用\n*   **禁用upcast-attention (dont-upcast-attention)**: 禁用高精度注意力计算，节省显存\n\n## 常规\n\n### 禁用xFormers优化\n\n**功能**：关闭 xFormers 库的优化功能。xFormers 是一个专门优化 Transformer 模型注意力机制的库，通常可以提高计算效率、减少内存使用并加快推理速度。禁用此优化后会：\n\n*   回退到标准的注意力计算方法\n*   可能增加内存使用和计算时间\n*   在某些情况下提供更稳定的运行环境\n\n**使用场景**：\n\n*   遇到与 xFormers 相关的兼容性问题时\n*   需要更精确的计算结果时（某些优化可能影响数值精度）\n*   在调试或排查问题时需要使用标准实现\n\n### 模型文件的默认哈希函数\n\n**选项**:\n\n*   `sha256`: 使用 SHA-256 算法进行哈希校验,安全性高但计算较慢\n*   `sha1`: 使用 SHA-1 算法,速度较快但安全性稍低\n*   `sha512`: 使用 SHA-512 算法,提供最高安全性但计算最慢\n*   `md5`: 使用 MD5 算法,速度最快但安全性较低\n\n**功能**：设置模型文件校验的哈希算法，用于验证文件完整性。不同的哈希算法在计算速度和安全性之间有不同的权衡。通常建议使用 sha256 作为默认选项，它能在安全性和性能之间取得较好的平衡。\n\n### 使 pytorch 在可以时使用较慢的确定性算法\n\n**功能**: 强制 PyTorch 在可能的情况下使用确定性算法，以提高结果的可重现性。 **说明**:\n\n*   启用后 PyTorch 会优先使用确定性算法而不是更快的非确定性算法\n*   相同的输入将产生相同的输出，有助于调试和结果验证\n*   确定性算法通常比非确定性算法运行更慢\n*   即使启用此设置，也不能完全保证在所有情况下都能产生完全相同的图像结果\n\n**使用场景**:\n\n*   科学研究需要严格的结果可重现性\n*   调试过程中需要稳定的输出结果\n*   生产环境中需要保证结果一致性\n\n### 不打印服务器输出\n\n**功能**：禁止在控制台显示服务器运行信息，保持界面整洁。 **说明**:\n\n*   启用后将不显示 ComfyUI 服务器的日志和运行信息\n*   可以减少控制台的信息干扰，使界面更加清爽\n*   在大量日志输出时可能略微提升系统性能\n*   默认为关闭状态(false)，即默认显示服务器输出\n\n**使用场景**:\n\n*   生产环境中不需要查看调试信息时\n*   希望保持控制台界面整洁时\n*   系统运行稳定无需监控日志时\n\n**注意**：在开发和调试过程中建议保持此选项关闭，以便及时查看服务器的运行状态和错误信息。\n\n### 禁用在文件中保存提示元数据\n\n**功能**：不在生成的图片中保存工作流信息，减少文件大小，但同时也意味着对应工作流信息的缺失，你无法再使用工作流输出的文件来重现对应的生成结果\n\n### 禁用所有自定义节点\n\n**功能**：禁止加载所有第三方扩展节点，通常用于在排查问题时使用，用于来定位对应的错误是否由于第三方扩展节点导致\n\n### 日志详细级别\n\n**功能**：控制日志输出的详细程度，用于调试和监控系统运行状态。 **选项**:\n\n*   `CRITICAL`: 仅输出严重错误信息，这些错误可能导致程序无法继续运行\n*   `ERROR`: 输出错误信息，表示某些功能无法正常工作\n*   `WARNING`: 输出警告信息，表示可能存在的问题但不影响主要功能\n*   `INFO`: 输出一般信息，包括系统运行状态和重要操作记录\n*   `DEBUG`: 输出最详细的调试信息，包括系统内部运行的细节\n\n**说明**:\n\n*   日志级别从上到下详细程度递增\n*   每个级别都会包含比它更高级别的所有日志信息\n*   建议在正常使用时设置为 INFO 级别\n*   在排查问题时可以设置为 DEBUG 级别以获取更多信息\n*   在生产环境中可以设置为 WARNING 或 ERROR 级别以减少日志量\n\n## 目录\n\n### 输入目录\n\n**功能**：设置输入文件（如图片、模型）的默认存放路径\n\n### 输出目录\n\n**功能**：设置生成结果的保存路径"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/runway/image-generation",
  "markdown": "# Runway API 节点 图像生成 ComfyUI 官方示例\n\nRunway 是一家专注于生成式AI的科技公司，提供强大的图像生成功能。其模型支持风格迁移、图像扩展和细节控制等特性。目前 ComfyUI 已集成 Runway API，你可以直接在 ComfyUI 中使用相关节点进行图像生成。 本篇指南中，我们将引导你完成下面的工作流:\n\n*   文生图\n*   参考生图\n\n## Runway Image 文生图 工作流\n\n### 1\\. 工作流文件下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![ComfyUI Runway Image Text to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/text_to_image.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Runway Image Text to Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Runway Text to Image` 的 `prompt` 中输入提示词\n2.  (可选) 设置调整 `ratio` 来设置不同的输出比例\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的生成。\n4.  等待 API 返回结果后，你可在 `Save Image` 节点中查看生成的图像（右键可以保存），对应的图像也会被保存至 `ComfyUI/output/` 目录下。\n\n## Runway Image 参考生图 工作流\n\n### 1\\. 工作流及输入图像下载\n\n下面的图片的`metadata`中已经包含工作流信息，请下载并拖入 ComfyUI 中加载对应工作流。 ![ComfyUI Runway Image Reference to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/runway_reference_to_image.png) 下载下面的图像用于输入 ![ComfyUI Runway Image Reference to Image Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/input.png)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Runway Image Reference to Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg) 你可参考图片中的序号来完成最基础的参考生图工作流运行：\n\n1.  在 `Load Image` 节点中加载提供的输入图像\n2.  在 `Runway Text to Image` 的 `prompt` 中输入提示词及进行尺寸调整\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图像的生成。\n4.  等待 API 返回结果后，你可在 `Save Image` 节点中查看生成的图像（右键可以保存），对应的图像也会被保存至 `ComfyUI/output/` 目录下。\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcomfyui-server%2Fcomms_overview",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcomfyui-server%2Fexecution_model_inversion_guide",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fspecs%2Fnodedef_json",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/tripo/model-generation",
  "markdown": "# Tripo API 节点模型生成 ComfyUI 官方示例\n\nTripo AI 是一家专注于生成式 AI 3D 建模的公司，它提供用户友好的平台和 API 服务，能够快速地将文本提示或2D图像（单张或多张）转换成高质量的3D模型。 目前 ComfyUI 已原生集成了对应 Tripo API ,现在你可以在 ComfyUI 中便捷地使用相关节点来进行模型生成 目前 ComfyUI 的 API 节点中已经支持 Tripo 以下模型生成能力：\n\n*   文生模型\n*   图生模型\n*   多视图模型生成\n*   骨骼绑定\n*   骨骼动画\n\n## 文生模型工作流\n\n### 1\\. 工作流文件下载\n\n下载下面的文件，并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/api_tripo_text_to_model.json)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Tripo: Text to Model` 节点的 `prompt` 中输入提示词\n    *   model： 可以选择不同的模型，目前仅 v1.4 模型支持 `Tripo: Refine Draft model` 的后续优化\n    *   style: 中可以设置不同的风格\n    *   texture\\_quality: 可以设置不同的纹理质量\n2.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行模型的生成，工作流完成后对应的模型会自动保存至 `ComfyUI/output/` 目录下\n3.  在 `Preview 3D` 节点中点击展开菜单\n4.  选择`Export` 可以直接将对应模型导出\n\n## 图生模型工作流\n\n### 1\\. 工作流文件下载\n\n下载下面的文件，并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/api_tripo_image_to_model.json)\n\n下载下面的图片作为输入图片 ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/panda.jpg)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Load Image` 节点中加载提供的输入图片\n2.  在 `Tripo: Image to Model` 节点中修改对应的参数设置\n    *   model： 可以选择不同的模型，目前仅 v1.4 模型支持 `Tripo: Refine Draft model` 的后续优化\n    *   style: 中可以设置不同的风格\n    *   texture\\_quality: 可以设置不同的纹理质量\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行模型的生成，工作流完成后对应的模型会自动保存至 `ComfyUI/output/` 目录下\n4.  模型下载请参考文生图部分的说明\n\n## 多视图模型生成工作流\n\n### 1\\. 工作流文件下载\n\n下载下面的文件，并拖入 ComfyUI 中加载对应工作流。\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/api_tripo_multiview_to_model.json)\n\n下载下面的图片作为输入图片 ![前视图](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/front.jpg) ![背视图](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/back.jpg)\n\n### 2\\. 按步骤完成工作流的运行\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg) 你可参考图片中的序号来完成最基础的文生图工作流运行：\n\n1.  在 `Load Image` 节点中分别加载提供的输入图片\n2.  在 `Tripo: Image to Model` 节点中修改对应的参数设置\n    *   model： 可以选择不同的模型，目前仅 v1.4 模型支持 `Tripo: Refine Draft model` 的后续优化\n    *   style: 中可以设置不同的风格\n    *   texture\\_quality: 可以设置不同的纹理质量\n3.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行模型的生成，工作流完成后对应的模型会自动保存至 `ComfyUI/output/` 目录下\n4.  其它视图输入可以参考步骤图中的示意将对应节点的模式设置为 `总是（always）` 来启用\n5.  模型下载请参考文生图部分的说明\n\n## 对应任务的后续任务处理\n\nTripo 的对应节点提供了对于同一任务的后续处理，只需要在相关节点中输入对应的`model_task_id` 即可,我们在相关模板中也已提供了对应的节点，你也可以按需通过修改对应节点模式来启用 ![Tripo 任务处理](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/other_nodes.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/cosmos/cosmos-predict2-video2world",
  "markdown": "# Cosmos Predict2 视频生成 ComfyUI 官方示例\n\nCosmos-Predict2 是由 NVIDIA 推出的新一代物理世界基础模型，专为物理 AI 场景下的高质量视觉生成与预测任务设计。 该模型具备极高的物理准确性、环境交互性和细节还原能力，能够真实模拟复杂的物理现象与动态场景。 Cosmos-Predict2 支持文本到图像（Text2Image）和视频到世界（Video2World）等多种生成方式，广泛应用于工业仿真、自动驾驶、城市规划、科学研究等领域，是推动智能视觉与物理世界深度融合的重要基础工具。 GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2) huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959) 本篇指南将引导你完成在 ComfyUI 中 **图生视频** 的工作流 对于文生图部分，请参考下面的部分\n\n[\n\n使用 Cosmos-Predict2 的进行文生图\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/image/cosmos/cosmos-predict2-t2i)\n\n## Cosmos Predict2 Video2World 工作流\n\n对于 2B 版本，在我们测试使用时，大约占用 16GB 的显存\n\n#### 1.下载工作流文件\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/cosmos/predict2/cosmos_predict2_2B_video2world_480p_16fps.json)\n\n请下载下面的图片作为输入文件： ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/cosmos/predict2/input.png)\n\n### 2.手动模型安装\n\n**Diffusion model**\n\n*   [cosmos\\_predict2\\_2B\\_video2world\\_480p\\_16fps.safetensors](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged/resolve/main/cosmos_predict2_2B_video2world_480p_16fps.safetensors)\n\n其它权重请访问 [Cosmos\\_Predict2\\_repackaged](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged) 进行下载 **Text encoder** [oldt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/resolve/main/text_encoders/oldt5_xxl_fp8_e4m3fn_scaled.safetensors) **VAE** [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors) 文件保存位置\n\n```\n📂 ComfyUI/\n├──📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └─── cosmos_predict2_2B_video2world_480p_16fps.safetensors\n│   ├── 📂 text_encoders/\n│   │   └─── oldt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── 📂 vae/\n│       └──  wan_2.1_vae.safetensors\n```\n\n### 3\\. 按步骤完成工作流运行\n\n![工作流使用步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/cosmos/cosmos_predict2_2B_video2world_480p_16fps_step_guide.jpg) 请参照图片序号进行逐步确认，来保证对应工作流的顺利运行\n\n1.  确保 `Load Diffusion Model` 节点加载了 `cosmos_predict2_2B_video2world_480p_16fps.safetensors`\n2.  确保 `Load CLIP` 节点加载了 `oldt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`\n4.  在 `Load Image` 节点中上传提供的输入图片\n5.  (可选)如果需要首尾帧控制，可以使用快捷键 `Ctrl(cmd) + B` 来启用尾帧输入\n6.  (可选) 你可以在 `ClipTextEncode` 节点中修改提示词\n7.  (可选) 修改 `CosmosPredict2ImageToVideoLatent` 节点中的尺寸和帧数\n8.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n9.  生成完成后对应的视频会自动保存到 `ComfyUI/output/` 目录下，你也可以在 `save video` 节点中预览或者调整保存位置"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fcustom-nodes%2Fbackend%2Fmore_on_inputs",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcore-concepts%2Fcustom-nodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcore-concepts%2Flinks",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcore-concepts%2Fnodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/outpaint",
  "markdown": "# ComfyUI 扩图（Outpaint）工作流示例 - ComfyUI\n\n本篇将引导了解 AI 绘图中扩图的概念，并在 ComfyUI 中完成扩图工作流生成。我们将接触以下内容：\n\n*   使用扩图工作流完成画面的扩展\n*   了解并使用 ComfyUI 中的扩图相关节点\n*   掌握扩图的基本操作流程\n\n## 关于扩图\n\n在 AI 图像生成过程中，我们经常会遇到这样的需求：已有的图片构图很好，但是画面范围太小，需要扩展画布来获得更大的场景，这时候就需要用到扩图功能。 这就像让 **画家(AI 绘图模型)** 在已有的画作基础上，向外延伸绘制更大的场景。我们需要告诉画家 **需要扩展的方向和范围**，画家会根据已有的画面内容，合理地延伸和扩展场景。 基本上它要求的内容与[局部重绘](https://docs.comfy.org/zh-CN/tutorials/basic/inpaint)相似，只不过我们用来**构建遮罩（Mask）的节点不同** 扩图的应用场景包括：\n\n*   **场景扩展：** 扩大原有画面的场景范围，展现更完整的环境\n*   **构图调整：** 通过扩展画布来优化整体构图\n*   **内容补充：** 为原有画面添加更多相关的场景元素\n\n### 准备工作\n\n#### 1\\. 模型安装\n\n*   [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)\n\n#### 2\\. 输入图片\n\n请准备一张你想要进行扩展的图片。在本例中，我们将使用下面这张图片作为示例： ![ComfyUI扩图输入图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/input.png)\n\n#### 3\\. 扩图工作流\n\n请下载下面的图片，并将其 **拖入** ComfyUI 界面或使用菜单 **工作流(Workflow)** —> **打开工作流(Open,快捷键 `Ctrl + O`)** 来加载这个扩图工作流 ![ComfyUI扩图工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpaint.png)\n\n### 扩图工作流使用讲解\n\n![ComfyUI 扩图工作流示意图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpainting_workflow.jpg) 扩图工作流的关键步骤如下：\n\n1.  请在 `加载模型(Load Checkpoint)` 节点中加载你本地安装的模型文件\n2.  请在 `加载图片(Load Image)` 节点中点击 `Upload` 按钮上传\n3.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成\n\n在这个工作流中主要是通过 `Pad Image for outpainting` 节点来控制图片的扩展方向和范围，其实这也是一个 [局部重绘(Inpaint)](https://docs.comfy.org/zh-CN/tutorials/basic/inpaint) 工作流，只不过我们用来构建遮罩（Mask）的节点不同。\n\n### Pad Image for outpainting 节点\n\n![Pad Image for outpainting 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/pad_image_for_outpainting.jpg) 这个节点接受一个输入图片，并输出一张扩展过的图像和对应的遮罩（Mask），其中遮罩由于对应的节点参数构建。\n\n#### 输入参数\n\n| 参数名称 | 作用  |\n| --- | --- |\n| `image` | 输入图片 |\n| `left` | 左侧填充量 |\n| `top` | 顶部填充量 |\n| `right` | 右侧填充量 |\n| `bottom` | 底部填充量 |\n| `feathering` | 控制原始图像与添加的填充内容之间的过渡平滑度，越大越平滑 |\n\n#### 输出参数\n\n| 参数名称 | 作用  |\n| --- | --- |\n| `image` | 输出`image`代表已填充的图像 |\n| `mask` | 输出`mask`指示原始图像和添加的填充区域 |\n\n#### 节点输出内容\n\n经过 `Pad Image for outpainting` 节点处理后，输出的图片和蒙版预览如下： ![Pad Image for outpainting 节点结果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg) 你可以看到对应的输出结果\n\n*   `Image` 输出的是扩展后的图像\n*   `Mask` 输出的是标记了扩展区域的蒙版"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/upscale",
  "markdown": "# ComfyUI 图像放大工作流 - ComfyUI\n\n## 什么是图像放大\n\n图像放大（Image Upscaling）是通过算法将低分辨率图像转换为高分辨率图像的过程。与传统插值放大不同，AI 放大模型（如 ESRGAN）能智能重建细节，保持图像质量。 比如默认通过 SD1.5 模型对于大尺寸的图片生成表现不佳，如果需要高分辨率，我们通常会先生成小尺寸的图像，然后使用图像放大来提升图片的分辨率。 当然本文介绍的只是诸多 ComfyUI 中图像放大方法中的一种，在这篇讲解中，我们将带你完成以下内容：\n\n*   下载并安装放大模型\n*   使用放大模型进行一次简单的放大\n*   结合文生图工作流，完成图像的放大\n\n## 下载并安装放大模型\n\n额外需要下载 ESRGAN 等放大模型（必须）：\n\n## 简单放大工作流\n\n### 1\\. 工作流及素材\n\n请下载下面的图片，并拖入到 ComfyUI 中，加载简单版本放大工作流 ![放大工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_workflow.png) 请下载下面这张小尺寸的图片作为输入 ![Upscale-input](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale-input.jpg) \n\n### 2\\. 工作流讲解\n\n![放大工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_simple_workflow.jpg)\n\n1.  在`加载放大模型(Load Upscale Model)`节点中选择我们之前下载的放大模型\n2.  在`加载图片(Load Image)`节点中选择我们之前准备的输入图片\n3.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成\n\n通过以上步骤，我们就可以完成一个图片的放大，你可以看到在这个工作流中，核心主要在于 `Load Upscale Model` 和 `Upscale Image(Using Model)` 的组合，他们通过接收一个图像的输入，然后使用放大模型将图像放大。\n\n## 结合文生图的放大工作流\n\n在完成了简单的放大工作流后，我们就可以尝试结合[文生图](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)的工作流来完成一个完整放大工作的流程，关于文生图的基础部分及相关模型要求，请参考[文生图](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)的部分的说明完成。 请将下面的图片下载并保存后拖入到 ComfyUI 中，加载结合文生图的放大工作流 ![结合文生图的放大工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/esrgan_example.png) 你可以看到在这个工作流里，就是在文生图工作流之后把对应的图片输入到放大工作流中完成了对应图片的放大。\n\n## 其它相关补充\n\n1.  **链式放大**：对于需要超高倍率放大的情况，可以串联多个放大节点（如先2x再4x）\n2.  **混合放大**：在生成工作流后接放大节点，实现”生成+增强”一体化流程\n3.  **对比测试**：不同模型对特定类型图片效果差异较大，建议同时测试多个模型"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/multiple-loras",
  "markdown": "# ComfyUI 应用多个 LoRA 示例 - ComfyUI\n\n在 [ComfyUI LoRA 使用示例](https://docs.comfy.org/zh-CN/tutorials/basic/lora) 中，我们介绍了如何在 ComfyUI 中加载并使用 LoRA 模型，也提及了该节点支持链式连接。 ![LoRA 节点链式连接](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png) 在本篇中我们将使用链式连接`Load LoRA`节点的方式来同时使用多个 LoRA 模型，在本示例中，我们将使用 [blindbox\\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) 和 [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856) 两个 LoRA 模型。 下图是这两个 LoRA 模型在同样参数下单独使用的效果 ![ComfyUI 中 LoRA 模型单独使用效果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/compare.png) 但通过多个 LoRA 模型链式连接后，我们可以在最终的效果中看到两种风格融合在一起的效果 ![ComfyUI 中多 LoRA 模型应用示例结果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)\n\n## 相关模型安装\n\n请下载 [dreamshaper\\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) 并保存至 `ComfyUI/models/checkpoints` 目录 请下载 [blindbox\\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) 并保存至 `ComfyUI/models/loras` 目录 请下载 [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model&format=SafeTensor&size=full&fp=fp16) 并保存至 `ComfyUI/models/loras` 目录\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - 多 LoRA 模型应用示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png) \n\n## 按步骤完成工作流的运行\n\n请参照下图步骤完成，确保工作流能够正常运行 ![ComfyUI 工作流 - 多 LoRA 模型应用示例流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/flow_diagram.png)\n\n1.  确保`Load Checkpoint`可以加载 **dreamshaper\\_8.safetensors**\n2.  确保`Load LoRA`可以加载 **blindbox\\_V1Mix.safetensors**\n3.  确保`Load LoRA`可以加载 **MoXinV1.safetensors**\n4.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n\n## 开始你的尝试\n\n1.  试着调整两个 `Load LoRA` 的 `strength_model` 参数，来修改不同 LoRA 模型对最终生成图片的影响\n2.  访问 [CivitAI](https://civitai.com/models) 网站，下载其它风格的 LoRA 模型，组合出你满意的效果"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets",
  "markdown": "# ComfyUI ControlNet 混合使用示例 - ComfyUI\n\n在 AI 图像生成中，单一的控制条件往往难以满足复杂场景的需求。混合使用多个 ControlNet 可以同时控制图像的不同区域或不同方面，实现更精确的图像生成控制。 在一些场景下，混合使用 ControlNet 可以利用不同控制条件的特性，来达到更精细的条件控制：\n\n1.  **场景复杂性**：复杂场景需要多种控制条件共同作用\n2.  **精细控制**：通过调整每个 ControlNet 的强度参数，可以精确控制各部分的影响程度\n3.  **互补效果**：不同类型的 ControlNet 可以互相补充，弥补单一控制的局限性\n4.  **创意表达**：组合不同控制可以产生独特的创意效果\n\n### 混合 ControlNet 的使用方法\n\n当我们混合使用多个 ControlNet 时，每个 ControlNet 会根据其应用的区域对图像生成过程施加影响。ComfyUI 通过 `Apply ControlNet` 节点的链式连接方式，允许多个 ControlNet 条件按顺序叠加应用混合控制条件： ![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)\n\n在本示例中，我们将使用 **Pose ControlNet** 和 **Scribble ControlNet** 的组合来生成一张包含多个元素的场景：左侧由 Pose ControlNet 控制的人物和右侧由 Scribble ControlNet 控制的猫咪滑板车。\n\n### 1\\. ControlNet 混合使用工作流素材\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - Mixing ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets.png) \n\n用于输入的 pose 图片（控制左侧人物姿态）: ![ComfyUI 工作流 - Mixing ControlNet 输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input.png) 用于输入的 scribble 图片（控制右侧猫咪和滑板车）: ![ComfyUI 工作流 - Mixing ControlNet 输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input_scribble.png)\n\n### 2\\. 手动模型安装\n\n*   [awpainting\\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)\n*   [control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── awpainting_v14.safetensors\n│   ├── controlnet/\n│   │   └── control_v11p_sd15_scribble_fp16.safetensors\n│   │   └── control_v11p_sd15_openpose_fp16.safetensors\n│   ├── vae/\n│   │   └── vae-ft-mse-840000-ema-pruned.safetensors\n```\n\n### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI 工作流 - Mixing ControlNet 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg) 按照图片中的数字标记，执行以下步骤：\n\n1.  确保`Load Checkpoint`可以加载 **awpainting\\_v14.safetensors**\n2.  确保`Load VAE`可以加载 **vae-ft-mse-840000-ema-pruned.safetensors**\n\n第一组 ControlNet 使用 Openpose 模型: 3. 确保`Load ControlNet Model`加载 **control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors** 4. 在`Load Image`中点击`Upload` 上传之前提供的 pose 图片 第二组 ControlNet 使用 Scribble 模型: 5. 确保`Load ControlNet Model`加载 **control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors** 6. 在`Load Image`中点击`Upload` 上传之前提供的 scribble 图片 7. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成\n\n## 工作流讲解\n\n#### 强度平衡\n\n当控制图像不同区域时，强度参数的平衡尤为重要：\n\n*   如果一个区域的 ControlNet 强度明显高于另一个，可能导致该区域的控制效果过强而抑制另一区域\n*   推荐为不同区域的 ControlNet 设置相似的强度值，例如都设为 1.0\n\n#### 提示词技巧\n\n在区域分治混合中，提示词需要同时包含两个区域的描述：\n\n```\n\"A woman in red dress, a cat riding a scooter, detailed background, high quality\"\n```\n\n这样的提示词同时涵盖了人物和猫咪滑板车，确保模型能够同时关注两个控制区域。\n\n## 同一主体多维控制的混合应用\n\n除了本例展示的区域分治混合外，另一种常见的混合方式是对同一主体进行多维控制。例如：\n\n*   **Pose + Depth**：控制人物姿势及空间感\n*   **Pose + Canny**：控制人物姿势及边缘细节\n*   **Pose + Reference**：控制人物姿势但参考特定风格\n\n在这种应用中，多个 ControlNet 的参考图应该对准同一主体，并调整各自的强度确保适当平衡。 通过组合不同类型的 ControlNet 并指定其控制区域，你可以对画面元素进行精确控制。"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-text-to-image",
  "markdown": "# ComfyUI Flux 文生图工作示例 - ComfyUI\n\n ![Flux](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_example.png) Flux 是目前最大的开源AI绘画模型之一，拥有 12B 参数，原始文件大小约为23GB。它由 [Black Forest Labs](https://blackforestlabs.ai/) 开发，该团队由前 Stable Diffusion 团队成员创立。 Flux 以其卓越的画面质量和灵活性而闻名，能够生成高质量、多样化的图像。 目前 Flux.1 模型主要有以下几个版本：\n\n*   **Flux.1 Pro：** 效果最佳模型，闭源模型，仅支持通过 API 调用。\n*   **[Flux.1 \\[dev\\]：](https://huggingface.co/black-forest-labs/FLUX.1-dev)** 开源但仅限非商业使用，从 Pro 版本蒸馏而来，效果接近Pro版。\n*   \\*\\*[Flux.1 \\[schnell\\]：](https://huggingface.co/black-forest-labs/FLUX.1-schnell)\\*\\*采用 Apache2.0 许可，仅需4步即可生成图像，适合低配置硬件。\n\n**Flux.1 模型特点**\n\n*   **混合架构：** 结合了 Transformer 网络和扩散模型的优势，有效整合文本与图像信息，提升生成图像与提示词的对齐精度，对复杂的提示词依旧有非常好的还原能力。\n*   **参数规模：** Flux 拥有 12B 参数，可捕捉更复杂的模式关系，生成更逼真、多样化的图像。\n*   **支持多种风格：** 支持多样化的风格，对各种类型的图像都有非常好的表现能力。\n\n在本篇示例中，我们将介绍使用 Flux.1 Dev 和 Flux.1 Schnell 两个版本进行文生图的示例，包括原始完整版模型和 FP8 Checkpoint 简化版本。\n\n*   **Flux 完整版本：** 效果最佳，但需要较大的显存资源（推荐16GB以上），需要安装多个模型文件。\n*   **Flux FP8 Checkpoint：** 仅需一个 fp8 版本的模型，但是质量相对完整版会有所降低。\n\n### Flux.1 Dev 完整版本工作流\n\n#### 1\\. 工作流文件\n\n请下载下面的图片，并拖入 ComfyUI 中加载工作流。 ![Flux Dev 原始版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_t5fp16.png) \n\n#### 2\\. 手动安装模型\n\n请下载下面的模型文件：\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) 当你的显存大于 32GB 时推荐使用。\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)\n\n文件保存位置：\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── t5xxl_fp16.safetensors\n│   ├── vae/\n│   │   └── ae.safetensors\n│   └── diffusion_models/\n│       └── flux1-dev.safetensors\n```\n\n#### 3\\. 按步骤检查确保工作流可以正常运行\n\n请参照下面的图片，确保各个模型文件都已经加载完成 ![ComfyUI Flux Dev工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg)\n\n1.  确保在`DualCLIPLoader`节点中下面的模型已加载：\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n2.  确保在`Load Diffusion Model`节点加载了`flux1-dev.safetensors`\n3.  确保在`Load VAE`节点中加载了`ae.safetensors`\n4.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n### Flux.1 Schnell 完整版本工作流\n\n#### 1\\. 工作流文件\n\n请下载下面的图片，并拖入 ComfyUI 中加载工作流。 ![Flux Schnell 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_t5fp8.png)\n\n#### 2\\. 手动安装模型\n\n完整模型文件列表：\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)\n\n文件保存位置：\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │   ├── clip_l.safetensors\n│   │   └── t5xxl_fp8_e4m3fn.safetensors\n│   ├── vae/\n│   │   └── ae.safetensors\n│   └── diffusion_models/\n│       └── flux1-schnell.safetensors\n```\n\n#### 3\\. 按步骤检查确保工作流可以正常运行\n\n![Flux Schnell 版本工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg)\n\n1.  确保在`DualCLIPLoader`节点中下面的模型已加载：\n    *   clip\\_name1: t5xxl\\_fp8\\_e4m3fn.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n2.  确保在`Load Diffusion Model`节点加载了`flux1-schnell.safetensors`\n3.  确保在`Load VAE`节点中加载了`ae.safetensors`\n4.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n## Fp8 Checkpoint 版文生图示例\n\nfp8 版本是对 flux1 原版 fp16 版本的量化版本，在一定程度上这个版本的质量会低于 fp16 版本，但同时它需要的显存也会更少，而且你仅需要安装一个模型文件即可尝试运行。\n\n### Flux.1 Dev fp8 Checkpoint 版工作流\n\n请下载下面的图片，并拖入 ComfyUI 中加载工作流。 ![Flux Dev fp8 Checkpoint 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_fp8.png) 请下载 [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true)并保存至 `ComfyUI/models/Checkpoints/` 目录下。 确保对应的 `Load Checkpoint` 节点加载了 `flux1-dev-fp8.safetensors`，即可测试运行。\n\n### Flux.1 Schnell fp8 Checkpoint 版工作流\n\n请下载下面的图片，并拖入 ComfyUI 中加载工作流。 ![Flux Schnell fp8 Checkpoint 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_fp8.png) 请下载[flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true)并保存至 `ComfyUI/models/Checkpoints/` 目录下。 确保对应的 `Load Checkpoint` 节点加载了 `flux1-schnell-fp8.safetensors`，即可测试运行。"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-fill-dev",
  "markdown": "# ComfyUI Flux.1 fill dev 示例\n\n![Flux.1 fill dev](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-fill-dev-demo.jpeg)\n\nFlux.1 fill dev 是 [Black Forest Labs](https://blackforestlabs.ai/) 推出的 ​[FLUX.1 Tools 套件](https://blackforestlabs.ai/flux-1-tools/) 中的核心工具之一，专为图像修复和扩展设计。 Flux.1 fill dev 的核心特点：\n\n*   强大的图像重绘(Inpainting)和扩绘(Outpainting)能力，生成效果仅次于商业版的 FLUX.1 Fill \\[pro\\]。\n*   出色的提示词理解和跟随能力，能够精确捕捉用户意图并与原图保持高度一致性。\n*   采用先进的引导蒸馏训练技术，使模型在保持高质量输出的同时更加高效。\n*   友好的许可条款，生成的输出可用于个人、科学和商业目的，具体请参见 [FLUX.1 \\[dev\\] 非商业许可证](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)。\n\n模型开源地址：[FLUX.1 \\[dev\\]](https://huggingface.co/black-forest-labs/FLUX.1-dev) 本文将基于 Flux.1 fill dev 模型来完成 Inpainting 和 Outpainting 的工作流， 如果你不太了解 Inpainting 和 Outpainting 的工作流可以参考 [ComfyUI 布局重绘示例](https://docs.comfy.org/zh-CN/tutorials/basic/inpaint) 和 [ComfyUI 扩图示例](https://docs.comfy.org/zh-CN/tutorials/basic/outpaint)，部分的相关说明。\n\n## Flux.1 Fill dev 工作流模型安装\n\n在开始之前，让我们先完成 Flux.1 Fill dev 模型文件的安装， inpainting 和 outpainting 的工作流中会使用完全相同的模型文件，如果你之前使用过完整版本的 [Flux.1 文生图工作流](https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-text-to-image)，那么在这个部分你仅需要下载 **flux1-fill-dev.safetensors** 这个模型文件。 不过由于下载对应模型需要同意对应的使用协议，所以请访问 [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev)页面，确保你参照下图同意了对应的协议。 ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_fill_dev_agreement.jpg) 完整模型列表：\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)\n\n文件保存位置：\n\n```\nComfyUI/\n├── models/\n│   ├── text_encoders/\n│   │    ├── clip_l.safetensors\n│   │    └── t5xxl_fp16.safetensors\n│   ├── vae/\n│   │    └── ae.safetensors\n│   └── diffusion_models/\n│        └── flux1-fill-dev.safetensors\n```\n\n## Flux.1 Fill dev inpainting 工作流\n\n### 1\\. Inpainting 工作流及相关素材\n\n请下载下面的图片，并拖入 ComfyUI 以加载对应的工作流 ![ComfyUI Flux.1 inpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint.png) 请下载下面的图片，我们将使用它来作为输入图片 ![ComfyUI Flux.1 inpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input.png) \n\n### 2\\. 参照图片序号检查完成工作流运行\n\n![ComfyUI Flux.1 Fill dev Inpainting 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_inpaint.jpg)\n\n1.  确保在`Load Diffusion Model`节点加载了`flux1-fill-dev.safetensors`\n2.  确保在`DualCLIPLoader`节点中下面的模型已加载：\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n3.  确保在`Load VAE`节点中加载了`ae.safetensors`\n4.  在`Load Image`节点中上传了文档中提供的输入图片，如果你使用的是不带蒙版的版本，记得使用遮罩编辑器完成蒙版的绘制\n5.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n## Flux.1 Fill dev Outpainting 工作流\n\n### 1\\. Outpainting 工作流\n\n请下载下面的图片，并拖入 ComfyUI 以加载对应的工作流 ![ComfyUI Flux.1 outpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint.png) 请下载下面的图片，我们将使用它来作为输入图片 ![ComfyUI Flux.1 outpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint_input.png) \n\n### 2\\. 参照图片序号检查完成工作流运行\n\n![ComfyUI Flux.1 Fill dev Outpainting 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_outpaint.jpg)\n\n1.  确保在`Load Diffusion Model`节点加载了`flux1-fill-dev.safetensors`\n2.  确保在`DualCLIPLoader`节点中下面的模型已加载：\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n3.  确保在`Load VAE`节点中加载了`ae.safetensors`\n4.  在`Load Image`节点中上传了文档中提供的输入图片\n5.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/moonvalley/moonvalley-video-generation",
  "markdown": "# Moonvalley API 节点 ComfyUI 官方示例\n\n![Legacy Browser](https://s1.hdslb.com/bfs/static/player/img/h5.png)\n\n您当前的浏览器不支持 HTML5 播放器\n\n请更换浏览器再试试哦~\n\nMoonvalley Marey Realism v1.5 是专为影视级创作打造的 AI 视频生成模型，该模型 **完全使用商业授权内容训练**，确保 **版权无忧，商用安全**。\n\n## 产品亮点\n\n*   极强的提示词理解力: 精准还原复杂提示词指令;\n*   原生 1080p 高清画质: 训练数据集基于 **1080P** 视频训练，输出画面细腻。\n*   真实物理与动态表现: 对物理运动模型、自然动态进行精准模拟，带来专业级别的真实感。\n*   复杂场景分层与高级光影效果: 支持复杂场景的前中后景分层，智能理解空间关系\n*   动作迁移和姿态迁移等生产级控制功能: 自动生成复合场景的真实光照。\n\n目前 Moonvalley 相关 API 节点，已在 ComfyUI 中原生支持，你可以在 ComfyUI 中使用 对应的 文生视频、图生视频、视频转绘等能力。\n\n### 1\\. 工作流文件下载\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_text_to_video.json)\n\n### 2\\. 按步骤完成工作流的运行\n\n![文本生视频作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg)\n\n1.  输入正向提示词（想要出现在画面中的内容）\n2.  输入负向提示词（不想要出现在画面中的内容）\n3.  修改视频输出分辨率\n4.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频的生成\n5.  等待 API 返回结果后，你可在 `Save Video` 节点中查看生成的视频，对应的视频也会被保存至 `ComfyUI/output/` 目录下\n\n## Moonvalley 图生视频工作流\n\n### 1\\. 工作流文件下载\n\n[\n\n下载 Json 格式工作流文件\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video.json)\n\n下载下面的图片作为输入图片 ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video_input.webp)\n\n### 2\\. 按步骤完成工作流的运行\n\n![图生视频工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg)\n\n1.  在 `Load Image` 节点中加载输入图像\n2.  输入正向提示词（想要出现在画面中的内容）\n3.  输入负向提示词（不想要出现在画面中的内容）\n4.  修改视频输出分辨率\n5.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频的生成\n6.  等待 API 返回结果后，你可在 `Save Video` 节点中查看生成的视频，对应的视频也会被保存至 `ComfyUI/output/` 目录下\n\n#### 0 个表情\n\n在此页面\n\n*   [产品亮点](#%E4%BA%A7%E5%93%81%E4%BA%AE%E7%82%B9)\n*   [Moonvalley 文生视频工作流](#moonvalley-%E6%96%87%E7%94%9F%E8%A7%86%E9%A2%91%E5%B7%A5%E4%BD%9C%E6%B5%81)\n*   [1\\. 工作流文件下载](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD)\n*   [2\\. 按步骤完成工作流的运行](#2-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C)\n*   [Moonvalley 图生视频工作流](#moonvalley-%E5%9B%BE%E7%94%9F%E8%A7%86%E9%A2%91%E5%B7%A5%E4%BD%9C%E6%B5%81)\n*   [1\\. 工作流文件下载](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD-2)\n*   [2\\. 按步骤完成工作流的运行](#2-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C-2)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/image/omnigen/omnigen2",
  "markdown": "# ComfyUI OmniGen2 原生工作流示例 - ComfyUI\n\nOmniGen2 是一个强大且高效的统一多模态生成模型，总参数量约 **7B**（3B 文本模型 + 4B 图像生成模型）。与 OmniGen v1 不同，OmniGen2 采用创新的双路径 Transformer 架构，具有完全独立的文本自回归模型和图像扩散模型，实现参数解耦和专门优化。\n\n### 模型亮点\n\n*   **视觉理解**：继承了 Qwen-VL-2.5 基础模型强大的图像内容解释和分析能力\n*   **文生图生成**：从文本提示创建高保真度和美观的图像\n*   **指令引导的图像编辑**：执行复杂的、基于指令的图像修改，在开源模型中达到最先进的性能\n*   **上下文生成**：多功能的能力，可以处理和灵活结合多样化的输入（包括人物、参考对象和场景），产生新颖且连贯的视觉输出\n\n### 技术特性\n\n*   **双路径架构**：基于 Qwen 2.5 VL（3B）文本编码器 + 独立扩散 Transformer（4B）\n*   **Omni-RoPE 位置编码**：支持多图像空间定位和身份区分\n*   **参数解耦设计**：避免文本生成对图像质量的负面影响\n*   支持复杂的文本理解和图像理解\n*   可控的图像生成和编辑\n*   优秀的细节保持能力\n*   统一架构支持多种图像生成任务\n*   文字生成能力：可以在图像中生成清晰的文字内容\n\n## OmniGen2 模型下载\n\n由于本文涉及不同工作流，对应的模型文件及安装位置如下，对应工作流中也已包含了模型文件下载信息： **Diffusion Models）**\n\n*   [omnigen2\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/omnigen2_fp16.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/resolve/main/split_files/vae/ae.safetensors)\n\n**Text Encoders）**\n\n*   [qwen\\_2.5\\_vl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_fp16.safetensors)\n\n文件保存位置：\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └── omnigen2_fp16.safetensors\n│   ├── 📂 vae/\n│   │   └── ae.safetensors\n│   └── 📂 text_encoders/\n│       └── qwen_2.5_vl_fp16.safetensors\n```\n\n## ComfyUI OmniGen2 文生图工作流\n\n### 1\\. 工作流文件下载\n\n![文生图工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_t2i.png)\n\n### 2\\. 按步骤完成工作流运行\n\n![工作流使用步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg) 请参照图片序号进行逐步确认，来保证对应工作流的顺利运行：\n\n1.  **加载主模型**：确保 `Load Diffusion Model` 节点加载了 `omnigen2_fp16.safetensors`\n2.  **加载文本编码器**：确保 `Load CLIP` 节点加载了 `qwen_2.5_vl_fp16.safetensors`\n3.  **加载 VAE**：确保 `Load VAE` 节点加载了 `ae.safetensors`\n4.  **设置图像尺寸**：在 `EmptySD3LatentImage` 节点设置生成图片的尺寸（推荐 1024x1024）\n5.  **输入提示词**：\n    *   在第一个 `CLipTextEncode` 节点中输入正向提示词（想要出现在图像中的内容）\n    *   在第二个 `CLipTextEncode` 节点中输入负向提示词（不想要出现在图像中的内容）\n6.  **开始生成**：点击 `Queue Prompt` 按钮，或使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行文生图\n7.  **查看结果**：生成完成后对应的图片会自动保存到 `ComfyUI/output/` 目录下，你也可以在 `SaveImage` 节点中预览\n\n## ComfyUI OmniGen2 图片编辑工作流\n\nOmniGen2 有丰富的图像编辑能力，并且支持为图像添加文本\n\n### 1\\. 工作流文件下载\n\n ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_image_edit.png) 下载下面的图片，我们将使用它作为输入图片。 ![输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/input_fairy.png) \n\n### 2\\. 按步骤完成工作流运行\n\n![工作流使用步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg)\n\n1.  **加载主模型**：确保 `Load Diffusion Model` 节点加载了 `omnigen2_fp16.safetensors`\n2.  **加载文本编码器**：确保 `Load CLIP` 节点加载了 `qwen_2.5_vl_fp16.safetensors`\n3.  **加载 VAE**：确保 `Load VAE` 节点加载了 `ae.safetensors`\n4.  **上传图像**：在 `Load Image` 节点中上传提供的图片\n5.  **输入提示词**：\n    *   在第一个 `CLipTextEncode` 节点中输入正向提示词（想要出现在图像中的内容）\n    *   在第二个 `CLipTextEncode` 节点中输入负向提示词（不想要出现在图像中的内容）\n6.  **开始生成**：点击 `Queue Prompt` 按钮，或使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行文生图\n7.  **查看结果**：生成完成后对应的图片会自动保存到 `ComfyUI/output/` 目录下，你也可以在 `SaveImage` 节点中预览\n\n### 3\\. 工作流补充说明\n\n*   如果你想要启用第二张图像输入 ，你可以将工作流中状态为粉紫色的节点使用快捷键 **Ctrl + B** 来启用对应的节点输入\n*   如果你想要自定义尺寸 ，可以删除链接 `EmptySD3LatentImage` 节点的 `Get image size` 节点，并输入自定义尺寸"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fcontrolnet%2Fdepth-t2i-adapter",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2F3d%2Fhunyuan3D-2",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/pose-controlnet-2-pass",
  "markdown": "# ComfyUI Pose ControlNet 使用示例 - ComfyUI\n\n## OpenPose 简介\n\n[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) 是由卡耐基梅隆大学（CMU）开发的开源实时多人姿态估计系统，是计算机视觉领域的重要技术突破。该系统能够同时检测图像中多个人的：\n\n*   **人体骨架**：18个关键点，包括头部、肩膀、手肘、手腕、髋部、膝盖和脚踝等\n*   **面部表情**：70个面部关键点，用于捕捉微表情和面部轮廓\n*   **手部细节**：21个手部关键点，精确表达手指姿势和手势\n*   **脚部姿态**：6个脚部关键点，记录站立姿势和动作细节\n\n![OpenPose 示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/openpose_example.jpg) 在 AI 图像生成领域，OpenPose 生成的骨骼结构图作为 ControlNet 的条件输入，能够精确控制生成人物的姿势、动作和表情，让我们能够按照预期的姿态和动作生成逼真的人物图像，极大提高了 AI 生成内容的可控性和实用价值。 特别针对早期 Stable diffusion 1.5 系列的模型，通过 OpenPose 生成的骨骼图，可以有效避免人物动作、肢体、表情畸变的问题。\n\n### 1\\. Pose ControlNet 工作流素材\n\n请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流 ![ComfyUI 工作流 - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)\n\n请下载下面的图片，我们将会将它作为输入 ![ComfyUI Pose 输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)\n\n### 2\\. 手动模型安装\n\n*   [control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)\n*   [majicmixRealistic\\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model&format=SafeTensor&size=pruned&fp=fp16)\n*   [japaneseStyleRealistic\\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model&format=SafeTensor&size=pruned&fp=fp16)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n\n```\nComfyUI/\n├── models/\n│   ├── checkpoints/\n│   │   └── majicmixRealistic_v7.safetensors\n│   │   └── japaneseStyleRealistic_v20.safetensors\n│   ├── vae/\n│   │   └── vae-ft-mse-840000-ema-pruned.safetensors\n│   └── controlnet/\n│       └── control_v11p_sd15_openpose_fp16.safetensors\n```\n\n### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI 工作流 - Pose ControlNet 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg) 按照图片中的数字标记，执行以下步骤：\n\n1.  确保`Load Checkpoint`可以加载 **majicmixRealistic\\_v7.safetensors**\n2.  确保`Load VAE`可以加载 **vae-ft-mse-840000-ema-pruned.safetensors**\n3.  确保`Load ControlNet Model`可以加载 **control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors**\n4.  在`Load Image`节点中点击选择按钮，上传之前提供的姿态输入图片，或者使用你自己的OpenPose骨骼图\n5.  确保`Load Checkpoint`可以加载 **japaneseStyleRealistic\\_v20.safetensors**\n6.  点击`Queue`按钮或使用快捷键`Ctrl(cmd) + Enter(回车)`来执行图片的生成\n\n## Pose ControlNet 二次图生图工作流讲解\n\n本工作流采用二次图生图（2-pass）的方式，将图像生成分为两个阶段：\n\n### 第一阶段：基础姿态图像生成\n\n在第一阶段，使用**majicmixRealistic\\_v7**模型结合Pose ControlNet生成初步的人物姿态图像：\n\n1.  首先通过`Load Checkpoint`加载majicmixRealistic\\_v7模型\n2.  通过`Load ControlNet Model`加载姿态控制模型\n3.  输入的姿态图被送入`Apply ControlNet`节点与正向和负向提示词条件结合\n4.  第一个`KSampler`节点（通常使用20-30步）生成基础的人物姿态图像\n5.  通过`VAE Decode`解码得到第一阶段的像素空间图像\n\n这个阶段主要关注正确的人物姿态、姿势和基本结构，确保生成的人物符合输入的骨骼姿态。\n\n### 第二阶段：风格优化与细节增强\n\n在第二阶段，将第一阶段的输出图像作为参考，使用**japaneseStyleRealistic\\_v20**模型进行风格化和细节增强：\n\n1.  第一阶段生成的图像通过`Upscale latent`节点创建的更大分辨率的潜在空间\n2.  第二个`Load Checkpoint`加载japaneseStyleRealistic\\_v20模型，这个模型专注于细节和风格\n3.  第二个`KSampler`节点使用较低的`denoise`强度（通常0.4-0.6）进行细化，保留第一阶段的基础结构\n4.  最终通过第二个`VAE Decode`和`Save Image`节点输出更高质量、更大分辨率的图像\n\n这个阶段主要关注风格统一性、细节丰富度和提升整体画面质量。\n\n## 二次图生图的优势\n\n与单次生成相比，二次图生图方法具有以下优势：\n\n1.  **更高分辨率**：通过二次处理可以生成超出单次生成能力的高分辨率图像\n2.  **风格混合**：可以结合不同模型的优势，如第一阶段使用写实模型，第二阶段使用风格化模型\n3.  **更好的细节**：第二阶段可以专注于优化细节，而不必担心整体结构\n4.  **精确控制**：姿态控制在第一阶段完成后，第二阶段可以专注于风格和细节的完善\n5.  **降低GPU负担**：分两次生成可以在有限的GPU资源下生成高质量大图\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/image/hidream/hidream-i1",
  "markdown": "# ComfyUI 原生版本 HiDream-I1 文生图工作流示例 - ComfyUI\n\n![HiDream-I1 演示](https://raw.githubusercontent.com/HiDream-ai/HiDream-I1/main/assets/demo.jpg) HiDream-I1 是智象未来(HiDream-ai)于2025年4月7日正式开源的文生图模型。该模型拥有17B参数规模，采用 [MIT 许可证](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE) 发布，支持用于个人项目、科学研究以及商用，目前在多项基准测试中该模型表现优异。\n\n## 模型特点\n\n**混合架构设计** 采用​​扩散模型（DiT）​​与​​混合专家系统（MoE）​​的结合架构：\n\n*   主体基于Diffusion Transformer（DiT），通过双流MMDiT模块处理多模态信息，单流DiT模块优化全局一致性。\n*   动态路由机制灵活分配计算资源，提升复杂场景处理能力，在色彩还原、边缘处理等细节上表现优异。\n\n**多模态文本编码器集成** 整合四个文本编码器：\n\n*   OpenCLIP ViT-bigG、OpenAI CLIP ViT-L（视觉语义对齐）\n*   T5-XXL（长文本解析）\n*   Llama-3.1-8B-Instruct（指令理解） 这一组合使其在颜色、数量、空间关系等复杂语义解析上达到SOTA水平，中文提示词支持显著优于同类开源模型。\n\n**原始模型版本** 智象未来(HiDream-ai)提供了三个版本的 HiDream-I1 模型，以满足不同场景的需求，下面是原始的模型仓库链接：\n\n*   完整版本：[🤗 HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full) 推理步数为 50\n*   蒸馏开发版本：[🤗 HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev) 推理步数为 28\n*   蒸馏快速版本：[🤗 HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) 推理步数为 16\n\n## 关于本篇工作流示例\n\n我们将在本篇示例中使用 ComfyOrg 的 repackaged 的版本，你可以在 [HiDream-I1\\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) 仓库中找到我们将在本篇示例中使用的所有模型文件。\n\n对应不同 ComfyUI 原生版本 HiDream-I1 工作流的模型要求基本上是相同的，只有使用过的 [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) 文件不同。 如果你不知道如何选择合适的版本，请参考以下建议：\n\n*   **HiDream-I1-Full** 可以生成质量最高的图像\n*   **HiDream-I1-Dev** 在生成较高质量的图像的同时，又兼顾速度\n*   **HiDream-I1-Fast** 只需要 16 步就可以生成图像，适合需要实时迭代的场景\n\n对于 **dev** 和 **fast** 版本并不需要负向提示词，所以请在采样时设置`cfg` 参数为 `1.0`，我们对应参数设置已在相关工作流中备注。\n\n### 模型安装\n\n下面的模型文件是我们会共用的模型文件，请点击对应的链接进行下载，并参照模型文件保存位置进行保存，对应的 **diffusion models** 模型我们会在对应工作流中引导你进行下载。 **text\\_encoders**：\n\n*   [clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)\n*   [clip\\_g\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)\n*   [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) 这个模型在许多的工作流中都有使用过，你可能已经下载了这个文件。\n*   [llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) 这个是 Flux 的 VAE 模型，如果你之前使用过 Flux 的工作流，你可能已经下载了这个文件。\n\n**diffusion models** 这部分我们将在对应工作流中具体引导下载对应的模型文件。 模型文件保存位置\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 text_encoders/\n│   │   ├─── clip_l_hidream.safetensors\n│   │   ├─── clip_g_hidream.safetensors\n│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors\n│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors\n│   └── 📂 vae/\n│   │   └── ae.safetensors\n│   └── 📂 diffusion_models/\n│       └── ...               # 将在对应版本的工作流中引导你进行安装            \n```\n\n### HiDream-I1 full 版本工作流\n\n#### 1\\. 模型文件下载\n\n请根据你的硬件情况选择合适的版本，点击链接并下载对应的模型文件保存到 `ComfyUI/models/diffusion_models/` 文件夹下。\n\n*   FP8 版本：[hidream\\_i1\\_full\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) 需要 16GB 以上的显存\n*   完整版本：[hidream\\_i1\\_full\\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) 需要 27GB 以上的显存\n\n#### 2\\. 工作流文件下载\n\n请下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流 ![HiDream-I1 full 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_full.png) \n\n#### 3\\. 按步骤完成工作流的运行\n\n![HiDream-I1 full 版本步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg) 按步骤完成工作流的运行\n\n1.  确保`Load Diffusion Model` 节点中使用的是 `hidream_i1_full_fp8.safetensors` 文件\n2.  确保`QuadrupleCLIPLoader` 中四个对应的 text encoder 被正确加载\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  确保`Load VAE` 节点中使用的是 `ae.safetensors` 文件\n4.  对于 **full** 版本你需要设置 `ModelSamplingSD3` 中的 `shift` 参数为 `3.0`\n5.  对于 `Ksampler` 节点，你需要进行以下设置\n    *   `steps` 设置为 `50`\n    *   `cfg` 设置为 `5.0`\n    *   (可选) `sampler` 设置为 `lcm`\n    *   (可选) `scheduler` 设置为 `normal`\n6.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片生成\n\n### HiDream-I1 dev 版本工作流\n\n#### 1\\. 模型文件下载\n\n请根据你的硬件情况选择合适的版本，点击链接并下载对应的模型文件保存到 `ComfyUI/models/diffusion_models/` 文件夹下。\n\n*   FP8 版本：[hidream\\_i1\\_dev\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) 需要 16GB 以上的显存\n*   完整版本：[hidream\\_i1\\_dev\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) 需要 27GB 以上的显存\n\n#### 2\\. 工作流文件下载\n\n请下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流 ![HiDream-I1 dev 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_dev.png)\n\n#### 3\\. 按步骤完成工作流的运行\n\n ![HiDream-I1 dev 版本步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg) 按步骤完成工作流的运行\n\n1.  确保`Load Diffusion Model` 节点中使用的是 `hidream_i1_dev_fp8.safetensors` 文件\n2.  确保`QuadrupleCLIPLoader` 中四个对应的 text encoder 被正确加载\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  确保`Load VAE` 节点中使用的是 `ae.safetensors` 文件\n4.  对于 **dev** 版本你需要设置 `ModelSamplingSD3` 中的 `shift` 参数为 `6.0`\n5.  对于 `Ksampler` 节点，你需要进行以下设置\n    *   `steps` 设置为 `28`\n    *   (重要) `cfg` 设置为 `1.0`\n    *   (可选) `sampler` 设置为 `lcm`\n    *   (可选) `scheduler` 设置为 `normal`\n6.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片生成\n\n### HiDream-I1 fast 版本工作流\n\n#### 1\\. 模型文件下载\n\n请根据你的硬件情况选择合适的版本，点击链接并下载对应的模型文件保存到 `ComfyUI/models/diffusion_models/` 文件夹下。\n\n*   FP8 版本：[hidream\\_i1\\_fast\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) 需要 16GB 以上的显存\n*   完整版本：[hidream\\_i1\\_fast\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_bf16.safetensors?download=true) 需要 27GB 以上的显存\n\n#### 2\\. 工作流文件下载\n\n请下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流 ![HiDream-I1 fast 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_fast.png)\n\n#### 3\\. 按步骤完成工作流的运行\n\n![HiDream-I1 fast 版本步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg) 按步骤完成工作流的运行\n\n1.  确保`Load Diffusion Model` 节点中使用的是 `hidream_i1_fast_fp8.safetensors` 文件\n2.  确保`QuadrupleCLIPLoader` 中四个对应的 text encoder 被正确加载\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  确保`Load VAE` 节点中使用的是 `ae.safetensors` 文件\n4.  对于 **fast** 版本你需要设置 `ModelSamplingSD3` 中的 `shift` 参数为 `3.0`\n5.  对于 `Ksampler` 节点，你需要进行以下设置\n    *   `steps` 设置为 `16`\n    *   (重要) `cfg` 设置为 `1.0`\n    *   (可选) `sampler` 设置为 `lcm`\n    *   (可选) `scheduler` 设置为 `normal`\n6.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片生成\n\n## 使用建议\n\n*   虽然 HiDream-I1 支持中文提示词，但建议还是优先使用英文提示词来保证准确性\n*   你可以使用 fast 版本来快速生成示例验证，然后再用完整版本的模型来生成较高质量的图像\n\n## 其它相关资源\n\n### GGUF 版本模型\n\n*   [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)\n*   [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)\n\n你需要使用 City96 的 [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) 中的 `Unet Loader (GGUF)`节点替换掉 `Load Diffusion Model` 节点来使用 GGUF 版本模型。\n\n*   [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF)\n\n### NF4 版本模型\n\n*   [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)\n*   使用 [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) 节点来使用 NF4 版本模型。\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-kontext-dev",
  "markdown": "# ComfyUI Flux Kontext Dev 原生工作流示例\n\n![Legacy Browser](https://s1.hdslb.com/bfs/static/player/img/h5.png)\n\n您当前的浏览器不支持 HTML5 播放器\n\n请更换浏览器再试试哦~\n\nFLUX.1 Kontext 是 Black Forest Labs 推出的突破性多模态图像编辑模型，支持文本和图像同时输入，能够智能理解图像上下文并执行精确编辑。其开发版是一个拥有 120 亿参数的开源扩散变压器模型，具有出色的上下文理解能力和角色一致性保持，即使经过多次迭代编辑，也能确保人物特征、构图布局等关键元素保持稳定。 与 FLUX.1 Kontext 套件具备相同的核心能力： 角色一致性：在多个场景和环境中保留图像的独特元素，例如图片中的参考角色或物体。 局部编辑：对图像中的特定元素进行有针对性的修改，而不影响其他部分。 风格参考：根据文本提示，在保留参考图像独特风格的同时生成新颖场景。 交互速度：图像生成和编辑的延迟极小。 虽然之前发布的 API 版本提供了最高的保真度和速度，但 FLUX.1 Kontext \\[Dev\\] 完全在本地机器上运行，为希望进行实验的开发者、研究人员和高级用户提供了无与伦比的灵活性。\n\n### 版本说明\n\n*   **\\[FLUX.1 Kontext \\[pro\\]** - 商业版本，专注快速迭代编辑\n*   **FLUX.1 Kontext \\[max\\]** - 实验版本，更强的提示遵循能力\n*   **FLUX.1 Kontext \\[dev\\]** - 开源版本（本教程使用），12B参数，主要用于研究\n\n目前在 ComfyUI 中，你可以使用所有的这些版本，其中 [Pro 及 Max 版本](https://docs.comfy.org/zh-CN/tutorials/api-nodes/black-forest-labs/flux-1-kontext) 可以通过 API 节点来进行调用，而 Dev 版本开源版本请参考本篇指南中的说明。\n\n## 工作流说明\n\n目前在本篇教程中，我们涉及了两类工作流，本质上他们其实是相同的，\n\n*   使用了组节点 **FLUX.1 Kontext Image Edit** 的工作流，使得整个界面和工作流复用起来变得简单\n*   而另一个工作流没有使用组节点，是完整的原始工作流。\n\n使用组节点的主要优点是工作流简洁，你可以复用组节点来实现复杂的工作流，快速复用节点组，另外在新版本的前端中，我们也为 Flux.1 Kontext Dev 增加了一个快速添加组节点的功能： ![快速添加组节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/selcetion_toolbox_edit.jpg)\n\n## 模型下载\n\n为了使本篇指南的工作流能够顺利运行，你先需要下载下面的模型文件,你也可以直接加载对应工作流下直接获取模型的下载链接，对应的工作流已经包含了模型文件的下载信息。 **Diffusion Model**\n\n*   [flux1-dev-kontext\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/flux1-kontext-dev_ComfyUI/resolve/main/split_files/diffusion_models/flux1-dev-kontext_fp8_scaled.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/blob/main/split_files/vae/ae.safetensors)\n\n**Text Encoder**\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/clip_l.safetensors)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors) 或 [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors)\n\n模型保存位置\n\n```\n📂 ComfyUI/\n├── 📂 models/\n│   ├── 📂 diffusion_models/\n│   │   └── flux1-dev-kontext_fp8_scaled.safetensors\n│   ├── 📂 vae/\n│   │   └── ae.safetensor\n│   └── 📂 text_encoders/\n│       ├── clip_l.safetensors\n│       └── t5xxl_fp16.safetensors 或者 t5xxl_fp8_e4m3fn_scaled.safetensors\n```\n\n## Flux.1 Kontext Dev Basic 工作流\n\n这个工作流是正常的工作流，不过使用了 `Load Image(from output)` 节点来加载需要编辑的图像可以让你更方便地获取到编辑后的图像，从而进行多轮次编辑\n\n### 1\\. 工作流及输入图片下载\n\n下载下面的文件，并拖入 ComfyUI 中加载对应工作流 ![ComfyUI Flux.1 Kontext Pro Image API 节点 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/flux_1_kontext_dev_basic.png) **输入图片** ![ComfyUI Flux Kontext 原生工作流输入](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/rabbit.jpg)\n\n### 2\\. 按步骤完成工作流的运行\n\n ![工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg) 你可参考图片中的序号来完成图工作流的运行：\n\n1.  在 `Load Diffusion Model` 节点中加载 `flux1-dev-kontext_fp8_scaled.safetensors` 模型\n2.  在 `DualCLIP Load` 节点中确保： `clip_l.safetensors` 及 `t5xxl_fp16.safetensors` 或 `t5xxl_fp8_e4m3fn_scaled.safetensors` 已经加载\n3.  在 `Load VAE` 节点中确保加载 `ae.safetensors` 模型\n4.  在 `Load Image(from output)` 节点中加载提供的输入图像\n5.  在 `CLIP Text Encode` 节点中修改提示词，仅支持英文\n6.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n## Flux.1 Kontext Dev Grouped 工作流\n\n这个工作流是使用组节点 **FLUX.1 Kontext Image Edit** 的工作流，使得整个界面和工作流复用起来变得简单 同时这个示例也使用了两个图像进行输入通过 `Image Stitch` 节点将两个图像拼接成一个图像，并使用 Flux.1 Kontext 进行编辑。\n\n### 1\\. 工作流及输入图片下载\n\n下载下面的文件，并拖入 ComfyUI 中加载对应工作流 ![ComfyUI Flux Kontext 原生工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/flux_1_kontext_dev_grouped.png) **输入图片** ![ComfyUI Flux Kontext 原生工作流输入](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/doll_1.webp) ![ComfyUI Flux Kontext 原生工作流输入](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/doll_2.webp)\n\n### 2\\. 按步骤完成工作流的运行\n\n ![工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_1_kontext_dev_grouped_step_guide.jpg) 你可参考图片中的序号来完成图工作流的运行：\n\n1.  在 `Load VAE` 节点中加载 `ae.safetensors` 模型\n2.  在 `Load Image` 节点中加载提供的第一个输入图像\n3.  在 `Load Image` 节点中加载提供的第二个输入图像\n4.  由于其它模型和相关节点都被组节点打包，你需要按照步骤图中的参考同样确保对应的模型已经正确加载，并书写提示词\n5.  点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流\n\n## 新增的 Flux.1 Kontext Dev 选择工具箱功能\n\n此次为了方便用户使用 Flux.1 Kontext 进行编辑，我们新增了选择工具箱功能，用户可以更加方便地快速添加 `FLUX.1 Kontext Image Edit` 组节点，具体可以查看下面的视频演示，当你选中 `Load Image` 节点时，就可以在选择工具箱中找到新增的编辑按钮\n\n![Legacy Browser](https://s1.hdslb.com/bfs/static/player/img/h5.png)\n\n您当前的浏览器不支持 HTML5 播放器\n\n请更换浏览器再试试哦~\n\n## Flux Kontext 提示词技巧\n\n### 1\\. 基础修改\n\n*   简单直接：`\"Change the car color to red\"`\n*   保持风格：`\"Change to daytime while maintaining the same style of the painting\"`\n\n### 2\\. 风格转换\n\n**原则：**\n\n*   明确命名风格：`\"Transform to Bauhaus art style\"`\n*   描述特征：`\"Transform to oil painting with visible brushstrokes, thick paint texture\"`\n*   保留构图：`\"Change to Bauhaus style while maintaining the original composition\"`\n\n### 3\\. 角色一致性\n\n**框架：**\n\n*   具体描述：`\"The woman with short black hair\"`而非`\"she\"`\n*   保留特征：`\"while maintaining the same facial features, hairstyle, and expression\"`\n*   分步修改：先改背景，再改动作\n\n### 4\\. 文本编辑\n\n*   使用引号：`\"Replace 'joy' with 'BFL'\"`\n*   保持格式：`\"Replace text while maintaining the same font style\"`\n\n## 常见问题解决\n\n### 角色变化过大\n\n❌ 错误：`\"Transform the person into a Viking\"` ✅ 正确：`\"Change the clothes to be a viking warrior while preserving facial features\"`\n\n### 构图位置改变\n\n❌ 错误：`\"Put him on a beach\"` ✅ 正确：`\"Change the background to a beach while keeping the person in the exact same position, scale, and pose\"`\n\n### 风格应用不准确\n\n❌ 错误：`\"Make it a sketch\"` ✅ 正确：`\"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\"`\n\n## 核心原则\n\n1.  **具体明确** - 使用精确描述，避免模糊词汇\n2.  **分步编辑** - 复杂修改分为多个简单步骤\n3.  **明确保留** - 说明哪些要保持不变\n4.  **动词选择** - 用”change”、“replace”而非”transform”\n\n## 最佳实践模板\n\n**对象修改：** `\"Change [object] to [new state], keep [content to preserve] unchanged\"` **风格转换：** `\"Transform to [specific style], while maintaining [composition/character/other] unchanged\"` **背景替换：** `\"Change the background to [new background], keep the subject in the exact same position and pose\"` **文本编辑：** `\"Replace '[original text]' with '[new text]', maintain the same font style\"`\n\n> **记住：** 越具体越好，Kontext 擅长理解详细指令并保持一致性。\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/wan-video",
  "markdown": "# ComfyUI Wan2.1 Video 示例 - ComfyUI\n\nWan2.1 Video 系列为阿里巴巴于 2025年2月开源的视频生成模型，其开源协议为 [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file)，提供 14B（140亿参数）和 1.3B（13亿参数）两个版本，覆盖文生视频（T2V）、图生视频（I2V）等多项任务。 该模型不仅在性能上超越现有开源模型，更重要的是其轻量级版本仅需 8GB 显存即可运行，大大降低了使用门槛。\n\n*   [Wan2.1 代码仓库](https://github.com/Wan-Video/Wan2.1)\n*   [Wan2.1 相关模型仓库](https://huggingface.co/Wan-AI)\n\n## Wan2.1 ComfyUI 原生（native）工作流示例\n\n## 模型安装\n\n本篇指南涉及的所有模型你都可以在[这里](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files)找到, 下面是本篇示例中将会使用到的共用的模型，你可以提前进行下载： 从**Text encoders** 选择一个版本进行下载，\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\n文件保存位置\n\n```\nComfyUI/\n├── models/\n│   ├── diffusion_models/\n│   ├── ...                  # 我们在对应的工作流中进行补充说明\n│   ├── text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors\n│   └── vae/\n│   │   └──  wan_2.1_vae.safetensors\n│   └── clip_vision/\n│       └──  clip_vision_h.safetensors   \n```\n\n在开始工作流前请下载 [wan2.1\\_t2v\\_1.3B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true)，并保存到 `ComfyUI/models/diffusion_models/` 目录下。\n\n> 如果你需要其它的 t2v 精度版本，请访问[这里](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models)进行下载\n\n### 1\\. 工作流文件下载\n\n下载下面的文件，并拖入 ComfyUI 以加载对应的工作流 ![Wan2.1 文生视频工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_t2v_1.3b.webp)\n\n### 2\\. 按流程完成工作流运行\n\n![ComfyUI Wan2.1 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg)\n\n1.  确保`Load Diffusion Model`节点加载了 `wan2.1_t2v_1.3B_fp16.safetensors` 模型\n2.  确保`Load CLIP`节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 模型\n3.  确保`Load VAE`节点加载了 `wan_2.1_vae.safetensors` 模型\n4.  （可选）可以在`EmptyHunyuanLatentVideo` 节点设置了视频的尺寸，如果有需要你可以修改\n5.  （可选）如果你需要修改提示词（正向及负向）请在序号`5` 的 `CLIP Text Encoder` 节点中进行修改\n6.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n\n## Wan2.1 图生视频工作流\n\n**由于 Wan Video 将 480P 和 720P 的模型分开** ，所以在本篇中我们将需要分别对两中清晰度的视频做出示例，除了对应模型不同之外，他们还有些许的参数差异\n\n### 480P 版本\n\n#### 1\\. 工作流及输入图片\n\n下载下面的图片，并拖入 ComfyUI 中来加载对应的工作流 ![Wan2.1 图生视频工作流 14B 480P Workflow 输入图片示例](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_480P.webp) 我们将使用下面的图片作为输入： ![Wan2.1 图生视频工作流 14B 480P Workflow 输入图片示例](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/flux_dev_example.png)\n\n#### 2\\. 模型下载\n\n请下载[wan2.1\\_i2v\\_480p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true)，并保存到 `ComfyUI/models/diffusion_models/` 目录下\n\n#### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI Wan2.1 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg)\n\n1.  确保`Load Diffusion Model`节点加载了 `wan2.1_i2v_480p_14B_fp16.safetensors` 模型\n2.  确保`Load CLIP`节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 模型\n3.  确保`Load VAE`节点加载了 `wan_2.1_vae.safetensors` 模型\n4.  确保`Load CLIP Vision`节点加载了 `clip_vision_h.safetensors` 模型\n5.  在`Load Image`节点中上传我们提供的输入图片\n6.  （可选）在`CLIP Text Encoder`节点中输入你想要生成的视频描述内容，\n7.  （可选）在`WanImageToVideo` 节点中设置了视频的尺寸，如果有需要你可以修改\n8.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n\n### 720P 版本\n\n#### 1\\. 工作流及输入图片\n\n下载下面的图片，并拖入 ComfyUI 中来加载对应的工作流 ![Wan2.1 图生视频工作流 14B 720P Workflow 输入图片示例](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_720P.webp) 我们将使用下面的图片作为输入： ![Wan2.1 图生视频工作流 14B 720P Workflow 输入图片示例](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/magician.png)\n\n#### 2\\. 模型下载\n\n请下载[wan2.1\\_i2v\\_720p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true)，并保存到 `ComfyUI/models/diffusion_models/` 目录下\n\n#### 3\\. 按步骤完成工作流的运行\n\n![ComfyUI Wan2.1 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg)\n\n1.  确保`Load Diffusion Model`节点加载了 `wan2.1_i2v_720p_14B_fp16.safetensors` 模型\n2.  确保`Load CLIP`节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 模型\n3.  确保`Load VAE`节点加载了 `wan_2.1_vae.safetensors` 模型\n4.  确保`Load CLIP Vision`节点加载了 `clip_vision_h.safetensors` 模型\n5.  在`Load Image`节点中上传我们提供的输入图片\n6.  （可选）在`CLIP Text Encoder`节点中输入你想要生成的视频描述内容，\n7.  （可选）在`WanImageToVideo` 节点中设置了视频的尺寸，如果有需要你可以修改\n8.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n\n#### 0 个表情"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/wan-ati",
  "markdown": "# Wan ATI ComfyUI 原生工作流教程 - ComfyUI\n\n**ATI（Any Trajectory Instruction）** 是由字节跳动团队提出的可控视频生成框架。ATI 基于 Wan2.1 实现，支持通过任意轨迹指令对视频中的物体、局部区域及摄像机运动进行统一控制。 项目地址：[https://github.com/bytedance/ATI](https://github.com/bytedance/ATI)\n\n## 主要特性\n\n*   **统一运动控制**：支持物体、局部、摄像机等多种运动类型的轨迹控制。\n*   **交互式轨迹编辑器**：可视化工具，用户可在图片上自由绘制、编辑运动轨迹。\n*   **兼容 Wan2.1**：基于 Wan2.1 官方实现，环境和模型结构兼容。\n*   **丰富的可视化工具**：支持输入轨迹、输出视频及轨迹可视化。\n\n### 1\\. 工作流下载\n\n下载下面的视频并拖入 ComfyUI 中，以加载对应的工作流\n\n我们将使用下面的素材作为输入: ![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/ati/input.jpg) \n\n### 2\\. 模型下载\n\n如果你没有成功下载工作流中的模型文件，可以尝试使用下面的链接手动下载 **Diffusion Model**\n\n*   [Wan2\\_1-I2V-ATI-14B\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**Text encoders** Chose one of following model\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**clip\\_vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)\n\nFile save location\n\n```\nComfyUI/\n├───📂 models/\n│   ├───📂 diffusion_models/\n│   │   └───Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors\n│   ├───📂 text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors # or other version\n│   ├───📂 clip_vision/\n│   │   └─── clip_vision_h.safetensors\n│   └───📂 vae/\n│       └──  wan_2.1_vae.safetensors\n```\n\n### 3\\. 按步骤完成工作流的运行\n\n![工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan_ati_guide.jpg) 请参照图片序号进行逐步确认，来保证对应工作流的顺利运行\n\n1.  确保`Load Diffusion Model`节点加载了 `Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors` 模型\n2.  确保`Load CLIP`节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 模型\n3.  确保`Load VAE`节点加载了 `wan_2.1_vae.safetensors` 模型\n4.  确保`Load CLIP Vision`节点加载了 `clip_vision_h.safetensors` 模型\n5.  在 `Load Image` 节点上传提供的输入图片\n6.  轨迹编辑： 目前 ComfyUI 中还未有对应的轨迹编辑器，你可以使用下面的链接来完成轨迹编辑\n    *   [在线轨迹编辑工具](https://comfyui-wiki.github.io/Trajectory-Annotation-Tool/)\n7.  如果你需要修改提示词（正向及负向）请在序号`5` 的 `CLIP Text Encoder` 节点中进行修改\n8.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成\n\n#### 0 个表情\n\n在此页面\n\n*   [主要特性](#%E4%B8%BB%E8%A6%81%E7%89%B9%E6%80%A7)\n*   [WAN ATI 轨迹控制工作流示例](#wan-ati-%E8%BD%A8%E8%BF%B9%E6%8E%A7%E5%88%B6%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A4%BA%E4%BE%8B)\n*   [1\\. 工作流下载](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E4%B8%8B%E8%BD%BD)\n*   [2\\. 模型下载](#2-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD)\n*   [3\\. 按步骤完成工作流的运行](#3-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/wan-flf",
  "markdown": "# ComfyUI Wan2.1 FLF2V 原生示例 - ComfyUI\n\nWan FLF2V（首尾帧视频生成）是由阿里通义万相团队推出的开源视频生成模型。其开源协议为 [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file)。 用户只需提供起始帧和结束帧两张图像，模型即可自动生成中间过渡帧，输出一段逻辑连贯、自然流畅的720p高清视频。 **核心技术亮点**\n\n1.  **首尾帧精准控制**：首尾帧匹配度达98%，通过起始和结束画面定义视频边界，模型智能填充中间动态变化，实现场景转换和物体形态演变等效果。\n2.  **稳定流畅视频生成**：采用CLIP语义特征和交叉注意力机制，视频抖动率比同类模型降低37%，确保转场自然流畅。\n3.  **多功能创作能力**：支持中英文字幕动态嵌入、二次元/写实/奇幻等多风格生成，适应不同创作需求。\n4.  **720p高清输出**：直接生成1280×720分辨率视频，无需后处理，适用于社交媒体和商业应用。\n5.  **开源生态支持**：模型权重、代码及训练框架全面开源，支持主流AI平台部署。\n\n**技术原理与架构**\n\n1.  **DiT架构**：基于扩散模型和Diffusion Transformer架构，结合Full Attention机制优化时空依赖建模，确保视频连贯性。\n2.  **三维因果变分编码器**：Wan-VAE技术将高清画面压缩至1/128尺寸，同时保留细微动态细节，显著降低显存需求。\n3.  **三阶段训练策略**：从480P分辨率开始预训练，逐步提升至720P，通过分阶段优化平衡生成质量与计算效率。\n\n**相关链接**\n\n*   **GitHub代码仓库**：[GitHub](https://github.com/Wan-Video/Wan2.1)\n*   **Hugging Face模型页**：[Hugging Face](https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P)\n*   **ModelScope（魔搭社区）**：[ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)\n\n### 1\\. 下载工作流文件及相关输入文件\n\n请下载下面的 WebP 保存下面的 WebP 文件，并拖入 ComfyUI 中来加载对应的工作流,对应工作流已嵌入对应的模型下载文件信息。 ![Wan2.1 FLF2V 720P f16 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/wan2.1_flf2v_720_f16.webp) 请下载下面的两张图片，我们将会作为作为视频的起始帧和结束帧 ![start_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/start_image.png) ![end_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/end_image.png)\n\n### 2.手动模型安装\n\n本篇指南涉及的所有模型你都可以在[这里](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files)找到。 **diffusion\\_models** 根据你的硬件情况选择一个版本进行下载，FP8 版本对显存要求低一些\n\n*   FP16:[wan2.1\\_flf2v\\_720p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp16.safetensors?download=true)\n*   FP8:[wan2.1\\_flf2v\\_720p\\_14B\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors)\n\n从**Text encoders** 选择一个版本进行下载，\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\n文件保存位置\n\n```\nComfyUI/\n├── models/\n│   ├── diffusion_models/\n│   │   └─── wan2.1_flf2v_720p_14B_fp16.safetensors          # 或者 FP8 版本\n│   ├── text_encoders/\n│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors           # 或者你选择的版本\n│   ├── vae/\n│   │   └──  wan_2.1_vae.safetensors\n│   └── clip_vision/\n│       └──  clip_vision_h.safetensors   \n```\n\n### 3\\. 按步骤完成工作流运行\n\n![Wan2.1 FLF2V 720P 原生工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg)\n\n1.  确保 `Load Diffusion Model` 节点加载了 `wan2.1_flf2v_720p_14B_fp16.safetensors` 或者 `wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors`\n2.  确保 `Load CLIP` 节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`\n4.  确保 `Load CLIP Vision` 节点加载了 `clip_vision_h.safetensors`\n5.  在 `Start_image` 节点上传起始帧\n6.  在 `End_image` 节点上传结束帧\n7.  （可选）修改 正向和负向的提示词（Prompt）使用中英文都可以\n8.  （**重要**）在 `WanFirstLastFrameToVideo` 修改对应视频的尺寸我们默认使用了 720 \\* 1280 的尺寸来，因为这是一个 720P 的尺寸来，因为这是一个720P的模型，所以使用较小的尺寸会无法获得较好的结果。\n9.  点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fvideo%2Fwan%2Ffun-camera",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fapi-nodes%2Fideogram%2Fideogram-v3",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fcustom-nodes%2Fjs%2Fjavascript_topbar_menu",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fapi-nodes%2Frunway%2Fimage-generation",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fvideo%2Fhunyuan-video",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fapi-nodes%2Fmoonvalley%2Fmoonvalley-video-generation",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fcontrolnet%2Fpose-controlnet-2-pass",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fimage%2Fhidream%2Fhidream-i1",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fflux%2Fflux-1-kontext-dev",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fvideo%2Fwan%2Fwan-video",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fvideo%2Fwan%2Fwan-ati",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
}]