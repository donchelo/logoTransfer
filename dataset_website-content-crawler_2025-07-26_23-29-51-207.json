[{
  "url": "https://docs.comfy.org/",
  "markdown": "# ComfyUI Official Documentation - ComfyUI\n\nGet Started\n\nComfyUI Official Documentation"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/create-text-to-image-prompt",
  "markdown": "# Create Text to Image Prompt\n\n#### Body\n\nHide child attributes\n\ninference\\_params.add\\_quality\\_guidance\n\nWhether to add quality guidance\n\ninference\\_params.caching\\_coefficient\n\nCaching coefficient for optimization\n\ninference\\_params.caching\\_cooldown\n\nNumber of caching cooldown steps\n\ninference\\_params.caching\\_warmup\n\nNumber of caching warmup steps\n\ninference\\_params.clip\\_value\n\nCLIP value for generation control\n\ninference\\_params.conditioning\\_frame\\_index\n\nIndex of the conditioning frame\n\ninference\\_params.cooldown\\_steps\n\nNumber of cooldown steps (calculated based on num\\_frames)\n\nFrames per second of the generated video\n\ninference\\_params.guidance\\_scale\n\nGuidance scale for generation control\n\nHeight of the generated video in pixels\n\ninference\\_params.negative\\_prompt\n\ninference\\_params.num\\_frames\n\nNumber of frames to generate\n\nRandom seed for generation (default: random)\n\ninference\\_params.shift\\_value\n\nShift value for generation control\n\nNumber of denoising steps\n\ninference\\_params.use\\_guidance\\_schedule\n\nWhether to use guidance scheduling\n\ninference\\_params.use\\_negative\\_prompts\n\nWhether to use negative prompts\n\ninference\\_params.use\\_timestep\\_transform\n\nWhether to use timestep transformation\n\ninference\\_params.warmup\\_steps\n\nNumber of warmup steps (calculated based on num\\_frames)\n\nWidth of the generated video in pixels\n\n#### Response\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/create-text-to-video-prompt",
  "markdown": "# Create Text to Video Prompt\n\n#### Body\n\nHide child attributes\n\ninference\\_params.add\\_quality\\_guidance\n\nWhether to add quality guidance\n\ninference\\_params.caching\\_coefficient\n\nCaching coefficient for optimization\n\ninference\\_params.caching\\_cooldown\n\nNumber of caching cooldown steps\n\ninference\\_params.caching\\_warmup\n\nNumber of caching warmup steps\n\ninference\\_params.clip\\_value\n\nCLIP value for generation control\n\ninference\\_params.conditioning\\_frame\\_index\n\nIndex of the conditioning frame\n\ninference\\_params.cooldown\\_steps\n\nNumber of cooldown steps (calculated based on num\\_frames)\n\nFrames per second of the generated video\n\ninference\\_params.guidance\\_scale\n\nGuidance scale for generation control\n\nHeight of the generated video in pixels\n\ninference\\_params.negative\\_prompt\n\ninference\\_params.num\\_frames\n\nNumber of frames to generate\n\nRandom seed for generation (default: random)\n\ninference\\_params.shift\\_value\n\nShift value for generation control\n\nNumber of denoising steps\n\ninference\\_params.use\\_guidance\\_schedule\n\nWhether to use guidance scheduling\n\ninference\\_params.use\\_negative\\_prompts\n\nWhether to use negative prompts\n\ninference\\_params.use\\_timestep\\_transform\n\nWhether to use timestep transformation\n\ninference\\_params.warmup\\_steps\n\nNumber of warmup steps (calculated based on num\\_frames)\n\nWidth of the generated video in pixels\n\n#### Response\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/create-image-to-video-prompt",
  "markdown": "# Create Image to Video Prompt\n\n#### Body\n\nHide child attributes\n\ninference\\_params.add\\_quality\\_guidance\n\nWhether to add quality guidance\n\ninference\\_params.caching\\_coefficient\n\nCaching coefficient for optimization\n\ninference\\_params.caching\\_cooldown\n\nNumber of caching cooldown steps\n\ninference\\_params.caching\\_warmup\n\nNumber of caching warmup steps\n\ninference\\_params.clip\\_value\n\nCLIP value for generation control\n\ninference\\_params.conditioning\\_frame\\_index\n\nIndex of the conditioning frame\n\ninference\\_params.cooldown\\_steps\n\nNumber of cooldown steps (calculated based on num\\_frames)\n\nFrames per second of the generated video\n\ninference\\_params.guidance\\_scale\n\nGuidance scale for generation control\n\nHeight of the generated video in pixels\n\ninference\\_params.negative\\_prompt\n\ninference\\_params.num\\_frames\n\nNumber of frames to generate\n\nRandom seed for generation (default: random)\n\ninference\\_params.shift\\_value\n\nShift value for generation control\n\nNumber of denoising steps\n\ninference\\_params.use\\_guidance\\_schedule\n\nWhether to use guidance scheduling\n\ninference\\_params.use\\_negative\\_prompts\n\nWhether to use negative prompts\n\ninference\\_params.use\\_timestep\\_transform\n\nWhether to use timestep transformation\n\ninference\\_params.warmup\\_steps\n\nNumber of warmup steps (calculated based on num\\_frames)\n\nWidth of the generated video in pixels\n\n#### Response\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/get_started/first_generation",
  "markdown": "# Getting Started with AI Image Generation\n\nThis guide aims to help you understand ComfyUIâ€™s basic operations and complete your first image generation. Weâ€™ll cover:\n\n1.  Loading example workflows\n    *   Loading from ComfyUIâ€™s workflow templates\n    *   Loading from images with workflow metadata\n2.  Model installation guidance\n    *   Automatic model installation\n    *   Manual model installation\n    *   Using ComfyUI Manager for model installation\n3.  Completing your first text-to-image generation\n\n## About Text-to-Image\n\nText-to-Image is a fundamental AI drawing feature that generates images from text descriptions. Itâ€™s one of the most commonly used functions in AI art generation. You can think of the process as telling your requirements (positive and negative prompts) to an artist (the drawing model), who will then create what you want. Detailed explanations about text-to-image will be covered in the [Text to Image](https://docs.comfy.org/tutorials/basic/text-to-image) chapter.\n\n## ComfyUI Text-to-Image Workflow Tutorial\n\n### 1\\. Launch ComfyUI\n\nMake sure youâ€™ve followed the installation guide to start ComfyUI and can successfully enter the ComfyUI interface. ![ComfyUI Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/comfyui-boot-screen.jpg) If you have not installed ComfyUI, please choose a suitable version to install based on your device.\n\n### 2\\. Load Default Text-to-Image Workflow\n\nComfyUI usually loads the default text-to-image workflow automatically when launched. However, you can try different methods to load workflows to familiarize yourself with ComfyUIâ€™s basic operations:\n\n ![ComfyUI Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1.jpg) Follow the numbered steps in the image:\n\n1.  Click the **Fit View** button in the bottom right to ensure any loaded workflow isnâ€™t hidden\n2.  Click the **folder icon (workflows)** in the sidebar\n3.  Click the **Browse example workflows** button at the top of the Workflows panel\n\nContinue with: ![Load Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg) \n\n4.  Select the first default workflow **Image Generation** to load it\n\nAlternatively, you can select **Browse workflow templates** from the workflow menu ![ComfyUI Menu - Browse Workflow Templates](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg) \n\n### 3\\. Model Installation\n\nMost ComfyUI installations donâ€™t include base models by default. After loading the workflow, if you donâ€™t have the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model installed, youâ€™ll see this prompt: ![Missing Models](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg) All models are stored in `<your ComfyUI installation>/ComfyUI/models/` with subfolders like `checkpoints`, `embeddings`, `vae`, `lora`, `upscale_model`, etc. ComfyUI detects models in these folders and paths configured in `extra_model_paths.yaml` at startup. ![ComfyUI Models Folder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg) You can install models through:\n\nAfter you click the **Download** button, ComfyUI will execute the download, and different behaviors will be performed depending on the version you are using.\n\nThe desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.![Model Download Progress](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg)If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.\n\n### 4\\. Load Model and Generate Your First Image\n\nAfter installing the model: ![Image Generation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)\n\n1.  In the **Load Checkpoint** node, ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected\n2.  Click `Queue` or press `Ctrl + Enter` to generate\n\nThe result will appear in the **Save Image** node. Right-click to save locally. ![ComfyUI First Image Generation Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg) For detailed text-to-image instructions, see our comprehensive guide:\n\n[\n\n## ComfyUI Text-to-Image Workflow Guide\n\nClick here for detailed text-to-image workflow instructions\n\n\n\n](https://docs.comfy.org/tutorials/basic/text-to-image)\n\n## Troubleshooting\n\n### Model Loading Issues\n\nIf the `Load Checkpoint` node shows no models or displays â€œnullâ€, verify your model installation location and try refreshing or restarting ComfyUI."
},
{
  "url": "https://docs.comfy.org/changelog",
  "markdown": "# Changelog - ComfyUI\n\nTrack ComfyUIâ€™s latest features, improvements, and bug fixes\n\n#### 0 reactions\n\n[Sign in](https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fchangelog) to add your reaction."
},
{
  "url": "https://docs.comfy.org/development/core-concepts/workflow",
  "markdown": "# Workflow - ComfyUI\n\n## A graph of nodes\n\nComfyUI is an environment for building and running generative content _**workflows**_. In this context, a workflow is defined as a collection of program objects called _**nodes**_ that are connected to each other, forming a network. This network is also known as a _**graph**_. A ComfyUI workflow can generate any type of media: image, video, audio, AI model, AI agent, and so on.\n\n## Sample workflows\n\nTo get started, try out some of the [official workflows](https://comfyanonymous.github.io/ComfyUI_examples). These use only the Core nodes included in the ComfyUI installation. A thriving community of developers has created a rich [ecosystem](https://registry.comfy.org/) of custom nodes to extend the functionality of ComfyUI.\n\n### Simple Example\n\n![simple workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/simple_workflow.jpeg)\n\n## Visual programming\n\nA node-based computer program like ComfyUI provides a level of power and flexibility that canâ€™t be achieved with traditional menu- and button-driven applications. The ComfyUI node graph is not limited by the tools provided in a traditional computer application. Itâ€™s a high-level _**visual programming environment**_ allowing users to design complex systems without needing to write program code or understand advanced mathematics. Many other computer applications use this same node graph paradigm. Examples include the compositing application called Nuke, the 3D programs Maya and Blender, the Unreal real-time graphics engine, and the interactive media authoring program called Max.\n\n### More Complex Example\n\n![complex workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/complex_workflow.jpeg)\n\n## Procedural framework\n\nAnother term used to describe a node-based application is _**procedural framework**_. Procedural means generative: some procedure or algorithm is employed to generate content such as a 3D model or a musical composition. ComfyUI is all of these things: a node graph, a visual programming environment, and a procedural framework. What makes ComfyUI different (and amazing!) is that its radically open structure allows us to generate any type of media asset such as picture, movie, sound, 3D model, AI model, etc. In the context of ComfyUI, the term _**workflow**_ is a synonym for the node network or graph. It corresponds to the _**scene graph**_ in a 3D or multimedia program: the network of all of the nodes within a particular disk file. 3D programs call this a _**scene file**_. Video editing, compositing, and multimedia programs usually call it a _**project file**_.\n\n## Saving workflows\n\nThe ComfyUI workflow is automatically saved in the metadata of any generated image, allowing users to open and use the graph that generated the image. A workflow can also be stored in a human-readable text file that follows the JSON data format. This is necessary for media formats that donâ€™t support metadata. ComfyUI workflows stored as JSON files are very small, allowing convenient versioning, archiving, and sharing of graphs, independently of any generated media.\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/nodes",
  "markdown": "# Nodes - ComfyUI\n\nIn ComfyUI, nodes are the fundamental building blocks for executing tasks. Each node is an independently built module, whether itâ€™s a **Comfy Core** node or a **Custom Node**, with its own unique functionality. Nodes connect to each other through links, allowing us to build complex functionality like assembling LEGO blocks. The combinations of different nodes create the unlimited possibilities of ComfyUI. ![Comfy Core K-Sampler Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg) For example, in the K-Sampler node, you can see it has multiple inputs and outputs, and also includes multiple parameter settings. These parameters determine the logic of node execution. Behind each node is well-written Python logic, allowing you to achieve corresponding functionality without having to write code yourself.\n\n## Nodes perform operations\n\nIn computer science, a _**node**_ is a container for information, usually including programmed instructions to perform some task. Nodes almost never exist in isolation, theyâ€™re almost always connected to other nodes in a networked graph. In ComfyUI, nodes take the visual form of boxes that are connected to each other. ComfyUI nodes are usually _**function operators**_. This means that they operate on some data to perform a function. A function is a process that accepts input data, performs some operation on it, and produces output data. In other words, nodes do some work, contributing to the completion of a task such as generating an image. So ComfyUI nodes almost always have at least one input or output, and usually have multiple inputs and outputs.\n\n## Different Node States\n\n![Node States](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/status.jpg) In ComfyUI, nodes have multiple states. Here are some common node states:\n\n1.  **Normal State**: The default state\n2.  **Running State**: The running state, typically displayed when a node is executing after you start running the workflow\n3.  **Error State**: Node error, typically displayed after running the workflow if thereâ€™s a problem with the nodeâ€™s input, indicated by red marking of the erroneous input node. You need to fix the problematic input to ensure the workflow runs correctly\n4.  **Missing State**: This state usually appears after importing workflows, with two possibilities:\n    *   Comfy Core native node missing: This usually happens because ComfyUI has been updated, but youâ€™re using an older version of ComfyUI. You need to update ComfyUI to resolve this issue\n    *   Custom node missing: The workflow uses custom nodes developed by third-party authors, but your local ComfyUI version doesnâ€™t have these custom nodes installed. You can use [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to find and install the missing custom nodes\n\n## Connections Between Nodes\n\nIn ComfyUI, nodes are connected through [links](https://docs.comfy.org/development/core-concepts/links), allowing data of the same type to flow between different processing units to achieve the final result. ![ComfyUI Node Links](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/inpaint.jpg) Each node receives some input, processes it through its module, and converts it to corresponding output. Connections between different nodes must conform to the data type requirements. In ComfyUI, we use different colors to distinguish node data types. Below are some basic data types: ![ComfyUI Node Data Types](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/data_type.jpg)\n\n| Data type | Color |\n| --- | --- |\n| diffusion model | lavender |\n| CLIP model | yellow |\n| VAE model | rose |\n| conditioning | orange |\n| latent image | pink |\n| pixel image | blue |\n| mask | green |\n| number (integer or float) | light green |\n| mesh | bright green |\n\nAs ComfyUI evolves, we may expand to more data types to meet the needs of more scenarios.\n\n### Connecting and Disconnecting Nodes\n\n![ComfyUI Node Connecting](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/link_nodes.gif) **Connecting**: Drag from the output point of one node to the input of the same color on another node to connect them **Disconnecting**: Click on the input endpoint and drag the mouse left button to disconnect, or cancel the connection through the midpoint menu of the link\n\n## Node Appearance\n\n![Node Appearance](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/index/node.jpg) We provide various style settings for you to customize the appearance of nodes:\n\n*   Modify styles\n*   Double-click the node title to modify the node name\n*   Switch node inputs between input sockets and widgets through the context menu\n*   Resize the node using the bottom right corner\n\n### Node Badges\n\n![Node Badges](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/badge.jpg) We provide multiple node badge display features, such as:\n\n*   Node ID\n*   Node source\n\nCurrently, **Comfy Core nodes** use a fox icon for display, while custom nodes use their names. This way you can quickly understand which node package a node comes from. You can set the corresponding display in the menu: ![Badge Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/badge_setting.jpg)\n\nNode context menus are mainly divided into two types:\n\n*   Context menu for the node itself\n*   Context menu for inputs/outputs\n\nBy right-clicking on a node, you can expand the corresponding node context menu: ![Node Context Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/context_menus_1.jpg) In the nodeâ€™s right-click context menu, you can:\n\n*   Adjust the nodeâ€™s color style\n*   Modify the title\n*   Clone, copy, or delete the node\n*   Set the nodeâ€™s mode\n\nIn this menu, besides appearance-related settings, the following menu operations are important:\n\n*   **Mode**: Set the nodeâ€™s mode: Always, Never, Bypass\n*   **Toggle between Widget and Input mode for node inputs**: Switch between widget and input mode for node inputs\n\n#### Mode\n\nFor modes, you may notice that we currently provide: Always, Never, On Event, On Trigger - four modes, but actually only **Always** and **Never** are effective. **On Event** and **On Trigger** are currently ineffective as we havenâ€™t fully implemented this feature. Additionally, you can understand **Bypass** as a mode. Below is an explanation of the available modes:\n\n*   **Always**: The default node mode. The node will execute whenever it runs for the first time or when any of its inputs change since the last execution\n*   **Never**: The node will never execute under any circumstances, as if itâ€™s been deleted. Subsequent nodes cannot read or receive any data from it\n*   **Bypass**: The node will never execute under any circumstances, but subsequent nodes can still try to obtain data that hasnâ€™t been processed by this node\n\nBelow is a comparison of the `Never` and `Bypass` modes: ![Never vs Bypass Mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/never_vs_bypass.jpg) In this comparison example, you can see that both workflows apply two LoRA models simultaneously, with the difference being that one `Load LoRA` node is set to `Never` mode while the other is set to `Bypass` mode.\n\n*   The node set to `Never` mode causes subsequent nodes to show errors because they donâ€™t receive any input data\n*   The node set to `Bypass` mode still allows subsequent nodes to receive unprocessed data, so they load the output data from the first `Load LoRA` node, allowing the subsequent workflow to continue running normally\n\n#### Switching Between Widget and Input Mode for Node Inputs\n\nIn some cases, we need to use output results from other nodes as input. In this case, we can switch between widget and input mode for node inputs. Hereâ€™s a very simple example: ![Switch Widget and Input Mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/switch_widget.jpg) By switching the K-Samplerâ€™s Seed from widget to input mode, multiple nodes can share the same seed, achieving variable uniformity across multiple samplers. Comparing the first node with the subsequent two nodes, you can see that the seed in the latter two nodes is in input mode. You can also convert it back to widget mode: ![Convert Input Mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/convert_input.jpg)\n\nThis context menu is mainly related to the data type of the corresponding input/output: ![Node Input/Output Context Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/context_menus_2.jpg) When dragging the input/output of a node, if a connection appears but you havenâ€™t connected to another nodeâ€™s input or output, releasing the mouse will pop up a context menu for the input/output, used to quickly add related types of nodes. You can adjust the number of node suggestions in the settings: ![Node Suggestion Count](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/node_suggestions.jpg)\n\nThe **Node Selection Toolbox** is a floating tool that provides quick operations for nodes. When you select a node, it hovers above the selected node. Through this toolbox, you can:\n\n*   Change the nodeâ€™s color\n*   Quickly set the node to Bypass mode (not execute during runtime)\n*   Lock the node\n*   Delete the node\n\nOf course, these functions can also be found in the right-click menu of the corresponding node. The node selection toolbox just provides a shortcut operation. If you want to disable this feature, you can turn it off in the settings. ![Disable Node Selection Toolbox](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/setting_selection_toolbox.jpg)\n\n## Node Groups\n\nIn ComfyUI, you can select multiple parts of a workflow simultaneously, then use the right-click menu to merge them into a node group, making that part a reusable module that can be repeatedly called in your ComfyUI.\n\n## Custom Nodes\n\nComfyUI includes many powerful nodes in the base installation package. These are known as **Comfy Core** nodes. Additionally, the ComfyUI community has created an amazing array of [_**custom nodes**_](https://registry.comfy.org/) to perform a wide variety of functions.\n\n## ComfyUI Manager\n\n![ComfyUI Manager interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png) The **ComfyUI Manager** window makes it easy to perform custom node management tasks such as search, install, update, disable, and uninstall. The Manager is included in the ComfyUI desktop application, but not in the ComfyUI server application.\n\n### Installing the ComfyUI Manager\n\nIf youâ€™re running the ComfyUI server application, you need to install the Manager. If ComfyUI is running, shut it down before proceeding. The first step is to install Git, a command-line application for software version control. Git will download the ComfyUI Manager from [github.com](https://github.com/). Download Git from [git-scm.com](https://git-scm.com/) and install it. Once Git is installed, navigate to the ComfyUI server program directory, to the folder labeled **custom\\_nodes**. Open up a command window or terminal. Make sure that the command line displays the current directory path as **custom\\_nodes**. Enter the following command. This will download the Manager. Technically, this is known as _cloning a Git repository_.\n\n```\ngit clone https://github.com/ltdrdata/ComfyUI-Manager.git\n```\n\nFor details or special cases, see [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation).\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/custom-nodes",
  "markdown": "# Custom Nodes - ComfyUI\n\n## About Custom Nodes\n\nAfter installing ComfyUI, youâ€™ll discover that it includes many built-in nodes. These native nodes are called **Comfy Core** nodes, which are officially maintained by ComfyUI. Additionally, there are numerous [**custom nodes**](https://registry.comfy.org/) created by various authors from the ComfyUI community. These custom nodes bring extensive functionality to ComfyUI, greatly expanding its capabilities and feature boundaries. In this guide, weâ€™ll cover various operations related to custom nodes, including installation, updates, disabling, uninstalling, and dependency installation. Anyone can develop their own custom extensions for ComfyUI and share them with others. You can find many community custom nodes [here](https://registry.comfy.org/). If you want to develop your own custom nodes, visit the section below to get started:\n\n[\n\n## Start Developing Custom Nodes\n\nLearn how to start developing a custom node\n\n\n\n](https://docs.comfy.org/custom-nodes/overview)\n\n## Custom Node Management\n\nIn this section we will cover:\n\n*   Installing custom nodes\n*   Installing node dependencies\n*   Custom node version control\n*   Uninstalling custom nodes\n*   Temporarily disabling custom nodes\n*   Handling custom node dependency conflicts\n\n### 1\\. Installing Custom Nodes\n\nCurrently, ComfyUI supports installing custom nodes through multiple methods, including:\n\n*   [Install via ComfyUI Manager (Recommended)](#install-via-comfyui-manager)\n*   Install via Git\n*   Manual installation\n\nWe recommend installing custom nodes through **ComfyUI Manager**, which is a highly significant tool in the ComfyUI custom node ecosystem. It makes custom node management (such as searching, installing, updating, disabling, and uninstalling) simple - you just need to search for the node you want to install in ComfyUI Manager and click install. However, since all custom nodes are currently stored on GitHub, for regions that cannot access GitHub normally, we have written detailed instructions for different custom node installation methods in this guide. Additionally, since we recommend using **ComfyUI Manager** for plugin management, we recommend using this tool for plugin management. You can find its source code [here](https://github.com/Comfy-Org/ComfyUI-Manager). Therefore, in this documentation, we will use installing ComfyUI Manager as a custom node installation example, and supplement how to use it for node management in the relevant introduction sections.\n\n### 2\\. Installing Node Dependencies\n\nCustom nodes all require the installation of related dependencies. For example, for ComfyUI-Manager, you can visit the [requirements.txt](https://github.com/Comfy-Org/ComfyUI-Manager/blob/main/requirements.txt) file to view the dependency package requirements. In the previous steps, we only cloned the custom node code locally and did not install the corresponding dependencies, so next we need to install the corresponding dependencies.\n\nIn the [Dependencies](https://docs.comfy.org/development/core-concepts/dependencies) chapter, we introduced the relevant content about dependencies in ComfyUI. ComfyUI is a **Python**\\-based project, and we built an independent **Python** runtime environment for running ComfyUI. All related dependencies need to be installed in this independent **Python** runtime environment. If you run `pip install -r requirements.txt` directly in the system-level terminal, the corresponding dependencies may be installed in the system-level **Python** environment, which will cause the dependencies to still be missing in ComfyUIâ€™s environment, preventing the corresponding custom nodes from running normally. So next we need to use ComfyUIâ€™s independent Python runtime environment to complete the dependency installation. Depending on different ComfyUI versions, we will use different methods to install the corresponding dependencies:\n\nFor ComfyUI Portable version, it uses an embedded Python located in the `\\ComfyUI_windows_portable\\python_embeded` directory. We need to use this Python to complete the dependency installation.First, start the terminal in the portable version directory, or use the `cd` command to navigate to the `\\ComfyUI_windows_portable\\` directory after starting the terminal.![Start Terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/custom_nodes/open_terminal.jpg)Ensure that the terminal directory is `\\ComfyUI_windows_portable\\`, as shown below for `D:\\ComfyUI_windows_portable\\`![Terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/custom_nodes/terminal.jpg)Then use `python_embeded\\python.exe` to complete the dependency installation:\n\n```\npython_embeded\\python.exe -m pip install -r ComfyUI\\custom_nodes\\ComfyUI-Manager\\requirements.txt\n```\n\nOf course, you can replace ComfyUI-Manager with the name of the custom node you actually installed, but make sure that a `requirements.txt` file exists in the corresponding node directory.\n\n### Custom Node Version Control\n\nCustom node version control is actually based on Git version control. You can manage node versions through Git, but ComfyUI Manager has already integrated this version management functionality very well. Many thanks to [@Dr.Lt.Data](https://github.com/ltdrdata) for bringing us such a convenient tool. In this section, we will still explain these two different plugin version management methods for you, but if you use ZIP packages for manual installation, the corresponding git version history information will be lost, making it impossible to perform version management.\n\n### Uninstalling Custom Nodes\n\nTo be updated\n\n### Temporarily Disabling Custom Nodes\n\nTo be updated\n\n### Custom Node Dependency Conflicts\n\nTo be updated\n\n## ComfyUI Manager\n\n![ComfyUI Manager Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png) This tool is currently included by default in the [Desktop version](https://docs.comfy.org/installation/desktop/windows), while in the [Portable version](https://docs.comfy.org/installation/comfyui_portable_windows), you need to refer to the installation instructions in the [Install Manager](#installing-custom-nodes) section of this document.\n\n### Installing the Manager\n\nIf you are running the ComfyUI server application, you need to install the manager. If ComfyUI is running, please close it before continuing. The first step is to install Git, which is a command-line application for software version control. Git will download the ComfyUI manager from [github.com](https://github.com/). Download and install Git from [git-scm.com](https://git-scm.com/). After installing Git, navigate to the ComfyUI server program directory and enter the folder labeled **custom\\_nodes**. Open a command window or terminal. Make sure the command line shows the current directory path as **custom\\_nodes**. Enter the following command. This will download the manager. Technically, this is called _cloning a Git repository_.\n\n### Detecting Missing Nodes\n\nAfter installing the manager, you can detect missing nodes in the manager. ![ComfyUI Manager Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png)\n\n## Developing a Custom Node\n\nIf you have some development capabilities, please start with the documentation below to learn how to begin developing a custom node.\n\n[\n\n## Start Developing Custom Nodes\n\nLearn how to start developing a custom node\n\n\n\n](https://docs.comfy.org/custom-nodes/overview)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/properties",
  "markdown": "# Properties - ComfyUI\n\n## Nodes are containers for properties\n\nNodes usually have _**properties**_. Also known as _**parameters**_ or _**attributes**_, node properties are variables that can be changed. Some properties can be adjusted manually by the user, using a data entry field called a _**widget**_. Other properties can be driven automatically by other nodes connected to the property _**input slot**_ or port. Usually, a property can be converted from widget to input and vice versa, allowing users to control property values manually or automatically. Properties can take many forms and hold many different types of information. For example, a **Load Checkpoint** node has a single property:Â  the file path to the generative model checkpoint file. A **KSampler** node has multiple properties such as the number of sampling **steps**, **CFG** scale, **sampler\\_name**, etc. ![node properties](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_properties.png)\n\n## Data types\n\nInformation can come in many different forms, called _**data types**_. For example, alphanumeric text is known as a _**string**_, a whole number is an _**integer**_, and a number with a decimal point is known as a _**floating point**_ number or _**float**_. New data types are always being added to ComfyUI. ComfyUI is written in the Python scripting language, which is very forgiving about data types. By contrast, the ComfyUI environment is very _**strongly typed**_. This means that different data types canâ€™t be mixed up. For example, we canâ€™t connect an image output to an integer input. This is a huge benefit to users, guiding them to proper workflow construction and preventing program errors.\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/links",
  "markdown": "# Links - ComfyUI\n\n## Links connect nodes\n\nIn the terminology of ComfyUI, the lines or curves between nodes are called _**links**_. Theyâ€™re also known as _**connections**_ or wires. Links can be displayed in several ways, such as curves, right angles, straight lines, or completely hidden. ![Link styles](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/link_styles.jpg) You can modify the link style in **Setup Menu** â€”> **Display (Lite Graph)** â€”> **Graph** â€”> **Link Render Mode**. ![Canvas Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/render_mode.jpg) You can also temporarily hide links in the **Canvas Menu**. ![Canvas Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/canvas_menu.jpg) Link display is crucial. Depending on the situation, it may be necessary to see all links. Especially when learning, sharing, or even just understanding workflows, the visibility of links enables users to follow the flow of data through the graph. For packaged workflows that arenâ€™t intended to be altered, it might make sense to hide the links to reduce clutter.\n\n### Reroute node\n\nIf legibility of the graph structure is important, then link wires can be manually routed in the 2D space of the graph with a tiny node called **Reroute**. Its purpose is to position the beginning and/or end points of link wires to ensure visibility. We can design a workflow so that link wires donâ€™t pass behind nodes, donâ€™t cross other link wires, and so on. ![ComfyUI Reroute node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/reroute.jpg) We are also continuously improving the native reroute functionality in litegraph. We recommend using this feature in the future to reorganize connections. ![ComfyUI Native Reroute](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/native_reroute.jpg)\n\n## Color-coding\n\nThe data type of node properties is indicated by color coding of input/output ports and link connection wires. We can always tell which inputs and outputs can be connected to one another by their color. Ports can only be connected to other ports of the same color to ensure matching data types. Common data types: ![ComfyUI Node Data Types](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/data_type.jpg)\n\n| Data type | Color |\n| --- | --- |\n| diffusion model | lavender |\n| CLIP model | yellow |\n| VAE model | rose |\n| conditioning | orange |\n| latent image | pink |\n| pixel image | blue |\n| mask | green |\n| number (integer or float) | light green |\n| mesh | bright green |\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/models",
  "markdown": "# Models - ComfyUI\n\n## Models are essential\n\nModels are essential building blocks for media generation workflows. They can be combined and mixed to achieve different creative effects. The word _**model**_ has many different meanings. Here, it means a data file carrying information that is required for a node graph to do its work. Specifically, itâ€™s a data structure that _models_ some function. As a verb, to model something means to represent it or provide an example. The primary example of a model data file in ComfyUI is an AI _**diffusion model**_. This is a large set of data that represents the complex relationships among text strings and images, making it possible to translate words into pictures or vice versa. Other examples of common models used for image generation are multimodal vision and language models such as CLIP, and upscaling models such as RealESRGAN.\n\n## Model files\n\nModel files are indispensable for generative media production. Without them, workflows cannot proceed effectively. Models are not included in the ComfyUI installation, but ComfyUI can often automatically download and install missing model files. Many models can be downloaded and installed from the **ComfyUI Manager** window. Models can also be found at websites such as [huggingface.co](https://huggingface.co/), [civitai.green](https://civitai.green/), and [github.com](https://github.com/).\n\n### Using Models in ComfyUI\n\n1.  Download and place them in the ComfyUI program directory\n    1.  Within the **models** folder, youâ€™ll find subfolders for various types of models, such as **checkpoints**\n    2.  The **ComfyUI Manager** helps to automate the process of searching, downloading, and installing\n    3.  Restart ComfyUI if itâ€™s running\n2.  In your workflow, create the node appropriate to the model type, e.g. **Load Checkpoint**, **Load LoRA**, **Load VAE**\n3.  In the loader node, choose the model you wish to use\n4.  Connect the loader node to other nodes in your workflow\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUIâ€™s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ lora/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n### File size\n\nModels can be extremely large files relative to image files. A typical uncompressed image may require a few megabytes of disk storage. Generative AI models can be tens of thousands of times larger, up to tens of gigabytes per model. They take up a great deal of disk space and take a long time to transfer over a network.\n\n## Model training and refinement\n\nA generative AI model is created by training a machine learning program on a very large set of data, such as pairs of images and text descriptions. An AI model doesnâ€™t store the training data explicitly, but rather it stores the correlations that are implicit within the data. Organizations and companies such as Stability AI and Black Forest Labs release â€œbaseâ€ models that carry large amounts of generic information. These are general purpose generative AI models. Commonly, the base models need to be _**refined**_ in order to get high quality generative outputs. A dedicated community of people work to refine the base models. The new, refined models produce better output, provide new or different functionality, and/or use fewer resources. Refined models can usually be run on systems with less computing power and/or memory.\n\n## Auxiliary models\n\nModel functionality can be extended with auxiliary models. For example, art directing a text-to-image workflow to achieve a specific result may be difficult or impossible using a diffusion model alone. Additional models can refine a diffusion model within the workflow graph to produce desired results. Examples include **LoRA** (Low Rank Adaptation), a small model that is trained on a specific subject; **ControlNet**, a model that helps control composition using a guide image; and **Inpainting**, a model that allows certain diffusion models to generate new content within an existing image. ![auxiliary models](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_auxiliary-model.png)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/core-concepts/dependencies",
  "markdown": "# Dependencies - ComfyUI\n\n## A workflow file depends on other files\n\nWe often obtain various workflow files from the community, but frequently find that the workflow cannot run directly after loading. This is because a workflow file depends on other files besides the workflow itself, such as media asset inputs, models, custom nodes, related Python dependencies, etc. ComfyUI workflows can only run normally when all relevant dependencies are satisfied. ComfyUI workflow dependencies mainly fall into the following categories:\n\n*   Assets (media files including audio, video, images, and other inputs)\n*   Custom nodes\n*   Python dependencies\n*   Models (such as Stable Diffusion models, etc.)\n\n## Assets\n\nAn AI model is an example of an _**asset**_. In media production, an asset is some media file that supplies input data. For example, a video editing program operates on movie files stored on disk. The editing programâ€™s project file holds links to these movie file assets, allowing non-destructive editing that doesnâ€™t alter the original movie files. ComfyUI works the same way. A workflow can only run if all of the required assets are found and loaded. Generative AI models, images, movies, and sounds are some examples of assets that a workflow might depend upon. These are therefore known as _**dependent assets**_ or _**asset dependencies**_.\n\n## Custom Nodes\n\nCustom nodes are an important component of ComfyUI that extend its functionality. They are created by the community and can be installed to add new capabilities to your workflows.\n\n## Python Dependencies\n\nComfyUI is a Python-based project. We build a standalone Python environment to run ComfyUI, and all related dependencies are installed in this isolated Python environment.\n\n### ComfyUI Dependencies\n\nYou can view ComfyUIâ€™s current dependencies in the [requirements.txt](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) file:\n\n```\ncomfyui-frontend-package==1.14.5\ntorch\ntorchsde\ntorchvision\ntorchaudio\nnumpy>=1.25.0\neinops\ntransformers>=4.28.1\ntokenizers>=0.13.3\nsentencepiece\nsafetensors>=0.4.2\naiohttp>=3.11.8\nyarl>=1.18.0\npyyaml\nPillow\nscipy\ntqdm\npsutil\n\n#non essential dependencies:\nkornia>=0.7.1\nspandrel\nsoundfile\nav\n```\n\nAs ComfyUI evolves, we may adjust dependencies accordingly, such as adding new dependencies or removing ones that are no longer needed. So if you use Git to update ComfyUI, you need to run the following command in the corresponding environment after pulling the latest updates:\n\n```\npip install -r requirements.txt\n```\n\nThis ensures that ComfyUIâ€™s dependencies are up to date for proper operation. You can also modify specific package dependency versions to upgrade or downgrade certain dependencies. Additionally, ComfyUIâ€™s frontend [ComfyUI\\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is currently maintained as a separate project. We update the `comfyui-frontend-package` dependency version after the corresponding version stabilizes. If you need to switch to a different frontend version, you can check the version information [here](https://pypi.org/project/comfyui-frontend-package/#history).\n\n### Custom Node Dependencies\n\nThanks to the efforts of many authors in the ComfyUI community, we can extend ComfyUIâ€™s functionality by using different custom nodes, enabling impressive creativity. Typically, each custom node has its own dependencies and a separate `requirements.txt` file. If you use [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) to install custom nodes, ComfyUI Manager will usually automatically install the corresponding dependencies. There are also cases where you need to install dependencies manually. Currently, all custom nodes are installed in the `ComfyUI/custom_nodes` directory. You need to navigate to the corresponding plugin directory in your ComfyUI Python environment and run `pip install -r requirements.txt` to install the dependencies. If youâ€™re using the [Windows Portable version](https://docs.comfy.org/installation/comfyui_portable_windows), you can use the following command in the `ComfyUI_windows_portable` directory:\n\n```\npython_embeded\\python.exe -m pip install -r ComfyUI\\custom_nodes\\<custom_node_name>\\requirements.txt\n```\n\nto install the dependencies for the corresponding node.\n\n### Dependency Conflicts\n\nDependency conflicts are a common issue when using ComfyUI. You might find that after installing or updating a custom node, previously installed custom nodes can no longer be found in ComfyUIâ€™s node library, or error pop-ups appear. One possible reason is dependency conflicts. There can be many reasons for dependency conflicts, such as:\n\n1.  Custom node version locking\n\nSome plugins may fix the exact version of a dependency library (e.g., `open_clip_torch==2.26.1`), while other plugins may require a higher version (e.g., `open_clip_torch>=2.29.0`), making it impossible to satisfy both version requirements simultaneously. **Solution**: You can try changing the fixed version dependency to a range constraint, such as `open_clip_torch>=2.26.1`, and then reinstall the dependencies to resolve these issues.\n\n2.  Environment pollution\n\nDuring the installation of custom node dependencies, it may overwrite versions of libraries already installed by other plugins. For example, multiple plugins may depend on `PyTorch` but require different CUDA versions, and the later installed plugin will break the existing environment. **Solutions**:\n\n*   You can try manually installing specific versions of dependencies in the Python virtual environment to resolve such issues.\n*   Or create different Python virtual environments for different plugins to resolve these issues.\n*   Try installing plugins one by one, restarting ComfyUI after each installation to observe if dependency conflicts occur.\n\n3.  Custom node dependency versions incompatible with ComfyUI dependency versions\n\nThese types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues. **Solution**: These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.\n\n## Models\n\nModels are a significant asset dependency for ComfyUI. Various custom nodes and workflows are built around specific models, such as the Stable Diffusion series, Flux series, Ltxv, and others. These models are an essential foundation for creation with ComfyUI, so we need to ensure that the models we use are properly available. Typically, our models are saved in the corresponding directory under `ComfyUI/models/`. Of course, you can also create an [extra\\_model\\_paths.yaml](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) by modifying the template to make additional model paths recognized by ComfyUI. This allows multiple ComfyUI instances to share the same model library, reducing disk usage.\n\n## Software\n\nAn advanced application like ComfyUI also has _**software dependencies**_. These are libraries of programming code and data that are required for the application to run. Custom nodes are examples of software dependencies. On an even more fundamental level, the Python programming environment is the ultimate dependency for ComfyUI. The correct version of Python is required to run a particular version of ComfyUI. Updates to Python, ComfyUI, and custom nodes can all be handled from the **ComfyUI Manager** window. ![ComfyUI Custom Nodes Manager](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_dependecies_custom-nodes-manager.png)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/interface/overview",
  "markdown": "# ComfyUI Interface Overview - ComfyUI\n\nThe visual interface is currently the way most users utilize ComfyUI to call the [ComfyUI Server](https://docs.comfy.org/development/comfyui-server/comms_overview) to generate corresponding media resources. It provides a visual interface for users to operate and organize workflows, debug workflows, and create amazing works. Typically, when you start the ComfyUI server, you will see an interface like this: ![ComfyUI Basic Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui_new_interface.jpg) If you are an earlier user, you may have seen the previous menu interface like this: ![ComfyUI Old Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui_old_interface.jpg) Currently, the [ComfyUI frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is a separate project, released and maintained as an independent pip package. If you want to contribute, you can fork this [repository](https://github.com/Comfy-Org/ComfyUI_frontend) and submit a pull request.\n\n## Localization Support\n\nCurrently, ComfyUI supports: English, Chinese, Russian, French, Japanese, and Korean. If you need to switch the interface language to your preferred language, you can click the **Settings gear icon** and then select your desired language under `Comfy` â€”> `Locale`. ![ComfyUI Localization Support](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/locale.jpg)\n\n### Workspace Areas\n\nBelow are the main interface areas of ComfyUI and a brief introduction to each part. ![ComfyUI Workspace](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui-new-interface-main.png) Currently, apart from the main workflow interface, the ComfyUI interface is mainly divided into the following parts:\n\n1.  Menu Bar: Provides workflow, editing, help menus, workflow execution, ComfyUI Manager entry, etc.\n2.  Sidebar Panel Switch Buttons: Used to switch between workflow history queue, node library, model library, local user workflow browsing, etc.\n3.  Theme Switch Button: Quickly switch between ComfyUIâ€™s default dark theme and light theme\n4.  Settings: Click to open the settings button\n5.  Canvas Menu: Provides zoom in, zoom out, and auto-fit operations for the ComfyUI canvas\n\n![ComfyUI Workspace](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui-new-interface-menu-bar.png) The image above shows the corresponding functions of the top menu bar, including common features, which we will explain in detail in the specific function usage section.\n\n![ComfyUI Sidebar Panel](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/side-panel.png) In the current ComfyUI, we provide four side panels with the following functions:\n\n1.  Workflow History Queue (Queue): All queue information for ComfyUI executing media content generation\n2.  Node Library: All nodes in ComfyUI, including `Comfy Core` and your installed custom nodes, can be found here\n3.  Model Library: Models in your local `ComfyUI/models` directory can be found here\n4.  Local User Workflows (Workflows): Your locally saved workflows can be found here\n\nCurrently, ComfyUI enables the new interface by default. If you prefer to use the old interface, you can click the **Settings gear icon** and then set `Use new menu` to `disabled` under `Comfy` â€”> `Menu` to switch to the old menu version.\n\nThe function annotations for the old menu interface are explained below: ![ComfyUI Old Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui-old-menu.png)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/interface/maskeditor",
  "markdown": "# Mask Editor - Create and Edit Masks in ComfyUI\n\nThe Mask Editor is a very useful feature in ComfyUI that allows users to create and edit masks within images without needing to use other applications. The Mask Editor is currently triggered through the `Load Image` node. After uploading an image, you can right-click on the node and select `Open in MaskEditor` from the menu to open the Mask Editor. ![ComfyUI Mask Editor](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/maskeditor/maskeditor.jpg) ![ComfyUI Mask Editor](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/maskeditor/maskeditor_ui.jpg) You can then click with your mouse on the image to create and edit masks.\n\n## Video Tutorial\n\nYour browser does not support the video tag.\n\n#### 0 reactions\n\nOn this page\n\n*   [Video Tutorial](#video-tutorial)"
},
{
  "url": "https://docs.comfy.org/troubleshooting/overview",
  "markdown": "# How to Troubleshoot and Solve ComfyUI Issues\n\n[\n\n## Custom Node Troubleshooting Guide\n\nCheck how to troubleshoot issues caused by custom nodes.\n\n\n\n](https://docs.comfy.org/troubleshooting/custom-node-issues)\n\n## Common Issues & Quick Fixes\n\nBefore diving into detailed troubleshooting, try these common solutions:\n\n### ComfyUI Wonâ€™t Start\n\n**Symptoms:** Application crashes on startup, black screen, or fails to load **Quick fixes:**\n\n1.  **Check system requirements** - Ensure your system meets the [minimum requirements](https://docs.comfy.org/installation/system_requirements)\n2.  **Update GPU drivers** - Download latest drivers from NVIDIA/AMD/Intel\n\n### Generation Fails or Produces Errors\n\n**Symptoms:** â€œPrompt execution failedâ€ dialog with â€œShow reportâ€ button, workflow stops executing **Quick fixes:**\n\n1.  **Click â€œShow reportâ€** - Read the detailed error message to identify the specific issue\n2.  **Check if itâ€™s a custom node issue** - [Follow our custom node troubleshooting guide](https://docs.comfy.org/troubleshooting/custom-node-issues)\n3.  **Verify model files** - See [Models documentation](https://docs.comfy.org/development/core-concepts/models) for model setup\n4.  **Check VRAM usage** - Close other applications using GPU memory\n\n### Slow Performance\n\n**Symptoms:** Very slow generation times, system freezing, out of memory errors **Quick fixes:**\n\n1.  **Lower resolution/batch size** - Reduce image size or number of images\n2.  **Use memory optimization flags** - See performance optimization section below\n3.  **Close unnecessary applications** - Free up RAM and VRAM\n4.  **Check CPU/GPU usage** - Use Task Manager to identify bottlenecks\n\n**Performance Optimization Commands:** For low VRAM systems:\n\n```\n# Low VRAM mode (uses cpu for text encoder)\npython main.py --lowvram\n\n# CPU mode (very slow but works with any hardware, only use as absolute last resort)\npython main.py --cpu\n```\n\nFor better performance:\n\n```\n# Disable previews (saves VRAM and processing)\npython main.py --preview-method none\n\n# Use optimized attention mechanisms\npython main.py --use-pytorch-cross-attention\npython main.py --use-flash-attention\n\n# Async weight offloading\npython main.py --async-offload\n```\n\nFor memory management:\n\n```\n# Reserve specific VRAM amount for OS (in GB)\npython main.py --reserve-vram 2\n\n# Disable smart memory management\npython main.py --disable-smart-memory\n\n# Use different caching strategies\npython main.py --cache-none      # Less RAM usage, but slower\npython main.py --cache-lru 10    # Cache 10 results, faster\npython main.py --cache-classic   # Use the old style (aggressive) caching. \n```\n\n## Installation-Specific Issues\n\n### Desktop App Issues\n\nFor comprehensive desktop installation troubleshooting, see the [Desktop Installation Guide](https://docs.comfy.org/installation/desktop/windows).\n\n*   **Unsupported device**: ComfyUI Desktop Windows only supports NVIDIA GPUs with CUDA. Use [ComfyUI Portable](https://docs.comfy.org/installation/comfyui_portable_windows) or [manual installation](https://docs.comfy.org/installation/manual_install) for other GPUs\n*   **Installation fails**: Run installer as administrator, ensure at least 15GB disk space\n*   **Maintenance page**: Check [mirror settings](https://docs.comfy.org/installation/desktop/windows#mirror-settings) if downloads fail\n*   **Missing models**: Models are not copied during migration, only linked. Verify model paths\n\n### Manual Installation Issues\n\n**Python version conflicts:**\n\n```\n# Check Python version (3.9+ required, 3.12 recommended)\npython --version\n\n# Use virtual environment (recommended)\npython -m venv comfyui_env\nsource comfyui_env/bin/activate  # Linux/Mac\ncomfyui_env\\Scripts\\activate     # Windows\n```\n\n**Package installation failures:**\n\n```\n# Update pip first\npython -m pip install --upgrade pip\n\n# Install dependencies\npip install -r requirements.txt\n\n# For NVIDIA GPUs (CUDA 12.8)\npip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128\n\n# For AMD GPUs (Linux only - ROCm 6.3)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3\n```\n\n### Linux-Specific Issues\n\n**LD\\_LIBRARY\\_PATH errors:** Common symptoms:\n\n*   â€œlibcuda.so.1: cannot open shared object fileâ€\n*   â€œlibnccl.so: cannot open shared object fileâ€\n*   â€œImportError: libnvinfer.so.X: cannot open shared object fileâ€\n\n**Solutions:**\n\n1.  **Modern PyTorch installations (most common):**\n\n```\n# For virtual environments with NVIDIA packages\nexport LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# For conda environments\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# Or find your Python site-packages automatically\nPYTHON_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# You may also need other NVIDIA libraries\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cuda_runtime/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cublas/lib:$LD_LIBRARY_PATH\n```\n\n2.  **Find what libraries you have:**\n\n```\n# Check installed NVIDIA packages\npython -c \"import site; import os; nvidia_path=os.path.join(site.getsitepackages()[0], 'nvidia'); print('NVIDIA libs:', [d for d in os.listdir(nvidia_path) if os.path.isdir(os.path.join(nvidia_path, d))] if os.path.exists(nvidia_path) else 'Not found')\"\n\n# Find missing libraries that PyTorch needs\npython -c \"import torch; print(torch.__file__)\"\nldd $(python -c \"import torch; print(torch.__file__.replace('__init__.py', 'lib/libtorch_cuda.so'))\")\n```\n\n3.  **Set permanently for your environment:**\n\n```\n# For virtual environments, add to activation script\necho 'export LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> $VIRTUAL_ENV/bin/activate\n\n# For conda environments\nconda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# For global bashrc (adjust Python version as needed)\necho 'export LD_LIBRARY_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> ~/.bashrc\n```\n\n4.  **Alternative: Use ldconfig:**\n\n```\n# Check current library cache\nldconfig -p | grep cuda\nldconfig -p | grep nccl\n\n# If missing, add library paths (requires root)\nsudo echo \"/usr/local/cuda/lib64\" > /etc/ld.so.conf.d/cuda.conf\nsudo ldconfig\n```\n\n5.  **Debug library loading:**\n\n```\n# Verbose library loading to see what's missing\nLD_DEBUG=libs python main.py 2>&1 | grep \"looking for\"\n\n# Check PyTorch CUDA availability\npython -c \"import torch; print('CUDA available:', torch.cuda.is_available()); print('CUDA version:', torch.version.cuda)\"\n```\n\nFor comprehensive model troubleshooting including architecture mismatches, missing models, and loading errors, see the dedicated [Model Issues](https://docs.comfy.org/troubleshooting/model-issues) page.\n\n## Network & API Issues\n\n### API Nodes Not Working\n\n**Symptoms:** API calls fail, timeout errors, quota exceeded **Solutions:**\n\n1.  **Check API key validity** - Verify keys in [user settings](https://docs.comfy.org/interface/user)\n2.  **Check account credits** - Ensure sufficient [API credits](https://docs.comfy.org/interface/credits)\n3.  **Verify internet connection** - Test with other online services\n4.  **Check service status** - Provider may be experiencing downtime\n\n### Connection Issues\n\n**Symptoms:** â€œFailed to connect to serverâ€, timeout errors **Solutions:**\n\n1.  **Check firewall settings** - Allow ComfyUI through firewall\n2.  **Try different port** - Default is 8188, try 8189 or 8190\n3.  **Disable VPN temporarily** - VPN may be blocking connections\n4.  **Check proxy settings** - Disable proxy if not required\n\n### Frontend Issues\n\n**â€œFrontend or Templates Package Not Updatedâ€:**\n\n```\n# After updating ComfyUI via Git, update frontend dependencies\npip install -r requirements.txt\n```\n\n**â€œCanâ€™t Find Custom Nodeâ€:**\n\n*   Disable node validation in ComfyUI settings\n\n**â€œError Toast About Workflow Failing Validationâ€:**\n\n*   Disable workflow validation in settings temporarily\n*   Report the issue to the ComfyUI team\n\n**Login Issues When Not on Localhost:**\n\n*   Normal login only works when accessing from localhost\n*   For LAN/remote access: Generate API key at [platform.comfy.org/login](https://platform.comfy.org/login)\n*   Use API key in login dialog or with `--api-key` command line argument\n\n## Hardware-Specific Issues\n\n### NVIDIA GPU Issues\n\n**â€œTorch not compiled with CUDA enabledâ€ error:**\n\n```\n# First uninstall torch\npip uninstall torch\n\n# Install stable PyTorch with CUDA 12.8\npip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128\n\n# For nightly builds (might have performance improvements)\npip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n\n# Verify CUDA support\npython -c \"import torch; print(torch.cuda.is_available())\"\n```\n\n**GPU not detected:**\n\n```\n# Check if GPU is visible\nnvidia-smi\n\n# Check driver version and CUDA compatibility\nnvidia-smi --query-gpu=driver_version --format=csv\n```\n\n### AMD GPU Issues\n\n**ROCm support (Linux only):**\n\n```\n# Install stable ROCm PyTorch (6.3.1 at the time of writing)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3\n\n# For nightly builds (ROCm 6.4 at the time of writing), which might have performance improvements)\npip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4\n```\n\n**Unsupported AMD GPUs:**\n\n```\n# For RDNA2 or older (6700, 6600)\nHSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py\n\n# For RDNA3 cards (7600)\nHSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py\n```\n\n**Performance optimization:**\n\n```\n# Enable experimental memory efficient attention (no longer necessary with PyTorch 2.4)\nTORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention\n\n# Enable tunable operations (slow first run, but faster subsequent runs)\nPYTORCH_TUNABLEOP_ENABLED=1 python main.py\n```\n\n### Apple Silicon (M1/M2/M3) Issues\n\n**MPS backend setup:**\n\n```\n# Install PyTorch nightly for Apple Silicon\n# Follow Apple's guide: https://developer.apple.com/metal/pytorch/\n\n# Check MPS availability\npython -c \"import torch; print(torch.backends.mps.is_available())\"\n\n# Launch ComfyUI\npython main.py\n```\n\n**If MPS causes issues:**\n\n```\n# Force CPU mode\npython main.py --cpu\n\n# With memory optimization\npython main.py --force-fp16 --cpu\n```\n\n### Intel GPU Issues\n\n**Option 1: Native PyTorch XPU support (Windows/Linux):**\n\n```\n# Install PyTorch nightly with XPU support\npip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu\n\n# Launch ComfyUI\npython main.py\n```\n\n**Option 2: Intel Extension for PyTorch (IPEX):**\n\n```\n# For Intel Arc A-Series Graphics\nconda install libuv\npip install torch==2.3.1.post0+cxx11.abi torchvision==0.18.1.post0+cxx11.abi torchaudio==2.3.1.post0+cxx11.abi intel-extension-for-pytorch==2.3.110.post0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n```\n\n## Getting Help & Reporting Bugs\n\n### Before Reporting a Bug\n\n1.  **Check if itâ€™s a known issue:**\n    *   Search [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n    *   Check [ComfyUI Forum](https://forum.comfy.org/)\n    *   Review [Discord discussions](https://discord.com/invite/comfyorg)\n2.  **Try basic troubleshooting:**\n    *   Test with [default workflow](https://docs.comfy.org/get_started/first_generation)\n    *   Disable all custom nodes (see [custom node troubleshooting](https://docs.comfy.org/troubleshooting/custom-node-issues))\n    *   Check console/terminal for error messages\n    *   If using comfy-cli, try updating: `comfy node update all`\n\n### How to Report Bugs Effectively\n\n#### For ComfyUI Core Issues\n\n**Where to report:** [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n\n#### For Desktop App Issues\n\n**Where to report:** [Desktop GitHub Issues](https://github.com/Comfy-Org/desktop/issues)\n\n#### For Frontend Issues\n\n**Where to report:** [Frontend GitHub Issues](https://github.com/Comfy-Org/ComfyUI_frontend/issues)\n\n#### For Custom Node Issues\n\n**Where to report:** Contact the specific custom node developer\n\n### Required Information\n\nWhen reporting any issue, include:\n\n*   **Official Forum:** [forum.comfy.org](https://forum.comfy.org/)\n*   **Discord:** [ComfyUI Discord Server](https://discord.com/invite/comfyorg)\n*   **Reddit:** [r/comfyui](https://reddit.com/r/comfyui)\n*   **YouTube:** [ComfyUI Tutorials](https://www.youtube.com/@comfyorg)\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/troubleshooting/model-issues",
  "markdown": "# How to Troubleshoot and Solve ComfyUI Model Issues\n\n## Model Architecture Mismatch\n\n**Symptoms:** Tensor dimension errors during generation, especially during VAE decode stage **Common error messages:**\n\n*   `Given groups=1, weight of size [64, 4, 3, 3], expected input[1, 16, 128, 128] to have 4 channels, but got 16 channels instead`\n*   `Given groups=1, weight of size [4, 4, 1, 1], expected input[1, 16, 144, 112] to have 4 channels, but got 16 channels instead`\n*   `Given groups=1, weight of size [320, 4, 3, 3], expected input[2, 16, 192, 128] to have 4 channels, but got 16 channels instead`\n*   `The size of tensor a (49) must match the size of tensor b (16) at non-singleton dimension 1`\n*   `Tensors must have same number of dimensions: got 2 and 3`\n*   `mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)`\n\n**Root cause:** Using models from different architecture families together\n\n### Solutions\n\n1.  **Verify model family compatibility:**\n    *   **Flux models** use 16-channel latent space with dual text encoder conditioning (CLIP-L + T5-XXL)\n    *   **SD1.5 models** use 4-channel latent space with single CLIP ViT-L/14 text encoder\n    *   **SDXL models** use 4-channel latent space with dual text encoders (CLIP ViT-L/14 + OpenCLIP ViT-bigG/14)\n    *   **SD3 models** use 16-channel latent space with triple text encoder conditioning (CLIP-L + OpenCLIP bigG + T5-XXL)\n    *   **ControlNet models** must match the architecture of the base checkpoint (SD1.5 ControlNets only work with SD1.5 checkpoints, SDXL ControlNets only work with SDXL checkpoints, etc.)\n2.  **Common mismatch scenarios and fixes:** **Flux + wrong VAE:**\n    \n    ```\n    Problem: Using taesd or sdxl_vae.safetensors with Flux checkpoint\n    Fix: Use ae.safetensors (Flux VAE) from Hugging Face Flux releases\n    ```\n    \n    **Flux + incorrect CLIP configuration:**\n    \n    ```\n    Problem: Using t5xxl_fp8_e4m3fn.safetensors in both CLIP slots of DualClipLoader\n    Fix: Use t5xxl_fp8_e4m3fn.safetensors in one slot and clip_l.safetensors in the other\n    ```\n    \n    **ControlNet architecture mismatch:**\n    \n    ```\n    Problem: SD1.5 ControlNet with SDXL checkpoint (or vice versa)\n    Error: \"mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)\"\n    Fix: Use ControlNet models designed for your checkpoint architecture\n         - SD1.5 checkpoints require SD1.5 ControlNets\n         - SDXL checkpoints require SDXL ControlNets\n    ```\n    \n3.  **Quick diagnostics:**\n    \n    ```\n    # Check if error occurs at VAE decode stage\n    # Look for \"expected input[X, Y, Z] to have N channels, but got M channels\"\n    # Y value indicates channel count: 4 = SD models, 16 = Flux models\n    ```\n    \n4.  **Prevention strategies:**\n    *   Keep all workflow models within the same architecture family\n    *   Download complete model packages from same source/release (often all in a Hugging Face repo)\n    *   When trying new models, start with the template workflows or official ComfyUI workflow examples before customizing\n\n## Missing Models Error\n\n**Example error message:**\n\n```\nPrompt execution failed\nPrompt outputs failed validation:\nCheckpointLoaderSimple:\n- Value not in list: ckpt_name: 'model-name.safetensors' not in []\n```\n\n### Solutions\n\n1.  **Download required models:**\n    *   Use ComfyUI Manager to auto-download models\n    *   Verify models are in correct subfolders\n2.  **Check model paths:**\n    *   **Checkpoints**: `models/checkpoints/`\n    *   **VAE**: `models/vae/`\n    *   **LoRA**: `models/loras/`\n    *   **ControlNet**: `models/controlnet/`\n    *   **Embeddings**: `models/embeddings/`\n3.  **Share models between UIs or use custom paths:**\n    *   See [ComfyUI Model Sharing and Custom Model Directory Configuration](https://docs.comfy.org/installation/comfyui_portable_windows#2-comfyui-model-sharing-and-custom-model-directory-configuration) for detailed instructions\n    *   Edit `extra_model_paths.yaml` file to add custom model directories\n\n### Model Search Path Configuration\n\nIf you have models in custom locations, see the detailed guide for [ComfyUI Model Sharing and Custom Model Directory Configuration](https://docs.comfy.org/installation/comfyui_portable_windows#2-comfyui-model-sharing-and-custom-model-directory-configuration) to configure ComfyUI to find them.\n\n## Model Loading Errors\n\n**Error message:** â€œError while deserializing headerâ€\n\n### Solutions\n\n1.  **Re-download the model** - File may be corrupted during download\n2.  **Check available disk space** - Ensure enough space for model loading (models can be 2-15GB+)\n3.  **Check file permissions** - Ensure ComfyUI can read the model files\n4.  **Test with different model** - Verify if issue is model-specific or system-wide\n\n## Model Performance Issues\n\n### Slow Model Loading\n\n**Symptoms:** Long delays when switching models or starting generation **Solutions:**\n\n1.  **Use faster storage:**\n    *   Move models to SSD if using HDD\n    *   Use NVMe SSD for best performance\n2.  **Adjust caching settings:**\n    \n    ```\n    python main.py --cache-classic       # Use the old style (aggressive) caching. \n    python main.py --cache-lru 10         # Increase size of LRU cache\n    ```\n    \n\n### Memory Issues with Large Models\n\n**â€œRuntimeError: CUDA out of memoryâ€:**\n\n```\n# Progressive memory reduction\npython main.py --lowvram          # First try\npython main.py --novram           # If lowvram insufficient  \npython main.py --cpu              # Last resort\n```\n\n**Model-specific memory optimization:**\n\n```\n# Force lower precision\npython main.py --force-fp16\n\n# Reduce attention memory usage\npython main.py --use-pytorch-cross-attention\n```"
},
{
  "url": "https://docs.comfy.org/troubleshooting/custom-node-issues",
  "markdown": "# How to Troubleshoot and Solve ComfyUI Issues\n\nHere is the overall approach for troubleshooting custom node issues:\n\n## How to disable all custom nodes?\n\nStart ComfyUI Desktop with custom nodes disabled from the settings menu ![Settings menu - Disable custom nodes](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/troubleshooting/desktop-diable-custom-node.jpg) or run the server manually:\n\n```\ncd path/to/your/comfyui\npython main.py --disable-all-custom-nodes\n```\n\n**Results:**\n\n*   âœ… **Issue disappears**: A custom node is causing the problem â†’ Continue to Step 2\n*   âŒ **Issue persists**: Not a custom node issue â†’ [Report the issue](#reporting-issues)\n\n## What is Binary Search?\n\nIn this document, we will introduce the binary search approach for troubleshooting custom node issues, which involves checking half of the custom nodes at a time until we locate the problematic node. Please refer to the flowchart below for the specific approach - enable half of the currently disabled nodes each time and check if the issue appears, until we identify which custom node is causing the issue\n\n## Two Troubleshooting Methods\n\nIn this document, we categorize custom nodes into two types for troubleshooting: ![Custom node types](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/troubleshooting/custom_node_type.jpg)\n\n*   A: Custom nodes with frontend extensions\n*   B: Regular custom nodes\n\nLetâ€™s first understand the potential issues and causes for different types of custom nodes:\n\nFor custom nodes, we can prioritize troubleshooting those with frontend extensions, as they cause the most issues. Their main conflicts arise from incompatibilities with ComfyUI frontend version updates.Common issues include:\n\n*   Workflows not executing\n*   Some nodes canâ€™t show preview images(such as save image node)\n*   Misaligned UI elements\n*   Unable to access ComfyUI frontend\n*   Completely broken UI or blank screen\n*   Unable to communicate normally with ComfyUI backend\n*   Node connections not working properly\n*   And more\n\nCommon causes for these issues:\n\n*   Frontend modifications during updates that custom nodes havenâ€™t adapted to yet\n*   Users updating ComfyUI without synchronously upgrading custom nodes, even though authors have released compatible versions\n*   Authors stopping maintenance, leading to incompatibility between custom node extensions and the ComfyUI frontend\n\n## Using Binary Search for Troubleshooting\n\nAmong these two different types of custom node issues, conflicts between custom node frontend extensions and ComfyUI are more common. Weâ€™ll prioritize troubleshooting these nodes first. Hereâ€™s the overall troubleshooting approach:\n\n### 1\\. Troubleshooting the Custom Nodesâ€™ Frontend Extensions\n\nUsing this method, you donâ€™t need to restart ComfyUI multiple times - just reload ComfyUI after enabling/disabling custom node frontend extensions. Plus, your troubleshooting scope is limited to nodes with frontend extensions, which greatly narrows down the search range.\n\n### 2\\. General Custom Node Troubleshooting\n\n## How to Fix the Issue\n\nOnce youâ€™ve identified the problematic custom node:\n\n### Option 1: Update the Node\n\n1.  Check if thereâ€™s an update available in ComfyUI Manager\n2.  Update the node and test again\n\n### Option 2: Replace the Node\n\n1.  Look for alternative custom nodes with similar functionality\n2.  Check the [ComfyUI Registry](https://registry.comfy.org/) for alternatives\n\n### Option 3: Report the Issue\n\nContact the custom node developer:\n\n1.  Find the nodeâ€™s GitHub repository\n2.  Create an issue with:\n    *   Your ComfyUI version\n    *   Error messages/logs\n    *   Steps to reproduce\n    *   Your operating system\n\n### Option 4: Remove or Disable the Node\n\nIf no fix is available and you donâ€™t need the functionality:\n\n1.  Remove the problematic node from `custom_nodes/` or disable it in the ComfyUI Manager interface\n2.  Restart ComfyUI\n\n## Reporting Issues\n\nIf the issue isnâ€™t caused by custom nodes, refer to the general [troubleshooting overview](https://docs.comfy.org/troubleshooting/overview) for other common problems.\n\n### For Custom Node-Specific Issues\n\nContact the custom node developer:\n\n*   Find the nodeâ€™s GitHub repository\n*   Create an issue with your ComfyUI version, error messages, reproduction steps, and OS\n*   Check the nodeâ€™s documentation and Issues page for known issues\n\n### For ComfyUI Core Issues\n\n*   **GitHub**: [ComfyUI Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n*   **Forum**: [Official ComfyUI Forum](https://forum.comfy.org/)\n\n### For Desktop App Issues\n\n*   **GitHub**: [ComfyUI Desktop Issues](https://github.com/Comfy-Org/desktop/issues)\n\n### For Frontend Issues\n\n*   **GitHub**: [ComfyUI Frontend Issues](https://github.com/Comfy-Org/ComfyUI_frontend/issues)"
},
{
  "url": "https://docs.comfy.org/community/contributing",
  "markdown": "# Contributing - ComfyUI\n\n##### Get Started\n\n*   [](https://docs.comfy.org/)\n\n*   [](https://docs.comfy.org/get_started/first_generation)\n*   [](https://docs.comfy.org/changelog)\n\n##### Basic Concepts\n\n*   [](https://docs.comfy.org/development/core-concepts/workflow)\n*   [](https://docs.comfy.org/development/core-concepts/nodes)\n*   [](https://docs.comfy.org/development/core-concepts/custom-nodes)\n*   [](https://docs.comfy.org/development/core-concepts/properties)\n*   [](https://docs.comfy.org/development/core-concepts/links)\n*   [](https://docs.comfy.org/development/core-concepts/models)\n*   [](https://docs.comfy.org/development/core-concepts/dependencies)\n\n##### Interface Guide\n\n*   [](https://docs.comfy.org/interface/overview)\n*   [](https://docs.comfy.org/interface/maskeditor)\n\n##### Tutorials\n\n##### Troubleshooting\n\n*   [](https://docs.comfy.org/troubleshooting/overview)\n*   [](https://docs.comfy.org/troubleshooting/model-issues)\n*   [](https://docs.comfy.org/troubleshooting/custom-node-issues)\n\n##### Community\n\n*   [](https://docs.comfy.org/community/contributing)\n\n### Create a PR\n\nFork the [repo](https://github.com/comfyanonymous/ComfyUI), and create a PR."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/overview",
  "markdown": "# ComfyUI Built-in Nodes - ComfyUI\n\nBuilt-in nodes are ComfyUIâ€™s default nodes. They are core functionalities of ComfyUI that you can use without installing any third-party custom node packages.\n\n## About built-in node document\n\nWe have now added built-in node help documentation, so the content of this section is periodically synced from [this repo](https://github.com/Comfy-Org/embedded-docs). We will update the content manually once a week.\n\n## Contribute\n\nIf you find any errors in the content, or want to contribute missing content, please submit an issue or PR in [this repo](https://github.com/Comfy-Org/embedded-docs) to help us improve."
},
{
  "url": "https://docs.comfy.org/development/overview",
  "markdown": "# Overview - ComfyUI\n\nComfyUI is a powerful GenAI inference engine that can be used to run AI models locally, create workflows, develop custom nodes, and be deployed as a server. ComfyUIâ€™s key capabilities are:\n\n*   **[Creating Workflows](https://docs.comfy.org/development/core-concepts/workflow)**: Workflows are a way to orchestrate AI models and automate tasks. They are a series of nodes that are connected together to form a pipeline.\n*   **[Custom Nodes](https://docs.comfy.org/custom-nodes/overview)**: Custom nodes can be written by anyone to extend the capabilities of ComfyUI for your own use. Nodes are written in Python and are published by the community.\n*   **Extensions**: Extensions are 3rd party applications that improve the UI of ComfyUI.\n*   **[Deployment](https://docs.comfy.org/development/comfyui-server/comms_overview)**: ComfyUI can be deployed in your own environment as an API endpoint. \\[Learn more\\]"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/text-to-image",
  "markdown": "# ComfyUI Text to Image Workflow\n\nThis guide aims to introduce you to ComfyUIâ€™s text-to-image workflow and help you understand the functionality and usage of various ComfyUI nodes. In this document, we will:\n\n*   Complete a text-to-image workflow\n*   Gain a basic understanding of diffusion model principles\n*   Learn about the functions and roles of workflow nodes\n*   Get an initial understanding of the SD1.5 model\n\nWeâ€™ll start by running a text-to-image workflow, followed by explanations of related concepts. Please choose the relevant sections based on your needs.\n\n## About Text to Image\n\n**Text to Image** is a fundamental process in AI art generation that creates images from text descriptions, with **diffusion models** at its core. The text-to-image process requires the following elements:\n\n*   **Artist:** The image generation model\n*   **Canvas:** The latent space\n*   **Image Requirements (Prompts):** Including positive prompts (elements you want in the image) and negative prompts (elements you donâ€™t want)\n\nThis text-to-image generation process can be simply understood as telling your requirements (positive and negative prompts) to an **artist (the image model)**, who then creates what you want based on these requirements.\n\n## ComfyUI Text to Image Workflow Example Guide\n\n### 1\\. Preparation\n\nEnsure you have at least one SD1.5 model file in your `ComfyUI/models/checkpoints` folder, such as [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) If you havenâ€™t installed it yet, please refer to the model installation section in [Getting Started with ComfyUI AI Art Generation](https://docs.comfy.org/get_started/first_generation).\n\n### 2\\. Loading the Text to Image Workflow\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![ComfyUI-Text to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/text-to-image-workflow.png)\n\n### 3\\. Loading the Model and Generating Your First Image\n\nAfter installing the image model, follow the steps in the image below to load the model and generate your first image ![Image Generation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg) Follow these steps according to the image numbers:\n\n1.  In the **Load Checkpoint** node, use the arrows or click the text area to ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected, and the left/right arrows donâ€™t show **null** text\n2.  Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute image generation\n\nAfter the process completes, you should see the resulting image in the **Save Image** node interface, which you can right-click to save locally ![ComfyUI First Image Generation Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)\n\n### 4\\. Start Experimenting\n\nTry modifying the text in the **CLIP Text Encoder** ![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg) The `Positive` connection to the KSampler node represents positive prompts, while the `Negative` connection represents negative prompts Here are some basic prompting principles for the SD1.5 model:\n\n*   Use English whenever possible\n*   Separate prompts with English commas `,`\n*   Use phrases rather than long sentences\n*   Use specific descriptions\n*   Use expressions like `(golden hour:1.2)` to increase the weight of specific keywords, making them more likely to appear in the image. `1.2` is the weight, `golden hour` is the keyword\n*   Use keywords like `masterpiece, best quality, 4k` to improve generation quality\n\nHere are several prompt examples you can try, or use your own prompts for generation: **1\\. Anime Style** Positive prompts:\n\n```\nanime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details\n\nmasterpiece, best quality, 4k\n```\n\nNegative prompts:\n\n```\nlow quality, blurry, deformed hands, extra fingers\n```\n\n**2\\. Realistic Style** Positive prompts:\n\n```\n(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), \nfull body, soft cinematic lighting, (golden hour:1.2), \n(fujifilm XT4:1.1), shallow depth of field, \n(skin texture details:1.3), (film grain:1.1), \ngentle wind flow, warm color grading, (perfect facial symmetry:1.3)\n```\n\nNegative prompts:\n\n```\n(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)\n```\n\n**3\\. Specific Artist Style** Positive prompts:\n\n```\nfantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style\n```\n\nNegative prompts:\n\n```\nblurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting\n```\n\n## Text to Image Working Principles\n\nThe entire text-to-image process can be understood as a **reverse diffusion process**. The [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) we downloaded is a pre-trained model that can **generate target images from pure Gaussian noise**. We only need to input our prompts, and it can generate target images through denoising random noise.\n\nWe need to understand two concepts:\n\n1.  **Latent Space:** Latent Space is an abstract data representation method in diffusion models. Converting images from pixel space to latent space reduces storage space and makes it easier to train diffusion models and reduce denoising complexity. Itâ€™s like architects using blueprints (latent space) for design rather than designing directly on the building (pixel space), maintaining structural features while significantly reducing modification costs\n2.  **Pixel Space:** Pixel Space is the storage space for images, which is the final image we see, used to store pixel values.\n\nIf you want to learn more about diffusion models, you can read these papers:\n\n*   [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)\n*   [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)\n*   [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)\n\n## ComfyUI Text to Image Workflow Node Explanation\n\n![ComfyUI Text to Image Workflow Explanation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/text-image-workflow.jpg)\n\n### A. Load Checkpoint Node\n\n![Load Checkpoint](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_checkpoint.jpg) This node is typically used to load the image generation model. A `checkpoint` usually contains three components: `MODEL (UNet)`, `CLIP`, and `VAE`\n\n*   `MODEL (UNet)`: The UNet model responsible for noise prediction and image generation during the diffusion process\n*   `CLIP`: The text encoder that converts our text prompts into vectors that the model can understand, as the model cannot directly understand text prompts\n*   `VAE`: The Variational AutoEncoder that converts images between pixel space and latent space, as diffusion models work in latent space while our images are in pixel space\n\n### B. Empty Latent Image Node\n\n![Empty Latent Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/empty_latent_image.jpg) Defines a latent space that outputs to the KSampler node. The Empty Latent Image node constructs a **pure noise latent space** You can think of its function as defining the canvas size, which determines the dimensions of our final generated image\n\n### C. CLIP Text Encoder Node\n\n![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg) Used to encode prompts, which are your requirements for the image\n\n*   The `Positive` condition input connected to the KSampler node represents positive prompts (elements you want in the image)\n*   The `Negative` condition input connected to the KSampler node represents negative prompts (elements you donâ€™t want in the image)\n\nThe prompts are encoded into semantic vectors by the `CLIP` component from the `Load Checkpoint` node and output as conditions to the KSampler node\n\n### D. KSampler Node\n\n![KSampler](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg) The **KSampler** is the core of the entire workflow, where the entire noise denoising process occurs, ultimately outputting a latent space image\n\nHereâ€™s an explanation of the KSampler node parameters:\n\n| Parameter Name | Description | Function |\n| --- | --- | --- |\n| **model** | Diffusion model used for denoising | Determines the style and quality of generated images |\n| **positive** | Positive prompt condition encoding | Guides generation to include specified elements |\n| **negative** | Negative prompt condition encoding | Suppresses unwanted content |\n| **latent\\_image** | Latent space image to be denoised | Serves as the input carrier for noise initialization |\n| **seed** | Random seed for noise generation | Controls generation randomness |\n| **control\\_after\\_generate** | Seed control mode after generation | Determines seed variation pattern in batch generation |\n| **steps** | Number of denoising iterations | More steps mean finer details but longer processing time |\n| **cfg** | Classifier-free guidance scale | Controls prompt constraint strength (too high leads to overfitting) |\n| **sampler\\_name** | Sampling algorithm name | Determines the mathematical method for denoising path |\n| **scheduler** | Scheduler type | Controls noise decay rate and step size allocation |\n| **denoise** | Denoising strength coefficient | Controls noise strength added to latent space, 0.0 preserves original input features, 1.0 is complete noise |\n\nIn the KSampler node, the latent space uses `seed` as an initialization parameter to construct random noise, and semantic vectors `Positive` and `Negative` are input as conditions to the diffusion model Then, based on the number of denoising steps specified by the `steps` parameter, denoising is performed. Each denoising step uses the denoising strength coefficient specified by the `denoise` parameter to denoise the latent space and generate a new latent space image\n\n### E. VAE Decode Node\n\n![VAE Decode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/vae_decode.jpg) Converts the latent space image output from the **KSampler** into a pixel space image\n\n### F. Save Image Node\n\n![Save Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/save_image.jpg) Previews and saves the decoded image from latent space to the local `ComfyUI/output` folder\n\n## Introduction to SD1.5 Model\n\n**SD1.5 (Stable Diffusion 1.5)** is an AI image generation model developed by [Stability AI](https://stability.ai/). Itâ€™s the foundational version of the Stable Diffusion series, trained on **512Ã—512** resolution images, making it particularly good at generating images at this resolution. With a size of about 4GB, it runs smoothly on **consumer-grade GPUs (e.g., 6GB VRAM)**. Currently, SD1.5 has a rich ecosystem, supporting various plugins (like ControlNet, LoRA) and optimization tools. As a milestone model in AI art generation, SD1.5 remains the best entry-level choice thanks to its open-source nature, lightweight architecture, and rich ecosystem. Although newer versions like SDXL/SD3 have been released, its value for consumer-grade hardware remains unmatched.\n\n### Basic Information\n\n*   **Release Date**: October 2022\n*   **Core Architecture**: Based on Latent Diffusion Model (LDM)\n*   **Training Data**: LAION-Aesthetics v2.5 dataset (approximately 590M training steps)\n*   **Open Source Features**: Fully open-source model/code/training data\n\n### Advantages and Limitations\n\nModel Advantages:\n\n*   Lightweight: Small size, only about 4GB, runs smoothly on consumer GPUs\n*   Low Entry Barrier: Supports a wide range of plugins and optimization tools\n*   Mature Ecosystem: Extensive plugin and tool support\n*   Fast Generation: Smooth operation on consumer GPUs\n\nModel Limitations:\n\n*   Detail Handling: Hands/complex lighting prone to distortion\n*   Resolution Limits: Quality degrades for direct 1024x1024 generation\n*   Prompt Dependency: Requires precise English descriptions for control"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher",
  "markdown": "# Retrieve permissions the user has for a given publisher\n\n#### Path Parameters\n\n#### Response\n\nThe response is of type `object`.\n\nRetrieve permissions the user has for a given publisher"
},
{
  "url": "https://docs.comfy.org/installation/system_requirements",
  "markdown": "# System Requirements - ComfyUI\n\nIn this guide, we will introduce the system requirements for installing ComfyUI. Due to frequent updates of ComfyUI, this document may not be updated in a timely manner. Please refer to the relevant instructions in [ComfyUI](https://github.com/comfyanonymous/ComfyUI). Regardless of which version of ComfyUI you use, it runs in a separate Python environment.\n\n### System Requirements\n\nCurrently, we support the following operating systems:\n\n*   Windows\n*   Linux\n*   macOS (supports Apple Silicon M1/M2)\n\nYou can refer to the following sections to learn about the installation methods for different systems and versions of ComfyUI. In the installation of different versions, we have simply described the system requirements.\n\n### Python Version\n\n*   Recommended Python 3.12\n*   Supports Python 3.13 (some custom nodes may not be compatible)\n\n### Supported Hardware\n\n*   NVIDIA GPU\n*   AMD GPU\n*   Intel GPU (includes Arc series, supports IPEX)\n*   Apple Silicon (M1/M2)\n*   Ascend NPU\n*   Cambricon MLU\n*   CPU (can use the â€”cpu parameter, slower)\n\nPlease refer to the [ComfyUI Windows and Linux manual installation section](https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#manual-install-windows-linux) for detailed installation steps.\n\n### Dependencies\n\n*   Install PyTorch\n*   Install all dependencies in the requirements.txt of ComfyUI\n\n[\n\n## Manual Installation\n\nPlease refer to the manual installation section for detailed installation steps.\n\n\n\n](https://docs.comfy.org/installation/manual_install)"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/publish-a-new-version-of-a-node",
  "markdown": "# Publish a new version of a node\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/publishers/{publisherId}/nodes/{nodeId}/versions \\\n  --header 'Authorization: Bearer <token>' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n  \"node\": {\n    \"author\": \"<string>\",\n    \"banner_url\": \"<string>\",\n    \"category\": \"<string>\",\n    \"created_at\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"downloads\": 123,\n    \"github_stars\": 123,\n    \"icon\": \"<string>\",\n    \"id\": \"<string>\",\n    \"latest_version\": {\n      \"changelog\": \"<string>\",\n      \"comfy_node_extract_status\": \"<string>\",\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"dependencies\": [\n        \"<string>\"\n      ],\n      \"deprecated\": true,\n      \"downloadUrl\": \"<string>\",\n      \"id\": \"<string>\",\n      \"node_id\": \"<string>\",\n      \"status\": \"NodeVersionStatusActive\",\n      \"status_reason\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"version\": \"<string>\"\n    },\n    \"license\": \"<string>\",\n    \"name\": \"<string>\",\n    \"preempted_comfy_node_names\": [\n      \"<string>\"\n    ],\n    \"publisher\": {\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"id\": \"<string>\",\n      \"logo\": \"<string>\",\n      \"members\": [\n        {\n          \"id\": \"<string>\",\n          \"role\": \"<string>\",\n          \"user\": {\n            \"email\": \"<string>\",\n            \"id\": \"<string>\",\n            \"name\": \"<string>\"\n          }\n        }\n      ],\n      \"name\": \"<string>\",\n      \"source_code_repo\": \"<string>\",\n      \"status\": \"PublisherStatusActive\",\n      \"support\": \"<string>\",\n      \"website\": \"<string>\"\n    },\n    \"rating\": 123,\n    \"repository\": \"<string>\",\n    \"search_ranking\": 123,\n    \"status\": \"NodeStatusActive\",\n    \"status_detail\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"tags\": [\n      \"<string>\"\n    ],\n    \"translations\": {}\n  },\n  \"node_version\": {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  },\n  \"personal_access_token\": \"<string>\"\n}'\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/update-changelog-and-deprecation-status-of-a-node-version",
  "markdown": "# Update changelog and deprecation status of a node version\n\n```\n{\n  \"changelog\": \"<string>\",\n  \"comfy_node_extract_status\": \"<string>\",\n  \"createdAt\": \"2023-11-07T05:31:56Z\",\n  \"dependencies\": [\n    \"<string>\"\n  ],\n  \"deprecated\": true,\n  \"downloadUrl\": \"<string>\",\n  \"id\": \"<string>\",\n  \"node_id\": \"<string>\",\n  \"status\": \"NodeVersionStatusActive\",\n  \"status_reason\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"version\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/unpublish-delete-a-specific-version-of-a-node",
  "markdown": "# Unpublish (delete) a specific version of a node\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Response\n\nVersion unpublished (deleted) successfully\n\n#### 0 reactions\n\nUnpublish (delete) a specific version of a node"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher-1",
  "markdown": "# Retrieve permissions the user has for a given publisher\n\n#### Path Parameters\n\n#### Response\n\n#### 0 reactions\n\n[Sign in](https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fretrieve-permissions-the-user-has-for-a-given-publisher-1) to add your reaction.\n\nRetrieve permissions the user has for a given publisher"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/delete-a-specific-personal-access-token",
  "markdown": "# Delete a specific personal access token\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Response\n\nToken deleted successfully\n\nDelete a specific personal access token"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/get-information-about-the-calling-user",
  "markdown": "# Get information about the calling user.\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Response\n\nThe response is of type `object`.\n\nGet information about the calling user."
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-all-publishers-for-a-given-user",
  "markdown": "# Retrieve all publishers for a given user\n\n```\n[\n  {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  }\n]\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-a-new-personal-access-token",
  "markdown": "# Create a new personal access token\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n\\[Output Only\\]The date and time the token was created.\n\nOptional. A more detailed description of the token's intended use.\n\nUnique identifier for the GitCommit\n\nRequired. The name of the token. Can be a simple description.\n\n\\[Output Only\\]. The personal access token. Only returned during creation.\n\n#### Response\n\nToken created successfully\n\nThe newly created personal access token.\n\n#### 0 reactions\n\nCreate a new personal access token"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/list-all-node-versions-given-some-filters",
  "markdown": "# List all node versions given some filters.\n\n```\n{\n  \"page\": 123,\n  \"pageSize\": 123,\n  \"total\": 123,\n  \"totalPages\": 123,\n  \"versions\": [\n    {\n      \"changelog\": \"<string>\",\n      \"comfy_node_extract_status\": \"<string>\",\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"dependencies\": [\n        \"<string>\"\n      ],\n      \"deprecated\": true,\n      \"downloadUrl\": \"<string>\",\n      \"id\": \"<string>\",\n      \"node_id\": \"<string>\",\n      \"status\": \"NodeVersionStatusActive\",\n      \"status_reason\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"version\": \"<string>\"\n    }\n  ]\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/create-video-to-video-prompt",
  "markdown": "# Create Video to Video Prompt\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/proxy/moonvalley/prompts/video-to-video \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n  \"control_type\": \"motion_control\",\n  \"inference_params\": {\n    \"add_quality_guidance\": true,\n    \"caching_coefficient\": 0.3,\n    \"caching_cooldown\": 3,\n    \"caching_warmup\": 3,\n    \"clip_value\": 3,\n    \"conditioning_frame_index\": 0,\n    \"cooldown_steps\": 36,\n    \"guidance_scale\": 15,\n    \"negative_prompt\": \"<string>\",\n    \"seed\": 123,\n    \"shift_value\": 3,\n    \"steps\": 80,\n    \"use_guidance_schedule\": true,\n    \"use_negative_prompts\": false,\n    \"use_timestep_transform\": true,\n    \"warmup_steps\": 24\n  },\n  \"prompt_text\": \"<string>\",\n  \"video_url\": \"<string>\",\n  \"webhook_url\": \"<string>\"\n}'\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/resize-a-video",
  "markdown": "# Resize a video - ComfyUI\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/proxy/moonvalley/prompts/video-to-video/resize \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n  \"control_type\": \"motion_control\",\n  \"inference_params\": {\n    \"add_quality_guidance\": true,\n    \"caching_coefficient\": 0.3,\n    \"caching_cooldown\": 3,\n    \"caching_warmup\": 3,\n    \"clip_value\": 3,\n    \"conditioning_frame_index\": 0,\n    \"cooldown_steps\": 36,\n    \"guidance_scale\": 15,\n    \"negative_prompt\": \"<string>\",\n    \"seed\": 123,\n    \"shift_value\": 3,\n    \"steps\": 80,\n    \"use_guidance_schedule\": true,\n    \"use_negative_prompts\": false,\n    \"use_timestep_transform\": true,\n    \"warmup_steps\": 24\n  },\n  \"prompt_text\": \"<string>\",\n  \"video_url\": \"<string>\",\n  \"webhook_url\": \"<string>\",\n  \"frame_position\": [\n    123\n  ],\n  \"frame_resolution\": [\n    123\n  ],\n  \"scale\": [\n    123\n  ]\n}'\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/get-prompt-details",
  "markdown": "# Get Prompt Details - ComfyUI\n\n```\n{\n  \"error\": {},\n  \"frame_conditioning\": {},\n  \"id\": \"<string>\",\n  \"inference_params\": {},\n  \"meta\": {},\n  \"model_params\": {},\n  \"output_url\": \"<string>\",\n  \"prompt_text\": \"<string>\",\n  \"status\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/releases/get-release-notes",
  "markdown": "# Get release notes - ComfyUI\n\n#### Query Parameters\n\nThe project to get release notes for\n\nAvailable options:\n\n`comfyui`,\n\n`comfyui_frontend`,\n\n`desktop`\n\nThe current version to filter release notes\n\nThe locale for the release notes\n\nAvailable options:\n\n`en`,\n\n`es`,\n\n`fr`,\n\n`ja`,\n\n`ko`,\n\n`ru`,\n\n`zh`\n\nThe platform requesting the release notes\n\n#### Response\n\nRelease notes retrieved successfully\n\nThe response is of type `object[]`."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fcreate-text-to-image-prompt",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-multiple-node-versions-in-a-single-request",
  "markdown": "# Retrieve multiple node versions in a single request\n\n```\n{\n  \"node_versions\": [\n    {\n      \"error_message\": \"<string>\",\n      \"identifier\": {\n        \"node_id\": \"<string>\",\n        \"version\": \"<string>\"\n      },\n      \"node_version\": {\n        \"changelog\": \"<string>\",\n        \"comfy_node_extract_status\": \"<string>\",\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"dependencies\": [\n          \"<string>\"\n        ],\n        \"deprecated\": true,\n        \"downloadUrl\": \"<string>\",\n        \"id\": \"<string>\",\n        \"node_id\": \"<string>\",\n        \"status\": \"NodeVersionStatusActive\",\n        \"status_reason\": \"<string>\",\n        \"supported_accelerators\": [\n          \"<string>\"\n        ],\n        \"supported_comfyui_frontend_version\": \"<string>\",\n        \"supported_comfyui_version\": \"<string>\",\n        \"supported_os\": [\n          \"<string>\"\n        ],\n        \"version\": \"<string>\"\n      },\n      \"status\": \"success\"\n    }\n  ]\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-a-node-by-comfyui-node-name",
  "markdown": "# Retrieve a node by ComfyUI node name\n\n```\n{\n  \"author\": \"<string>\",\n  \"banner_url\": \"<string>\",\n  \"category\": \"<string>\",\n  \"created_at\": \"2023-11-07T05:31:56Z\",\n  \"description\": \"<string>\",\n  \"downloads\": 123,\n  \"github_stars\": 123,\n  \"icon\": \"<string>\",\n  \"id\": \"<string>\",\n  \"latest_version\": {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  },\n  \"license\": \"<string>\",\n  \"name\": \"<string>\",\n  \"preempted_comfy_node_names\": [\n    \"<string>\"\n  ],\n  \"publisher\": {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  },\n  \"rating\": 123,\n  \"repository\": \"<string>\",\n  \"search_ranking\": 123,\n  \"status\": \"NodeStatusActive\",\n  \"status_detail\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"tags\": [\n    \"<string>\"\n  ],\n  \"translations\": {}\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/api-nodes/upload-files",
  "markdown": "# Upload Files - ComfyUI\n\n[ComfyUI home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo/dark.svg)](https://docs.comfy.org/)\n\n##### Registry\n\n*   [POST\n    \n    Retrieve multiple node versions in a single request\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-multiple-node-versions-in-a-single-request)\n*   [GET\n    \n    Retrieve a node by ComfyUI node name\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-a-node-by-comfyui-node-name)\n*   [GET\n    \n    Retrieves a list of nodes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes)\n*   [GET\n    \n    Retrieves a list of nodes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes-1)\n*   [GET\n    \n    Retrieve a specific node by ID\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-a-specific-node-by-id)\n*   [GET\n    \n    Returns a node version to be installed.\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/returns-a-node-version-to-be-installed)\n*   [POST\n    \n    Add review to a specific version of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/add-review-to-a-specific-version-of-a-node)\n*   [POST\n    \n    Create Node Translations\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-node-translations)\n*   [GET\n    \n    List all versions of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/list-all-versions-of-a-node)\n*   [GET\n    \n    Retrieve a specific version of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-a-specific-version-of-a-node)\n*   [GET\n    \n    list comfy-nodes for certain node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/list-comfy-nodes-for-certain-node)\n*   [POST\n    \n    create comfy-nodes for certain node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-comfy-nodes-for-certain-node)\n*   [GET\n    \n    get specify comfy-node based on its id\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/get-specify-comfy-node-based-on-its-id)\n*   [GET\n    \n    Retrieve all publishers\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-all-publishers)\n*   [POST\n    \n    Create a new publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-a-new-publisher)\n*   [GET\n    \n    Validate if a publisher username is available\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/validate-if-a-publisher-username-is-available)\n*   [GET\n    \n    Retrieve a publisher by ID\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-a-publisher-by-id)\n*   [PUT\n    \n    Update a publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/update-a-publisher)\n*   [DEL\n    \n    Delete a publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/delete-a-publisher)\n*   [GET\n    \n    Retrieve all nodes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-all-nodes)\n*   [POST\n    \n    Create a new custom node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-a-new-custom-node)\n*   [GET\n    \n    Retrieve all nodes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-all-nodes-1)\n*   [PUT\n    \n    Update a specific node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/update-a-specific-node)\n*   [DEL\n    \n    Delete a specific node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/delete-a-specific-node)\n*   [POST\n    \n    Claim nodeId into publisherId for the authenticated publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/claim-nodeid-into-publisherid-for-the-authenticated-publisher)\n*   [GET\n    \n    Retrieve permissions the user has for a given publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher)\n*   [POST\n    \n    Publish a new version of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/publish-a-new-version-of-a-node)\n*   [PUT\n    \n    Update changelog and deprecation status of a node version\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/update-changelog-and-deprecation-status-of-a-node-version)\n*   [DEL\n    \n    Unpublish (delete) a specific version of a node\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/unpublish-delete-a-specific-version-of-a-node)\n*   [GET\n    \n    Retrieve permissions the user has for a given publisher\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher-1)\n*   [POST\n    \n    Create a new personal access token\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/create-a-new-personal-access-token)\n*   [DEL\n    \n    Delete a specific personal access token\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/delete-a-specific-personal-access-token)\n*   [GET\n    \n    Get information about the calling user.\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/get-information-about-the-calling-user)\n*   [GET\n    \n    Retrieve all publishers for a given user\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/retrieve-all-publishers-for-a-given-user)\n*   [GET\n    \n    List all node versions given some filters.\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/registry/list-all-node-versions-given-some-filters)\n\n##### API Nodes\n\n*   [POST\n    \n    Create Image to Video Prompt\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/create-image-to-video-prompt)\n*   [POST\n    \n    Create Text to Image Prompt\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/create-text-to-image-prompt)\n*   [POST\n    \n    Create Text to Video Prompt\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/create-text-to-video-prompt)\n*   [POST\n    \n    Create Video to Video Prompt\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/create-video-to-video-prompt)\n*   [POST\n    \n    Resize a video\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/resize-a-video)\n*   [GET\n    \n    Get Prompt Details\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/get-prompt-details)\n*   [POST\n    \n    Upload Files\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/api-nodes/upload-files)\n\n##### Releases\n\n*   [GET\n    \n    Get release notes\n    \n    \n    \n    ](https://docs.comfy.org/api-reference/releases/get-release-notes)\n\n[ComfyUI home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo/dark.svg)](https://docs.comfy.org/)\n\nAPI Nodes\n\nPOST\n\n/\n\nproxy\n\n/\n\nmoonvalley\n\n/\n\nuploads\n\nUpload Files\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/proxy/moonvalley/uploads \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@example-file.txt\n```\n\n```\n{\n  \"access_url\": \"<string>\"\n}\n```\n\n#### Body\n\nmultipart/form-data\n\n[â€‹](#body-file)\n\nfile\n\nfile\n\n#### Response\n\n200 - application/json\n\nFile uploaded successfully\n\n[â€‹](#response-access-url)\n\naccess\\_url\n\nstring\n\n#### 0 reactions\n\n[Sign in](https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fupload-files) to add your reaction.\n\n#### 0 comments\n\n[Sign in with GitHub](https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fupload-files)\n\nWas this page helpful?\n\n[Suggest edits](https://github.com/comfy-org/docs/edit/main/api-reference/api-nodes/upload-files.mdx)\n\n[Previous](https://docs.comfy.org/api-reference/api-nodes/get-prompt-details)\n\n[\n\nGet release notesFetch release notes from Strapi with caching\n\nNext\n\n\n\n](https://docs.comfy.org/api-reference/releases/get-release-notes)\n\nUpload Files\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/proxy/moonvalley/uploads \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form file=@example-file.txt\n```\n\n```\n{\n  \"access_url\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes-1",
  "markdown": "# Retrieves a list of nodes\n\n```\n{\n  \"limit\": 123,\n  \"nodes\": [\n    {\n      \"author\": \"<string>\",\n      \"banner_url\": \"<string>\",\n      \"category\": \"<string>\",\n      \"created_at\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"downloads\": 123,\n      \"github_stars\": 123,\n      \"icon\": \"<string>\",\n      \"id\": \"<string>\",\n      \"latest_version\": {\n        \"changelog\": \"<string>\",\n        \"comfy_node_extract_status\": \"<string>\",\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"dependencies\": [\n          \"<string>\"\n        ],\n        \"deprecated\": true,\n        \"downloadUrl\": \"<string>\",\n        \"id\": \"<string>\",\n        \"node_id\": \"<string>\",\n        \"status\": \"NodeVersionStatusActive\",\n        \"status_reason\": \"<string>\",\n        \"supported_accelerators\": [\n          \"<string>\"\n        ],\n        \"supported_comfyui_frontend_version\": \"<string>\",\n        \"supported_comfyui_version\": \"<string>\",\n        \"supported_os\": [\n          \"<string>\"\n        ],\n        \"version\": \"<string>\"\n      },\n      \"license\": \"<string>\",\n      \"name\": \"<string>\",\n      \"preempted_comfy_node_names\": [\n        \"<string>\"\n      ],\n      \"publisher\": {\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"description\": \"<string>\",\n        \"id\": \"<string>\",\n        \"logo\": \"<string>\",\n        \"members\": [\n          {\n            \"id\": \"<string>\",\n            \"role\": \"<string>\",\n            \"user\": {\n              \"email\": \"<string>\",\n              \"id\": \"<string>\",\n              \"name\": \"<string>\"\n            }\n          }\n        ],\n        \"name\": \"<string>\",\n        \"source_code_repo\": \"<string>\",\n        \"status\": \"PublisherStatusActive\",\n        \"support\": \"<string>\",\n        \"website\": \"<string>\"\n      },\n      \"rating\": 123,\n      \"repository\": \"<string>\",\n      \"search_ranking\": 123,\n      \"status\": \"NodeStatusActive\",\n      \"status_detail\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"tags\": [\n        \"<string>\"\n      ],\n      \"translations\": {}\n    }\n  ],\n  \"page\": 123,\n  \"total\": 123,\n  \"totalPages\": 123\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes",
  "markdown": "# Retrieves a list of nodes\n\n```\n{\n  \"limit\": 123,\n  \"nodes\": [\n    {\n      \"author\": \"<string>\",\n      \"banner_url\": \"<string>\",\n      \"category\": \"<string>\",\n      \"created_at\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"downloads\": 123,\n      \"github_stars\": 123,\n      \"icon\": \"<string>\",\n      \"id\": \"<string>\",\n      \"latest_version\": {\n        \"changelog\": \"<string>\",\n        \"comfy_node_extract_status\": \"<string>\",\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"dependencies\": [\n          \"<string>\"\n        ],\n        \"deprecated\": true,\n        \"downloadUrl\": \"<string>\",\n        \"id\": \"<string>\",\n        \"node_id\": \"<string>\",\n        \"status\": \"NodeVersionStatusActive\",\n        \"status_reason\": \"<string>\",\n        \"supported_accelerators\": [\n          \"<string>\"\n        ],\n        \"supported_comfyui_frontend_version\": \"<string>\",\n        \"supported_comfyui_version\": \"<string>\",\n        \"supported_os\": [\n          \"<string>\"\n        ],\n        \"version\": \"<string>\"\n      },\n      \"license\": \"<string>\",\n      \"name\": \"<string>\",\n      \"preempted_comfy_node_names\": [\n        \"<string>\"\n      ],\n      \"publisher\": {\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"description\": \"<string>\",\n        \"id\": \"<string>\",\n        \"logo\": \"<string>\",\n        \"members\": [\n          {\n            \"id\": \"<string>\",\n            \"role\": \"<string>\",\n            \"user\": {\n              \"email\": \"<string>\",\n              \"id\": \"<string>\",\n              \"name\": \"<string>\"\n            }\n          }\n        ],\n        \"name\": \"<string>\",\n        \"source_code_repo\": \"<string>\",\n        \"status\": \"PublisherStatusActive\",\n        \"support\": \"<string>\",\n        \"website\": \"<string>\"\n      },\n      \"rating\": 123,\n      \"repository\": \"<string>\",\n      \"search_ranking\": 123,\n      \"status\": \"NodeStatusActive\",\n      \"status_detail\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"tags\": [\n        \"<string>\"\n      ],\n      \"translations\": {}\n    }\n  ],\n  \"page\": 123,\n  \"total\": 123,\n  \"totalPages\": 123\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-a-specific-node-by-id",
  "markdown": "# Retrieve a specific node by ID\n\n```\n{\n  \"author\": \"<string>\",\n  \"banner_url\": \"<string>\",\n  \"category\": \"<string>\",\n  \"created_at\": \"2023-11-07T05:31:56Z\",\n  \"description\": \"<string>\",\n  \"downloads\": 123,\n  \"github_stars\": 123,\n  \"icon\": \"<string>\",\n  \"id\": \"<string>\",\n  \"latest_version\": {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  },\n  \"license\": \"<string>\",\n  \"name\": \"<string>\",\n  \"preempted_comfy_node_names\": [\n    \"<string>\"\n  ],\n  \"publisher\": {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  },\n  \"rating\": 123,\n  \"repository\": \"<string>\",\n  \"search_ranking\": 123,\n  \"status\": \"NodeStatusActive\",\n  \"status_detail\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"tags\": [\n    \"<string>\"\n  ],\n  \"translations\": {}\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/BasicScheduler",
  "markdown": "# BasicScheduler - ComfyUI Built-in Node Documentation\n\nThe `BasicScheduler` node is designed to compute a sequence of sigma values for diffusion models based on the provided scheduler, model, and denoising parameters. It dynamically adjusts the total number of steps based on the denoise factor to fine-tune the diffusion process, providing precise â€œrecipesâ€ for different stages in advanced sampling processes that require fine control (such as multi-stage sampling).\n\n## Inputs\n\n| Parameter | Data Type | Input Type | Default | Range | Metaphor Description | Technical Purpose |\n| --- | --- | --- | --- | --- | --- | --- |\n| `model` | MODEL | Input | \\-  | \\-  | **Canvas Type**: Different canvas materials need different paint formulas | Diffusion model object, determines sigma calculation basis |\n| `scheduler` | COMBO\\[STRING\\] | Widget | \\-  | 9 options | **Mixing Technique**: Choose how paint concentration changes | Scheduling algorithm, controls noise decay mode |\n| `steps` | INT | Widget | 20  | 1-10000 | **Mixing Count**: 20 mixes vs 50 mixes precision difference | Sampling steps, affects generation quality and speed |\n| `denoise` | FLOAT | Widget | 1.0 | 0.0-1.0 | **Creation Intensity**: Control level from fine-tuning to repainting | Denoising strength, supports partial repainting scenarios |\n\n### Scheduler Types\n\nBased on source code `comfy.samplers.SCHEDULER_NAMES`, supports the following 9 schedulers:\n\n| Scheduler Name | Characteristics | Use Cases | Noise Decay Pattern |\n| --- | --- | --- | --- |\n| **normal** | Standard linear | General scenarios, balanced | Uniform decay |\n| **karras** | Smooth transition | High quality, detail-rich | Smooth non-linear decay |\n| **exponential** | Exponential decay | Fast generation, efficiency | Exponential rapid decay |\n| **sgm\\_uniform** | SGM uniform | Specific model optimization | SGM optimized decay |\n| **simple** | Simple scheduling | Quick testing, basic use | Simplified decay |\n| **ddim\\_uniform** | DDIM uniform | DDIM sampling optimization | DDIM specific decay |\n| **beta** | Beta distribution | Special distribution needs | Beta function decay |\n| **linear\\_quadratic** | Linear quadratic | Complex scenario optimization | Quadratic function decay |\n| **kl\\_optimal** | KL optimal | Theoretical optimization | KL divergence optimized decay |\n\n## Outputs\n\n| Parameter | Data Type | Output Type | Metaphor Description | Technical Meaning |\n| --- | --- | --- | --- | --- |\n| `sigmas` | SIGMAS | Output | **Paint Recipe Chart**: Detailed paint concentration list for step-by-step use | Noise level sequence, guides diffusion model denoising process |\n\n## Node Role: Artistâ€™s Color Mixing Assistant\n\nImagine you are an artist creating a clear image from a chaotic mixture of paint (noise). `BasicScheduler` acts like your **professional color mixing assistant**, whose job is to prepare a series of precise paint concentration recipes:\n\n### Workflow\n\n*   **Step 1**: Use 90% concentration paint (high noise level)\n*   **Step 2**: Use 80% concentration paint\n*   **Step 3**: Use 70% concentration paint\n*   **â€¦**\n*   **Final Step**: Use 0% concentration (clean canvas, no noise)\n\n### Color Assistantâ€™s Special Skills\n\n**Different mixing methods (scheduler)**:\n\n*   **â€œkarrasâ€ mixing method**: Paint concentration changes very smoothly, like professional artistâ€™s gradient technique\n*   **â€œexponentialâ€ mixing method**: Paint concentration decreases rapidly, suitable for quick creation\n*   **â€œlinearâ€ mixing method**: Paint concentration decreases uniformly, stable and controllable\n\n**Fine control (steps)**:\n\n*   **20 mixes**: Quick painting, efficiency priority\n*   **50 mixes**: Fine painting, quality priority\n\n**Creation intensity (denoise)**:\n\n*   **1.0 = Complete new creation**: Start completely from blank canvas\n*   **0.5 = Half transformation**: Keep half of original painting, transform half\n*   **0.2 = Fine adjustment**: Only make subtle adjustments to original painting\n\n### Collaboration with Other Nodes\n\n`BasicScheduler` (Color Assistant) â†’ Prepare Recipe â†’ `SamplerCustom` (Artist) â†’ Actual Painting â†’ Completed Work"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/returns-a-node-version-to-be-installed",
  "markdown": "# Returns a node version to be installed.\n\n```\n{\n  \"changelog\": \"<string>\",\n  \"comfy_node_extract_status\": \"<string>\",\n  \"createdAt\": \"2023-11-07T05:31:56Z\",\n  \"dependencies\": [\n    \"<string>\"\n  ],\n  \"deprecated\": true,\n  \"downloadUrl\": \"<string>\",\n  \"id\": \"<string>\",\n  \"node_id\": \"<string>\",\n  \"status\": \"NodeVersionStatusActive\",\n  \"status_reason\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"version\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/add-review-to-a-specific-version-of-a-node",
  "markdown": "# Add review to a specific version of a node\n\n```\n{\n  \"author\": \"<string>\",\n  \"banner_url\": \"<string>\",\n  \"category\": \"<string>\",\n  \"created_at\": \"2023-11-07T05:31:56Z\",\n  \"description\": \"<string>\",\n  \"downloads\": 123,\n  \"github_stars\": 123,\n  \"icon\": \"<string>\",\n  \"id\": \"<string>\",\n  \"latest_version\": {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  },\n  \"license\": \"<string>\",\n  \"name\": \"<string>\",\n  \"preempted_comfy_node_names\": [\n    \"<string>\"\n  ],\n  \"publisher\": {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  },\n  \"rating\": 123,\n  \"repository\": \"<string>\",\n  \"search_ranking\": 123,\n  \"status\": \"NodeStatusActive\",\n  \"status_detail\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"tags\": [\n    \"<string>\"\n  ],\n  \"translations\": {}\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/list-all-versions-of-a-node",
  "markdown": "# List all versions of a node\n\n```\n[\n  {\n    \"changelog\": \"<string>\",\n    \"comfy_node_extract_status\": \"<string>\",\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"dependencies\": [\n      \"<string>\"\n    ],\n    \"deprecated\": true,\n    \"downloadUrl\": \"<string>\",\n    \"id\": \"<string>\",\n    \"node_id\": \"<string>\",\n    \"status\": \"NodeVersionStatusActive\",\n    \"status_reason\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"version\": \"<string>\"\n  }\n]\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-node-translations",
  "markdown": "# Create Node Translations - ComfyUI\n\n#### Path Parameters\n\nThe unique identifier of the node.\n\n#### Body\n\n#### Response\n\nDetailed information about a specific node\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/list-comfy-nodes-for-certain-node",
  "markdown": "# list comfy-nodes for certain node\n\n```\n{\n  \"comfy_nodes\": [\n    {\n      \"category\": \"<string>\",\n      \"comfy_node_name\": \"<string>\",\n      \"deprecated\": true,\n      \"description\": \"<string>\",\n      \"experimental\": true,\n      \"function\": \"<string>\",\n      \"input_types\": \"<string>\",\n      \"output_is_list\": [\n        true\n      ],\n      \"return_names\": \"<string>\",\n      \"return_types\": \"<string>\"\n    }\n  ],\n  \"totalNumberOfPages\": 123\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-a-specific-version-of-a-node",
  "markdown": "# Retrieve a specific version of a node\n\n```\n{\n  \"changelog\": \"<string>\",\n  \"comfy_node_extract_status\": \"<string>\",\n  \"createdAt\": \"2023-11-07T05:31:56Z\",\n  \"dependencies\": [\n    \"<string>\"\n  ],\n  \"deprecated\": true,\n  \"downloadUrl\": \"<string>\",\n  \"id\": \"<string>\",\n  \"node_id\": \"<string>\",\n  \"status\": \"NodeVersionStatusActive\",\n  \"status_reason\": \"<string>\",\n  \"supported_accelerators\": [\n    \"<string>\"\n  ],\n  \"supported_comfyui_frontend_version\": \"<string>\",\n  \"supported_comfyui_version\": \"<string>\",\n  \"supported_os\": [\n    \"<string>\"\n  ],\n  \"version\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-comfy-nodes-for-certain-node",
  "markdown": "# create comfy-nodes for certain node\n\ncreate comfy-nodes for certain node\n\n```\ncurl --request POST \\\n  --url https://api.comfy.org/nodes/{nodeId}/versions/{version}/comfy-nodes \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n  \"cloud_build_info\": {\n    \"build_id\": \"<string>\",\n    \"location\": \"<string>\",\n    \"project_id\": \"<string>\",\n    \"project_number\": \"<string>\"\n  },\n  \"nodes\": {},\n  \"reason\": \"<string>\",\n  \"status\": \"<string>\",\n  \"success\": true\n}'\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncodeFlux",
  "markdown": "# ClipTextEncodeFlux - ComfyUI Built-in Node Documentation\n\n`CLIPTextEncodeFlux` is an advanced text encoding node in ComfyUI, specifically designed for the Flux architecture. It uses a dual-encoder mechanism (CLIP-L and T5XXL) to process both structured keywords and detailed natural language descriptions, providing the Flux model with more accurate and comprehensive text understanding for improved text-to-image generation quality. This node is based on a dual-encoder collaboration mechanism:\n\n1.  The `clip_l` input is processed by the CLIP-L encoder, extracting style, theme, and other keyword featuresâ€”ideal for concise descriptions.\n2.  The `t5xxl` input is processed by the T5XXL encoder, which excels at understanding complex and detailed natural language scene descriptions.\n3.  The outputs from both encoders are fused, and combined with the `guidance` parameter to generate unified conditioning embeddings (`CONDITIONING`) for downstream Flux sampler nodes, controlling how closely the generated content matches the text description.\n\n## Inputs\n\n| Parameter | Data Type | Input Method | Default | Range | Description |\n| --- | --- | --- | --- | --- | --- |\n| `clip` | CLIP | Node input | None | \\-  | Must be a CLIP model supporting the Flux architecture, including both CLIP-L and T5XXL encoders |\n| `clip_l` | STRING | Text box | None | Up to 77 tokens | Suitable for concise keyword descriptions, such as style or theme |\n| `t5xxl` | STRING | Text box | None | Nearly unlimited | Suitable for detailed natural language descriptions, expressing complex scenes and details |\n| `guidance` | FLOAT | Slider | 3.5 | 0.0 - 100.0 | Controls the influence of text conditions on the generation process; higher values mean stricter adherence to the text |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| `CONDITIONING` | CONDITIONING | Contains the fused embeddings from both encoders and the guidance parameter, used for conditional image generation |\n\n## Usage Examples\n\n### Prompt Examples\n\n*   **clip\\_l input** (keyword style):\n    *   Use structured, concise keyword combinations\n    *   Example: `masterpiece, best quality, portrait, oil painting, dramatic lighting`\n    *   Focus on style, quality, and main subject\n*   **t5xxl input** (natural language description):\n    *   Use complete, fluent scene descriptions\n    *   Example: `A highly detailed portrait in oil painting style, featuring dramatic chiaroscuro lighting that creates deep shadows and bright highlights, emphasizing the subject's features with renaissance-inspired composition.`\n    *   Focus on scene details, spatial relationships, and lighting effects\n\n### Notes\n\n1.  Make sure to use a CLIP model compatible with the Flux architecture\n2.  It is recommended to fill in both `clip_l` and `t5xxl` to leverage the dual-encoder advantage\n3.  Note the 77-token limit for `clip_l`\n4.  Adjust the `guidance` parameter based on the generated results"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncodeHunyuanDit",
  "markdown": "# ClipTextEncodeHunyuanDit - ComfyUI Built-in Node Documentation\n\nThe `CLIPTextEncodeHunyuanDiT` nodeâ€™s main function is to convert input text into a form that the model can understand. It is an advanced conditioning node specifically designed for the dual text encoder architecture of the HunyuanDiT model. Its primary role is like a translator, converting our text descriptions into â€œmachine languageâ€ that the AI model can understand. The `bert` and `mt5xl` inputs prefer different types of prompt inputs.\n\n## Inputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip` | CLIP | A CLIP model instance used for text tokenization and encoding, which is core to generating conditions. |\n| `bert` | STRING | Text input for encoding, prefers phrases and keywords, supports multiline and dynamic prompts. |\n| `mt5xl` | STRING | Another text input for encoding, supports multiline and dynamic prompts (multilingual), can use complete sentences and complex descriptions. |\n\n## Outputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `CONDITIONING` | CONDITIONING | The encoded conditional output used for further processing in generation tasks. |\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncodeSdxl",
  "markdown": "# ClipTextEncodeSdxl - ComfyUI Built-in Node Documentation\n\nThis node is designed to encode text input using a CLIP model specifically customized for the SDXL architecture. It uses a dual encoder system (CLIP-L and CLIP-G) to process text descriptions, resulting in more accurate image generation.\n\n## Inputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip` | CLIP | CLIP model instance used for text encoding. |\n| `width` | INT | Specifies the image width in pixels, default 1024. |\n| `height` | INT | Specifies the image height in pixels, default 1024. |\n| `crop_w` | INT | Width of the crop area in pixels, default 0. |\n| `crop_h` | INT | Height of the crop area in pixels, default 0. |\n| `target_width` | INT | Target width for the output image, default 1024. |\n| `target_height` | INT | Target height for the output image, default 1024. |\n| `text_g` | STRING | Global text description for overall scene description. |\n| `text_l` | STRING | Local text description for detail description. |\n\n## Outputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `CONDITIONING` | CONDITIONING | Contains encoded text and conditional information needed for image generation. |\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/get-specify-comfy-node-based-on-its-id",
  "markdown": "# get specify comfy-node based on its id\n\n#### Path Parameters\n\n#### Response\n\nComy Nodes created successfully\n\nThe response is of type `object`.\n\nget specify comfy-node based on its id"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-all-publishers",
  "markdown": "# Retrieve all publishers - ComfyUI\n\n```\n[\n  {\n    \"createdAt\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"id\": \"<string>\",\n    \"logo\": \"<string>\",\n    \"members\": [\n      {\n        \"id\": \"<string>\",\n        \"role\": \"<string>\",\n        \"user\": {\n          \"email\": \"<string>\",\n          \"id\": \"<string>\",\n          \"name\": \"<string>\"\n        }\n      }\n    ],\n    \"name\": \"<string>\",\n    \"source_code_repo\": \"<string>\",\n    \"status\": \"PublisherStatusActive\",\n    \"support\": \"<string>\",\n    \"website\": \"<string>\"\n  }\n]\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-a-new-publisher",
  "markdown": "# Create a new publisher - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Body\n\n#### Response\n\nPublisher created successfully\n\nThe response is of type `object`."
},
{
  "url": "https://docs.comfy.org/api-reference/registry/validate-if-a-publisher-username-is-available",
  "markdown": "# Validate if a publisher username is available\n\n#### Query Parameters\n\nThe publisher username to validate.\n\n#### Response\n\nUsername validation result\n\nThe response is of type `object`.\n\nValidate if a publisher username is available"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-1-1-pro-ultra-image",
  "markdown": "# Flux 1.1 \\[pro\\] Ultra Image - ComfyUI Native Node Documentation\n\n![ComfyUI Native Flux 1.1 [pro] Ultra Image node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg) The Flux 1.1 \\[pro\\] Ultra Image node allows you to generate ultra-high-resolution images through text prompts, directly connecting to Black Forest Labsâ€™ latest image generation API. This node supports two main usage modes:\n\n1.  **Text-to-Image**: Generate high-quality images from text prompts (when no image input is used)\n2.  **Image-to-Image**: Combine existing images with prompts to create new images that blend features from both (Remix mode)\n\nThis node supports Ultra mode through API calls, capable of generating images at 4 times the resolution of standard Flux 1.1 \\[pro\\] (up to 4MP), without sacrificing prompt adherence, and maintaining super-fast generation times of just 10 seconds. Compared to other high-resolution models, itâ€™s more than 2.5 times faster.\n\n## Parameter Description\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | String | \"\"  | Text description for generating the image |\n| prompt\\_upsampling | Boolean | False | Whether to use prompt upsampling technique to enhance details. When enabled, automatically modifies prompts for more creative generation, but results become non-deterministic (same seed wonâ€™t produce exactly the same result) |\n| seed | Integer | 0   | Random seed value, controls generation randomness |\n| aspect\\_ratio | String | â€16:9â€ | Width-to-height ratio of the image, must be between 1:4 and 4:1 |\n| raw | Boolean | False | When set to True, generates less processed, more natural-looking images |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image\\_prompt | Image | None | Optional input, used for Image-to-Image (Remix) mode |\n| image\\_prompt\\_strength | Float | 0.1 | Active when `image_prompt` is input, adjusts the blend between prompt and image prompt. Higher values make output closer to input image, range is 0.0-1.0 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | Image | Generated high-resolution image result |\n\n## Usage Examples\n\nPlease visit the tutorial below to see corresponding usage examples\n\n*   [Flux 1.1 Pro Ultra Image API Node ComfyUI Official Example Workflow](https://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)\n\n## How It Works\n\nFlux 1.1 \\[pro\\] Ultra mode uses optimized deep learning architecture and efficient GPU acceleration technology to achieve high-resolution image generation without sacrificing speed. When a request is sent to the API, the system parses the prompt, applies appropriate parameters, then computes the image in parallel, finally generating and returning the high-resolution result. Compared to regular models, Ultra mode particularly focuses on detail preservation and consistency at large scales, ensuring impressive quality even at 4MP high resolution.\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\nclass FluxProUltraImageNode(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    MINIMUM_RATIO = 1 / 4\n    MAXIMUM_RATIO = 4 / 1\n    MINIMUM_RATIO_STR = \"1:4\"\n    MAXIMUM_RATIO_STR = \"4:1\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"prompt_upsampling\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result).\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"16:9\",\n                        \"tooltip\": \"Aspect ratio of image; must be between 1:4 and 4:1.\",\n                    },\n                ),\n                \"raw\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"When True, generate less processed, more natural-looking images.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image_prompt\": (IO.IMAGE,),\n                \"image_prompt_strength\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.1,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Blend between the prompt and the image prompt.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, aspect_ratio: str):\n        try:\n            validate_aspect_ratio(\n                aspect_ratio,\n                minimum_ratio=cls.MINIMUM_RATIO,\n                maximum_ratio=cls.MAXIMUM_RATIO,\n                minimum_ratio_str=cls.MINIMUM_RATIO_STR,\n                maximum_ratio_str=cls.MAXIMUM_RATIO_STR,\n            )\n        except Exception as e:\n            return str(e)\n        return True\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/bfl\"\n\n    def api_call(\n        self,\n        prompt: str,\n        aspect_ratio: str,\n        prompt_upsampling=False,\n        raw=False,\n        seed=0,\n        image_prompt=None,\n        image_prompt_strength=0.1,\n        auth_token=None,\n        **kwargs,\n    ):\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/bfl/flux-pro-1.1-ultra/generate\",\n                method=HttpMethod.POST,\n                request_model=BFLFluxProUltraGenerateRequest,\n                response_model=BFLFluxProGenerateResponse,\n            ),\n            request=BFLFluxProUltraGenerateRequest(\n                prompt=prompt,\n                prompt_upsampling=prompt_upsampling,\n                seed=seed,\n                aspect_ratio=validate_aspect_ratio(\n                    aspect_ratio,\n                    minimum_ratio=self.MINIMUM_RATIO,\n                    maximum_ratio=self.MAXIMUM_RATIO,\n                    minimum_ratio_str=self.MINIMUM_RATIO_STR,\n                    maximum_ratio_str=self.MAXIMUM_RATIO_STR,\n                ),\n                raw=raw,\n                image_prompt=(\n                    image_prompt\n                    if image_prompt is None\n                    else convert_image_to_base64(image_prompt)\n                ),\n                image_prompt_strength=(\n                    None if image_prompt is None else round(image_prompt_strength, 2)\n                ),\n            ),\n            auth_token=auth_token,\n        )\n        output_image = handle_bfl_synchronous_operation(operation)\n        return (output_image,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1",
  "markdown": "# Ideogram V1 - ComfyUI Native Node Documentation\n\n![ComfyUI Native Ideogram V1 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg) The Ideogram V1 node allows you to generate images with high-quality text rendering capabilities using Ideogramâ€™s text-to-image API.\n\n## Parameter Description\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the content to generate |\n| turbo | boolean | False | Whether to use turbo mode (faster but possibly lower quality) |\n| aspect\\_ratio | select | â€1:1â€ | Image aspect ratio |\n| magic\\_prompt\\_option | select | â€AUTOâ€ | Determines whether to use MagicPrompt in generation, options: AUTO, ON, OFF |\n| seed | integer | 0   | Random seed value (0-2147483647) |\n| negative\\_prompt | string | \"\"  | Specifies elements you donâ€™t want in the image |\n| num\\_images | integer | 1   | Number of images to generate (1-8) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image result |\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\nclass IdeogramV1(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V1 model.\n\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"turbo\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to use turbo mode (faster generation, potentially lower quality)\",\n                    }\n                ),\n            },\n            \"optional\": {\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V2_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Description of what to exclude from the image\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v1\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        turbo=False,\n        aspect_ratio=\"1:1\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        negative_prompt=\"\",\n        num_images=1,\n        auth_token=None,\n    ):\n        # Determine the model based on turbo setting\n        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)\n        model = \"V_1_TURBO\" if turbo else \"V_1\"\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/ideogram/generate\",\n                method=HttpMethod.POST,\n                request_model=IdeogramGenerateRequest,\n                response_model=IdeogramGenerateResponse,\n            ),\n            request=IdeogramGenerateRequest(\n                image_request=ImageRequest(\n                    prompt=prompt,\n                    model=model,\n                    num_images=num_images,\n                    seed=seed,\n                    aspect_ratio=aspect_ratio if aspect_ratio != \"ASPECT_1_1\" else None,\n                    magic_prompt_option=(\n                        magic_prompt_option if magic_prompt_option != \"AUTO\" else None\n                    ),\n                    negative_prompt=negative_prompt if negative_prompt else None,\n                )\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2",
  "markdown": "# Ideogram V2 - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Ideogram V2 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg) The Ideogram V2 node allows you to generate more refined images using Ideogramâ€™s second-generation AI model, with significant improvements in text rendering, image quality, and overall aesthetics.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the content to generate |\n| turbo | boolean | False | Whether to use turbo mode (faster generation, possibly lower quality) |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| aspect\\_ratio | dropdown | â€1:1â€ | Image aspect ratio, effective when resolution is set to â€œAutoâ€ |\n| resolution | dropdown | â€Autoâ€ | Output image resolution, if not set to â€œAutoâ€, it will override the aspect\\_ratio setting |\n| magic\\_prompt\\_option | dropdown | â€AUTOâ€ | Determines whether to use MagicPrompt feature during generation, options are \\[â€œAUTOâ€, â€œONâ€, â€œOFFâ€\\] |\n| seed | integer | 0   | Random seed value, range 0-2147483647 |\n| style\\_type | dropdown | â€NONEâ€ | Generation style type (V2 only), options are \\[â€œAUTOâ€, â€œGENERALâ€, â€œREALISTICâ€, â€œDESIGNâ€, â€œRENDER\\_3Dâ€, â€œANIMEâ€\\] |\n| negative\\_prompt | string | \"\"  | Specifies elements you donâ€™t want to appear in the image |\n| num\\_images | integer | 1   | Number of images to generate, range 1-8 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image(s) |\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\n\nclass IdeogramV2(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V2 model.\n\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"turbo\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to use turbo mode (faster generation, potentially lower quality)\",\n                    }\n                ),\n            },\n            \"optional\": {\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V2_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation. Ignored if resolution is not set to AUTO.\",\n                    },\n                ),\n                \"resolution\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V1_RES_MAP.keys()),\n                        \"default\": \"Auto\",\n                        \"tooltip\": \"The resolution for image generation. If not set to AUTO, this overrides the aspect_ratio setting.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"style_type\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"GENERAL\", \"REALISTIC\", \"DESIGN\", \"RENDER_3D\", \"ANIME\"],\n                        \"default\": \"NONE\",\n                        \"tooltip\": \"Style type for generation (V2 only)\",\n                    },\n                ),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Description of what to exclude from the image\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n                #\"color_palette\": (\n                #    IO.STRING,\n                #    {\n                #        \"multiline\": False,\n                #        \"default\": \"\",\n                #        \"tooltip\": \"Color palette preset name or hex colors with weights\",\n                #    },\n                #),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v2\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        turbo=False,\n        aspect_ratio=\"1:1\",\n        resolution=\"Auto\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        style_type=\"NONE\",\n        negative_prompt=\"\",\n        num_images=1,\n        color_palette=\"\",\n        auth_token=None,\n    ):\n        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)\n        resolution = V1_V1_RES_MAP.get(resolution, None)\n        # Determine the model based on turbo setting\n        model = \"V_2_TURBO\" if turbo else \"V_2\"\n\n        # Handle resolution vs aspect_ratio logic\n        # If resolution is not AUTO, it overrides aspect_ratio\n        final_resolution = None\n        final_aspect_ratio = None\n\n        if resolution != \"AUTO\":\n            final_resolution = resolution\n        else:\n            final_aspect_ratio = aspect_ratio if aspect_ratio != \"ASPECT_1_1\" else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/ideogram/generate\",\n                method=HttpMethod.POST,\n                request_model=IdeogramGenerateRequest,\n                response_model=IdeogramGenerateResponse,\n            ),\n            request=IdeogramGenerateRequest(\n                image_request=ImageRequest(\n                    prompt=prompt,\n                    model=model,\n                    num_images=num_images,\n                    seed=seed,\n                    aspect_ratio=final_aspect_ratio,\n                    resolution=final_resolution,\n                    magic_prompt_option=(\n                        magic_prompt_option if magic_prompt_option != \"AUTO\" else None\n                    ),\n                    style_type=style_type if style_type != \"NONE\" else None,\n                    negative_prompt=negative_prompt if negative_prompt else None,\n                    color_palette=color_palette if color_palette else None,\n                )\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-a-publisher-by-id",
  "markdown": "# Retrieve a publisher by ID\n\n```\n{\n  \"createdAt\": \"2023-11-07T05:31:56Z\",\n  \"description\": \"<string>\",\n  \"id\": \"<string>\",\n  \"logo\": \"<string>\",\n  \"members\": [\n    {\n      \"id\": \"<string>\",\n      \"role\": \"<string>\",\n      \"user\": {\n        \"email\": \"<string>\",\n        \"id\": \"<string>\",\n        \"name\": \"<string>\"\n      }\n    }\n  ],\n  \"name\": \"<string>\",\n  \"source_code_repo\": \"<string>\",\n  \"status\": \"PublisherStatusActive\",\n  \"support\": \"<string>\",\n  \"website\": \"<string>\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image",
  "markdown": "# Luma Image to Image - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Luma Image to Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg) The Luma Image to Image node allows you to modify existing images using Luma AI technology based on text prompts, while preserving certain features and structure of the original image.\n\nThis node connects to Luma AIâ€™s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail, particularly excelling at generating photorealistic content and artistic style images.\n\nThe Luma Image to Image node analyzes the input image and combines it with text prompts to guide the modification process. It uses Luma AIâ€™s generation models to make creative changes to images based on prompts. Node process:\n\nThe image\\_weight parameter controls the degree of influence from the original image - values closer to 0 will preserve more of the original image features, while values closer to 1 allow for more substantial modifications.\n\n```\n\nclass LumaImageModifyNode(ComfyNodeABC):\n    \"\"\"\n    Modifies images synchronously based on prompt and aspect ratio.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE,),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"image_weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.02,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of the image; the closer to 0.0, the less the image will be modified.\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaImageModel],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {},\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        image: torch.Tensor,\n        image_weight: float,\n        seed,\n        auth_token=None,\n        **kwargs,\n    ):\n        # first, upload image\n        download_urls = upload_images_to_comfyapi(\n            image, max_images=1, auth_token=auth_token\n        )\n        image_url = download_urls[0]\n        # next, make Luma call with download url provided\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations/image\",\n                method=HttpMethod.POST,\n                request_model=LumaImageGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaImageGenerationRequest(\n                prompt=prompt,\n                model=model,\n                modify_image_ref=LumaModifyImageRef(\n                    url=image_url, weight=round(image_weight, 2)\n                ),\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        img_response = requests.get(response_poll.assets.image)\n        img = process_image_response(img_response)\n        return (img,)\n\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/update-a-publisher",
  "markdown": "# Update a publisher - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n#### Response\n\nPublisher updated successfully\n\nThe response is of type `object`."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2",
  "markdown": "# OpenAI DALLÂ·E 2 - ComfyUI Native Node Documentation\n\n![ComfyUI Native Stability AI Stable Image Ultra Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg) The OpenAI DALLÂ·E 2 node allows you to use OpenAIâ€™s DALLÂ·E 2 API to generate creative images from text descriptions.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt for DALLÂ·E to generate images, supports multi-line input |\n| seed | integer | 0   | The result is not actually related to the seed, this parameter only determines whether to re-execute |\n| size | select | â€1024x1024â€ | Output image size, options: 256x256, 512x512, 1024x1024 |\n| n   | integer | 1   | Number of images to generate, range 1-8 |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | None | Optional reference image for image editing |\n| mask | mask | None | Optional mask for inpainting (white areas will be replaced) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image(s) |\n\n## Features\n\n*   Basic function: Generate images from text prompts\n*   Image editing: When both image and mask parameters are provided, performs image editing (white masked areas will be replaced)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass OpenAIDalle2(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's DALLÂ·E 2 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are shortâ€‘lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for DALLÂ·E\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"256x256\", \"512x512\", \"1024x1024\"],\n                        \"default\": \"1024x1024\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"How many images to generate\",\n                    },\n                ),\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        image=None,\n        mask=None,\n        n=1,\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"dall-e-2\"\n        path = \"/proxy/openai/images/generations\"\n        content_type = \"application/json\"\n        request_class = OpenAIImageGenerationRequest\n        img_binary = None\n\n        if image is not None and mask is not None:\n            path = \"/proxy/openai/images/edits\"\n            content_type = \"multipart/form-data\"\n            request_class = OpenAIImageEditRequest\n\n            input_tensor = image.squeeze().cpu()\n            height, width, channels = input_tensor.shape\n            rgba_tensor = torch.ones(height, width, 4, device=\"cpu\")\n            rgba_tensor[:, :, :channels] = input_tensor\n\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n            rgba_tensor[:, :, 3] = 1 - mask.squeeze().cpu()\n\n            rgba_tensor = downscale_image_tensor(rgba_tensor.unsqueeze(0)).squeeze()\n\n            image_np = (rgba_tensor.numpy() * 255).astype(np.uint8)\n            img = Image.fromarray(image_np)\n            img_byte_arr = io.BytesIO()\n            img.save(img_byte_arr, format=\"PNG\")\n            img_byte_arr.seek(0)\n            img_binary = img_byte_arr  # .getvalue()\n            img_binary.name = \"image.png\"\n        elif image is not None or mask is not None:\n            raise Exception(\"Dall-E 2 image editing requires an image AND a mask\")\n\n        # Build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=path,\n                method=HttpMethod.POST,\n                request_model=request_class,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=request_class(\n                model=model,\n                prompt=prompt,\n                n=n,\n                size=size,\n                seed=seed,\n            ),\n            files=(\n                {\n                    \"image\": img_binary,\n                }\n                if img_binary\n                else None\n            ),\n            content_type=content_type,\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-all-nodes",
  "markdown": "# Retrieve all nodes - ComfyUI\n\n```\n[\n  {\n    \"author\": \"<string>\",\n    \"banner_url\": \"<string>\",\n    \"category\": \"<string>\",\n    \"created_at\": \"2023-11-07T05:31:56Z\",\n    \"description\": \"<string>\",\n    \"downloads\": 123,\n    \"github_stars\": 123,\n    \"icon\": \"<string>\",\n    \"id\": \"<string>\",\n    \"latest_version\": {\n      \"changelog\": \"<string>\",\n      \"comfy_node_extract_status\": \"<string>\",\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"dependencies\": [\n        \"<string>\"\n      ],\n      \"deprecated\": true,\n      \"downloadUrl\": \"<string>\",\n      \"id\": \"<string>\",\n      \"node_id\": \"<string>\",\n      \"status\": \"NodeVersionStatusActive\",\n      \"status_reason\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"version\": \"<string>\"\n    },\n    \"license\": \"<string>\",\n    \"name\": \"<string>\",\n    \"preempted_comfy_node_names\": [\n      \"<string>\"\n    ],\n    \"publisher\": {\n      \"createdAt\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"id\": \"<string>\",\n      \"logo\": \"<string>\",\n      \"members\": [\n        {\n          \"id\": \"<string>\",\n          \"role\": \"<string>\",\n          \"user\": {\n            \"email\": \"<string>\",\n            \"id\": \"<string>\",\n            \"name\": \"<string>\"\n          }\n        }\n      ],\n      \"name\": \"<string>\",\n      \"source_code_repo\": \"<string>\",\n      \"status\": \"PublisherStatusActive\",\n      \"support\": \"<string>\",\n      \"website\": \"<string>\"\n    },\n    \"rating\": 123,\n    \"repository\": \"<string>\",\n    \"search_ranking\": 123,\n    \"status\": \"NodeStatusActive\",\n    \"status_detail\": \"<string>\",\n    \"supported_accelerators\": [\n      \"<string>\"\n    ],\n    \"supported_comfyui_frontend_version\": \"<string>\",\n    \"supported_comfyui_version\": \"<string>\",\n    \"supported_os\": [\n      \"<string>\"\n    ],\n    \"tags\": [\n      \"<string>\"\n    ],\n    \"translations\": {}\n  }\n]\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/create-a-new-custom-node",
  "markdown": "# Create a new custom node\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n#### Response\n\nNode created successfully\n\nThe response is of type `object`."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb",
  "markdown": "# Recraft Color RGB - ComfyUI Native Node Documentation\n\n ![ComfyUI Native Recraft Color RGB Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg) The Recraft Color RGB node lets you define precise RGB color values to control colors in Recraft image generation.\n\n## Node Function\n\nThis node creates a color configuration object that connects to the Recraft Controls node to specify colors used in generated images.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| r   | integer | 0   | Red channel (0-255) |\n| g   | integer | 0   | Green channel (0-255) |\n| b   | integer | 0   | Blue channel (0-255) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_color | Recraft Color | Color config object to connect to Recraft Controls |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftColorRGBNode:\n    \"\"\"\n    Create Recraft Color by choosing specific RGB values.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.COLOR,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    RETURN_NAMES = (\"recraft_color\",)\n    FUNCTION = \"create_color\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"r\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Red value of color.\"\n                }),\n                \"g\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Green value of color.\"\n                }),\n                \"b\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Blue value of color.\"\n                }),\n            },\n            \"optional\": {\n                \"recraft_color\": (RecraftIO.COLOR,),\n            }\n        }\n\n    def create_color(self, r: int, g: int, b: int, recraft_color: RecraftColorChain=None):\n        recraft_color = recraft_color.clone() if recraft_color else RecraftColorChain()\n        recraft_color.add(RecraftColor(r, g, b))\n        return (recraft_color, )\n\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/delete-a-publisher",
  "markdown": "# Delete a publisher - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Response\n\nPublisher deleted successfully\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls",
  "markdown": "# Recraft Controls - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Controls Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg) The Recraft Controls node lets you define control parameters (like colors and background colors) to guide Recraftâ€™s image generation process. This node combines multiple control inputs into a unified control object.\n\n## Parameters\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| colors | Recraft Color | Color controls for image generation |\n| background\\_color | Recraft Color | Background color control |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_controls | Recraft Controls | Control config object for Recraft generation nodes |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## How It Works\n\nNode process:\n\n1.  Collects input control parameters (colors and background\\_color)\n2.  Combines these parameters into a structured control object\n3.  Outputs this control object for connecting to Recraft generation nodes\n\nWhen connected to Recraft generation nodes, these control parameters influence the AI generation process. The AI considers multiple factors beyond just the text promptâ€™s semantic content. If color inputs are configured, the AI will try to use these colors appropriately in the generated image.\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftControlsNode:\n    \"\"\"\n    Create Recraft Controls for customizing Recraft generation.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.CONTROLS,)\n    RETURN_NAMES = (\"recraft_controls\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_controls\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n            },\n            \"optional\": {\n                \"colors\": (RecraftIO.COLOR,),\n                \"background_color\": (RecraftIO.COLOR,),\n            }\n        }\n\n    def create_controls(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None):\n        return (RecraftControls(colors=colors, background_color=background_color), )\n\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/retrieve-all-nodes-1",
  "markdown": "# Retrieve all nodes - ComfyUI\n\n```\n{\n  \"limit\": 123,\n  \"nodes\": [\n    {\n      \"author\": \"<string>\",\n      \"banner_url\": \"<string>\",\n      \"category\": \"<string>\",\n      \"created_at\": \"2023-11-07T05:31:56Z\",\n      \"description\": \"<string>\",\n      \"downloads\": 123,\n      \"github_stars\": 123,\n      \"icon\": \"<string>\",\n      \"id\": \"<string>\",\n      \"latest_version\": {\n        \"changelog\": \"<string>\",\n        \"comfy_node_extract_status\": \"<string>\",\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"dependencies\": [\n          \"<string>\"\n        ],\n        \"deprecated\": true,\n        \"downloadUrl\": \"<string>\",\n        \"id\": \"<string>\",\n        \"node_id\": \"<string>\",\n        \"status\": \"NodeVersionStatusActive\",\n        \"status_reason\": \"<string>\",\n        \"supported_accelerators\": [\n          \"<string>\"\n        ],\n        \"supported_comfyui_frontend_version\": \"<string>\",\n        \"supported_comfyui_version\": \"<string>\",\n        \"supported_os\": [\n          \"<string>\"\n        ],\n        \"version\": \"<string>\"\n      },\n      \"license\": \"<string>\",\n      \"name\": \"<string>\",\n      \"preempted_comfy_node_names\": [\n        \"<string>\"\n      ],\n      \"publisher\": {\n        \"createdAt\": \"2023-11-07T05:31:56Z\",\n        \"description\": \"<string>\",\n        \"id\": \"<string>\",\n        \"logo\": \"<string>\",\n        \"members\": [\n          {\n            \"id\": \"<string>\",\n            \"role\": \"<string>\",\n            \"user\": {\n              \"email\": \"<string>\",\n              \"id\": \"<string>\",\n              \"name\": \"<string>\"\n            }\n          }\n        ],\n        \"name\": \"<string>\",\n        \"source_code_repo\": \"<string>\",\n        \"status\": \"PublisherStatusActive\",\n        \"support\": \"<string>\",\n        \"website\": \"<string>\"\n      },\n      \"rating\": 123,\n      \"repository\": \"<string>\",\n      \"search_ranking\": 123,\n      \"status\": \"NodeStatusActive\",\n      \"status_detail\": \"<string>\",\n      \"supported_accelerators\": [\n        \"<string>\"\n      ],\n      \"supported_comfyui_frontend_version\": \"<string>\",\n      \"supported_comfyui_version\": \"<string>\",\n      \"supported_os\": [\n        \"<string>\"\n      ],\n      \"tags\": [\n        \"<string>\"\n      ],\n      \"translations\": {}\n    }\n  ],\n  \"page\": 123,\n  \"total\": 123,\n  \"totalPages\": 123\n}\n```"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/update-a-specific-node",
  "markdown": "# Update a specific node - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n#### Response\n\nNode updated successfully\n\nThe response is of type `object`."
},
{
  "url": "https://docs.comfy.org/api-reference/registry/delete-a-specific-node",
  "markdown": "# Delete a specific node - ComfyUI\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Response\n\nNode deleted successfully"
},
{
  "url": "https://docs.comfy.org/api-reference/registry/claim-nodeid-into-publisherid-for-the-authenticated-publisher",
  "markdown": "# Claim nodeId into publisherId for the authenticated publisher\n\n#### Authorizations\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n#### Body\n\n#### Response\n\nNode claimed successfully\n\nClaim nodeId into publisherId for the authenticated publisher"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fcreate-text-to-video-prompt",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/installation/desktop/windows",
  "markdown": "# Windows Desktop Version - ComfyUI\n\n**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files. You can quickly migrate from an existing [ComfyUI Portable version](https://docs.comfy.org/installation/comfyui_portable_windows) to the Desktop version. ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop) ComfyUI Desktop hardware requirements:\n\n*   NVIDIA GPU\n\nThis tutorial will guide you through the software installation process and explain related configuration details.\n\n## ComfyUI Desktop (Windows) Download\n\nPlease click the button below to download the installation package for Windows **ComfyUI Desktop**\n\n[\n\nDownload for Windows (NVIDIA)\n\n](https://download.comfy.org/windows/nsis/x64)\n\n## ComfyUI Desktop Installation Steps\n\nDouble-click the downloaded installation package file, which will first perform an automatic installation and create a **ComfyUI Desktop** shortcut on the desktop ![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-shortcut.jpg) Double-click the corresponding shortcut to enter ComfyUI initialization settings\n\n### ComfyUI Desktop Initialization Process\n\n## First Image Generation\n\nAfter successful installation, you can refer to the section below to start your ComfyUI journey~\n\n[\n\n## First Image Generation\n\nThis tutorial will guide you through your first model installation and text-to-image generation\n\n\n\n](https://docs.comfy.org/get_started/first_generation)\n\n## How to Update ComfyUI Desktop\n\nCurrently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings ![ComfyUI Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg) You can also choose to manually check for available updates in the `Menu` â€”> `Help` â€”> `Check for Updates` ![ComfyUI Desktop Check for Updates](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_check_for_updates.jpg)\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUIâ€™s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ lora/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## Desktop Python Environment\n\nThe desktop installation will create a Python virtual environment in your chosen installation directory, typically a hidden `.venv` folder. If you need to handle dependencies for ComfyUI plugins, youâ€™ll need to do so within this environment. Using the system command line directly risks installing dependencies to the system environment, so please follow the instructions below to activate the appropriate environment.\n\n### How to use the Desktop Python environment?\n\nYou can use the built-in terminal in the desktop app to access the Python environment. ![ComfyUI Desktop Terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_terminal.jpg) \n\n1.  Click the icon in the menu bar to open the bottom panel\n2.  Click `Terminal` to open the terminal\n3.  If you want to check the Python installation location for the corresponding environment, you can use the following command\n\n```\n  python -c \"import sys; print(sys.executable)\"\n```\n\n## How to Uninstall ComfyUI Desktop\n\nFor **ComfyUI Desktop** you can use the system uninstall function in Windows Settings to complete software uninstallation ![ComfyUI Desktop Uninstallation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-uninstall-comfyui.jpg) If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:\n\n*   C:\\\\Users<YOUR\\_USERNAME>\\\\AppData\\\\Local@comfyorgcomfyui-electron-updater\n*   C:\\\\Users<YOUR\\_USERNAME>\\\\AppData\\\\Local\\\\Programs@comfyorgcomfyui-electron\n*   C:\\\\Users<YOUR\\_USERNAME>\\\\AppData\\\\Roaming\\\\ComfyUI\n\nThe above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:\n\n*   models files\n*   custom nodes\n*   input/output directories\n\n## Troubleshooting\n\n### Display unsupported devices\n\n![ComfyUI Installation Steps - Unsupported Device](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-0.jpg) Since ComfyUI Desktop (Windows) only supports **NVIDIA GPUs with CUDA**, you may see this screen if your device is not supported\n\n*   Please switch to a supported device\n*   Or consider using [ComfyUI Portable](https://docs.comfy.org/installation/comfyui_portable_windows) or through [manual installation](https://docs.comfy.org/installation/manual_install) to use ComfyUI\n\n### â€‹Error identificationâ€‹\n\nIf installation fails, you should see the following screen ![ComfyUI Installation Failed](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg) It is recommended to take these steps to find the error cause:\n\n1.  Click `Show Terminal` to view error output\n2.  Click `Open Logs` to view installation logs\n3.  Visit official forum to search for error reports\n4.  Click `Reinstall` to try reinstalling\n\nBefore submitting feedback, itâ€™s recommended to provide the **error output** and **log files** to tools like **GPT** ![ComfyUI Installation Failed - Error Log](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg) ![ComfyUI Installation Failed - GPT Feedback](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg) As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.\n\n### Feedback Installation Failure\n\nIf you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:\n\n*   Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)\n*   Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)\n\nWhen submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:\n\n1.  Log Files\n\n| Filename | Description | Location |\n| --- | --- | --- |\n| main.log | Contains logs related to desktop application and server startup from the Electron process |     |\n| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output |     |\n\n![ComfyUI Log Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)\n\n2.  Configuration Files\n\n| Filename | Description | Location |\n| --- | --- | --- |\n| extra\\_model\\_paths.yaml | Contains additional paths where ComfyUI will search for models and custom nodes |     |\n| config.json | Contains application configuration. This file should not be edited directly |     |\n\n![ComfyUI Config Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fcreate-image-to-video-prompt",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/installation/comfyui_portable_windows",
  "markdown": "# ComfyUI(portable) Windows - ComfyUI\n\n**ComfyUI Portable** is a standalone packaged complete ComfyUI Windows version that has integrated an independent **Python (python\\_embeded)** required for ComfyUI to run. You only need to extract it to use it. Currently, the portable version supports running through **Nvidia GPU** or **CPU**. This guide section will walk you through installing ComfyUI Portable.\n\n## Download ComfyUI Portable\n\nYou can get the latest ComfyUI Portable download link by clicking the link below\n\n[\n\nDownload ComfyUI Portable\n\n](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)\n\nAfter downloading, you can use decompression software like [7-ZIP](https://7-zip.org/) to extract the compressed package The file structure and description after extracting the portable version are as follows:\n\n```\nComfyUI_windows_portable\nâ”œâ”€â”€ ðŸ“‚ComfyUI                   // ComfyUI main program\nâ”œâ”€â”€ ðŸ“‚python_embeded            // Independent Python environment\nâ”œâ”€â”€ ðŸ“‚update                    // Batch scripts for upgrading portable version\nâ”œâ”€â”€ README_VERY_IMPORTANT.txt   // ComfyUI Portable usage instructions in English\nâ”œâ”€â”€ run_cpu.bat                 // Double click to start ComfyUI (CPU only)\nâ””â”€â”€ run_nvidia_gpu.bat          // Double click to start ComfyUI (Nvidia GPU)\n```\n\n## How to Launch ComfyUI\n\nDouble click either `run_nvidia_gpu.bat` or `run_cpu.bat` depending on your computerâ€™s configuration to launch ComfyUI. You will see the command running as shown in the image below ![ComfyUI Portable Command Prompt](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui-portable-cmd.png) When you see something similar to the image\n\n```\nTo see the GUI go to: http://127.0.0.1:8188\n```\n\nAt this point, your ComfyUI service has started. Normally, ComfyUI will automatically open your default browser and navigate to `http://127.0.0.1:8188`. If it doesnâ€™t open automatically, please manually open your browser and visit this address.\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUIâ€™s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ lora/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## First Image Generation\n\nAfter successful installation, you can refer to the section below to start your ComfyUI journey~\n\n[\n\n## First Image Generation\n\nThis tutorial will guide you through your first model installation and text-to-image generation\n\n\n\n](https://docs.comfy.org/get_started/first_generation)\n\n## Additional ComfyUI Portable Instructions\n\n### 1\\. Upgrading ComfyUI Portable\n\nYou can use the batch commands in the update folder to upgrade your ComfyUI Portable version\n\n```\nComfyUI_windows_portable\nâ””â”€ ðŸ“‚update\n   â”œâ”€â”€ update.py\n   â”œâ”€â”€ update_comfyui.bat                          // Update ComfyUI to the latest commit version\n   â”œâ”€â”€ update_comfyui_and_python_dependencies.bat  // Only use when you have issues with your runtime environment\n   â””â”€â”€ update_comfyui_stable.bat                   // Update ComfyUI to the latest stable version\n```\n\n### 2\\. Setting Up LAN Access for ComfyUI Portable\n\nIf your ComfyUI is running on a local network and you want other devices to access ComfyUI, you can modify the `run_nvidia_gpu.bat` or `run_cpu.bat` file using Notepad to complete the configuration. This is mainly done by adding `--listen` to specify the listening address. Below is an example of the `run_nvidia_gpu.bat` file command with the `--listen` parameter added\n\n```\n.\\python_embeded\\python.exe -s ComfyUI\\main.py --listen --windows-standalone-build\npause\n```\n\nAfter enabling ComfyUI, you will notice the final running address will become\n\n```\nStarting server\n\nTo see the GUI go to: http://0.0.0.0:8188\nTo see the GUI go to: http://[::]:8188\n```\n\nYou can press `WIN + R` and type `cmd` to open the command prompt, then enter `ipconfig` to view your local IP address. Other devices can then access ComfyUI by entering `http://your-local-IP:8188` in their browser."
},
{
  "url": "https://docs.comfy.org/installation/manual_install",
  "markdown": "# How to install ComfyUI manually in different systems\n\nFor the installation of ComfyUI, it is mainly divided into several steps:\n\n1.  Create a virtual environment(avoid polluting the system-level Python environment)\n2.  Clone the ComfyUI code repository\n3.  Install dependencies\n4.  Start ComfyUI\n\nYou can also refer to [ComfyUI CLI](https://docs.comfy.org/comfy-cli/getting-started) to install ComfyUI, it is a command line tool that can easily install ComfyUI and manage its dependencies.\n\n## Create a virtual environment\n\n[Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI. Create an environment with Conda.\n\n```\nconda create -n comfyenv\nconda activate comfyenv\n```\n\n## Clone the ComfyUI code repository\n\nYou need to ensure that you have installed [Git](https://git-scm.com/downloads) on your system. First, you need to open the terminal (command line), then clone the code repository.\n\n```\ngit clone git@github.com:comfyanonymous/ComfyUI.git\n```\n\n## Install GPU and ComfyUI dependencies\n\n## How to update ComfyUI\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUIâ€™s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ lora/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```"
},
{
  "url": "https://docs.comfy.org/installation/install_custom_node",
  "markdown": "# How to Install Custom Nodes in ComfyUI\n\n## What are Custom Nodes ?\n\nCustom nodes are extensions for ComfyUI that add new functionality like advanced image processing, machine learning fine-tuning, color adjustments, and more. These community-developed nodes can significantly expand ComfyUIâ€™s core capabilities.\n\nAll custom node installations require completing these two steps:\n\n1.  Clone the node code to the `ComfyUI/custom_nodes` directory\n2.  Install the required Python dependencies\n\nThis guide covers three installation methods. Hereâ€™s a comparison of their pros and cons. While [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) isnâ€™t yet part of the core dependencies, it will be in the future. We still provide other installation guides to meet different needs.\n\n| Method | Advantages | Disadvantages |\n| --- | --- | --- |\n| **ComfyUI Manager** (Recommended) | 1\\. Automated installation  <br>2\\. Dependency handling  <br>3\\. GUI interface | Cannot directly search for nodes not registered in the registry |\n| **Git Clone** | Can install nodes not registered in the registry | 1\\. Requires Git knowledge  <br>2\\. Manual dependency handling  <br>3\\. Installation risks |\n| **Repository ZIP Download** | 1\\. No Git required  <br>2\\. Manual control | 1\\. Manual dependency handling  <br>2\\. No version control  <br>3\\. Installation risks |\n\nTip: Before installing custom nodes, check the pluginâ€™s README file to understand installation methods, usage, and requirements like specific models, dependency versions, and common issue solutions.\n\n## Method 1: ComfyUI Manager (Recommended)\n\n## Method 2: Manual Installation Using Git\n\nSuitable for new nodes not found in Manager or when specific versions are needed. Requires [Git](https://git-scm.com/) installed on your system.\n\n## Method 3: ZIP Download Installation\n\nSuitable for users who cannot use Git or Manager"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fchangelog",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image",
  "markdown": "# Stability AI Stable Diffusion 3.5 - ComfyUI Native Node Documentation\n\n![ComfyUI Native Stability AI Stable Diffusion 3.5 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg) The Stability AI Stable Diffusion 3.5 Image node uses Stability AIâ€™s Stable Diffusion 3.5 API to generate high-quality images. It supports both text-to-image and image-to-image generation, capable of creating detailed visual content from text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | What you want to see in the output image. Strong, descriptive prompts that clearly define elements, colors and themes will yield better results |\n| model | select | \\-  | Choose which Stability SD 3.5 model to use |\n| aspect\\_ratio | select | â€1:1â€ | Width to height ratio of generated image |\n| style\\_preset | select | â€Noneâ€ | Optional preset style for the desired image |\n| cfg\\_scale | float | 4.0 | How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt). Range: 1.0 - 10.0, Step: 0.1 |\n| seed | integer | 0   | Random seed for noise generation (0-4294967294) |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image. When provided, the node switches to image-to-image mode |\n| negative\\_prompt | string | \"\"  | Keywords of what you donâ€™t want to see in the output image. This is an advanced feature |\n| image\\_denoise | float | 0.5 | Denoising strength for input image. 0.0 yields image identical to input, 1.0 is as if no image was provided at all. Range: 0.0 - 1.0, Step: 0.01. Only effective when image is provided |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image |\n\n## Usage Example\n\n[\n\n## Stability AI Stable Diffusion 3.5 Image Workflow Example\n\nStability AI Stable Diffusion 3.5 Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)\n\n## Notes\n\n*   When an input image is provided, the node switches from text-to-image mode to image-to-image mode\n*   In image-to-image mode, aspect ratio parameters are ignored\n*   Mode selection automatically switches based on whether an image is provided:\n    *   No image provided: text-to-image mode\n    *   Image provided: image-to-image mode\n*   If style\\_preset is set to â€œNoneâ€, no preset style will be applied\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-07)\\]\n\n```\nclass StabilityStableImageSD_3_5Node:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Stability AI\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results.\"\n                    },\n                ),\n                \"model\": ([x.value for x in Stability_SD3_5_Model],),\n                \"aspect_ratio\": ([x.value for x in StabilityAspectRatio],\n                    {\n                        \"default\": StabilityAspectRatio.ratio_1_1,\n                        \"tooltip\": \"Aspect ratio of generated image.\",\n                    },\n                ),\n                \"style_preset\": (get_stability_style_presets(),\n                    {\n                        \"tooltip\": \"Optional desired style of generated image.\",\n                    },\n                ),\n                \"cfg_scale\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 4.0,\n                        \"min\": 1.0,\n                        \"max\": 10.0,\n                        \"step\": 0.1,\n                        \"tooltip\": \"How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 4294967294,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (IO.IMAGE,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"Keywords of what you do not wish to see in the output image. This is an advanced feature.\"\n                    },\n                ),\n                \"image_denoise\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(self, model: str, prompt: str, aspect_ratio: str, style_preset: str, seed: int, cfg_scale: float,\n                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,\n                 auth_token=None):\n        validate_string(prompt, strip_whitespace=False)\n        # prepare image binary if image present\n        image_binary = None\n        mode = Stability_SD3_5_GenerationMode.text_to_image\n        if image is not None:\n            image_binary = tensor_to_bytesio(image, total_pixels=1504*1504).read()\n            mode = Stability_SD3_5_GenerationMode.image_to_image\n            aspect_ratio = None\n        else:\n            image_denoise = None\n\n        if not negative_prompt:\n            negative_prompt = None\n        if style_preset == \"None\":\n            style_preset = None\n\n        files = {\n            \"image\": image_binary\n        }\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/stability/v2beta/stable-image/generate/sd3\",\n                method=HttpMethod.POST,\n                request_model=StabilityStable3_5Request,\n                response_model=StabilityStableUltraResponse,\n            ),\n            request=StabilityStable3_5Request(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                aspect_ratio=aspect_ratio,\n                seed=seed,\n                strength=image_denoise,\n                style_preset=style_preset,\n                cfg_scale=cfg_scale,\n                model=model,\n                mode=mode,\n            ),\n            files=files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.finish_reason != \"SUCCESS\":\n            raise Exception(f\"Stable Diffusion 3.5 Image generation failed: {response_api.finish_reason}.\")\n\n        image_data = base64.b64decode(response_api.image)\n        returned_image = bytesio_to_image_tensor(BytesIO(image_data))\n\n        return (returned_image,)\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fnodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fworkflow",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/custom-nodes/overview",
  "markdown": "# Overview - ComfyUI\n\nCustom nodes allow you to implement new features and share them with the wider community. A custom node is like any Comfy node: it takes input, does something to it, and produces an output. While some custom nodes perform highly complex tasks, many just do one thing. Hereâ€™s an example of a simple node that takes an image and inverts it. ![Unique Images Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/invert_image_node.png)\n\n## Client-Server Model\n\nComfy runs in a client-server model. The server, written in Python, handles all the real work: data-processing, models, image diffusion etc. The client, written in Javascript, handles the user interface. Comfy can also be used in API mode, in which a workflow is sent to the server by a non-Comfy client (such as another UI, or a command line script). Custom nodes can be placed into one of four categories:\n\n### Server side only\n\nThe majority of Custom Nodes run purely on the server side, by defining a Python class that specifies the input and output types, and provides a function that can be called to process inputs and produce an output.\n\n### Client side only\n\nA few Custom Nodes provide a modification to the client UI, but do not add core functionality. Despite the name, they may not even add new nodes to the system.\n\n### Independent Client and Server\n\nCustom nodes may provide additional server features, and additional (related) UI features (such as a new widget to deal with a new data type). In most cases, communication between the client and server can be handled by the Comfy data flow control.\n\n### Connected Client and Server\n\nIn a small number of cases, the UI features and the server need to interact with each other directly."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video",
  "markdown": "# Google Veo2 Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native Google Veo2 Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg) The Google Veo2 Video node generates high-quality videos from text descriptions using Googleâ€™s Veo2 API technology, converting text prompts into dynamic video content.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text description of the video content to generate |\n| aspect\\_ratio | select | â€16:9â€ | Output video aspect ratio, â€œ16:9â€ or â€œ9:16â€ |\n| negative\\_prompt | string | \"\"  | Text describing what to avoid in the video |\n| duration\\_seconds | integer | 5   | Video duration, 5-8 seconds |\n| enhance\\_prompt | boolean | True | Whether to use AI to enhance the prompt |\n| person\\_generation | select | â€ALLOWâ€ | Allow or block person generation, â€œALLOWâ€ or â€œBLOCKâ€ |\n| seed | integer | 0   | Random seed, 0 means randomly generated |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | None | Optional reference image to guide video creation |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass VeoVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos from text prompts using Google's Veo API.\n\n    This node can create videos from text descriptions and optional image inputs,\n    with control over parameters like aspect ratio, duration, and more.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text description of the video\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"16:9\", \"9:16\"],\n                        \"default\": \"16:9\",\n                        \"tooltip\": \"Aspect ratio of the output video\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Negative text prompt to guide what to avoid in the video\",\n                    },\n                ),\n                \"duration_seconds\": (\n                    IO.INT,\n                    {\n                        \"default\": 5,\n                        \"min\": 5,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"Duration of the output video in seconds\",\n                    },\n                ),\n                \"enhance_prompt\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": True,\n                        \"tooltip\": \"Whether to enhance the prompt with AI assistance\",\n                    }\n                ),\n                \"person_generation\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"ALLOW\", \"BLOCK\"],\n                        \"default\": \"ALLOW\",\n                        \"tooltip\": \"Whether to allow generating people in the video\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFF,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation (0 for random)\",\n                    },\n                ),\n                \"image\": (IO.IMAGE, {\n                    \"default\": None,\n                    \"tooltip\": \"Optional reference image to guide video generation\",\n                }),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (IO.VIDEO,)\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Veo\"\n    DESCRIPTION = \"Generates videos from text prompts using Google's Veo API\"\n    API_NODE = True\n\n    def generate_video(\n        self,\n        prompt,\n        aspect_ratio=\"16:9\",\n        negative_prompt=\"\",\n        duration_seconds=5,\n        enhance_prompt=True,\n        person_generation=\"ALLOW\",\n        seed=0,\n        image=None,\n        auth_token=None,\n    ):\n        # Prepare the instances for the request\n        instances = []\n\n        instance = {\n            \"prompt\": prompt\n        }\n\n        # Add image if provided\n        if image is not None:\n            image_base64 = convert_image_to_base64(image)\n            if image_base64:\n                instance[\"image\"] = {\n                    \"bytesBase64Encoded\": image_base64,\n                    \"mimeType\": \"image/png\"\n                }\n\n        instances.append(instance)\n\n        # Create parameters dictionary\n        parameters = {\n            \"aspectRatio\": aspect_ratio,\n            \"personGeneration\": person_generation,\n            \"durationSeconds\": duration_seconds,\n            \"enhancePrompt\": enhance_prompt,\n        }\n\n        # Add optional parameters if provided\n        if negative_prompt:\n            parameters[\"negativePrompt\"] = negative_prompt\n        if seed > 0:\n            parameters[\"seed\"] = seed\n\n        # Initial request to start video generation\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/veo/generate\",\n                method=HttpMethod.POST,\n                request_model=Veo2GenVidRequest,\n                response_model=Veo2GenVidResponse\n            ),\n            request=Veo2GenVidRequest(\n                instances=instances,\n                parameters=parameters\n            ),\n            auth_token=auth_token\n        )\n\n        initial_response = initial_operation.execute()\n        operation_name = initial_response.name\n\n        logging.info(f\"Veo generation started with operation name: {operation_name}\")\n\n        # Define status extractor function\n        def status_extractor(response):\n            # Only return \"completed\" if the operation is done, regardless of success or failure\n            # We'll check for errors after polling completes\n            return \"completed\" if response.done else \"pending\"\n\n        # Define progress extractor function\n        def progress_extractor(response):\n            # Could be enhanced if the API provides progress information\n            return None\n\n        # Define the polling operation\n        poll_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=\"/proxy/veo/poll\",\n                method=HttpMethod.POST,\n                request_model=Veo2GenVidPollRequest,\n                response_model=Veo2GenVidPollResponse\n            ),\n            completed_statuses=[\"completed\"],\n            failed_statuses=[],  # No failed statuses, we'll handle errors after polling\n            status_extractor=status_extractor,\n            progress_extractor=progress_extractor,\n            request=Veo2GenVidPollRequest(\n                operationName=operation_name\n            ),\n            auth_token=auth_token,\n            poll_interval=5.0\n        )\n\n        # Execute the polling operation\n        poll_response = poll_operation.execute()\n\n        # Now check for errors in the final response\n        # Check for error in poll response\n        if hasattr(poll_response, 'error') and poll_response.error:\n            error_message = f\"Veo API error: {poll_response.error.message} (code: {poll_response.error.code})\"\n            logging.error(error_message)\n            raise Exception(error_message)\n\n        # Check for RAI filtered content\n        if (hasattr(poll_response.response, 'raiMediaFilteredCount') and\n            poll_response.response.raiMediaFilteredCount > 0):\n\n            # Extract reason message if available\n            if (hasattr(poll_response.response, 'raiMediaFilteredReasons') and\n                poll_response.response.raiMediaFilteredReasons):\n                reason = poll_response.response.raiMediaFilteredReasons[0]\n                error_message = f\"Content filtered by Google's Responsible AI practices: {reason} ({poll_response.response.raiMediaFilteredCount} videos filtered.)\"\n            else:\n                error_message = f\"Content filtered by Google's Responsible AI practices ({poll_response.response.raiMediaFilteredCount} videos filtered.)\"\n\n            logging.error(error_message)\n            raise Exception(error_message)\n\n        # Extract video data\n        video_data = None\n        if poll_response.response and hasattr(poll_response.response, 'videos') and poll_response.response.videos and len(poll_response.response.videos) > 0:\n            video = poll_response.response.videos[0]\n\n            # Check if video is provided as base64 or URL\n            if hasattr(video, 'bytesBase64Encoded') and video.bytesBase64Encoded:\n                # Decode base64 string to bytes\n                video_data = base64.b64decode(video.bytesBase64Encoded)\n            elif hasattr(video, 'gcsUri') and video.gcsUri:\n                # Download from URL\n                video_url = video.gcsUri\n                video_response = requests.get(video_url)\n                video_data = video_response.content\n            else:\n                raise Exception(\"Video returned but no data or URL was provided\")\n        else:\n            raise Exception(\"Video generation completed but no video was returned\")\n\n        if not video_data:\n            raise Exception(\"No video data was returned\")\n\n        logging.info(\"Video generation completed successfully\")\n\n        # Convert video data to BytesIO object\n        video_io = io.BytesIO(video_data)\n\n        # Return VideoFromFile object\n        return (VideoFromFile(video_io),)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v",
  "markdown": "# Kling Text to Video (Camera Control) - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Text to Video (Camera Control) Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg) The Kling Text to Video (Camera Control) node converts text into videos with professional camera movements. It extends the standard Kling Text to Video node by adding camera control capabilities.\n\n```\n\nclass KlingCameraControlT2VNode(KlingTextToVideoNode):\n    \"\"\"\n    Kling Text to Video Camera Control Node. This node is a text to video node, but it supports controlling the camera.\n    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingText2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingText2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"camera_control\": (\n                    \"CAMERA_CONTROL\",\n                    {\n                        \"tooltip\": \"Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Transform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text.\"\n\n    def api_call(\n        self,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        camera_control: Optional[CameraControl] = None,\n        auth_token: Optional[str] = None,\n    ):\n        return super().api_call(\n            model_name=\"kling-v1-5\",\n            cfg_scale=cfg_scale,\n            mode=\"pro\",\n            aspect_ratio=aspect_ratio,\n            duration=\"5\",\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            camera_control=camera_control,\n            auth_token=auth_token,\n        )\n\n\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls",
  "markdown": "# Kling Camera Controls - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Kling Camera Controls Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg) The Kling Camera Controls node defines virtual camera behavior parameters to control camera movement and view changes during Kling video generation.\n\n**Note**: At least one non-zero camera control parameter is required for the effect to work.\n\n**Note**: Not all model and mode combinations support camera control. Please check the Kling API documentation for details.\n\n```\n\nclass KlingCameraControls(KlingNodeBase):\n    \"\"\"Kling Camera Controls Node\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"camera_control_type\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\n                            camera_control_type.value\n                            for camera_control_type in CameraType\n                        ],\n                        \"default\": \"simple\",\n                        \"tooltip\": \"Predefined camera movements type. simple: Customizable camera movement. down_back: Camera descends and moves backward. forward_up: Camera moves forward and tilts up. right_turn_forward: Rotate right and move forward. left_turn_forward: Rotate left and move forward.\",\n                    },\n                ),\n                \"horizontal_movement\": get_camera_control_input_config(\n                    \"Controls camera's movement along horizontal axis (x-axis). Negative indicates left, positive indicates right\"\n                ),\n                \"vertical_movement\": get_camera_control_input_config(\n                    \"Controls camera's movement along vertical axis (y-axis). Negative indicates downward, positive indicates upward.\"\n                ),\n                \"pan\": get_camera_control_input_config(\n                    \"Controls camera's rotation in vertical plane (x-axis). Negative indicates downward rotation, positive indicates upward rotation.\",\n                    default=0.5,\n                ),\n                \"tilt\": get_camera_control_input_config(\n                    \"Controls camera's rotation in horizontal plane (y-axis). Negative indicates left rotation, positive indicates right rotation.\",\n                ),\n                \"roll\": get_camera_control_input_config(\n                    \"Controls camera's rolling amount (z-axis). Negative indicates counterclockwise, positive indicates clockwise.\",\n                ),\n                \"zoom\": get_camera_control_input_config(\n                    \"Controls change in camera's focal length. Negative indicates narrower field of view, positive indicates wider field of view.\",\n                ),\n            }\n        }\n\n    DESCRIPTION = \"Kling Camera Controls Node. Not all model and mode combinations support camera control. Please refer to the Kling API documentation for more information.\"\n    RETURN_TYPES = (\"CAMERA_CONTROL\",)\n    RETURN_NAMES = (\"camera_control\",)\n    FUNCTION = \"main\"\n\n    @classmethod\n    def VALIDATE_INPUTS(\n        cls,\n        horizontal_movement: float,\n        vertical_movement: float,\n        pan: float,\n        tilt: float,\n        roll: float,\n        zoom: float,\n    ) -> bool | str:\n        if not is_valid_camera_control_configs(\n            [\n                horizontal_movement,\n                vertical_movement,\n                pan,\n                tilt,\n                roll,\n                zoom,\n            ]\n        ):\n            return \"Invalid camera control configs: at least one of the values must be non-zero\"\n        return True\n\n    def main(\n        self,\n        camera_control_type: str,\n        horizontal_movement: float,\n        vertical_movement: float,\n        pan: float,\n        tilt: float,\n        roll: float,\n        zoom: float,\n    ) -> tuple[CameraControl]:\n        return (\n            CameraControl(\n                type=CameraType(camera_control_type),\n                config=CameraConfig(\n                    horizontal=horizontal_movement,\n                    vertical=vertical_movement,\n                    pan=pan,\n                    roll=roll,\n                    tilt=tilt,\n                    zoom=zoom,\n                ),\n            ),\n        )\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video",
  "markdown": "# Kling Image to Video - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg) The Kling Image to Video node converts static images into dynamic video content using Klingâ€™s image-to-video API.\n\n## Parameters\n\n### Basic Parameters\n\nAll parameters below are required:\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| start\\_frame | Image | \\-  | Input source image |\n| prompt | String | \"\"  | Text prompt describing video action and content |\n| negative\\_prompt | String | \"\"  | Elements to avoid in the video |\n| cfg\\_scale | Float | 7.0 | Controls how closely to follow the prompt |\n| model\\_name | Select | â€kling-v1-5â€ | Model type to use |\n| aspect\\_ratio | Select | â€16:9â€ | Output video aspect ratio |\n| duration | Select | â€5sâ€ | Generated video duration |\n| mode | Select | â€proâ€ | Video generation mode |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n| video\\_id | String | Unique video identifier |\n| duration | String | Actual video duration |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass KlingImage2VideoNode(KlingNodeBase):\n    \"\"\"Kling Image to Video Node\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"model_name\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"model_name\",\n                    enum_type=KlingVideoGenModelName,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"mode\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"mode\",\n                    enum_type=KlingVideoGenMode,\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=KlingVideoGenAspectRatio,\n                ),\n                \"duration\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"duration\",\n                    enum_type=KlingVideoGenDuration,\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (\"VIDEO\", \"STRING\", \"STRING\")\n    RETURN_NAMES = (\"VIDEO\", \"video_id\", \"duration\")\n    DESCRIPTION = \"Kling Image to Video Node\"\n\n    def get_response(self, task_id: str, auth_token: str) -> KlingImage2VideoResponse:\n        return poll_until_finished(\n            auth_token,\n            ApiEndpoint(\n                path=f\"{PATH_IMAGE_TO_VIDEO}/{task_id}\",\n                method=HttpMethod.GET,\n                request_model=KlingImage2VideoRequest,\n                response_model=KlingImage2VideoResponse,\n            ),\n        )\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        model_name: str,\n        cfg_scale: float,\n        mode: str,\n        aspect_ratio: str,\n        duration: str,\n        camera_control: Optional[KlingCameraControl] = None,\n        end_frame: Optional[torch.Tensor] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_I2V)\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_IMAGE_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=KlingImage2VideoRequest,\n                response_model=KlingImage2VideoResponse,\n            ),\n            request=KlingImage2VideoRequest(\n                model_name=KlingVideoGenModelName(model_name),\n                image=tensor_to_base64_string(start_frame),\n                image_tail=(\n                    tensor_to_base64_string(end_frame)\n                    if end_frame is not None\n                    else None\n                ),\n                prompt=prompt,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                cfg_scale=cfg_scale,\n                mode=KlingVideoGenMode(mode),\n                aspect_ratio=KlingVideoGenAspectRatio(aspect_ratio),\n                duration=KlingVideoGenDuration(duration),\n                camera_control=camera_control,\n            ),\n            auth_token=auth_token,\n        )\n\n        task_creation_response = initial_operation.execute()\n        validate_task_creation_response(task_creation_response)\n        task_id = task_creation_response.data.task_id\n\n        final_response = self.get_response(task_id, auth_token)\n        validate_video_result_response(final_response)\n\n        video = get_video_from_response(final_response)\n        return video_result_to_node_output(video)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts",
  "markdown": "# Luma Concepts - ComfyUI Native Node Documentation\n\n![ComfyUI Native Luma Concepts Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg) The Luma Concepts node allows you to apply predefined camera concepts to the Luma generation process, providing precise control over camera angles and perspectives without complex prompt descriptions.\n\n## Node Function\n\nThis node serves as a helper tool for Luma generation nodes, enabling users to select and apply predefined camera concepts. These concepts include different shooting angles (like overhead or low angle), camera distances (like close-up or long shot), and movement styles (like push-in or follow). It simplifies the creative workflow by providing an intuitive way to control camera effects in the generated output.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| concept1 | select | First camera concept choice, includes various presets and â€œnoneâ€ |\n| concept2 | select | Second camera concept choice, includes various presets and â€œnoneâ€ |\n| concept3 | select | Third camera concept choice, includes various presets and â€œnoneâ€ |\n| concept4 | select | Fourth camera concept choice, includes various presets and â€œnoneâ€ |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPTS | Optional Camera Concepts to merge with selected concepts |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPT | Combined object containing all selected concepts |\n\n## Usage Examples\n\n[\n\n## Luma Text to Video Workflow Example\n\nLuma Text to Video Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)[\n\n## Luma Image to Video Workflow Example\n\nLuma Image to Video Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)\n\n## How It Works\n\nThe Luma Concepts node offers a variety of predefined camera concepts including:\n\n*   Camera distances (close-up, medium shot, long shot)\n*   View angles (eye level, overhead, low angle)\n*   Movement types (push-in, follow, orbit)\n*   Special effects (handheld, stabilized, floating)\n\nUsers can select up to 4 concepts to use together. The node creates an object containing the selected camera concepts, which is then passed to Luma generation nodes. During generation, Luma AI uses these camera concepts to influence the viewpoint and composition of the output, ensuring the results reflect the chosen photographic effects. By combining multiple camera concepts, users can create complex camera guidance without writing detailed prompt descriptions. This is particularly useful when specific camera angles or compositions are needed.\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass LumaConceptsNode(ComfyNodeABC):\n    \"\"\"\n    Holds one or more Camera Concepts for use with Luma Text to Video and Luma Image to Video nodes.\n    \"\"\"\n\n    RETURN_TYPES = (LumaIO.LUMA_CONCEPTS,)\n    RETURN_NAMES = (\"luma_concepts\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_concepts\"\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"concept1\": (get_luma_concepts(include_none=True),),\n                \"concept2\": (get_luma_concepts(include_none=True),),\n                \"concept3\": (get_luma_concepts(include_none=True),),\n                \"concept4\": (get_luma_concepts(include_none=True),),\n            },\n            \"optional\": {\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to add to the ones chosen here.\"\n                    },\n                ),\n            },\n        }\n\n    def create_concepts(\n        self,\n        concept1: str,\n        concept2: str,\n        concept3: str,\n        concept4: str,\n        luma_concepts: LumaConceptChain = None,\n    ):\n        chain = LumaConceptChain(str_list=[concept1, concept2, concept3, concept4])\n        if luma_concepts is not None:\n            chain = luma_concepts.clone_and_merge(chain)\n        return (chain,)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fcustom-nodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fproperties",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video",
  "markdown": "# Luma Image to Video - ComfyUI Native API Node Documentation\n\n![ComfyUI Native Luma Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg) The Luma Image to Video node uses Luma AIâ€™s technology to transform static images into smooth, dynamic videos, bringing your images to life.\n\n## Node Function\n\nThis node connects to Luma AIâ€™s image-to-video API, allowing users to create dynamic videos from input images. It understands the image content and generates natural, coherent motion while maintaining the original visual style. Combined with text prompts, users can precisely control the videoâ€™s dynamic effects.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing video motion and content |\n| model | select | \\-  | Video generation model to use |\n| resolution | select | â€540pâ€ | Output video resolution |\n| duration | select | \\-  | Video length options |\n| loop | boolean | False | Whether to loop the video |\n| seed | integer | 0   | Seed value for node rerun, results are nondeterministic |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| first\\_image | image | First frame of video (required if no last\\_image) |\n| last\\_image | image | Last frame of video (required if no first\\_image) |\n| luma\\_concepts | LUMA\\_CONCEPTS | Concepts for controlling camera motion and shot style |\n\n### Requirements\n\n*   Either **first\\_image** or **last\\_image** must be provided\n*   Each image input (first\\_image and last\\_image) accepts only 1 image\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Usage Example\n\n[\n\n## Luma Image to Video Workflow Example\n\nLuma Image to Video Workflow Tutorial\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass LumaImageToVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt, input images, and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaVideoModel],),\n                # \"aspect_ratio\": ([ratio.value for ratio in LumaAspectRatio], {\n                #     \"default\": LumaAspectRatio.ratio_16_9,\n                # }),\n                \"resolution\": (\n                    [resolution.value for resolution in LumaVideoOutputResolution],\n                    {\n                        \"default\": LumaVideoOutputResolution.res_540p,\n                    },\n                ),\n                \"duration\": ([dur.value for dur in LumaVideoModelOutputDuration],),\n                \"loop\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"first_image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"First frame of generated video.\"},\n                ),\n                \"last_image\": (IO.IMAGE, {\"tooltip\": \"Last frame of generated video.\"}),\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        resolution: str,\n        duration: str,\n        loop: bool,\n        seed,\n        first_image: torch.Tensor = None,\n        last_image: torch.Tensor = None,\n        luma_concepts: LumaConceptChain = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        if first_image is None and last_image is None:\n            raise Exception(\n                \"At least one of first_image and last_image requires an input.\"\n            )\n        keyframes = self._convert_to_keyframes(first_image, last_image, auth_token)\n        duration = duration if model != LumaVideoModel.ray_1_6 else None\n        resolution = resolution if model != LumaVideoModel.ray_1_6 else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations\",\n                method=HttpMethod.POST,\n                request_model=LumaGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaGenerationRequest(\n                prompt=prompt,\n                model=model,\n                aspect_ratio=LumaAspectRatio.ratio_16_9,  # ignored, but still needed by the API for some reason\n                resolution=resolution,\n                duration=duration,\n                loop=loop,\n                keyframes=keyframes,\n                concepts=luma_concepts.create_api_model() if luma_concepts else None,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.assets.video)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n    def _convert_to_keyframes(\n        self,\n        first_image: torch.Tensor = None,\n        last_image: torch.Tensor = None,\n        auth_token=None,\n    ):\n        if first_image is None and last_image is None:\n            return None\n        frame0 = None\n        frame1 = None\n        if first_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                first_image, max_images=1, auth_token=auth_token\n            )\n            frame0 = LumaImageReference(type=\"image\", url=download_urls[0])\n        if last_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                last_image, max_images=1, auth_token=auth_token\n            )\n            frame1 = LumaImageReference(type=\"image\", url=download_urls[0])\n        return LumaKeyframes(frame0=frame0, frame1=frame1)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v",
  "markdown": "# Kling Image to Video (Camera Control) - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Image to Video (Camera Control) Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg) The Kling Image to Video (Camera Control) node converts static images into videos with professional camera movements. It supports camera controls like zoom, rotation, pan, tilt and first-person view while maintaining focus on the original image content.\n\n```\n\nclass KlingCameraControlI2VNode(KlingImage2VideoNode):\n    \"\"\"\n    Kling Image to Video Camera Control Node. This node is a image to video node, but it supports controlling the camera.\n    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"camera_control\": (\n                    \"CAMERA_CONTROL\",\n                    {\n                        \"tooltip\": \"Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Transform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image.\"\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        camera_control: CameraControl,\n        auth_token: Optional[str] = None,\n    ):\n        return super().api_call(\n            model_name=\"kling-v1-5\",\n            start_frame=start_frame,\n            cfg_scale=cfg_scale,\n            mode=\"pro\",\n            aspect_ratio=aspect_ratio,\n            duration=\"5\",\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            camera_control=camera_control,\n            auth_token=auth_token,\n        )\n\n\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video",
  "markdown": "# MiniMax Text to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native MiniMax Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg) The MiniMax Text to Video node connects to MiniMaxâ€™s API to generate high-quality, smooth videos from text prompts. It supports different video generation models to create short video clips in various styles.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt\\_text | String | \"\"  | Text prompt that guides the video generation |\n| model | Select | â€T2V-01â€ | Video model to use, options are â€œT2V-01â€ and â€œT2V-01-Directorâ€ |\n| seed | Integer | 0   | Random seed for noise generation, defaults to 0 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass MinimaxTextToVideoNode:\n    \"\"\"\n    Generates videos synchronously based on a prompt, and optional parameters using Minimax's API.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt_text\": (\n                    \"STRING\",\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt to guide the video generation\",\n                    },\n                ),\n                \"model\": (\n                    [\n                        \"T2V-01\",\n                        \"T2V-01-Director\",\n                    ],\n                    {\n                        \"default\": \"T2V-01\",\n                        \"tooltip\": \"Model to use for video generation\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from prompts using Minimax's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Minimax\"\n    API_NODE = True\n    OUTPUT_NODE = True\n\n    def generate_video(\n        self,\n        prompt_text,\n        seed=0,\n        model=\"T2V-01\",\n        image: torch.Tensor=None, # used for ImageToVideo\n        subject: torch.Tensor=None, # used for SubjectToVideo\n        auth_token=None,\n    ):\n        '''\n        Function used between Minimax nodes - supports T2V, I2V, and S2V, based on provided arguments.\n        '''\n        # upload image, if passed in\n        image_url = None\n        if image is not None:\n            image_url = upload_images_to_comfyapi(image, max_images=1, auth_token=auth_token)[0]\n\n        # TODO: figure out how to deal with subject properly, API returns invalid params when using S2V-01 model\n        subject_reference = None\n        if subject is not None:\n            subject_url = upload_images_to_comfyapi(subject, max_images=1, auth_token=auth_token)[0]\n            subject_reference = [SubjectReferenceItem(image=subject_url)]\n\n\n        video_generate_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/video_generation\",\n                method=HttpMethod.POST,\n                request_model=MinimaxVideoGenerationRequest,\n                response_model=MinimaxVideoGenerationResponse,\n            ),\n            request=MinimaxVideoGenerationRequest(\n                model=Model(model),\n                prompt=prompt_text,\n                callback_url=None,\n                first_frame_image=image_url,\n                subject_reference=subject_reference,\n                prompt_optimizer=None,\n            ),\n            auth_token=auth_token,\n        )\n        response = video_generate_operation.execute()\n\n        task_id = response.task_id\n        if not task_id:\n            raise Exception(f\"Minimax generation failed: {response.base_resp}\")\n\n        video_generate_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/query/video_generation\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=MinimaxTaskResultResponse,\n                query_params={\"task_id\": task_id},\n            ),\n            completed_statuses=[\"Success\"],\n            failed_statuses=[\"Fail\"],\n            status_extractor=lambda x: x.status.value,\n            auth_token=auth_token,\n        )\n        task_result = video_generate_operation.execute()\n\n        file_id = task_result.file_id\n        if file_id is None:\n            raise Exception(\"Request was not successful. Missing file ID.\")\n        file_retrieve_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/files/retrieve\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=MinimaxFileRetrieveResponse,\n                query_params={\"file_id\": int(file_id)},\n            ),\n            request=EmptyRequest(),\n            auth_token=auth_token,\n        )\n        file_result = file_retrieve_operation.execute()\n\n        file_url = file_result.file.download_url\n        if file_url is None:\n            raise Exception(\n                f\"No video was found in the response. Full response: {file_result.model_dump()}\"\n            )\n        logging.info(f\"Generated video URL: {file_url}\")\n\n        video_io = download_url_to_bytesio(file_url)\n        if video_io is None:\n            error_msg = f\"Failed to download video from {file_url}\"\n            logging.error(error_msg)\n            raise Exception(error_msg)\n        return (VideoFromFile(video_io),)\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fmodels",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video",
  "markdown": "# Pika 2.2 Image to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native Pika 2.2 Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg) The Pika 2.2 Image to Video node connects to Pikaâ€™s latest 2.2 API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | Image | \\-  | Input image to convert to video |\n| prompt\\_text | String | \"\"  | Text prompt describing video motion and content |\n| negative\\_prompt | String | \"\"  | Elements to avoid in the video |\n| seed | Integer | 0   | Random seed for generation |\n| resolution | Select | â€1080pâ€ | Output video resolution |\n| duration | Select | â€5sâ€ | Length of generated video |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## How It Works\n\nThe node sends the input image and parameters (prompts, resolution, duration, etc.) to Pikaâ€™s API server as multipart form data. The API processes this and returns the generated video. Users can control the output by adjusting the prompts, negative prompts, random seed and other parameters.\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\n\nclass PikaImageToVideoV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Image to Video Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"The image to convert to video\"},\n                ),\n                **cls.get_base_inputs_types(PikaBodyGenerate22I2vGenerate22I2vPost),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    DESCRIPTION = \"Sends an image and prompt to the Pika API v2.2 to generate a video.\"\n    RETURN_TYPES = (\"VIDEO\",)\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika 2.2 Image to Video.\"\"\"\n        # Convert image to BytesIO\n        image_bytes_io = tensor_to_bytesio(image)\n        image_bytes_io.seek(0)  # Reset stream position\n\n        # Prepare file data for multipart upload\n        pika_files = {\"image\": (\"image.png\", image_bytes_io, \"image/png\")}\n\n        # Prepare non-file data using the Pydantic model\n        pika_request_data = PikaBodyGenerate22I2vGenerate22I2vPost(\n            promptText=prompt_text,\n            negativePrompt=negative_prompt,\n            seed=seed,\n            resolution=resolution,\n            duration=duration,\n        )\n\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_IMAGE_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22I2vGenerate22I2vPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=pika_request_data,\n            files=pika_files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Flinks",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes",
  "markdown": "# Pika 2.2 Scenes - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Pika 2.2 Scenes Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg) The Pika 2.2 Scenes node allows you to upload multiple images and generate a high-quality video incorporating these elements. It uses Pikaâ€™s 2.2 API to create smooth scene transitions between the images.\n\nThe Pika 2.2 Scenes node analyzes all input images and creates a video containing these image elements. The node sends the images and parameters to Pikaâ€™s API server, which processes them and returns the generated video. Users can guide the video style and content through prompts, and exclude unwanted elements using negative prompts. The node supports up to 5 input images as ingredients and generates the final video based on the specified combination mode, resolution, duration, and aspect ratio.\n\n```\n\nclass PikaScenesV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Scenes Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        image_ingredient_input = (\n            IO.IMAGE,\n            {\"tooltip\": \"Image that will be used as ingredient to create a video.\"},\n        )\n        return {\n            \"required\": {\n                **cls.get_base_inputs_types(\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                ),\n                \"ingredients_mode\": model_field_to_node_input(\n                    IO.COMBO,\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                    \"ingredientsMode\",\n                    enum_type=IngredientsMode,\n                    default=\"creative\",\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.FLOAT,\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                    \"aspectRatio\",\n                    step=0.001,\n                    min=0.4,\n                    max=2.5,\n                    default=1.7777777777777777,\n                ),\n            },\n            \"optional\": {\n                \"image_ingredient_1\": image_ingredient_input,\n                \"image_ingredient_2\": image_ingredient_input,\n                \"image_ingredient_3\": image_ingredient_input,\n                \"image_ingredient_4\": image_ingredient_input,\n                \"image_ingredient_5\": image_ingredient_input,\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    DESCRIPTION = \"Combine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them.\"\n    RETURN_TYPES = (\"VIDEO\",)\n\n    def api_call(\n        self,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        ingredients_mode: str,\n        aspect_ratio: float,\n        image_ingredient_1: Optional[torch.Tensor] = None,\n        image_ingredient_2: Optional[torch.Tensor] = None,\n        image_ingredient_3: Optional[torch.Tensor] = None,\n        image_ingredient_4: Optional[torch.Tensor] = None,\n        image_ingredient_5: Optional[torch.Tensor] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika Scenes 2.2.\"\"\"\n        all_image_bytes_io = []\n        for image in [\n            image_ingredient_1,\n            image_ingredient_2,\n            image_ingredient_3,\n            image_ingredient_4,\n            image_ingredient_5,\n        ]:\n            if image is not None:\n                image_bytes_io = tensor_to_bytesio(image)\n                image_bytes_io.seek(0)\n                all_image_bytes_io.append(image_bytes_io)\n\n        # Prepare files data for multipart upload\n        pika_files = [\n            (\"images\", (f\"image_{i}.png\", image_bytes_io, \"image/png\"))\n            for i, image_bytes_io in enumerate(all_image_bytes_io)\n        ]\n\n        # Prepare non-file data using the Pydantic model\n        pika_request_data = PikaBodyGenerate22C2vGenerate22PikascenesPost(\n            ingredientsMode=ingredients_mode,\n            promptText=prompt_text,\n            negativePrompt=negative_prompt,\n            seed=seed,\n            resolution=resolution,\n            duration=duration,\n            aspectRatio=aspect_ratio,\n        )\n\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_PIKASCENES,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=pika_request_data,\n            files=pika_files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/comms_overview",
  "markdown": "# Server Overview - ComfyUI\n\nThe Comfy server runs on top of the [aiohttp framework](https://docs.aiohttp.org/), which in turn uses [asyncio](https://pypi.org/project/asyncio/). Messages from the server to the client are sent by socket messages through the `send_sync` method of the server, which is an instance of `PromptServer` (defined in `server.py`). They are processed by a socket event listener registered in `api.js`. See [messages](https://docs.comfy.org/development/comfyui-server/comms_messages). Messages from the client to the server are sent by the `api.fetchApi()` method defined in `api.js`, and are handled by http routes defined by the server. See [routes](https://docs.comfy.org/development/comfyui-server/comms_routes)."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video",
  "markdown": "# PixVerse Image to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native PixVerse Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg) The PixVerse Image to Video node uses PixVerseâ€™s API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | Image | \\-  | Input image to convert to video |\n| prompt | String | \"\"  | Text prompt describing video motion/content |\n| negative\\_prompt | String | \"\"  | Elements to avoid in the video |\n| seed | Integer | \\-1 | Random seed (-1 for random) |\n| quality | Select | â€highâ€ | Output video quality level |\n| aspect\\_ratio | Select | â€r16\\_9â€ | Output video aspect ratio |\n| duration | Select | â€seconds\\_4â€ | Length of generated video |\n| motion\\_mode | Select | â€standardâ€ | Video motion style |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | Optional PixVerse template |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\nclass PixverseImageToVideoNode(ComfyNodeABC):\n    \"\"\"\n    Pixverse Image to Video\n\n    Generates videos from an image and prompts.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"prompt\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n                \"negative_prompt\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n                \"seed\": (\"INT\", {\"default\": -1, \"min\": -1, \"max\": 0xffffffffffffffff}),\n                \"quality\": (list(PixverseQuality.__members__.keys()), {\"default\": \"high\"}),\n                \"aspect_ratio\": (list(PixverseAspectRatio.__members__.keys()), {\"default\": \"r16_9\"}),\n                \"duration\": (list(PixverseDuration.__members__.keys()), {\"default\": \"seconds_4\"}),\n                \"motion_mode\": (list(PixverseMotionMode.__members__.keys()), {\"default\": \"standard\"}),\n            },\n            \"optional\": {\n                \"pixverse_template\": (\"PIXVERSE_TEMPLATE\",),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from an image and prompts using Pixverse's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Pixverse\"\n    API_NODE = True\n    OUTPUT_NODE = True\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video",
  "markdown": "# PixVerse Transition Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native PixVerse Transition Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg) The Pixverse Transition Video node connects to PixVerseâ€™s API to generate smooth video transitions between two images. It automatically creates all intermediate frames to produce fluid transformations, perfect for morphing effects, scene transitions, and object evolution.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| first\\_frame | Image | \\-  | Starting frame image |\n| last\\_frame | Image | \\-  | Ending frame image |\n| prompt | String | \"\"  | Text prompt describing video and transition |\n| quality | Select | â€PixverseQuality.res\\_540pâ€ | Output video quality |\n| duration\\_seconds | Select | \\-  | Length of generated video |\n| motion\\_mode | Select | \\-  | Video motion style |\n| seed | Integer | 0   | Random seed (range: 0-2147483647) |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| negative\\_prompt | String | \"\"  | Elements to avoid in video |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | Optional style preset |\n\n### Parameter Constraints\n\n*   When quality is set to 1080p, motion\\_mode is forced to normal and duration\\_seconds to 5 seconds\n*   When duration\\_seconds is not 5 seconds, motion\\_mode is forced to normal\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## Source Code\n\n```\n\nclass PixverseTransitionVideoNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"first_frame\": (\n                    IO.IMAGE,\n                ),\n                \"last_frame\": (\n                    IO.IMAGE,\n                ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"quality\": (\n                    [resolution.value for resolution in PixverseQuality],\n                    {\n                        \"default\": PixverseQuality.res_540p,\n                    },\n                ),\n                \"duration_seconds\": ([dur.value for dur in PixverseDuration],),\n                \"motion_mode\": ([mode.value for mode in PixverseMotionMode],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"pixverse_template\": (\n                    PixverseIO.TEMPLATE,\n                    {\n                        \"tooltip\": \"An optional template to influence style of generation, created by the Pixverse Template node.\"\n                    }\n                )\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        first_frame: torch.Tensor,\n        last_frame: torch.Tensor,\n        prompt: str,\n        quality: str,\n        duration_seconds: int,\n        motion_mode: str,\n        seed,\n        negative_prompt: str=None,\n        pixverse_template: int=None,\n        auth_token=None,\n        **kwargs,\n    ):\n        first_frame_id = upload_image_to_pixverse(first_frame, auth_token=auth_token)\n        last_frame_id = upload_image_to_pixverse(last_frame, auth_token=auth_token)\n\n        # 1080p is limited to 5 seconds duration\n        # only normal motion_mode supported for 1080p or for non-5 second duration\n        if quality == PixverseQuality.res_1080p:\n            motion_mode = PixverseMotionMode.normal\n            duration_seconds = PixverseDuration.dur_5\n        elif duration_seconds != PixverseDuration.dur_5:\n            motion_mode = PixverseMotionMode.normal\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/pixverse/video/transition/generate\",\n                method=HttpMethod.POST,\n                request_model=PixverseTransitionVideoRequest,\n                response_model=PixverseVideoResponse,\n            ),\n            request=PixverseTransitionVideoRequest(\n                first_frame_img=first_frame_id,\n                last_frame_img=last_frame_id,\n                prompt=prompt,\n                quality=quality,\n                duration=duration_seconds,\n                motion_mode=motion_mode,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                template_id=pixverse_template,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.Resp is None:\n            raise Exception(f\"Pixverse request failed: '{response_api.ErrMsg}'\")\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/pixverse/video/result/{response_api.Resp.video_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=PixverseGenerationStatusResponse,\n            ),\n            completed_statuses=[PixverseStatus.successful],\n            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],\n            status_extractor=lambda x: x.Resp.status,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.Resp.url)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video",
  "markdown": "# PixVerse Text to Video - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in PixVerse Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg) The PixVerse Text to Video node connects to PixVerseâ€™s text-to-video API, allowing users to generate high-quality videos from text descriptions. Users can customize their creations by adjusting various parameters like video quality, duration, and motion mode.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the video content |\n| aspect\\_ratio | select | \\-  | Output video aspect ratio |\n| quality | select | PixverseQuality.res\\_540p | Video quality level |\n| duration\\_seconds | select | \\-  | Video duration |\n| motion\\_mode | select | \\-  | Video motion mode |\n| seed | integer | 0   | Random seed for consistent generation results |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| negative\\_prompt | string | \"\"  | Elements to exclude from the video |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | Optional template for style settings |\n\n### Limitations\n\n*   1080p quality only supports normal motion mode with 5-second duration\n*   Non 5-second durations only support normal motion mode\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\n\nclass PixverseTextToVideoNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in PixverseAspectRatio],\n                ),\n                \"quality\": (\n                    [resolution.value for resolution in PixverseQuality],\n                    {\n                        \"default\": PixverseQuality.res_540p,\n                    },\n                ),\n                \"duration_seconds\": ([dur.value for dur in PixverseDuration],),\n                \"motion_mode\": ([mode.value for mode in PixverseMotionMode],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"pixverse_template\": (\n                    PixverseIO.TEMPLATE,\n                    {\n                        \"tooltip\": \"An optional template to influence style of generation, created by the Pixverse Template node.\"\n                    }\n                )\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        aspect_ratio: str,\n        quality: str,\n        duration_seconds: int,\n        motion_mode: str,\n        seed,\n        negative_prompt: str=None,\n        pixverse_template: int=None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # 1080p is limited to 5 seconds duration\n        # only normal motion_mode supported for 1080p or for non-5 second duration\n        if quality == PixverseQuality.res_1080p:\n            motion_mode = PixverseMotionMode.normal\n            duration_seconds = PixverseDuration.dur_5\n        elif duration_seconds != PixverseDuration.dur_5:\n            motion_mode = PixverseMotionMode.normal\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/pixverse/video/text/generate\",\n                method=HttpMethod.POST,\n                request_model=PixverseTextVideoRequest,\n                response_model=PixverseVideoResponse,\n            ),\n            request=PixverseTextVideoRequest(\n                prompt=prompt,\n                aspect_ratio=aspect_ratio,\n                quality=quality,\n                duration=duration_seconds,\n                motion_mode=motion_mode,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                template_id=pixverse_template,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.Resp is None:\n            raise Exception(f\"Pixverse request failed: '{response_api.ErrMsg}'\")\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/pixverse/video/result/{response_api.Resp.video_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=PixverseGenerationStatusResponse,\n            ),\n            completed_statuses=[PixverseStatus.successful],\n            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],\n            status_extractor=lambda x: x.Resp.status,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.Resp.url)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/conditioning/video-models/wan-vace-to-video",
  "markdown": "# Wan Vace To Video - ComfyUI Built-in Node Documentation\n\n![Wan Vace To Video](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg) The Wan Vace To Video node allows you to generate videos through text prompts and supports multiple input methods, including text, images, videos, masks, and control signals. This node combines input conditions (prompts), control videos, and masks to generate high-quality videos. It first preprocesses and encodes the inputs, then applies the conditional information to generate the final video latent representation. When a reference image is provided, it serves as the initial reference for the video. Control videos and masks can be used to guide the generation process, making the generated video more aligned with expectations.\n\n## Parameter Description\n\n### Required Parameters\n\n| Parameter | Type | Default | Range | Description |\n| --- | --- | --- | --- | --- |\n| positive | CONDITIONING | \\-  | \\-  | Positive prompt condition |\n| negative | CONDITIONING | \\-  | \\-  | Negative prompt condition |\n| vae | VAE | \\-  | \\-  | VAE model for encoding/decoding |\n| width | INT | 832 | 16-MAX\\_RESOLUTION | Video width, step size 16 |\n| height | INT | 480 | 16-MAX\\_RESOLUTION | Video height, step size 16 |\n| length | INT | 81  | 1-MAX\\_RESOLUTION | Number of video frames, step size 4 |\n| batch\\_size | INT | 1   | 1-4096 | Batch size |\n| strength | FLOAT | 1.0 | 0.0-1000.0 | Condition strength, step size 0.01 |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| control\\_video | IMAGE | Control video for guiding the generation process |\n| control\\_masks | MASK | Control masks defining which areas should be controlled |\n| reference\\_image | IMAGE | Reference image as starting point or reference (single image) |\n\n### Output Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| positive | CONDITIONING | Processed positive prompt condition |\n| negative | CONDITIONING | Processed negative prompt condition |\n| latent | LATENT | Generated video latent representation |\n| trim\\_latent | INT | Parameter for trimming latent representation, default value is 0. When a reference image is provided, this value is set to the shape size of the reference image in latent space. It indicates how much content from the reference image downstream nodes should trim from the generated latent representation to ensure proper control of the reference imageâ€™s influence in the final video output. |\n\n## Source Code\n\n\\[Source code update time: 2025-05-15\\]\n\n```\nclass WanVaceToVideo:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"vae\": (\"VAE\", ),\n                             \"width\": (\"INT\", {\"default\": 832, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                             \"height\": (\"INT\", {\"default\": 480, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                             \"length\": (\"INT\", {\"default\": 81, \"min\": 1, \"max\": nodes.MAX_RESOLUTION, \"step\": 4}),\n                             \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1000.0, \"step\": 0.01}),\n                },\n                \"optional\": {\"control_video\": (\"IMAGE\", ),\n                             \"control_masks\": (\"MASK\", ),\n                             \"reference_image\": (\"IMAGE\", ),\n                }}\n\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\", \"INT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\", \"trim_latent\")\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/video_models\"\n\n    EXPERIMENTAL = True\n\n    def encode(self, positive, negative, vae, width, height, length, batch_size, strength, control_video=None, control_masks=None, reference_image=None):\n        latent_length = ((length - 1) // 4) + 1\n        if control_video is not None:\n            control_video = comfy.utils.common_upscale(control_video[:length].movedim(-1, 1), width, height, \"bilinear\", \"center\").movedim(1, -1)\n            if control_video.shape[0] < length:\n                control_video = torch.nn.functional.pad(control_video, (0, 0, 0, 0, 0, 0, 0, length - control_video.shape[0]), value=0.5)\n        else:\n            control_video = torch.ones((length, height, width, 3)) * 0.5\n\n        if reference_image is not None:\n            reference_image = comfy.utils.common_upscale(reference_image[:1].movedim(-1, 1), width, height, \"bilinear\", \"center\").movedim(1, -1)\n            reference_image = vae.encode(reference_image[:, :, :, :3])\n            reference_image = torch.cat([reference_image, comfy.latent_formats.Wan21().process_out(torch.zeros_like(reference_image))], dim=1)\n\n        if control_masks is None:\n            mask = torch.ones((length, height, width, 1))\n        else:\n            mask = control_masks\n            if mask.ndim == 3:\n                mask = mask.unsqueeze(1)\n            mask = comfy.utils.common_upscale(mask[:length], width, height, \"bilinear\", \"center\").movedim(1, -1)\n            if mask.shape[0] < length:\n                mask = torch.nn.functional.pad(mask, (0, 0, 0, 0, 0, 0, 0, length - mask.shape[0]), value=1.0)\n\n        control_video = control_video - 0.5\n        inactive = (control_video * (1 - mask)) + 0.5\n        reactive = (control_video * mask) + 0.5\n\n        inactive = vae.encode(inactive[:, :, :, :3])\n        reactive = vae.encode(reactive[:, :, :, :3])\n        control_video_latent = torch.cat((inactive, reactive), dim=1)\n        if reference_image is not None:\n            control_video_latent = torch.cat((reference_image, control_video_latent), dim=2)\n\n        vae_stride = 8\n        height_mask = height // vae_stride\n        width_mask = width // vae_stride\n        mask = mask.view(length, height_mask, vae_stride, width_mask, vae_stride)\n        mask = mask.permute(2, 4, 0, 1, 3)\n        mask = mask.reshape(vae_stride * vae_stride, length, height_mask, width_mask)\n        mask = torch.nn.functional.interpolate(mask.unsqueeze(0), size=(latent_length, height_mask, width_mask), mode='nearest-exact').squeeze(0)\n\n        trim_latent = 0\n        if reference_image is not None:\n            mask_pad = torch.zeros_like(mask[:, :reference_image.shape[2], :, :])\n            mask = torch.cat((mask_pad, mask), dim=1)\n            latent_length += reference_image.shape[2]\n            trim_latent = reference_image.shape[2]\n\n        mask = mask.unsqueeze(0)\n        positive = node_helpers.conditioning_set_values(positive, {\"vace_frames\": control_video_latent, \"vace_mask\": mask, \"vace_strength\": strength})\n        negative = node_helpers.conditioning_set_values(negative, {\"vace_frames\": control_video_latent, \"vace_mask\": mask, \"vace_strength\": strength})\n\n        latent = torch.zeros([batch_size, 16, latent_length, height // 8, width // 8], device=comfy.model_management.intermediate_device())\n        out_latent = {}\n        out_latent[\"samples\"] = latent\n        return (positive, negative, out_latent, trim_latent)\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Finterface%2Foverview",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/interface/settings/overview",
  "markdown": "# ComfyUI Settings Overview - ComfyUI\n\nDetailed description of ComfyUI settings overview\n\nThis section covers detailed setting descriptions in the ComfyUI frontend settings menu. All user settings are automatically saved to the `ComfyUI/user/default/comfy.settings.json` file. You can use the `Ctrl + ,` keyboard shortcut to open the settings panel, then click on the corresponding setting options to configure them. Since custom nodes can also register corresponding setting categories in the menu, our official documentation currently only includes native setting content. Additionally, some setting options are **only effective for ComfyUI Desktop**, which we have noted on the corresponding pages.\n\n## ComfyUI Settings Menu\n\n[\n\n## User\n\nUser settings related to ComfyUI account, mainly used for logging into ComfyUI account to use API nodes\n\n\n\n](https://docs.comfy.org/interface/user)[\n\n## Credits\n\nEntry for purchasing credits and credit balance history, only visible after logging into ComfyUI account\n\n\n\n](https://docs.comfy.org/interface/credits)[\n\n## Comfy\n\nDetailed description of ComfyUI core setting options\n\n\n\n](https://docs.comfy.org/interface/settings/comfy)[\n\n## Lite Graph\n\nDetailed description of Canvas (Lite Graph) setting options in ComfyUI\n\n\n\n](https://docs.comfy.org/interface/settings/lite-graph)[\n\n## Appearance\n\nModify ComfyUI appearance options such as themes, background colors, sidebar position, etc.\n\n\n\n](https://docs.comfy.org/interface/appearance)[\n\n## Extension\n\nManage the enable/disable status of frontend extension plugins in ComfyUI\n\n\n\n](https://docs.comfy.org/interface/settings/extension)[\n\n## 3D\n\nSome setting options for 3D node initialization\n\n\n\n](https://docs.comfy.org/interface/settings/3d)[\n\n## Comfy Desktop\n\nDesktop update settings, mirror settings, etc. (only effective for ComfyUI Desktop)\n\n\n\n](https://docs.comfy.org/interface/settings/comfy-desktop)[\n\n## Mask Editor\n\nAdjust mask editor usage preferences\n\n\n\n](https://docs.comfy.org/interface/settings/mask-editor)[\n\n## Keybinding\n\nModify ComfyUI keyboard shortcut settings\n\n\n\n](https://docs.comfy.org/interface/shortcuts)[\n\n## About\n\nLearn about current ComfyUI version information, device runtime information, etc., which is very useful for daily feedback\n\n\n\n](https://docs.comfy.org/interface/settings/about)[\n\n## Server Config\n\nModify ComfyUI configuration file, this setting is only effective for ComfyUI Desktop\n\n\n\n](https://docs.comfy.org/interface/settings/server-config)"
},
{
  "url": "https://docs.comfy.org/changelog/index",
  "markdown": "# Changelog - ComfyUI\n\n**Advanced Sampling & Training Infrastructure Improvements**This release introduces significant enhancements to sampling algorithms, training capabilities, and node functionality for AI researchers and workflow creators:\n\n## New Sampling & Generation Features\n\n*   **SA-Solver Sampler**: New reconstructed SA-Solver sampling algorithm providing enhanced numerical stability and quality for complex generation workflows\n*   **Experimental CFGNorm Node**: Advanced classifier-free guidance normalization for improved control over generation quality and style consistency\n*   **Nested Dual CFG Support**: Added nested style configuration to DualCFGGuider node, offering more sophisticated guidance control patterns\n*   **SamplingPercentToSigma Node**: New utility node for precise sigma calculation from sampling percentages, improving workflow flexibility\n\n## Enhanced Training Capabilities\n\n*   **Multi Image-Caption Dataset Support**: LoRA training node now handles multiple image-caption datasets simultaneously, streamlining training workflows\n*   **Better Training Loop Implementation**: Optimized training algorithms for improved convergence and stability during model fine-tuning\n*   **Enhanced Error Detection**: Added model detection error hints for LoRA operations, providing clearer feedback when issues occur\n\n## Platform & Performance Improvements\n\n*   **Async Node Support**: Full support for asynchronous node functions with earlier execution optimization, improving workflow performance for I/O intensive operations\n*   **Chroma Flexibility**: Un-hardcoded patch\\_size parameter in Chroma, allowing better adaptation to different model configurations\n*   **LTXV VAE Decoder**: Switched to improved default padding mode for better image quality with LTXV models\n*   **Safetensors Memory Management**: Added workaround for mmap issues, improving reliability when loading large model files\n\n## API & Integration Enhancements\n\n*   **Custom Prompt IDs**: API now allows specifying prompt IDs for better workflow tracking and management\n*   **Kling API Optimization**: Increased polling timeout to prevent user timeouts during video generation workflows\n*   **History Token Cleanup**: Removed sensitive tokens from history items for improved security\n*   **Python 3.9 Compatibility**: Fixed compatibility issues ensuring broader platform support\n\n## Bug Fixes & Stability\n\n*   **MaskComposite Fixes**: Resolved errors when destination masks have 2 dimensions, improving mask workflow reliability\n*   **Fresca Input/Output**: Corrected input and output handling for Fresca model workflows\n*   **Reference Bug Fixes**: Resolved incorrect reference bugs in Gemini node implementations\n*   **Line Ending Standardization**: Automated detection and removal of Windows line endings for cross-platform consistency\n\n## Developer Experience\n\n*   **Warning Systems**: Added torch import mistake warnings to catch common configuration issues\n*   **Template Updates**: Multiple template version updates (0.1.36, 0.1.37, 0.1.39) for improved custom node development\n*   **Documentation**: Enhanced fast\\_fp16\\_accumulation documentation in portable configurations\n\nThese improvements make ComfyUI more robust for production workflows while introducing powerful new sampling techniques and training capabilities essential for advanced AI research and creative applications.\n\n**Advanced Sampling & Model Control Enhancements**This release delivers significant improvements to sampling algorithms and model control systems, particularly benefiting advanced AI researchers and workflow creators:\n\n## New Sampling Capabilities\n\n*   **TCFG Node**: Enhanced classifier-free guidance control for more nuanced generation control in your workflows\n*   **ER-SDE Sampler**: Migrated from VE to VP algorithm with new sampler node, providing better numerical stability for complex generation tasks\n*   **Skip Layer Guidance (SLG)**: Alternative implementation for precise layer-level control during inference, perfect for advanced model steering workflows\n\n## Enhanced Development Tools\n\n*   **Custom Node Management**: New `--whitelist-custom-nodes` argument pairs with `--disable-all-custom-nodes` for precise development control\n*   **Performance Optimizations**: Dual CFG node now optimizes automatically when CFG is 1.0, reducing computational overhead\n*   **GitHub Actions Integration**: Automated release webhook notifications keep developers informed of new updates\n\n## Image Processing Improvements\n\n*   **New Transform Nodes**: Added ImageRotate and ImageFlip nodes for enhanced image manipulation workflows\n*   **ImageColorToMask Fix**: Corrected mask value returns for more accurate color-based masking operations\n*   **3D Model Support**: Upload 3D models to custom subfolders for better organization in complex projects\n\n## Guidance & Conditioning Enhancements\n\n*   **PerpNeg Guider**: Updated with improved pre and post-CFG handling plus performance optimizations\n*   **Latent Conditioning Fix**: Resolved issues with conditioning at index > 0 for multi-step workflows\n*   **Denoising Steps**: Added denoising step support to several samplers for cleaner outputs\n\n## Platform Stability\n\n*   **PyTorch Compatibility**: Fixed contiguous memory issues with PyTorch nightly builds\n*   **FP8 Fallback**: Automatic fallback to regular operations when FP8 operations encounter exceptions\n*   **Audio Processing**: Removed deprecated torchaudio.save function dependencies with warning fixes\n\n## Model Integration\n\n*   **Moonvalley Nodes**: Added native support for Moonvalley model workflows\n*   **Scheduler Reordering**: Simple scheduler now defaults first for better user experience\n*   **Template Updates**: Multiple template version updates (0.1.31-0.1.35) for improved custom node development\n\n## Security & Safety\n\n*   **Safe Loading**: Added warnings when loading files unsafely, with documentation noting that checkpoint files are loaded safely by default\n*   **File Validation**: Enhanced checkpoint loading safety measures for secure workflow execution\n\nThese improvements make ComfyUI more robust for production workflows while expanding creative possibilities for AI artists working with advanced sampling techniques and model control systems.\n\n**Enhanced Model Support & Workflow Reliability**This release brings significant improvements to model compatibility and workflow stability:\n\n*   **Expanded Model Documentation**: Added comprehensive support documentation for Flux Kontext and Omnigen 2 models, making it easier for creators to integrate these powerful models into their workflows\n*   **VAE Encoding Improvements**: Removed unnecessary random noise injection during VAE encoding, resulting in more consistent and predictable outputs across workflow runs\n*   **Memory Management Fix**: Resolved a critical memory estimation bug specifically affecting Kontext model usage, preventing out-of-memory errors and improving workflow stability\n\nThese changes enhance the reliability of advanced model workflows while maintaining ComfyUIâ€™s flexibility for AI artists and researchers working with cutting-edge generative models.\n\n**Major Model Support Additions**\n\n*   **Cosmos Predict2 Support**: Full implementation for both text-to-image (2B and 14B models) and image-to-video generation workflows, expanding video creation capabilities\n*   **Enhanced Flux Compatibility**: Chroma Text Encoder now works seamlessly with regular Flux models, improving text conditioning quality\n*   **LoRA Training Integration**: New native LoRA training node using weight adapter scheme, enabling direct model fine-tuning within ComfyUI workflows\n\n**Performance & Hardware Optimizations**\n\n*   **AMD GPU Enhancements**: Enabled FP8 operations and PyTorch attention on GFX1201 and other compatible AMD GPUs for faster inference\n*   **Apple Silicon Fixes**: Addressed long-standing FP16 attention issues on Apple devices, improving stability for Mac users\n*   **Flux Model Stability**: Resolved black image generation issues with certain Flux models in FP16 precision\n\n**Advanced Sampling Improvements**\n\n*   **Rectified Flow (RF) Samplers**: Added SEEDS and multistep DPM++ SDE samplers with RF support, providing more sampling options for cutting-edge models\n*   **ModelSamplingContinuousEDM**: New cosmos\\_rflow option for enhanced sampling control with Cosmos models\n*   **Memory Optimization**: Improved memory estimation for Cosmos models with uncapped resolution support\n\n**Developer & Integration Features**\n\n*   **SQLite Database Support**: Enhanced data management capabilities for custom nodes and workflow storage\n*   **PyProject.toml Integration**: Automatic web folder registration and settings configuration from pyproject files\n*   **Frontend Flexibility**: Support for semver suffixes and prerelease frontend versions for custom deployments\n*   **Tokenizer Enhancements**: Configurable min\\_length settings with tokenizer\\_data for better text processing\n\n**Quality of Life Improvements**\n\n*   **Kontext Aspect Ratio Fix**: Resolved widget-only limitation, now works properly in all connection modes\n*   **SaveLora Consistency**: Standardized filename format across all save nodes for better file organization\n*   **Python Version Warnings**: Added alerts for outdated Python installations to prevent compatibility issues\n*   **WebcamCapture Fixes**: Corrected IS\\_CHANGED signature for reliable live input workflows\n\nThis release significantly expands ComfyUIâ€™s model ecosystem support while delivering crucial stability improvements and enhanced hardware compatibility across different platforms.\n\nThis release brings powerful new workflow utilities and performance optimizations for ComfyUI creators:\n\n## New Workflow Tools\n\n*   **ImageStitch Node**: Concatenate multiple images seamlessly in your workflows - perfect for creating comparison grids or composite outputs\n*   **GetImageSize Node**: Extract image dimensions with batch processing support, essential for dynamic sizing workflows\n*   **Regex Replace Node**: Advanced text manipulation capabilities for prompt engineering and string processing workflows\n\n## Enhanced Model Compatibility\n\n*   **Improved Tensor Handling**: Streamlined list processing makes complex multi-model workflows more reliable\n*   **BFL API Optimization**: Refined support for Kontext \\[pro\\] and \\[max\\] models with cleaner node interfaces\n*   **Performance Boost**: Fused multiply-add operations in chroma processing for faster generation times\n\n## Developer Experience Improvements\n\n*   **Custom Node Support**: Added pyproject.toml support for better custom node dependency management\n*   **Help Menu Integration**: New help system in the Node Library sidebar for faster node discovery\n*   **API Documentation**: Enhanced API nodes documentation for workflow automation\n\n## Frontend & UI Enhancements\n\n*   **Frontend Updated to v1.21.7**: Multiple stability fixes and performance improvements\n*   **Custom API Base Support**: Better subpath handling for custom deployment configurations\n*   **Security Hardening**: XSS vulnerability fixes for safer workflow sharing\n\n## Bug Fixes & Stability\n\n*   **Pillow Compatibility**: Updated deprecated API calls to maintain compatibility with latest image processing libraries\n*   **ROCm Support**: Improved version detection for AMD GPU users\n*   **Template Updates**: Enhanced project templates for custom node development\n\nThese updates strengthen ComfyUIâ€™s foundation for complex AI workflows while making the platform more accessible to new users through improved documentation and helper tools."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Finterface%2Fmaskeditor",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/interface/user",
  "markdown": "# Account Management - ComfyUI\n\nThe account system was added to support `API Nodes`, which enable calls to closed-source model APIs, greatly expanding the possibilities of ComfyUI. Since these API calls consume tokens, we have added a corresponding user system. Currently, we support the following login methods:\n\n*   Email login\n*   Google login\n*   Github login\n*   API Key login (for non-whitelisted site authorization)\n\nWe will provide relevant login requirements and explanations in this document.\n\n## ComfyUI Version Requirements\n\nYou may need to use at least [ComfyUI v0.3.0](https://github.com/comfyanonymous/ComfyUI/releases/tag/v0.3.30) to use the account system. Ensure that the corresponding frontend version is at least `1.17.11`. Sometimes the frontend may fail to install and revert to an older version, so please check if the frontend version is greater than `1.17.11` in `Settings` -> `About`. In some regions, network restrictions may prevent normal access to the login API, causing timeouts or failures. Before logging in, please **ensure that your network environment does not restrict access to the corresponding API**, and make sure you can access sites like Google or Github.\n\n## Network Requirements\n\nTo login to ComfyUI account, you must be in a secure network environment:\n\n*   Only allow access from `127.0.0.1` or `localhost`.\n*   Do not support using the `--listen` parameter to access the API node through a local network.\n*   If you are using a non-SSL certificate or a site that does not start with `https`, you may not be able to successfully log in.\n*   You may not be able to log in on a site that is not in our whitelist (but you can log in using an API Key now).\n*   Ensure you can connect to our service normally (some regions may require a proxy).\n\n## How to Log In\n\nLog in via `Settings` -> `User`: ![ComfyUI User Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user.jpg)\n\n## Login Methods\n\n![user-login](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user-login.jpg) If this is your first login, please create an account first.\n\n## Logging in with an API Key\n\nSince not all ComfyUI deployments are on our domain authorization whitelist, we have provided API Key login in a recent update (2025-05-10) for logging in through non-whitelisted sites. Below are the steps for logging in with an API Key:\n\n## Post-Login Status\n\nAfter logging in, a login button is displayed in the top menu bar of the ComfyUI interface. You can open the corresponding login interface through this button and log out of the corresponding account in the settings menu. ![user-logged](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user-logged.jpg) ![menu-user-logged](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/menu-user-logged.jpg)\n\n## Frequently Asked Questions"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcore-concepts%2Fdependencies",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/interface/credits",
  "markdown": "# Credits Management - ComfyUI\n\nThe credit system was added to support the `API Nodes`, as calling closed-source AI models requires token consumption, so proper credit management is necessary. By default, the credits interface is not displayed. Please first log in to your ComfyUI account in `Settings` -> `User`, and then you can view your associated accountâ€™s credit information in `Settings` -> `Credits`. ![ComfyUI Credits Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/menu-credits.jpg)\n\n## How to Purchase Credits?\n\nBelow is a demonstration video for purchasing credits:\n\nDetailed steps are as follows:\n\n## Frequently Asked Questions"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/tripo/model-generation",
  "markdown": "# Tripo API Node Model Generation ComfyUI Official Example\n\nTripo AI is a company focused on generative AI 3D modeling. It provides user-friendly platforms and API services that can quickly convert text prompts or 2D images (single or multiple) into high-quality 3D models. ComfyUI has now natively integrated the corresponding Tripo API, allowing you to conveniently use the related nodes in ComfyUI for model generation. Currently, ComfyUIâ€™s API nodes support the following Tripo model generation capabilities:\n\n*   Text-to-model\n*   Image-to-model\n*   Multi-view model generation\n*   Rig model\n*   Retarget rigged model\n\n## Text-to-Model Workflow\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/api_tripo_text_to_model.json)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic text-to-model workflow execution:\n\n1.  In the `Tripo: Text to Model` node, input your prompt in the `prompt` field\n    *   model: You can select different models, currently only v1.4 model supports subsequent optimization with `Tripo: Refine Draft model`\n    *   style: You can set different styles\n    *   texture\\_quality: You can set different texture qualities\n2.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/` directory\n3.  In the `Preview 3D` node, click to expand the menu\n4.  Select `Export` to directly export the corresponding model\n\n## Image-to-Model Workflow\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/api_tripo_image_to_model.json)\n\nDownload the image below as input image ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/panda.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic image-to-model workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  In the `Tripo: Image to Model` node, modify the corresponding parameter settings\n    *   model: You can select different models, currently only v1.4 model supports subsequent optimization with `Tripo: Refine Draft model`\n    *   style: You can set different styles\n    *   texture\\_quality: You can set different texture qualities\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/` directory\n4.  For model download, please refer to the instructions in the text-to-model section\n\n## Multi-view Model Generation Workflow\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/api_tripo_multiview_to_model.json)\n\nDownload the images below as input images ![Front View](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/front.jpg) ![Back View](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/back.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic multi-view to model workflow execution:\n\n1.  In the `Load Image` nodes, load the provided input images respectively\n2.  In the `Tripo: Image to Model` node, modify the corresponding parameter settings\n    *   model: You can select different models, currently only v1.4 model supports subsequent optimization with `Tripo: Refine Draft model`\n    *   style: You can set different styles\n    *   texture\\_quality: You can set different texture qualities\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/` directory\n4.  For other view inputs, you can refer to the step diagram and set the corresponding node mode to `Always` to enable it\n5.  For model download, please refer to the instructions in the text-to-model section\n\n## Subsequent Task Processing for the Same Task\n\nTripoâ€™s corresponding nodes provide subsequent processing for the same task, you only need to input the corresponding `model_task_id` in the relevant nodes, and we have also provided the corresponding nodes in the relevant templates, you can also modify the corresponding node mode as needed to enable it ![Tripo Task Processing](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/other_nodes.jpg)"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/datatypes",
  "markdown": "# Datatypes - ComfyUI\n\nThese are the most important built in datatypes. You can also [define your own](https://docs.comfy.org/custom-nodes/backend/more_on_inputs#custom-datatypes). Datatypes are used on the client side to prevent a workflow from passing the wrong form of data into a node - a bit like strong typing. The JavaScript client side code will generally not allow a node output to be connected to an input of a different datatype, although a few exceptions are noted below.\n\n## Comfy datatypes\n\n### COMBO\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype: defined as `list[str]`, output value is `str`\n\nRepresents a dropdown menu widget. Unlike other datatypes, `COMBO` it is not specified in `INPUT_TYPES` by a `str`, but by a `list[str]` corresponding to the options in the dropdown list, with the first option selected by default. `COMBO` inputs are often dynamically generated at run time. For instance, in the built-in `CheckpointLoaderSimple` node, you find\n\n```\n\"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), )\n```\n\nor they might just be a fixed list of options,\n\n```\n\"play_sound\": ([\"no\",\"yes\"], {}),\n```\n\n### Primitive and reroute\n\nPrimitive and reroute nodes only exist on the client side. They do not have an intrinsic datatype, but when connected they take on the datatype of the input or output to which they have been connected (which is why they canâ€™t connect to a `*` inputâ€¦)\n\n## Python datatypes\n\n### INT\n\n*   Additional parameters in `INPUT_TYPES`:\n    *   `default` is required\n    *   `min` and `max` are optional\n*   Python datatype `int`\n\n### FLOAT\n\n*   Additional parameters in `INPUT_TYPES`:\n    *   `default` is required\n    *   `min`, `max`, `step` are optional\n*   Python datatype `float`\n\n### STRING\n\n*   Additional parameters in `INPUT_TYPES`:\n    *   `default` is required\n*   Python datatype `str`\n\n### BOOLEAN\n\n*   Additional parameters in `INPUT_TYPES`:\n    *   `default` is required\n*   Python datatype `bool`\n\n## Tensor datatypes\n\n### IMAGE\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype `torch.Tensor` with _shape_ \\[B,H,W,C\\]\n\nA batch of `B` images, height `H`, width `W`, with `C` channels (generally `C=3` for `RGB`).\n\n### LATENT\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype `dict`, containing a `torch.Tensor` with _shape_ \\[B,C,H,W\\]\n\nThe `dict` passed contains the key `samples`, which is a `torch.Tensor` with _shape_ \\[B,C,H,W\\] representing a batch of `B` latents, with `C` channels (generally `C=4` for existing stable diffusion models), height `H`, width `W`. The height and width are 1/8 of the corresponding image size (which is the value you set in the Empty Latent Image node). Other entries in the dictionary contain things like latent masks.\n\n### MASK\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype `torch.Tensor` with _shape_ \\[H,W\\] or \\[B,C,H,W\\]\n\n### AUDIO\n\n*   No additional parameters in `INPUT_TYPES`\n*   Python datatype `dict`, containing a `torch.Tensor` with _shape_ \\[B, C, T\\] and a sample rate.\n\nThe `dict` passed contains the key `waveform`, which is a `torch.Tensor` with _shape_ \\[B, C, T\\] representing a batch of `B` audio samples, with `C` channels (`C=2` for stereo and `C=1` for mono), and `T` time steps (i.e., the number of audio samples). The `dict` contains another key `sample_rate`, which indicates the sampling rate of the audio.\n\n## Custom Sampling datatypes\n\n### Noise\n\nThe `NOISE` datatype represents a _source_ of noise (not the actual noise itself). It can be represented by any Python object that provides a method to generate noise, with the signature `generate_noise(self, input_latent:Tensor) -> Tensor`, and a property, `seed:Optional[int]`.\n\nWhen noise is to be added, the latent is passed into this method, which should return a `Tensor` of the same shape containing the noise. See the [noise mixing example](https://docs.comfy.org/custom-nodes/backend/snippets#creating-noise-variations)\n\n### Sampler\n\nThe `SAMPLER` datatype represents a sampler, which is represented as a Python object providing a `sample` method. Stable diffusion sampling is beyond the scope of this guide; see `comfy/samplers.py` if you want to dig into this part of the code.\n\n### Sigmas\n\nThe `SIGMAS` datatypes represents the values of sigma before and after each step in the sampling process, as produced by a scheduler. This is represented as a one-dimensional tensor, of length `steps+1`, where each element represents the noise expected to be present before the corresponding step, with the final value representing the noise present after the final step. A `normal` scheduler, with 20 steps and denoise of 1, for an SDXL model, produces:\n\n```\ntensor([14.6146, 10.7468,  8.0815,  6.2049,  4.8557,  \n         3.8654,  3.1238,  2.5572,  2.1157,  1.7648,  \n         1.4806,  1.2458,  1.0481,  0.8784,  0.7297,  \n         0.5964,  0.4736,  0.3555,  0.2322,  0.0292,  0.0000])\n```\n\n### Guider\n\nA `GUIDER` is a generalisation of the denoising process, as â€˜guidedâ€™ by a prompt or any other form of conditioning. In Comfy the guider is represented by a `callable` Python object providing a `__call__(*args, **kwargs)` method which is called by the sample. The `__call__` method takes (in `args[0]`) a batch of noisy latents (tensor `[B,C,H,W]`), and returns a prediction of the noise (a tensor of the same shape).\n\n## Model datatypes\n\nThere are a number of more technical datatypes for stable diffusion models. The most significant ones are `MODEL`, `CLIP`, `VAE` and `CONDITIONING`. Working with these is (for the time being) beyond the scope of this guide!\n\n## Additional Parameters\n\nBelow is a list of officially supported keys that can be used in the â€˜extra optionsâ€™ portion of an input definition.\n\n| Key | Description |\n| --- | --- |\n| `default` | The default value of the widget |\n| `min` | The minimum value of a number (`FLOAT` or `INT`) |\n| `max` | The maximum value of a number (`FLOAT` or `INT`) |\n| `step` | The amount to increment or decrement a widget |\n| `label_on` | The label to use in the UI when the bool is `True` (`BOOL`) |\n| `label_off` | The label to use in the UI when the bool is `False` (`BOOL`) |\n| `defaultInput` | Defaults to an input socket rather than a supported widget |\n| `forceInput` | `defaultInput` and also donâ€™t allow converting to a widget |\n| `multiline` | Use a multiline text box (`STRING`) |\n| `placeholder` | Placeholder text to display in the UI when empty (`STRING`) |\n| `dynamicPrompts` | Causes the front-end to evaluate dynamic prompts |\n| `lazy` | Declares that this input uses [Lazy Evaluation](https://docs.comfy.org/custom-nodes/backend/lazy_evaluation) |\n| `rawLink` | When a link exists, rather than receiving the evaluated value, you will receive the link (i.e. `[\"nodeId\", <outputIndex>]`). Primarily useful when your node uses [Node Expansion](https://docs.comfy.org/custom-nodes/backend/expansion). |"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/expansion",
  "markdown": "# Node Expansion - ComfyUI\n\nNormally, when a node is executed, that execution function immediately returns the output results of that node. â€œNode Expansionâ€ is a relatively advanced technique that allows nodes to return a new subgraph of nodes that should take its place in the graph. This technique is what allows custom nodes to implement loops.\n\n### A Simple Example\n\nFirst, hereâ€™s a simple example of what node expansion looks like:\n\n```\ndef load_and_merge_checkpoints(self, checkpoint_path1, checkpoint_path2, ratio):\n    from comfy_execution.graph_utils import GraphBuilder # Usually at the top of the file\n    graph = GraphBuilder()\n    checkpoint_node1 = graph.node(\"CheckpointLoaderSimple\", checkpoint_path=checkpoint_path1)\n    checkpoint_node2 = graph.node(\"CheckpointLoaderSimple\", checkpoint_path=checkpoint_path2)\n    merge_model_node = graph.node(\"ModelMergeSimple\", model1=checkpoint_node1.out(0), model2=checkpoint_node2.out(0), ratio=ratio)\n    merge_clip_node = graph.node(\"ClipMergeSimple\", clip1=checkpoint_node1.out(1), clip2=checkpoint_node2.out(1), ratio=ratio)\n    return {\n        # Returning (MODEL, CLIP, VAE) outputs\n        \"result\": (merge_model_node.out(0), merge_clip_node.out(0), checkpoint_node1.out(2)),\n        \"expand\": graph.finalize(),\n    }\n```\n\nWhile this same node could previously be implemented by manually calling into ComfyUI internals, using expansion means that each subnode will be cached separately (so if you change `model2`, you donâ€™t have to reload `model1`).\n\n### Requirements\n\nIn order to perform node expansion, a node must return a dictionary with the following keys:\n\n1.  `result`: A tuple of the outputs of the node. This may be a mix of finalized values (like you would return from a normal node) and node outputs.\n2.  `expand`: The finalized graph to perform expansion on. See below if you are not using the `GraphBuilder`.\n\n#### Additional Requirements if not using GraphBuilder\n\nThe format expected from the `expand` key is the same as the ComfyUI API format. The following requirements are handled by the `GraphBuilder`, but must be handled manually if you choose to forego it:\n\n1.  Node IDs must be unique across the entire graph. (This includes between multiple executions of the same node due to the use of lists.)\n2.  Node IDs must be deterministic and consistent between multiple executions of the graph (including partial executions due to caching).\n\nEven if you donâ€™t want to use the `GraphBuilder` for actually building the graph (e.g. because youâ€™re loading the raw json of the graph from a file), you can use the `GraphBuilder.alloc_prefix()` function to generate a prefix and `comfy.graph_utils.add_graph_prefix` to fix existing graphs to meet these requirements.\n\n### Efficient Subgraph Caching\n\nWhile you can pass non-literal inputs to nodes within the subgraph (like torch tensors), this can inhibit caching _within_ the subgraph. When possible, you should pass links to subgraph objects rather than the node itself. (You can declare an input as a `rawLink` within the inputâ€™s [Additional Parameters](https://docs.comfy.org/custom-nodes/backend/datatypes#additional-parameters) to do this easily.)"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/images_and_masks",
  "markdown": "# Images, Latents, and Masks - ComfyUI\n\nWhen working with these datatypes, you will need to know about the `torch.Tensor` class. Complete documentation is [here](https://pytorch.org/docs/stable/tensors.html), or an introduction to the key concepts required for Comfy [here](https://docs.comfy.org/custom-nodes/backend/tensors).\n\nMost of the concepts below are illustrated in the [example code snippets](https://docs.comfy.org/custom-nodes/backend/snippets).\n\nAn IMAGE is a `torch.Tensor` with shape `[B,H,W,C]`, `C=3`. If you are going to save or load images, you will need to convert to and from `PIL.Image` format - see the code snippets below! Note that some `pytorch` operations offer (or expect) `[B,C,H,W]`, known as â€˜channel firstâ€™, for reasons of computational efficiency. Just be careful.\n\n### Working with PIL.Image\n\nIf you want to load and save images, youâ€™ll want to use PIL:\n\n```\nfrom PIL import Image, ImageOps\n```\n\n## Masks\n\nA MASK is a `torch.Tensor` with shape `[B,H,W]`. In many contexts, masks have binary values (0 or 1), which are used to indicate which pixels should undergo specific operations. In some cases values between 0 and 1 are used indicate an extent of masking, (for instance, to alter transparency, adjust filters, or composite layers).\n\n### Masks from the Load Image Node\n\nThe `LoadImage` node uses an imageâ€™s alpha channel (the â€œAâ€ in â€œRGBAâ€) to create MASKs. The values from the alpha channel are normalized to the range \\[0,1\\] (torch.float32) and then inverted. The `LoadImage` node always produces a MASK output when loading an image. Many images (like JPEGs) donâ€™t have an alpha channel. In these cases, `LoadImage` creates a default mask with the shape `[1, 64, 64]`.\n\n### Understanding Mask Shapes\n\nIn libraries like `numpy`, `PIL`, and many others, single-channel images (like masks) are typically represented as 2D arrays, shape `[H,W]`. This means the `C` (channel) dimension is implicit, and thus unlike IMAGE types, batches of MASKs have only three dimensions: `[B, H, W]`. It is not uncommon to encounter a mask which has had the `B` dimension implicitly squeezed, giving a tensor `[H,W]`. To use a MASK, you will often have to match shapes by unsqueezing to produce a shape `[B,H,W,C]` with `C=1` To unsqueezing the `C` dimension, so you should `unsqueeze(-1)`, to unsqueeze `B`, you `unsqueeze(0)`. If your node receives a MASK as input, you would be wise to always check `len(mask.shape)`.\n\n## Latents\n\nA LATENT is a `dict`; the latent sample is referenced by the key `samples` and has shape `[B,C,H,W]`, with `C=4`."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftroubleshooting%2Foverview",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/lazy_evaluation",
  "markdown": "# Lazy Evaluation - ComfyUI\n\nBy default, all `required` and `optional` inputs are evaluated before a node can be run. Sometimes, however, an input wonâ€™t necessarily be used and evaluating it would result in unnecessary processing. Here are some examples of nodes where lazy evaluation may be beneficial:\n\n1.  A `ModelMergeSimple` node where the ratio is either `0.0` (in which case the first model doesnâ€™t need to be loaded) or `1.0` (in which case the second model doesnâ€™t need to be loaded).\n2.  Interpolation between two images where the ratio (or mask) is either entirely `0.0` or entirely `1.0`.\n3.  A Switch node where one input determines which of the other inputs will be passed through.\n\n### Creating Lazy Inputs\n\nThere are two steps to making an input a â€œlazyâ€ input. They are:\n\n1.  Mark the input as lazy in the dictionary returned by `INPUT_TYPES`\n2.  Define a method named `check_lazy_status` (note: _not_ a class method) that will be called prior to evaluation to determine if any more inputs are necessary.\n\nTo demonstrate these, weâ€™ll make a â€œMixImagesâ€ node that interpolates between two images according to a mask. If the entire mask is `0.0`, we donâ€™t need to evaluate any part of the tree leading up to the second image. If the entire mask is `1.0`, we can skip evaluating the first image.\n\n#### Defining `INPUT_TYPES`\n\nDeclaring that an input is lazy is as simple as adding a `lazy: True` key-value pair to the inputâ€™s options dictionary.\n\n```\n@classmethod\ndef INPUT_TYPES(cls):\n    return {\n        \"required\": {\n            \"image1\": (\"IMAGE\",{\"lazy\": True}),\n            \"image2\": (\"IMAGE\",{\"lazy\": True}),\n            \"mask\": (\"MASK\",),\n        },\n    }\n```\n\nIn this example, `image1` and `image2` are both marked as lazy inputs, but `mask` will always be evaluated.\n\n#### Defining `check_lazy_status`\n\nA `check_lazy_status` method is called if there are one or more lazy inputs that are not yet available. This method receives the same arguments as the standard execution function. All available inputs are passed in with their final values while unavailable lazy inputs have a value of `None`. The responsibility of the `check_lazy_status` function is to return a list of the names of any lazy inputs that are needed to proceed. If all lazy inputs are available, the function should return an empty list. Note that `check_lazy_status` may be called multiple times. (For example, you might find after evaluating one lazy input that you need to evaluate another.)\n\n```\ndef check_lazy_status(self, mask, image1, image2):\n    mask_min = mask.min()\n    mask_max = mask.max()\n    needed = []\n    if image1 is None and (mask_min != 1.0 or mask_max != 1.0):\n        needed.append(\"image1\")\n    if image2 is None and (mask_min != 0.0 or mask_max != 0.0):\n        needed.append(\"image2\")\n    return needed\n```\n\n### Full Example\n\n```\nclass LazyMixImages:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image1\": (\"IMAGE\",{\"lazy\": True}),\n                \"image2\": (\"IMAGE\",{\"lazy\": True}),\n                \"mask\": (\"MASK\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"mix\"\n\n    CATEGORY = \"Examples\"\n\n    def check_lazy_status(self, mask, image1, image2):\n        mask_min = mask.min()\n        mask_max = mask.max()\n        needed = []\n        if image1 is None and (mask_min != 1.0 or mask_max != 1.0):\n            needed.append(\"image1\")\n        if image2 is None and (mask_min != 0.0 or mask_max != 0.0):\n            needed.append(\"image2\")\n        return needed\n\n    # Not trying to handle different batch sizes here just to keep the demo simple\n    def mix(self, mask, image1, image2):\n        mask_min = mask.min()\n        mask_max = mask.max()\n        if mask_min == 0.0 and mask_max == 0.0:\n            return (image1,)\n        elif mask_min == 1.0 and mask_max == 1.0:\n            return (image2,)\n\n        result = image1 * (1. - mask) + image2 * mask,\n        return (result[0],)\n```\n\n## Execution Blocking\n\nWhile Lazy Evaluation is the recommended way to â€œdisableâ€ part of a graph, there are times when you want to disable an `OUTPUT` node that doesnâ€™t implement lazy evaluation itself. If itâ€™s an output node that you developed yourself, you should just add lazy evaluation as follows:\n\n1.  Add a required (if this is a new node) or optional (if you care about backward compatibility) input for `enabled` that defaults to `True`\n2.  Make all other inputs `lazy` inputs\n3.  Only evaluate the other inputs if `enabled` is `True`\n\nIf itâ€™s not a node you control, you can make use of a `comfy_execution.graph.ExecutionBlocker`. This special object can be returned as an output from any socket. Any nodes which receive an `ExecutionBlocker` as input will skip execution and return that `ExecutionBlocker` for any outputs.\n\n### Usage\n\nThere are two ways to construct and use an `ExecutionBlocker`\n\n1.  Pass `None` into the constructor to silently block execution. This is useful for cases where blocking execution is part of a successful run â€” like disabling an output.\n\n```\ndef silent_passthrough(self, passthrough, blocked):\n    if blocked:\n        return (ExecutionBlocker(None),)\n    else:\n        return (passthrough,)\n```\n\n2.  Pass a string into the constructor to display an error message when a node is blocked due to receiving the object. This can be useful if you want to display a meaningful error message if someone uses a meaningless output â€” for example, the `VAE` output when loading a model that doesnâ€™t contain VAEs.\n\n```\ndef load_checkpoint(self, ckpt_name):\n    ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n    model, clip, vae = load_checkpoint(ckpt_path)\n    if vae is None:\n        # This error is more useful than a \"'NoneType' has no attribute\" error\n        # in a later node\n        vae = ExecutionBlocker(f\"No VAE contained in the loaded model {ckpt_name}\")\n    return (model, clip, vae)\n```"
},
{
  "url": "https://docs.comfy.org/comfy-cli/getting-started",
  "markdown": "# Getting Started - ComfyUI\n\n### Overview\n\n`comfy-cli` is a [command line tool](https://github.com/Comfy-Org/comfy-cli) that makes it easier to install and manage Comfy.\n\n### Install CLI\n\nTo get shell completion hints:\n\n```\ncomfy --install-completion\n```\n\n### Install ComfyUI\n\nCreate a virtual environment with any Python version greater than 3.9.\n\nInstall ComfyUI\n\n### Run ComfyUI\n\n### Manage Custom Nodes\n\n```\ncomfy node install <NODE_NAME>\n```\n\nWe use `cm-cli` for installing custom nodes. See the [docs](https://github.com/ltdrdata/ComfyUI-Manager/blob/main/docs/en/cm-cli.md) for more information.\n\n### Manage Models\n\nDownloading models with `comfy-cli` is easy. Just run:\n\n```\ncomfy model download <url> models/checkpoints\n```\n\n### Contributing\n\nWe encourage contributions to comfy-cli! If you have suggestions, ideas, or bug reports, please open an issue on our [GitHub repository](https://github.com/Comfy-Org/comfy-cli/issues). If you want to contribute code, fork the repository and submit a pull request. Refer to the [Dev Guide](https://github.com/Comfy-Org/comfy-cli/blob/main/DEV_README.md) for further details.\n\n### Analytics\n\nWe track usage of the CLI to improve the user experience. You can disable this by running:\n\nTo re-enable tracking, run:"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipSetLastLayer",
  "markdown": "# ClipSetLastLayer - ComfyUI Built-in Node Documentation\n\n`CLIP Set Last Layer` is a core node in ComfyUI for controlling the processing depth of CLIP models. It allows users to precisely control where the CLIP text encoder stops processing, affecting both the depth of text understanding and the style of generated images. Imagine the CLIP model as a 24-layer intelligent brain:\n\n*   Shallow layers (1-8): Recognize basic letters and words\n*   Middle layers (9-16): Understand grammar and sentence structure\n*   Deep layers (17-24): Grasp abstract concepts and complex semantics\n\n`CLIP Set Last Layer` works like a **â€œthinking depth controllerâ€**: \\-1: Use all 24 layers (complete understanding) -2: Stop at layer 23 (slightly simplified) -12: Stop at layer 13 (medium understanding) -24: Use only layer 1 (basic understanding)\n\n## Inputs\n\n| Parameter | Data Type | Default | Range | Description |\n| --- | --- | --- | --- | --- |\n| `clip` | CLIP | \\-  | \\-  | The CLIP model to be modified |\n| `stop_at_clip_layer` | INT | \\-1 | \\-24 to -1 | Specifies which layer to stop at, -1 uses all layers, -24 uses only the first layer |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| clip | CLIP | The modified CLIP model with the specified layer set as the last one |\n\n## Why Set the Last Layer\n\n*   **Performance Optimization**: Like not needing a PhD to understand simple sentences, sometimes shallow understanding is enough and faster\n*   **Style Control**: Different levels of understanding produce different artistic styles\n*   **Compatibility**: Some models might perform better at specific layers"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncode",
  "markdown": "# ClipTextEncode - ComfyUI Built-in Node Documentation\n\n`CLIP Text Encode (CLIPTextEncode)` acts like a translator, converting your creative text prompts into a special â€œlanguageâ€ that AI can understand, helping the AI accurately interpret what kind of image you want to create. Imagine communicating with a foreign artist - you need a translator to help accurately convey the artwork you want. This node acts as that translator, using the CLIP model (an AI model trained on vast amounts of image-text pairs) to understand your text descriptions and convert them into â€œinstructionsâ€ that the AI art model can understand.\n\n## Inputs\n\n| Parameter | Data Type | Input Method | Default | Range | Description |\n| --- | --- | --- | --- | --- | --- |\n| text | STRING | Text Input | Empty | Any text | Like detailed instructions to an artist, enter your image description here. Supports multi-line text for detailed descriptions. |\n| clip | CLIP | Model Selection | None | Loaded CLIP models | Like choosing a specific translator, different CLIP models are like different translators with slightly different understandings of artistic styles. |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| CONDITIONING | CONDITIONING | These are the translated â€œpainting instructionsâ€ containing detailed creative guidance that the AI model can understand. These instructions tell the AI model how to create an image matching your description. |\n\n## Usage Tips\n\n1.  **Basic Text Prompt Usage**\n    *   Write detailed descriptions like youâ€™re writing a short essay\n    *   More specific descriptions lead to more accurate results\n    *   Use English commas to separate different descriptive elements\n2.  **Special Feature: Using Embedding Models**\n    *   Embedding models are like preset art style packages that can quickly apply specific artistic effects\n    *   Currently supports .safetensors, .pt, and .bin file formats, and you donâ€™t necessarily need to use the complete model name\n    *   How to use:\n        \n        1.  Place the embedding model file (in .pt format) in the `ComfyUI/models/embeddings` folder\n        2.  Use `embedding:model_name` in your text Example: If you have a model called `EasyNegative.pt`, you can use it like this:\n        \n        ```\n        a beautiful landscape, embedding:EasyNegative, high quality\n        ```\n        \n3.  **Prompt Weight Adjustment**\n    *   Use parentheses to adjust the importance of certain descriptions\n    *   For example: `(beautiful:1.2)` will make the â€œbeautifulâ€ feature more prominent\n    *   Regular parentheses `()` have a default weight of 1.1\n    *   Use keyboard shortcuts `ctrl + up/down arrow` to quickly adjust weights\n    *   The weight adjustment step size can be modified in settings\n4.  **Important Notes**\n    *   Ensure the CLIP model is properly loaded\n    *   Use positive and clear text descriptions\n    *   When using embedding models, make sure the file name is correct and compatible with your current main modelâ€™s architecture\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)\n*   [Usage Tips](#usage-tips)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipVisionEncode",
  "markdown": "# ClipVisionEncode - ComfyUI Built-in Node Documentation\n\nThe `CLIP Vision Encode` node is an image encoding node in ComfyUI, used to convert input images into visual feature vectors through the CLIP Vision model. This node is an important bridge connecting image and text understanding, and is widely used in various AI image generation and processing workflows. **Node Functionality**\n\n*   **Image feature extraction**: Converts input images into high-dimensional feature vectors\n*   **Multimodal bridging**: Provides a foundation for joint processing of images and text\n*   **Conditional generation**: Provides visual conditions for image-based conditional generation\n\n## Inputs\n\n| Parameter Name | Data Type | Description |\n| --- | --- | --- |\n| `clip_vision` | CLIP\\_VISION | CLIP vision model, usually loaded via the CLIPVisionLoader node |\n| `image` | IMAGE | The input image to be encoded |\n| `crop` | Dropdown | Image cropping method, options: center (center crop), none (no crop) |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| CLIP\\_VISION\\_OUTPUT | CLIP\\_VISION\\_OUTPUT | Encoded visual features |\n\nThis output object contains:\n\n*   `last_hidden_state`: The last hidden state\n*   `image_embeds`: Image embedding vector\n*   `penultimate_hidden_states`: The penultimate hidden state\n*   `mm_projected`: Multimodal projection result (if available)\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/Canny",
  "markdown": "# Canny - ComfyUI Built-in Node Documentation\n\nExtract all edge lines from photos, like using a pen to outline a photo, drawing out the contours and detail boundaries of objects.\n\n## Working Principle\n\nImagine you are an artist who needs to use a pen to outline a photo. The Canny node acts like an intelligent assistant, helping you decide where to draw lines (edges) and where not to. This process is like a screening job:\n\n*   **High threshold** is the â€œmust draw line standardâ€: only very obvious and clear contour lines will be drawn, such as facial contours of people and building frames\n*   **Low threshold** is the â€œdefinitely donâ€™t draw line standardâ€: edges that are too weak will be ignored to avoid drawing noise and meaningless lines\n*   **Middle area**: edges between the two standards will be drawn together if they connect to â€œmust draw linesâ€, but wonâ€™t be drawn if they are isolated\n\nThe final output is a black and white image, where white parts are detected edge lines and black parts are areas without edges.\n\n## Inputs\n\n| Parameter Name | Data Type | Input Type | Default | Range | Function Description |\n| --- | --- | --- | --- | --- | --- |\n| `image` | IMAGE | Input | \\-  | \\-  | Original photo that needs edge extraction |\n| `low_threshold` | FLOAT | Widget | 0.4 | 0.01-0.99 | Low threshold, determines how weak edges to ignore. Lower values preserve more details but may produce noise |\n| `high_threshold` | FLOAT | Widget | 0.8 | 0.01-0.99 | High threshold, determines how strong edges to preserve. Higher values only keep the most obvious contour lines |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| `image` | IMAGE | Black and white edge image, white lines are detected edges, black areas are parts without edges |\n\n## Parameter Comparison\n\n![Original Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/canny/input.webp) ![Parameter Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/canny/compare.webp) **Common Issues:**\n\n*   Broken edges: Try lowering high threshold\n*   Too much noise: Raise low threshold\n*   Missing important details: Lower low threshold\n*   Edges too rough: Check input image quality and resolution"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/CheckpointLoaderSimple",
  "markdown": "# CheckpointLoaderSimple - ComfyUI Built-in Node Documentation\n\nThis is a model loader node that loads model files from specified locations and decomposes them into three core components: the main model, text encoder, and image encoder/decoder. This node automatically detects all model files in the `ComfyUI/models/checkpoints` folder, as well as additional paths configured in your `extra_model_paths.yaml` file.\n\n1.  **Model Compatibility**: Ensure the selected model is compatible with your workflow. Different model types (such as SD1.5, SDXL, Flux, etc.) need to be paired with corresponding samplers and other nodes\n2.  **File Management**: Place model files in the `ComfyUI/models/checkpoints` folder, or configure other paths through extra\\_model\\_paths.yaml\n3.  **Interface Refresh**: If new model files are added while ComfyUI is running, you need to refresh the browser (Ctrl+R) to see the new files in the dropdown list\n\n## Inputs\n\n| Parameter | Data Type | Input Type | Default | Range | Description |\n| --- | --- | --- | --- | --- | --- |\n| `ckpt_name` | STRING | Widget | null | All model files in checkpoints folder | Select the checkpoint model file name to load, which determines the AI model used for subsequent image generation |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| `MODEL` | MODEL | The main diffusion model used for image denoising generation, the core component of AI image creation |\n| `CLIP` | CLIP | The model used for encoding text prompts, converting text descriptions into information that AI can understand |\n| `VAE` | VAE | The model used for image encoding and decoding, responsible for converting between pixel space and latent space |\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipLoader",
  "markdown": "# ClipLoader - ComfyUI Built-in Node Documentation\n\nThis node is primarily used for loading CLIP text encoder models independently. The model files can be detected in the following paths:\n\n*   â€œComfyUI/models/text\\_encoders/â€\n*   â€œComfyUI/models/clip/â€\n\n> If you save a model after ComfyUI has started, youâ€™ll need to refresh the ComfyUI frontend to get the latest model file path list\n\nSupported model formats:\n\n*   `.ckpt`\n*   `.pt`\n*   `.pt2`\n*   `.bin`\n*   `.pth`\n*   `.safetensors`\n*   `.pkl`\n*   `.sft`\n\nFor more details on the latest model file loading, please refer to [folder\\_paths](https://github.com/comfyanonymous/ComfyUI/blob/master/folder_paths.py)\n\n## Inputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip_name` | COMBO\\[STRING\\] | Specifies the name of the CLIP model to be loaded. This name is used to locate the model file within a predefined directory structure. |\n| `type` | COMBO\\[STRING\\] | Determines the type of CLIP model to load. As ComfyUI supports more models, new types will be added here. Please check the `CLIPLoader` class definition in [node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py) for details. |\n| `device` | COMBO\\[STRING\\] | Choose the device for loading the CLIP model. `default` will run the model on GPU, while selecting `CPU` will force loading on CPU. |\n\n### Device Options Explained\n\n**When to choose â€œdefaultâ€:**\n\n*   Have sufficient GPU memory\n*   Want the best performance\n*   Let the system optimize memory usage automatically\n\n**When to choose â€œcpuâ€:**\n\n*   Insufficient GPU memory\n*   Need to reserve GPU memory for other models (like UNet)\n*   Running in a low VRAM environment\n*   Debugging or special purpose needs\n\n**Performance Impact** Running on CPU will be much slower than GPU, but it can save valuable GPU memory for other more important model components. In memory-constrained environments, putting the CLIP model on CPU is a common optimization strategy.\n\n### Supported Combinations\n\n| Model Type | Corresponding Encoder |\n| --- | --- |\n| stable\\_diffusion | clip-l |\n| stable\\_cascade | clip-g |\n| sd3 | t5 xxl/ clip-g / clip-l |\n| stable\\_audio | t5 base |\n| mochi | t5 xxl |\n| cosmos | old t5 xxl |\n| lumina2 | gemma 2 2B |\n| wan | umt5 xxl |\n\nAs ComfyUI updates, these combinations may expand. For details, please refer to the `CLIPLoader` class definition in [node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py)\n\n## Outputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip` | CLIP | The loaded CLIP model, ready for use in downstream tasks or further processing. |\n\n## Additional Notes\n\nCLIP models play a core role as text encoders in ComfyUI, responsible for converting text prompts into numerical representations that diffusion models can understand. You can think of them as translators, responsible for translating your text into a language that large models can understand. Of course, different models have their own â€œdialects,â€ so different CLIP encoders are needed between different architectures to complete the text encoding process."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipVisionLoader",
  "markdown": "# Load CLIP Vision - ComfyUI Built-in Node Documentation\n\nThis node automatically detects models located in the `ComfyUI/models/clip_vision` folder, as well as any additional model paths configured in the `extra_model_paths.yaml` file. If you add models after starting ComfyUI, please **refresh the ComfyUI interface** to ensure the latest model files are listed.\n\n## Inputs\n\n| Field | Data Type | Description |\n| --- | --- | --- |\n| `clip_name` | COMBO\\[STRING\\] | Lists all supported model files in the `ComfyUI/models/clip_vision` folder. |\n\n## Outputs\n\n| Field | Data Type | Description |\n| --- | --- | --- |\n| `clip_vision` | CLIP\\_VISION | Loaded CLIP Vision model, ready for encoding images or other vision-related tasks. |\n\nOn this page\n\n*   [Inputs](#inputs)\n*   [Outputs](#outputs)"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_about_panel_badges",
  "markdown": "# About Panel Badges - ComfyUI\n\nThe About Panel Badges API allows extensions to add custom badges to the ComfyUI about page. These badges can display information about your extension and contain links to documentation, source code, or other resources.\n\n## Basic Usage\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  aboutPageBadges: [\n    {\n      label: \"Documentation\",\n      url: \"https://example.com/docs\",\n      icon: \"pi pi-file\"\n    },\n    {\n      label: \"GitHub\",\n      url: \"https://github.com/username/repo\",\n      icon: \"pi pi-github\"\n    }\n  ]\n});\n```\n\n## Badge Configuration\n\nEach badge requires all of these properties:\n\n```\n{\n  label: string,           // Text to display on the badge\n  url: string,             // URL to open when badge is clicked\n  icon: string             // Icon class (e.g., PrimeVue icon)\n}\n```\n\n## Icon Options\n\nBadge icons use PrimeVueâ€™s icon set. Here are some commonly used icons:\n\n*   Documentation: `pi pi-file` or `pi pi-book`\n*   GitHub: `pi pi-github`\n*   External link: `pi pi-external-link`\n*   Information: `pi pi-info-circle`\n*   Download: `pi pi-download`\n*   Website: `pi pi-globe`\n*   Discord: `pi pi-discord`\n\nFor a complete list of available icons, refer to the [PrimeVue Icons documentation](https://primevue.org/icons/).\n\n## Example\n\n```\napp.registerExtension({\n  name: \"BadgeExample\",\n  aboutPageBadges: [\n    {\n      label: \"Website\",\n      url: \"https://example.com\",\n      icon: \"pi pi-home\"\n    },\n    {\n      label: \"Donate\",\n      url: \"https://example.com/donate\",\n      icon: \"pi pi-heart\"\n    },\n    {\n      label: \"Documentation\",\n      url: \"https://example.com/docs\",\n      icon: \"pi pi-book\"\n    }\n  ]\n});\n```\n\nBadges appear in the About panel of the Settings dialog, which can be accessed via the gear icon in the top-right corner of the ComfyUI interface."
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_commands_keybindings",
  "markdown": "# Commands and Keybindings - ComfyUI\n\nThe Commands and Keybindings API allows extensions to register custom commands and associate them with keyboard shortcuts. This enables users to quickly trigger actions without using the mouse.\n\n## Basic Usage\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  // Register commands\n  commands: [\n    {\n      id: \"myCommand\",\n      label: \"My Command\",\n      function: () => {\n        console.log(\"Command executed!\");\n      }\n    }\n  ],\n  // Associate keybindings with commands\n  keybindings: [\n    {\n      combo: { key: \"k\", ctrl: true },\n      commandId: \"myCommand\"\n    }\n  ]\n});\n```\n\n## Command Configuration\n\nEach command requires an `id`, `label`, and `function`:\n\n```\n{\n  id: string,              // Unique identifier for the command\n  label: string,           // Display name for the command\n  function: () => void     // Function to execute when command is triggered\n}\n```\n\n## Keybinding Configuration\n\nEach keybinding requires a `combo` and `commandId`:\n\n```\n{\n  combo: {                 // Key combination\n    key: string,           // The main key (single character or special key)\n    ctrl?: boolean,        // Require Ctrl key (optional)\n    shift?: boolean,       // Require Shift key (optional)\n    alt?: boolean,         // Require Alt key (optional)\n    meta?: boolean         // Require Meta/Command key (optional)\n  },\n  commandId: string        // ID of the command to trigger\n}\n```\n\n### Special Keys\n\nFor non-character keys, use one of these values:\n\n*   Arrow keys: `\"ArrowUp\"`, `\"ArrowDown\"`, `\"ArrowLeft\"`, `\"ArrowRight\"`\n*   Function keys: `\"F1\"` through `\"F12\"`\n*   Other special keys: `\"Escape\"`, `\"Tab\"`, `\"Enter\"`, `\"Backspace\"`, `\"Delete\"`, `\"Home\"`, `\"End\"`, `\"PageUp\"`, `\"PageDown\"`\n\n## Command Examples\n\n```\napp.registerExtension({\n  name: \"CommandExamples\",\n  commands: [\n    {\n      id: \"runWorkflow\",\n      label: \"Run Workflow\",\n      function: () => {\n        app.queuePrompt();\n      }\n    },\n    {\n      id: \"clearWorkflow\",\n      label: \"Clear Workflow\",\n      function: () => {\n        if (confirm(\"Clear the workflow?\")) {\n          app.graph.clear();\n        }\n      }\n    },\n    {\n      id: \"saveWorkflow\",\n      label: \"Save Workflow\",\n      function: () => {\n        app.graphToPrompt().then(workflow => {\n          const blob = new Blob([JSON.stringify(workflow)], {type: \"application/json\"});\n          const url = URL.createObjectURL(blob);\n          const a = document.createElement(\"a\");\n          a.href = url;\n          a.download = \"workflow.json\";\n          a.click();\n          URL.revokeObjectURL(url);\n        });\n      }\n    }\n  ]\n});\n```\n\n## Keybinding Examples\n\n```\napp.registerExtension({\n  name: \"KeybindingExamples\",\n  commands: [\n    /* Commands defined above */\n  ],\n  keybindings: [\n    // Ctrl+R to run workflow\n    {\n      combo: { key: \"r\", ctrl: true },\n      commandId: \"runWorkflow\"\n    },\n    // Ctrl+Shift+C to clear workflow\n    {\n      combo: { key: \"c\", ctrl: true, shift: true },\n      commandId: \"clearWorkflow\"\n    },\n    // Ctrl+S to save workflow\n    {\n      combo: { key: \"s\", ctrl: true },\n      commandId: \"saveWorkflow\"\n    },\n    // F5 to run workflow (alternative)\n    {\n      combo: { key: \"F5\" },\n      commandId: \"runWorkflow\"\n    }\n  ]\n});\n```\n\n## Notes and Limitations\n\n*   Keybindings defined in the ComfyUI core cannot be overwritten by extensions. Check the core keybindings in these source files:\n    *   [Core Commands](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/composables/useCoreCommands.ts)\n    *   [Core Menu Commands](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreMenuCommands.ts)\n    *   [Core Keybindings](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreKeybindings.ts)\n    *   [Reserved Key Combos](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/reservedKeyCombos.ts)\n*   Some key combinations are reserved by the browser (like Ctrl+F for search) and cannot be overridden\n*   If multiple extensions register the same keybinding, the behavior is undefined"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_dialog",
  "markdown": "# Dialog API - ComfyUI\n\nThe Dialog API provides standardized dialogs that work consistently across desktop and web environments. Extension authors will find the prompt and confirm methods most useful.\n\n## Basic Usage\n\n### Prompt Dialog\n\n```\n// Show a prompt dialog\napp.extensionManager.dialog.prompt({\n  title: \"User Input\",\n  message: \"Please enter your name:\",\n  defaultValue: \"User\"\n}).then(result => {\n  if (result !== null) {\n    console.log(`Input: ${result}`);\n  }\n});\n```\n\n### Confirm Dialog\n\n```\n// Show a confirmation dialog\napp.extensionManager.dialog.confirm({\n  title: \"Confirm Action\",\n  message: \"Are you sure you want to continue?\",\n  type: \"default\"\n}).then(result => {\n  console.log(result ? \"User confirmed\" : \"User cancelled\");\n});\n```\n\n## API Reference\n\n### Prompt\n\n```\napp.extensionManager.dialog.prompt({\n  title: string,             // Dialog title\n  message: string,           // Message/question to display\n  defaultValue?: string      // Initial value in the input field (optional)\n}).then((result: string | null) => {\n  // result is the entered text, or null if cancelled\n});\n```\n\n### Confirm\n\n```\napp.extensionManager.dialog.confirm({\n  title: string,             // Dialog title\n  message: string,           // Message to display\n  type?: \"default\" | \"overwrite\" | \"delete\" | \"dirtyClose\" | \"reinstall\", // Dialog type (optional)\n  itemList?: string[],       // List of items to display (optional)\n  hint?: string              // Hint text to display (optional)\n}).then((result: boolean | null) => {\n  // result is true if confirmed, false if denied, null if cancelled\n});\n```\n\nFor other specialized dialogs available in ComfyUI, extension authors can refer to the `dialogService.ts` file in the source code."
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_examples",
  "markdown": "# Annotated Examples - ComfyUI\n\nA growing collection of fragments of example codeâ€¦\n\nThe main background menu (right-click on the canvas) is generated by a call to  \n`LGraph.getCanvasMenuOptions`. One way to add your own menu options is to hijack this call:\n\n```\n/* in setup() */\n    const original_getCanvasMenuOptions = LGraphCanvas.prototype.getCanvasMenuOptions;\n    LGraphCanvas.prototype.getCanvasMenuOptions = function () {\n        // get the basic options \n        const options = original_getCanvasMenuOptions.apply(this, arguments);\n        options.push(null); // inserts a divider\n        options.push({\n            content: \"The text for the menu\",\n            callback: async () => {\n                // do whatever\n            }\n        })\n        return options;\n    }\n```\n\nWhen you right click on a node, the menu is similarly generated by `node.getExtraMenuOptions`. But instead of returning an options object, this one gets it passed inâ€¦\n\n```\n/* in beforeRegisterNodeDef() */\nif (nodeType?.comfyClass==\"MyNodeClass\") { \n    const original_getExtraMenuOptions = nodeType.prototype.getExtraMenuOptions;\n    nodeType.prototype.getExtraMenuOptions = function(_, options) {\n        original_getExtraMenuOptions?.apply(this, arguments);\n        options.push({\n            content: \"Do something fun\",\n            callback: async () => {\n                // fun thing\n            }\n        })\n    }   \n}\n```\n\nIf you want a submenu, provide a callback which uses `LiteGraph.ContextMenu` to create it:\n\n```\nfunction make_submenu(value, options, e, menu, node) {\n    const submenu = new LiteGraph.ContextMenu(\n        [\"option 1\", \"option 2\", \"option 3\"],\n        { \n            event: e, \n            callback: function (v) { \n                // do something with v (==\"option x\")\n            }, \n            parentMenu: menu, \n            node:node\n        }\n    )\n}\n\n/* ... */\n    options.push(\n        {\n            content: \"Menu with options\",\n            has_submenu: true,\n            callback: make_submenu,\n        }\n    )\n```\n\n## Capture UI events\n\nThis works just like youâ€™d expect - find the UI element in the DOM and add an eventListener. `setup()` is a good place to do this, since the page has fully loaded. For instance, to detect a click on the â€˜Queueâ€™ button:\n\n```\nfunction queue_button_pressed() { console.log(\"Queue button was pressed!\") }\ndocument.getElementById(\"queue-button\").addEventListener(\"click\", queue_button_pressed);\n```\n\n## Detect when a workflow starts\n\nThis is one of many `api` events:\n\n```\nimport { api } from \"../../scripts/api.js\";\n/* in setup() */\n    function on_execution_start() { \n        /* do whatever */\n    }\n    api.addEventListener(\"execution_start\", on_execution_start);\n```\n\n## Detect an interrupted workflow\n\nA simple example of hijacking the api:\n\n```\nimport { api } from \"../../scripts/api.js\";\n/* in setup() */\n    const original_api_interrupt = api.interrupt;\n    api.interrupt = function () {\n        /* Do something before the original method is called */\n        original_api_interrupt.apply(this, arguments);\n        /* Or after */\n    }\n```\n\n## Catch clicks on your node\n\n`node` has a mouseDown method you can hijack. This time weâ€™re careful to pass on any return value.\n\n```\nasync nodeCreated(node) {\n    if (node?.comfyClass === \"My Node Name\") {\n        const original_onMouseDown = node.onMouseDown;\n        node.onMouseDown = function( e, pos, canvas ) {\n            alert(\"ouch!\");\n            return original_onMouseDown?.apply(this, arguments);\n        }        \n    }\n}\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_hooks",
  "markdown": "# Comfy Hooks - ComfyUI\n\n## Extension hooks\n\nAt various points during Comfy execution, the application calls `#invokeExtensionsAsync` or `#invokeExtensions` with the name of a hook. These invoke, on all registered extensions, the appropriately named method (if present), such as `setup` in the example above. Comfy provides a variety of hooks for custom extension code to use to modify client behavior.\n\nA few of the most significant hooks are described below. As Comfy is being actively developed, from time to time additional hooks are added, so search for `#invokeExtensions` in `app.js` to find all available hooks. See also the [sequence](#call-sequences) in which hooks are invoked.\n\n### Commonly used hooks\n\nStart with `beforeRegisterNodeDef`, which is used by the majority of extensions, and is often the only one needed.\n\n#### beforeRegisterNodeDef()\n\nCalled once for each node type (the list of nodes available in the `AddNode` menu), and is used to modify the behaviour of the node.\n\n```\nasync beforeRegisterNodeDef(nodeType, nodeData, app) \n```\n\nThe object passed in the `nodeType` parameter essentially serves as a template for all nodes that will be created of this type, so modifications made to `nodeType.prototype` will apply to all nodes of this type. `nodeData` is an encapsulation of aspects of the node defined in the Python code, such as its category, inputs, and outputs. `app` is a reference to the main Comfy app object (which you have already imported anyway!)\n\nThe usual idiom is to check `nodeType.ComfyClass`, which holds the Python class name corresponding to this node, to see if you need to modify the node. Often this means modifying the custom nodes that you have added, although you may sometimes need to modify the behavior of other nodes (or other custom nodes might modify yours!), in which case care should be taken to ensure interoperability.\n\nA very common idiom in `beforeRegisterNodeDef` is to â€˜hijackâ€™ an existing method:\n\n```\nasync beforeRegisterNodeDef(nodeType, nodeData, app) {\n\tif (nodeType.comfyClass==\"MyNodeClass\") { \n\t\tconst onConnectionsChange = nodeType.prototype.onConnectionsChange;\n\t\tnodeType.prototype.onConnectionsChange = function (side,slot,connect,link_info,output) {     \n\t\t\tconst r = onConnectionsChange?.apply(this, arguments);   \n\t\t\tconsole.log(\"Someone changed my connection!\");\n\t\t\treturn r;\n\t\t}\n\t}\n}\n```\n\nIn this idiom the existing prototype method is stored, and then replaced. The replacement calls the original method (the `?.apply` ensures that if there wasnâ€™t one this is still safe) and then performs additional operations. Depending on your code logic, you may need to place the `apply` elsewhere in your replacement code, or even make calling it conditional. When hijacking a method in this way, you will want to look at the core comfy code (breakpoints are your friend) to check and conform with the method signature.\n\n#### nodeCreated()\n\nCalled when a specific instance of a node gets created (right at the end of the `ComfyNode()` function on `nodeType` which serves as a constructor). In this hook you can make modifications to individual instances of your node.\n\n#### init()\n\nCalled when the Comfy webpage is loaded (or reloaded). The call is made after the graph object has been created, but before any nodes are registered or created. It can be used to modify core Comfy behavior by hijacking methods of the app, or of the graph (a `LiteGraph` object). This is discussed further in [Comfy Objects](https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking).\n\n#### setup()\n\nCalled at the end of the startup process. A good place to add event listeners (either for Comfy events, or DOM events), or adding to the global menus, both of which are discussed elsewhere.\n\n### Call sequences\n\nThese sequences were obtained by insert logging code into the Comfy `app.js` file. You may find similar code helpful in understanding the execution flow.\n\n```\n/* approx line 220 at time of writing: */\n\t#invokeExtensions(method, ...args) {\n\t\tconsole.log(`invokeExtensions      ${method}`) // this line added\n\t\t// ...\n\t}\n/* approx line 250 at time of writing: */\n\tasync #invokeExtensionsAsync(method, ...args) {\n\t\tconsole.log(`invokeExtensionsAsync ${method}`) // this line added\n\t\t// ...\n\t}\n```\n\n#### Web page load\n\n```\ninvokeExtensionsAsync init\ninvokeExtensionsAsync addCustomNodeDefs\ninvokeExtensionsAsync getCustomWidgets\ninvokeExtensionsAsync beforeRegisterNodeDef    [repeated multiple times]\ninvokeExtensionsAsync registerCustomNodes\ninvokeExtensionsAsync beforeConfigureGraph\ninvokeExtensionsAsync nodeCreated\ninvokeExtensions      loadedGraphNode\ninvokeExtensionsAsync afterConfigureGraph\ninvokeExtensionsAsync setup\n```\n\n#### Loading workflow\n\n```\ninvokeExtensionsAsync beforeConfigureGraph\ninvokeExtensionsAsync beforeRegisterNodeDef   [zero, one, or multiple times]\ninvokeExtensionsAsync nodeCreated             [repeated multiple times]\ninvokeExtensions      loadedGraphNode         [repeated multiple times]\ninvokeExtensionsAsync afterConfigureGraph\n```\n\n#### Adding new node\n\n```\ninvokeExtensionsAsync nodeCreated\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/latent/video/trim-video-latent",
  "markdown": "# TrimVideoLatent Node - ComfyUI\n\n![ComfyUI TrimVideoLatent Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/latent/video/trim-video-latent.jpg) The TrimVideoLatent node is used to trim video frames in latent space (LATENT). It is commonly used when processing video latent sequences to remove unwanted frames from the beginning, achieving â€œforward trimmingâ€ of the video. Basic usage: Input the video latent data to be trimmed into samples, and set trim\\_amount to the number of frames to trim. The node will trim the specified number of frames from the beginning of the video and output the remaining latent sequence. Typical scenarios: Used in video generation, video editing and other scenarios to remove unwanted leading frames, or to work with other nodes to achieve video segment splicing and processing.\n\n## Parameters\n\n### Input Parameters\n\n| Parameter | Type | Required | Default | Description |\n| --- | --- | --- | --- | --- |\n| samples | LATENT | Yes | None | Input latent video data |\n| trim\\_amount | INT | Yes | 0   | Number of frames to trim (from start) |\n\n### Output Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| samples | LATENT | Trimmed video latent data |\n\n## Usage Example\n\n[\n\n## Wan2.1 VACE Video Generation Workflow Example\n\nWan2.1 VACE Video Generation Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/video/wan/vace)\n\n### Source Code\n\n```\nclass TrimVideoLatent:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"trim_amount\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 99999}),\n                             }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/video\"\n\n    EXPERIMENTAL = True\n\n    def op(self, samples, trim_amount):\n        samples_out = samples.copy()\n\n        s1 = samples[\"samples\"]\n        samples_out[\"samples\"] = s1[:, :, trim_amount:]\n        return (samples_out,)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipMergeSimple",
  "markdown": "# ClipMergeSimple - ComfyUI Built-in Node Documentation\n\n`CLIPMergeSimple` is an advanced model merging node used to combine two CLIP text encoder models based on a specified ratio. This node specializes in merging two CLIP models based on a specified ratio, effectively blending their characteristics. It selectively applies patches from one model to another, excluding specific components like position IDs and logit scale, to create a hybrid model that combines features from both source models.\n\n## Inputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip1` | CLIP | The first CLIP model to be merged. It serves as the base model for the merging process. |\n| `clip2` | CLIP | The second CLIP model to be merged. Its key patches, except for position IDs and logit scale, are applied to the first model based on the specified ratio. |\n| `ratio` | FLOAT | Range `0.0 - 1.0`, determines the proportion of features from the second model to blend into the first model. A ratio of 1.0 means fully adopting the second modelâ€™s features, while 0.0 retains only the first modelâ€™s features. |\n\n## Outputs\n\n| Parameter | Data Type | Description |\n| --- | --- | --- |\n| `clip` | CLIP | The resulting merged CLIP model, incorporating features from both input models according to the specified ratio. |\n\n## Merging Mechanism Explained\n\n### Merging Algorithm\n\nThe node uses weighted averaging to merge the two models:\n\n1.  **Clone Base Model**: First clones `clip1` as the base model\n2.  **Get Patches**: Obtains all key patches from `clip2`\n3.  **Filter Special Keys**: Skips keys ending with `.position_ids` and `.logit_scale`\n4.  **Apply Weighted Merge**: Uses the formula `(1.0 - ratio) * clip1 + ratio * clip2`\n\n### Ratio Parameter Explained\n\n*   **ratio = 0.0**: Fully uses clip1, ignores clip2\n*   **ratio = 0.5**: 50% contribution from each model\n*   **ratio = 1.0**: Fully uses clip2, ignores clip1\n\n## Use Cases\n\n1.  **Model Style Fusion**: Combine characteristics of CLIP models trained on different data\n2.  **Performance Optimization**: Balance strengths and weaknesses of different models\n3.  **Experimental Research**: Explore combinations of different CLIP encoders"
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/comms_routes",
  "markdown": "# Routes - ComfyUI\n\nThe server defines a series of `get` and `post` methods which can be found by searching for `@routes` in `server.py`. When you submit a workflow in the web client, it is posted to `/prompt` which validates the prompt and adds it to an execution queue, returning either a `prompt_id` and `number` (the position in the queue), or `error` and `node_errors` if validation fails. The prompt queue is defined in `execution.py`, which also defines the `PromptExecutor` class.\n\n### Built in routes\n\n`server.py` defines the following routes:\n\n| path | get/post | purpose |\n| --- | --- | --- |\n| `/` | get | load the comfy webpage |\n| `/embeddings` | get | retrieve a list of the names of embeddings available |\n| `/extensions` | get | retrieve a list of the extensions registering a `WEB_DIRECTORY` |\n| `/workflow_templates` | get | retrieve a map of custom node modules and associated template workflows |\n| `/upload/image` | post | upload an image |\n| `/upload/mask` | post | upload a mask |\n| `/view` | get | view an image. Lots of options, see `@routes.get(\"/view\")` in `server.py` |\n| `/view_metadata`/ | get | retrieve metadata for a model |\n| `/system_stats` | get | retrieve information about the system (python version, devices, vram etc) |\n| `/prompt` | get | retrieve current status |\n| `/prompt` | post | submit a prompt to the queue |\n| `/object_info` | get | retrieve details of all node types |\n| `/object_info/{node_class}` | get | retrieve details of one node type |\n| `/history` | get | retrieve the queue history |\n| `/history/{prompt_id}` | get | retrieve the queue history for a specific prompt |\n| `/history` | post | clear history or delete history item |\n| `/queue` | get | retrieve the state of the queue |\n| `/interrupt` | post | stop the current workflow |\n| `/free` | post | free memory by unloading specified models |\n\n### Custom routes\n\nIf you want to send a message from the client to the server during execution, you will need to add a custom route to the server. For anything complicated, you will need to dive into the [aiohttp framework docs](https://docs.aiohttp.org/), but most cases can be handled as follows:\n\n```\nfrom server import PromptServer\nfrom aiohttp import web\nroutes = PromptServer.instance.routes\n@routes.post('/my_new_path')\nasync def my_function(request):\n    the_data = await request.post()\n    # the_data now holds a dictionary of the values sent\n    MyClass.handle_my_message(the_data)\n    return web.json_response({})\n```\n\nThe client can use this new route by sending a `FormData` object with code something like this, which would result in `the_data`, in the above code, containing `message` and `node_id` keys:\n\n```\nimport { api } from \"../../scripts/api.js\";\nfunction send_message(node_id, message) {\n    const body = new FormData();\n    body.append('message',message);\n    body.append('node_id', node_id);\n    api.fetchApi(\"/my_new_path\", { method: \"POST\", body, });\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipSave",
  "markdown": "# ClipSave - ComfyUI Built-in Node Documentation\n\nThe `CLIPSave` node is designed for saving CLIP text encoder models in SafeTensors format. This node is part of advanced model merging workflows and is typically used in conjunction with nodes like `CLIPMergeSimple` and `CLIPMergeAdd`. The saved files use the SafeTensors format to ensure security and compatibility.\n\n## Inputs\n\n| Parameter | Data Type | Required | Default Value | Description |\n| --- | --- | --- | --- | --- |\n| clip | CLIP | Yes | \\-  | The CLIP model to be saved |\n| filename\\_prefix | STRING | Yes | â€clip/ComfyUIâ€ | The prefix path for the saved file |\n| prompt | PROMPT | Hidden | \\-  | Workflow prompt information (for metadata) |\n| extra\\_pnginfo | EXTRA\\_PNGINFO | Hidden | \\-  | Additional PNG information (for metadata) |\n\n## Outputs\n\nThis node has no defined output types. It saves the processed files to the `ComfyUI/output/` folder.\n\n### Multi-file Saving Strategy\n\nThe node saves different components based on the CLIP model type:\n\n| Prefix Type | File Suffix | Description |\n| --- | --- | --- |\n| `clip_l.` | `_clip_l` | CLIP-L text encoder |\n| `clip_g.` | `_clip_g` | CLIP-G text encoder |\n| Empty prefix | No suffix | Other CLIP components |\n\n## Usage Notes\n\n1.  **File Location**: All files are saved in the `ComfyUI/output/` directory\n2.  **File Format**: Models are saved in SafeTensors format for security\n3.  **Metadata**: Includes workflow information and PNG metadata if available\n4.  **Naming Convention**: Uses the specified prefix plus appropriate suffixes based on model type"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/Load3D",
  "markdown": "# Load3D - ComfyUI Built-in Node Documentation\n\nThe Load3D node is a core node for loading and processing 3D model files. When loading the node, it automatically retrieves available 3D resources from `ComfyUI/input/3d/`. You can also upload supported 3D files for preview using the upload function. **Supported Formats** Currently, this node supports multiple 3D file formats, including `.gltf`, `.glb`, `.obj`, `.fbx`, and `.stl`. **3D Node Preferences** Some related preferences for 3D nodes can be configured in ComfyUIâ€™s settings menu. Please refer to the following documentation for corresponding settings: [Settings Menu - 3D](https://docs.comfy.org/interface/settings/3d) Besides regular node outputs, Load3D has lots of 3D view-related settings in the canvas menu.\n\n## Inputs\n\n| Parameter Name | Type | Description | Default | Range |\n| --- | --- | --- | --- | --- |\n| model\\_file | File Selection | 3D model file path, supports upload, defaults to reading model files from `ComfyUI/input/3d/` | \\-  | Supported formats |\n| width | INT | Canvas rendering width | 1024 | 1-4096 |\n| height | INT | Canvas rendering height | 1024 | 1-4096 |\n\n## Outputs\n\n| Parameter Name | Data Type | Description |\n| --- | --- | --- |\n| image | IMAGE | Canvas rendered image |\n| mask | MASK | Mask containing current model position |\n| mesh\\_path | STRING | Model file path |\n| normal | IMAGE | Normal map |\n| lineart | IMAGE | Line art image output, corresponding `edge_threshold` can be adjusted in the canvas model menu |\n| camera\\_info | LOAD3D\\_CAMERA | Camera information |\n| recording\\_video | VIDEO | Recorded video (only when recording exists) |\n\nAll corresponding outputs preview ![View Operation Demo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/load3d_outputs.jpg) \n\n## Canvas Area Description\n\nThe Load3D nodeâ€™s Canvas area contains numerous view operations, including:\n\n*   Preview view settings (grid, background color, preview view)\n*   Camera control: Control FOV, camera type\n*   Global illumination intensity: Adjust lighting intensity\n*   Video recording: Record and export videos\n*   Model export: Supports `GLB`, `OBJ`, `STL` formats\n*   And more\n\n![Load 3D Node UI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/load3d_ui.jpg)\n\n1.  Contains multiple menus and hidden menus of the Load 3D node\n2.  Menu for `resizing preview window` and `canvas video recording`\n3.  3D view operation axis\n4.  Preview thumbnail\n5.  Preview size settings, scale preview view display by setting dimensions and then resizing window\n\n### 1\\. View Operations\n\nView control operations:\n\n*   Left-click + drag: Rotate the view\n*   Right-click + drag: Pan the view\n*   Middle wheel scroll or middle-click + drag: Zoom in/out\n*   Coordinate axis: Switch views\n\n![Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu.jpg) In the canvas, some settings are hidden in the menu. Click the menu button to expand different menus\n\n*   1.  Scene: Contains preview window grid, background color, preview settings\n*   2.  Model: Model rendering mode, texture materials, up direction settings\n*   3.  Camera: Switch between orthographic and perspective views, and set the perspective angle size\n*   4.  Light: Scene global illumination intensity\n*   5.  Export: Export model to other formats (GLB, OBJ, STL)\n\n#### Scene\n\n![scene menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_scene.jpg) The Scene menu provides some basic scene setting functions\n\n1.  Show/Hide grid\n2.  Set background color\n3.  Click to upload a background image\n4.  Hide the preview\n\n#### Model\n\n![Menu_Scene](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_model.jpg) The Model menu provides some model-related functions\n\n1.  **Up direction**: Determine which axis is the up direction for the model\n2.  **Material mode**: Switch model rendering modes - Original, Normal, Wireframe, Lineart\n\n#### Camera\n\n![menu_modelmenu_camera](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_camera.jpg) This menu provides switching between orthographic and perspective views, and perspective angle size settings\n\n1.  **Camera**: Quickly switch between orthographic and orthographic views\n2.  **FOV**: Adjust FOV angle\n\n#### Light\n\n![menu_modelmenu_camera](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_light.jpg) Through this menu, you can quickly adjust the sceneâ€™s global illumination intensity\n\n#### Export\n\n![menu_export](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_export.jpg) This menu provides the ability to quickly convert and export model formats\n\nThe right menu has two main functions:\n\n1.  **Reset view ratio**: After clicking the button, the view will adjust the canvas rendering area ratio according to the set width and height\n2.  **Video recording**: Allows you to record current 3D view operations as video, allows import, and can be output as `recording_video` to subsequent nodes"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/sampling/ksampler",
  "markdown": "# Ksampler - ComfyUI Built-in Node Documentation\n\n![Ksampler](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/sampling/ksampler.jpg) The KSampler node performs multi-step denoising sampling on latent images. It combines positive and negative conditions (prompts) and uses specified sampling algorithms and schedulers to generate high-quality latent images. It is commonly used in AI image generation workflows like text-to-image and image-to-image.\n\n## Parameter Description\n\n### Input Parameters\n\n| Parameter | Type | Required | Default | Description |\n| --- | --- | --- | --- | --- |\n| model | MODEL | Yes | None | Model used for denoising (e.g. Stable Diffusion model) |\n| seed | INT | Yes | 0   | Random seed to ensure reproducible results |\n| steps | INT | Yes | 20  | Number of denoising steps - more steps mean finer details but slower generation |\n| cfg | FLOAT | Yes | 8.0 | Classifier-Free Guidance scale - higher values better match prompts but too high impacts quality |\n| sampler\\_name | Enum | Yes | None | Name of sampling algorithm, affects generation speed, style and quality |\n| scheduler | Enum | Yes | None | Scheduler that controls the noise removal process |\n| positive | CONDITIONING | Yes | None | Positive conditions describing desired image content |\n| negative | CONDITIONING | Yes | None | Negative conditions describing content to exclude |\n| latent\\_image | LATENT | Yes | None | Latent image to denoise, usually noise or output from previous step |\n| denoise | FLOAT | Yes | 1.0 | Denoising strength - 1.0 for full denoising, lower values preserve original structure, suitable for image-to-image |\n\n### Output Parameters\n\n| Output | Type | Description |\n| --- | --- | --- |\n| samples | LATENT | Denoised latent image that can be decoded to final image |\n\n## Usage Examples\n\n[\n\n## Stable diffusion 1.5 Text-to-Image Workflow Example\n\nStable diffusion 1.5 Text-to-Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/basic/text-to-image)[\n\n## Stable diffusion 1.5 Image-to-Image Workflow Example\n\nStable diffusion 1.5 Image-to-Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/basic/image-to-image)\n\n## Source Code\n\n\\[Updated on May 15, 2025\\]\n\n```\n\ndef common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n    latent_image = latent[\"samples\"]\n    latent_image = comfy.sample.fix_empty_latent_channels(model, latent_image)\n\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = comfy.sample.prepare_noise(latent_image, seed, batch_inds)\n\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]\n\n    callback = latent_preview.prepare_callback(model, steps)\n    disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,\n                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    out = latent.copy()\n    out[\"samples\"] = samples\n    return (out, )\n\n\nclass KSampler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"model\": (\"MODEL\", {\"tooltip\": \"The model used for denoising the input latent.\"}),\n                \"seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff, \"control_after_generate\": True, \"tooltip\": \"The random seed used for creating the noise.\"}),\n                \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000, \"tooltip\": \"The number of steps used in the denoising process.\"}),\n                \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01, \"tooltip\": \"The Classifier-Free Guidance scale balances creativity and adherence to the prompt. Higher values result in images more closely matching the prompt however too high values will negatively impact quality.\"}),\n                \"sampler_name\": (comfy.samplers.KSampler.SAMPLERS, {\"tooltip\": \"The algorithm used when sampling, this can affect the quality, speed, and style of the generated output.\"}),\n                \"scheduler\": (comfy.samplers.KSampler.SCHEDULERS, {\"tooltip\": \"The scheduler controls how noise is gradually removed to form the image.\"}),\n                \"positive\": (\"CONDITIONING\", {\"tooltip\": \"The conditioning describing the attributes you want to include in the image.\"}),\n                \"negative\": (\"CONDITIONING\", {\"tooltip\": \"The conditioning describing the attributes you want to exclude from the image.\"}),\n                \"latent_image\": (\"LATENT\", {\"tooltip\": \"The latent image to denoise.\"}),\n                \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01, \"tooltip\": \"The amount of denoising applied, lower values will maintain the structure of the initial image allowing for image to image sampling.\"}),\n            }\n        }\n\n    RETURN_TYPES = (\"LATENT\",)\n    OUTPUT_TOOLTIPS = (\"The denoised latent.\",)\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling\"\n    DESCRIPTION = \"Uses the provided model, positive and negative conditioning to denoise the latent image.\"\n\n    def sample(self, model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0):\n        return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_bottom_panel_tabs",
  "markdown": "# Bottom Panel Tabs - ComfyUI\n\nThe Bottom Panel Tabs API allows extensions to add custom tabs to the bottom panel of the ComfyUI interface. This is useful for adding features like logs, debugging tools, or custom panels.\n\n## Basic Usage\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  bottomPanelTabs: [\n    {\n      id: \"customTab\",\n      title: \"Custom Tab\",\n      type: \"custom\",\n      render: (el) => {\n        el.innerHTML = '<div>This is my custom tab content</div>';\n      }\n    }\n  ]\n});\n```\n\n## Tab Configuration\n\nEach tab requires an `id`, `title`, and `type`, along with a render function:\n\n```\n{\n  id: string,              // Unique identifier for the tab\n  title: string,           // Display title shown on the tab\n  type: string,            // Tab type (usually \"custom\")\n  icon?: string,           // Icon class (optional)\n  render: (element) => void // Function that populates the tab content\n}\n```\n\nThe `render` function receives a DOM element where you should insert your tabâ€™s content.\n\n## Interactive Elements\n\nYou can add interactive elements like buttons:\n\n```\napp.registerExtension({\n  name: \"InteractiveTabExample\",\n  bottomPanelTabs: [\n    {\n      id: \"controlsTab\",\n      title: \"Controls\",\n      type: \"custom\",\n      render: (el) => {\n        el.innerHTML = `\n          <div style=\"padding: 10px;\">\n            <button id=\"runBtn\">Run Workflow</button>\n          </div>\n        `;\n        \n        // Add event listeners\n        el.querySelector('#runBtn').addEventListener('click', () => {\n          app.queuePrompt();\n        });\n      }\n    }\n  ]\n});\n```\n\n## Using React Components\n\nYou can mount React components in bottom panel tabs:\n\n```\n// Import React dependencies in your extension\nimport React from \"react\";\nimport ReactDOM from \"react-dom/client\";\n\n// Simple React component\nfunction TabContent() {\n  const [count, setCount] = React.useState(0);\n  \n  return (\n    <div style={{ padding: \"10px\" }}>\n      <h3>React Component</h3>\n      <p>Count: {count}</p>\n      <button onClick={() => setCount(count + 1)}>Increment</button>\n    </div>\n  );\n}\n\n// Register the extension with React content\napp.registerExtension({\n  name: \"ReactTabExample\",\n  bottomPanelTabs: [\n    {\n      id: \"reactTab\",\n      title: \"React Tab\",\n      type: \"custom\",\n      render: (el) => {\n        const container = document.createElement(\"div\");\n        container.id = \"react-tab-container\";\n        el.appendChild(container);\n        \n        // Mount React component\n        ReactDOM.createRoot(container).render(\n          <React.StrictMode>\n            <TabContent />\n          </React.StrictMode>\n        );\n      }\n    }\n  ]\n});\n```\n\n## Standalone Registration\n\nYou can also register tabs outside of `registerExtension`:\n\n```\napp.extensionManager.registerBottomPanelTab({\n  id: \"standAloneTab\",\n  title: \"Stand-Alone Tab\",\n  type: \"custom\",\n  render: (el) => {\n    el.innerHTML = '<div>This tab was registered independently</div>';\n  }\n});\n```\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/execution_model_inversion_guide",
  "markdown": "# Execution Model Inversion Guide - ComfyUI\n\n[PR #2666](https://github.com/comfyanonymous/ComfyUI/pull/2666) inverts the execution model from a back-to-front recursive model to a front-to-back topological sort. While most custom nodes should continue to â€œjust workâ€, this page is intended to serve as a guide for custom node creators to the things that _could_ break.\n\n## Breaking Changes\n\n### Monkey Patching\n\nAny code that monkey patched the execution model is likely to stop working. Note that the performance of execution with this PR exceeds that with the most popular monkey patches, so many of them will be unnecessary.\n\n### Optional Input Validation\n\nPrior to this PR, only nodes that were connected to outputs exclusively through a string of `\"required\"` inputs were actually validated. If you had custom nodes that were only ever connected to `\"optional\"` inputs, you previously wouldnâ€™t have been seeing that they failed validation.\n\nHere are some of the things that could cause you to fail validation along with recommended solutions:\n\n*   Use of reserved [Additional Parameters](https://docs.comfy.org/custom-nodes/backend/datatypes#additional-parameters) like `min` and `max` on types that arenâ€™t comparable (e.g. dictionaries) in order to configure custom widgets.\n    *   Change the additional parameters used to non-reserved keys like `uiMin` and `uiMax`. _(Recommended Solution)_\n        \n        ```\n        @classmethod\n        def INPUT_TYPES(cls):\n            return {\n                \"required\": {\n                    \"my_size\": (\"VEC2\", {\"uiMin\": 0.0, \"uiMax\": 1.0}),\n                }\n            }\n        ```\n        \n    *   Define a custom [VALIDATE\\_INPUTS](https://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) function with this input so validation of it is skipped. _(Quick Solution)_\n        \n        ```\n        @classmethod\n        def VALIDATE_INPUTS(cls, my_size):\n            return True\n        ```\n        \n*   Use of composite types (e.g. `CUSTOM_A,CUSTOM_B`)\n    *   (When used as output) Define and use a wrapper like `MakeSmartType` [seen here in the PRâ€™s unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R2)\n        \n        ```\n        class MyCustomNode:\n        \n            @classmethod\n            def INPUT_TYPES(cls):\n                return {\n                    \"required\": {\n                        \"input\": (MakeSmartType(\"FOO,BAR\"), {}),\n                    }\n                }\n        \n            RETURN_TYPES = (MakeSmartType(\"FOO,BAR\"),)\n        \n            # ...\n        ```\n        \n    *   (When used as input) Define a custom[VALIDATE\\_INPUTS](https://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) function that takes a `input_types` argument so type validation is skipped.\n        \n        ```\n        @classmethod\n        def VALIDATE_INPUTS(cls, input_types):\n            return True\n        ```\n        \n    *   (Supports both, convenient) Define and use the `@VariantSupport` decorator [seen here in the PRâ€™s unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R15)\n        \n        ```\n        @VariantSupport\n        class MyCustomNode:\n        \n            @classmethod\n            def INPUT_TYPES(cls):\n                return {\n                    \"required\": {\n                        \"input\": (\"FOO,BAR\", {}),\n                    }\n                }\n            \n            RETURN_TYPES = (MakeSmartType(\"FOO,BAR\"),)\n        \n            # ...\n        ```\n        \n*   The use of lists (e.g. `[1, 2, 3]`) as constants in the graph definition (e.g. to represent a const `VEC3` input). This would have required a front-end extension before. Previously, lists of size exactly `2` would have failed anyway â€” they would have been treated as broken links.\n    *   Wrap the lists in a dictionary like `{ \"value\": [1, 2, 3] }`\n\n### Execution Order\n\nExecution order has always changed depending on which nodes happen to have which IDs, but it may now change depending on which values are cached as well. In general, the execution order should be considered non-deterministic and subject to change (beyond what is enforced by the graphâ€™s structure). Donâ€™t rely on the execution order. _HIC SUNT DRACONES_\n\n## New Functionality\n\n### Validation Changes\n\nA number of features were added to the `VALIDATE_INPUTS` function in order to lessen the impact of the [Optional Input Validation](#optional-input-validation) mentioned above.\n\n*   Default validation will now be skipped for inputs which are received by the `VALIDATE_INPUTS` function.\n*   The `VALIDATE_INPUTS` function can now take `**kwargs` which causes all inputs to be treated as validated by the node creator.\n*   The `VALIDATE_INPUTS` function can take an input named `input_types`. This input will be a dict mapping each input (connected via a link) to the type of the connected output. When this argument exists, type validation for the nodeâ€™s inputs is skipped.\n\nYou can read more at [VALIDATE\\_INPUTS](https://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs).\n\n### Lazy Evaluation\n\nInputs can be evaluated lazily (i.e. you can wait to see if they are needed before evaluating the attached node and all its ancestors). See [Lazy Evaluation](https://docs.comfy.org/custom-nodes/backend/lazy_evaluation) for more information.\n\n### Node Expansion\n\nAt runtime, nodes can expand into a subgraph of nodes. This is what allows loops to be implemented (via tail-recursion). See [Node Expansion](https://docs.comfy.org/custom-nodes/backend/expansion) for more information."
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/api-key-integration",
  "markdown": "# Integration of API Key to use ComfyUI API nodes\n\nStarting from [PR #8041](https://github.com/comfyanonymous/ComfyUI/pull/8041), ComfyUI supports directly using built-in API nodes through API Keys, without requiring a specific frontend interface (you can even run without a frontend). This means you can create workflows that combine:\n\n*   local OS models\n*   tools from the custom node community\n*   popular paid models\n\nThen run everything together by simply sending the prompt to the Comfy webserver API, letting it handle all the orchestration. This is helpful for users who want to use Comfy as a backend service, via the command line, with their own frontend, etc.\n\n## Prerequisites\n\nUsing API Key to call ComfyUIâ€™s built-in API nodes requires:\n\n*   API Key for the corresponding account\n*   Sufficient account credits\n\nTo use API Key to call ComfyUIâ€™s built-in API nodes, you need to first register an account on [ComfyUI Platform](https://platform.comfy.org/login) and create an API key\n\n[\n\n## Login with API Key\n\nPlease refer to the User Interface section to learn how to login with API Key\n\n\n\n](https://docs.comfy.org/interface/user#logging-in-with-an-api-key)\n\nYou need to ensure your ComfyUI account has sufficient credits to test the corresponding features.\n\n[\n\n## Credits\n\nPlease refer to the Credits section to learn how to purchase credits for your account\n\n\n\n](https://docs.comfy.org/interface/credits)\n\n## Python Example\n\nHere is an example of how to send a workflow containing API nodes to the ComfyUI API using Python code:\n\n```\n\"\"\"Using API nodes when running ComfyUI headless or with alternative frontend\n\nYou can execute a ComfyUI workflow that contains API nodes by including an API key in the prompt.\nThe API key should be added to the `extra_data` field of the payload.\nBelow we show an example of how to do this.\n\nSee more:\n\n- API nodes overview: https://docs.comfy.org/tutorials/api-nodes/overview\n- To generate an API key, login here: https://platform.comfy.org/login\n\"\"\"\n\nimport json\nfrom urllib import request\n\nSERVER_URL = \"http://127.0.0.1:8188\"\n\n# We have a prompt/job (workflow in \"API format\") that contains API nodes.\nworkflow_with_api_nodes = \"\"\"{\n  \"11\": {\n    \"inputs\": {\n      \"prompt\": \"A dreamy, surreal half-body portrait of a young woman meditating. She has a short, straight bob haircut dyed in pastel pink, with soft bangs covering her forehead. Her eyes are gently closed, and her hands are raised in a calm, open-palmed meditative pose, fingers slightly curved, as if levitating or in deep concentration. She wears a colorful dress made of patchwork-like pastel tiles, featuring clouds, stars, and rainbows. Around her float translucent, iridescent soap bubbles reflecting the rainbow hues. The background is a fantastical sky filled with cotton-candy clouds and vivid rainbow waves, giving the entire scene a magical, dreamlike atmosphere. Emphasis on youthful serenity, whimsical ambiance, and vibrant soft lighting.\",\n      \"prompt_upsampling\": false,\n      \"seed\": 589991183902375,\n      \"aspect_ratio\": \"1:1\",\n      \"raw\": false,\n      \"image_prompt_strength\": 0.4000000000000001,\n      \"image_prompt\": [\n        \"14\",\n        0\n      ]\n    },\n    \"class_type\": \"FluxProUltraImageNode\",\n    \"_meta\": {\n      \"title\": \"Flux 1.1 [pro] Ultra Image\"\n    }\n  },\n  \"12\": {\n    \"inputs\": {\n      \"filename_prefix\": \"ComfyUI\",\n      \"images\": [\n        \"11\",\n        0\n      ]\n    },\n    \"class_type\": \"SaveImage\",\n    \"_meta\": {\n      \"title\": \"Save Image\"\n    }\n  },\n  \"14\": {\n    \"inputs\": {\n      \"image\": \"example.png\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"Load Image\"\n    }\n  }\n}\"\"\"\n\n\nprompt = json.loads(workflow_with_api_nodes)\npayload = {\n    \"prompt\": prompt,\n    # Add the `api_key_comfy_org` to the payload.\n    # You can first get the key from the associated user if handling multiple clients.\n    \"extra_data\": {\n        \"api_key_comfy_org\": \"comfyui-87d01e28d*******************************************************\"  # replace with actual key\n    },\n}\ndata = json.dumps(payload).encode(\"utf-8\")\nreq = request.Request(f\"{SERVER_URL}/prompt\", data=data)\nrequest.urlopen(req)\n\n```\n\n*   [API nodes overview](https://docs.comfy.org/tutorials/api-nodes/overview)\n*   [Account management](https://docs.comfy.org/interface/user)\n*   [Credits](https://docs.comfy.org/interface/credits)"
},
{
  "url": "https://docs.comfy.org/comfy-cli/troubleshooting",
  "markdown": "# Getting Started - ComfyUI\n\n### Prerequisites\n\nYou need to have git installed on your system. Install it [here](https://git-scm.com/downloads).\n\nOn this page\n\n*   [Prerequisites](#prerequisites)"
},
{
  "url": "https://docs.comfy.org/comfy-cli/reference",
  "markdown": "# Reference - ComfyUI\n\n## CLI\n\n## Nodes\n\n**Usage**:\n\n```\n$ comfy node [OPTIONS] COMMAND [ARGS]...\n```\n\n**Options**:\n\n*   `--install-completion`: Install completion for the current shell.\n*   `--show-completion`: Show completion for the current shell, to copy it or customize the installation.\n*   `--help`: Show this message and exit.\n\n**Commands**:\n\n*   `deps-in-workflow`\n*   `disable`\n*   `enable`\n*   `fix`\n*   `install`\n*   `install-deps`\n*   `reinstall`\n*   `restore-dependencies`\n*   `restore-snapshot`\n*   `save-snapshot`: Save a snapshot of the current ComfyUIâ€¦\n*   `show`\n*   `simple-show`\n*   `uninstall`\n*   `update`\n\n### `deps-in-workflow`\n\n**Usage**:\n\n```\n$ deps-in-workflow [OPTIONS]\n```\n\n**Options**:\n\n*   `--workflow TEXT`: Workflow file (.json/.png) \\[required\\]\n*   `--output TEXT`: Workflow file (.json/.png) \\[required\\]\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `disable`\n\n**Usage**:\n\n```\n$ disable [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: disable custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `enable`\n\n**Usage**:\n\n```\n$ enable [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: enable custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `fix`\n\n**Usage**:\n\n**Arguments**:\n\n*   `ARGS...`: fix dependencies for specified custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `install`\n\n**Usage**:\n\n```\n$ install [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: install custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `install-deps`\n\n**Usage**:\n\n**Options**:\n\n*   `--deps TEXT`: Dependency spec file (.json)\n*   `--workflow TEXT`: Workflow file (.json/.png)\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `reinstall`\n\n**Usage**:\n\n```\n$ reinstall [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: reinstall custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `restore-dependencies`\n\n**Usage**:\n\n```\n$ restore-dependencies [OPTIONS]\n```\n\n**Options**:\n\n*   `--help`: Show this message and exit.\n\n### `restore-snapshot`\n\n**Usage**:\n\n```\n$ restore-snapshot [OPTIONS] PATH\n```\n\n**Arguments**:\n\n*   `PATH`: \\[required\\]\n\n**Options**:\n\n*   `--help`: Show this message and exit.\n\n### `save-snapshot`\n\nSave a snapshot of the current ComfyUI environment **Usage**:\n\n```\n$ save-snapshot [OPTIONS]\n```\n\n**Options**:\n\n*   `--output TEXT`: Specify the output file path. (.json/.yaml)\n*   `--help`: Show this message and exit.\n\n### `show`\n\n**Usage**:\n\n**Arguments**:\n\n*   `ARGS...`: \\[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list\\] \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `simple-show`\n\n**Usage**:\n\n```\n$ simple-show [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: \\[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list\\] \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `uninstall`\n\n**Usage**:\n\n```\n$ uninstall [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: uninstall custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n### `update`\n\n**Usage**:\n\n```\n$ update [OPTIONS] ARGS...\n```\n\n**Arguments**:\n\n*   `ARGS...`: update custom nodes \\[required\\]\n\n**Options**:\n\n*   `--channel TEXT`: Specify the operation mode\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: Show this message and exit.\n\n## Models\n\n**Usage**:\n\n```\n$ comfy model [OPTIONS] COMMAND [ARGS]...\n```\n\n**Options**:\n\n*   `--install-completion`: Install completion for the current shell.\n*   `--show-completion`: Show completion for the current shell, to copy it or customize the installation.\n*   `--help`: Show this message and exit.\n\n**Commands**:\n\n*   `download`: Download a model to a specified relativeâ€¦\n*   `list`: Display a list of all models currentlyâ€¦\n*   `remove`: Remove one or more downloaded models,â€¦\n\n### `download`\n\nDownload a model to a specified relative path if it is not already downloaded. **Usage**:\n\n**Options**:\n\n*   `--url TEXT`: The URL from which to download the model \\[required\\]\n*   `--relative-path TEXT`: The relative path from the current workspace to install the model. \\[default: models/checkpoints\\]\n*   `--help`: Show this message and exit.\n\n### `list`\n\nDisplay a list of all models currently downloaded in a table format. **Usage**:\n\n**Options**:\n\n*   `--relative-path TEXT`: The relative path from the current workspace where the models are stored. \\[default: models/checkpoints\\]\n*   `--help`: Show this message and exit.\n\n### `remove`\n\nRemove one or more downloaded models, either by specifying them directly or through an interactive selection. **Usage**:\n\n**Options**:\n\n*   `--relative-path TEXT`: The relative path from the current workspace where the models are stored. \\[default: models/checkpoints\\]\n*   `--model-names TEXT`: List of model filenames to delete, separated by spaces\n*   `--help`: Show this message and exit."
},
{
  "url": "https://docs.comfy.org/development/comfyui-server/comms_messages",
  "markdown": "# Messages - ComfyUI\n\nDuring execution (or when the state of the queue changes), the `PromptExecutor` sends messages back to the client through the `send_sync` method of `PromptServer`. These messages are received by a socket event listener defined in `api.js` (at time of writing around line 90, or search for `this.socket.addEventListener`), which creates a `CustomEvent` object for any known message type, and dispatches it to any registered listeners. An extension can register to receive events (normally done in the `setup()` function) following the standard Javascript idiom:\n\n```\napi.addEventListener(message_type, messageHandler);\n```\n\nIf the `message_type` is not one of the built in ones, it will be added to the list of known message types automatically. The message `messageHandler` will be called with a `CustomEvent` object, which extends the event raised by the socket to add a `.detail` property, which is a dictionary of the data sent by the server. So usage is generally along the lines of:\n\n```\nfunction messageHandler(event) {\n    if (event.detail.node == aNodeIdThatIsInteresting) {\n        // do something with event.detail.other_things\n    }\n}\n```\n\n### Built in message types\n\nDuring execution (or when the state of the queue changes), the `PromptExecutor` sends the following messages back to the client through the `send_sync` method of `PromptServer`. An extension can register as a listener for any of these.\n\n| event | when | data |\n| --- | --- | --- |\n| `execution_start` | When a prompt is about to run | `prompt_id` |\n| `execution_error` | When an error occurs during execution | `prompt_id`, plus additional information |\n| `execution_interrupted` | When execution is stopped by a node raising `InterruptProcessingException` | `prompt_id`, `node_id`, `node_type` and `executed` (a list of executed nodes) |\n| `execution_cached` | At the start of execution | `prompt_id`, `nodes` (a list of nodes which are being skipped because their cached outputs can be used) |\n| `execution_success` | When all nodes from the prompt have been successfully executed | `prompt_id`, `timestamp` |\n| `executing` | When a new node is about to be executed | `node` (node id or `None` to indicate completion), `prompt_id` |\n| `executed` | When a node returns a ui element | `node` (node id), `prompt_id`, `output` |\n| `progress` | During execution of a node that implements the required hook | `node` (node id), `prompt_id`, `value`, `max` |\n| `status` | When the state of the queue changes | `exec_info`, a dictionary holding `queue_remaining`, the number of entries in the queue |\n\n### Using executed\n\nDespite the name, an `executed` message is not sent whenever a node completes execution (unlike `executing`), but only when the node returns a ui update. To do this, the main function needs to return a dictionary instead of a tuple:\n\n```\n# at the end of my main method\n        return { \"ui\":a_new_dictionary, \"result\": the_tuple_of_output_values }\n```\n\n`a_new_dictionary` will then be sent as the value of `output` in an `executed` message. The `result` key can be omitted if the node has no outputs (see, for instance, the code for `SaveImage` in `nodes.py`)\n\n### Custom message types\n\nAs indicated above, on the client side, a custom message type can be added simply by registering as a listener for a unique message type name.\n\n```\napi.addEventListener(\"my.custom.message\", messageHandler);\n```\n\nOn the server, the code is equally simple:\n\n```\nfrom server import PromptServer\n# then, in your main execution function (normally)\n        PromptServer.instance.send_sync(\"my.custom.message\", a_dictionary)\n```\n\n#### Getting node\\_id\n\nMost of the built-in messages include the current node id in the value of `node`. Itâ€™s likely that you will want to do the same. The node\\_id is available on the server side through a hidden input, which is obtained with the `hidden` key in the `INPUT_TYPES` dictionary:\n\n```\n    @classmethod    \n    def INPUT_TYPES(s):\n        return {\"required\" : { }, # whatever your required inputs are \n                \"hidden\": { \"node_id\": \"UNIQUE_ID\" } } # Add the hidden key\n\n    def my_main_function(self, required_inputs, node_id):\n        # do some things\n        PromptServer.instance.send_sync(\"my.custom.message\", {\"node\": node_id, \"other_things\": etc})\n```\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/custom-nodes/tips",
  "markdown": "# Tips - ComfyUI\n\n### Recommended Development Lifecycle"
},
{
  "url": "https://docs.comfy.org/registry/overview",
  "markdown": "# Overview - ComfyUI\n\n## Introduction\n\nThe Registry is a public collection of custom nodes. Developers can publish, version, deprecate, and track metrics related to their custom nodes. ComfyUI users can discover, install, and rate custom nodes from the registry.\n\n## Why use the Registry?\n\nThe Comfy Registry helps the community by standardizing the development of custom nodes: Â  **Node Versioning:** Developers frequently publish new versions of their custom nodes which often break workflows that rely on them. With registry nodes being [semantically versioned](https://semver.org/), users can now choose to safely upgrade, deprecate, or lock their node versions in place, knowing in advance how their actions will impact their workflows. The workflow JSON will store the version of the node used, so you can always reliably reproduce your workflows. Â  **Node Security:** The registry will serve as a backend for the [ComfyUI-manager](https://github.com/ltdrdata/ComfyUI-Manager). All nodes will be scanned for malicious behaviour such as custom pip wheels, arbitrary system calls, etc. Nodes that pass these checks will have a verification flag () beside their name on the UI-manager. For a list of security standards, see the [standards](https://docs.comfy.org/registry/standards). Â  **Search:** Search across all nodes on the Registry to find existing nodes for your workflow.x\n\n## Publishing Nodes\n\nGet started publishing your first node by following the [tutorial](https://docs.comfy.org/registry/publishing).\n\n## Frequently Asked Questions"
},
{
  "url": "https://docs.comfy.org/custom-nodes/workflow_templates",
  "markdown": "# Workflow templates - ComfyUI\n\nIf you have example workflow files associated with your custom nodes then ComfyUI can show these to the user in the template browser (`Workflow`/`Browse Templates` menu). Workflow templates are a great way to support people getting started with your nodes. All you have to do as a node developer is to create an `example_workflows` folder and place the `json` files there. Optionally you can place `jpg` files with the same name to be shown as the template thumbnail. Under the hood ComfyUI statically serves these files along with an endpoint (`/api/workflow_templates`) that returns the collection of workflow templates.\n\n## Example\n\nUnder `ComfyUI-MyCustomNodeModule/example_workflows/` directory:\n\n*   `My_example_workflow_1.json`\n*   `My_example_workflow_1.jpg`\n*   `My_example_workflow_2.json`\n\nIn this example ComfyUIâ€™s template browser shows a category called `ComfyUI-MyCustomNodeModule` with two items, one of which has a thumbnail."
},
{
  "url": "https://docs.comfy.org/custom-nodes/help_page",
  "markdown": "# Help Page - ComfyUI\n\n## Node Documentation with Markdown\n\nCustom nodes can include rich markdown documentation that will be displayed in the UI instead of the generic node description. This provides users with detailed information about your nodeâ€™s functionality, parameters, and usage examples.\n\n## Setup\n\nTo add documentation for your nodes:\n\n1.  Create a `docs` folder inside your `WEB_DIRECTORY`\n2.  Add markdown files named after your nodes (the names of your nodes are the dictionary keys in the `NODE_CLASS_MAPPINGS` dictionary used to register the nodes):\n    *   `WEB_DIRECTORY/docs/NodeName.md` - Default documentation\n    *   `WEB_DIRECTORY/docs/NodeName/en.md` - English documentation\n    *   `WEB_DIRECTORY/docs/NodeName/zh.md` - Chinese documentation\n    *   Add other locales as needed (e.g., `fr.md`, `de.md`, etc.)\n\nThe system will automatically load the appropriate documentation based on the userâ€™s locale, falling back to `NodeName.md` if a localized version is not available.\n\n## Supported Markdown Features\n\n*   Standard markdown syntax (headings, lists, code blocks, etc.)\n*   Images using markdown syntax: `![alt text](image.png)`\n*   HTML media elements with specific attributes:\n    *   `<video>` and `<source>` tags\n    *   Allowed attributes: `controls`, `autoplay`, `loop`, `muted`, `preload`, `poster`\n\n## Example Structure\n\n```\nmy-custom-node/\nâ”œâ”€â”€ __init__.py\nâ”œâ”€â”€ web/              # WEB_DIRECTORY\nâ”‚   â”œâ”€â”€ js/\nâ”‚   â”‚   â””â”€â”€ my-node.js\nâ”‚   â””â”€â”€ docs/\nâ”‚       â”œâ”€â”€ MyNode.md           # Fallback documentation\nâ”‚       â””â”€â”€ MyNode/\nâ”‚           â”œâ”€â”€ en.md           # English version\nâ”‚           â””â”€â”€ zh.md           # Chinese version\n```\n\n## Example Markdown File\n\n```\n# My Custom Node\n\nThis node processes images using advanced algorithms.\n\n## Parameters\n\n- **image**: Input image to process\n- **strength**: Processing strength (0.0 - 1.0)\n\n## Usage\n\n![example usage](example.png)\n\n<video controls loop muted>\n  <source src=\"demo.mp4\" type=\"video/mp4\">\n</video>\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/walkthrough",
  "markdown": "# Getting Started - ComfyUI\n\nThis page will take you step-by-step through the process of creating a custom node. Our example will take a batch of images, and return one of the images. Initially, the node will return the image which is, on average, the lightest in color; weâ€™ll then extend it to have a range of selection criteria, and then finally add some client side code. This page assumes very little knowledge of Python or Javascript. After this walkthrough, dive into the details of [backend code](https://docs.comfy.org/custom-nodes/backend/server_overview), and [frontend code](https://docs.comfy.org/custom-nodes/backend/server_overview).\n\n## Write a basic node\n\n### Prerequisites\n\n*   A working ComfyUI [installation](https://docs.comfy.org/installation/manual_install). For development, we recommend installing ComfyUI manually.\n*   A working comfy-cli [installation](https://docs.comfy.org/comfy-cli/getting-started).\n\n### Setting up\n\n```\ncd ComfyUI/custom_nodes\ncomfy node scaffold\n```\n\nAfter answering a few questions, youâ€™ll have a new directory set up.\n\n```\n ~  % comfy node scaffold\nYou've downloaded .cookiecutters/cookiecutter-comfy-extension before. Is it okay to delete and re-download it? [y/n] (y): y\n  [1/9] full_name (): Comfy\n  [2/9] email (you@gmail.com): me@comfy.org\n  [3/9] github_username (your_github_username): comfy\n  [4/9] project_name (My Custom Nodepack): FirstComfyNode\n  [5/9] project_slug (firstcomfynode): \n  [6/9] project_short_description (A collection of custom nodes for ComfyUI): \n  [7/9] version (0.0.1): \n  [8/9] Select open_source_license\n    1 - GNU General Public License v3\n    2 - MIT license\n    3 - BSD license\n    4 - ISC license\n    5 - Apache Software License 2.0\n    6 - Not open source\n    Choose from [1/2/3/4/5/6] (1): 1\n  [9/9] include_web_directory_for_custom_javascript [y/n] (n): y\nInitialized empty Git repository in firstcomfynode/.git/\nâœ“ Custom node project created successfully!\n```\n\n### Defining the node\n\nAdd the following code to the end of `src/nodes.py`:\n\n```\nclass ImageSelector:\n    CATEGORY = \"example\"\n    @classmethod    \n    def INPUT_TYPES(s):\n        return { \"required\":  { \"images\": (\"IMAGE\",), } }\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"choose_image\"\n```\n\nA custom node is defined using a Python class, which must include these four things: `CATEGORY`, which specifies where in the add new node menu the custom node will be located, `INPUT_TYPES`, which is a class method defining what inputs the node will take (see [later](https://docs.comfy.org/custom-nodes/backend/server_overview#input-types) for details of the dictionary returned), `RETURN_TYPES`, which defines what outputs the node will produce, and `FUNCTION`, the name of the function that will be called when the node is executed.\n\n### The main function\n\nThe main function, `choose_image`, receives named arguments as defined in `INPUT_TYPES`, and returns a `tuple` as defined in `RETURN_TYPES`. Since weâ€™re dealing with images, which are internally stored as `torch.Tensor`,\n\nThen add the function to your class. The datatype for image is `torch.Tensor` with shape `[B,H,W,C]`, where `B` is the batch size and `C` is the number of channels - 3, for RGB. If we iterate over such a tensor, we will get a series of `B` tensors of shape `[H,W,C]`. The `.flatten()` method turns this into a one dimensional tensor, of length `H*W*C`, `torch.mean()` takes the mean, and `.item()` turns a single value tensor into a Python float.\n\n```\ndef choose_image(self, images):\n    brightness = list(torch.mean(image.flatten()).item() for image in images)\n    brightest = brightness.index(max(brightness))\n    result = images[brightest].unsqueeze(0)\n    return (result,)\n```\n\nNotes on those last two lines:\n\n*   `images[brightest]` will return a Tensor of shape `[H,W,C]`. `unsqueeze` is used to insert a (length 1) dimension at, in this case, dimension zero, to give us `[B,H,W,C]` with `B=1`: a single image.\n*   in `return (result,)`, the trailing comma is essential to ensure you return a tuple.\n\n### Register the node\n\nTo make Comfy recognize the new node, it must be available at the package level. Modify the `NODE_CLASS_MAPPINGS` variable at the end of `src/nodes.py`. You must restart ComfyUI to see any changes.\n\n```\n\nNODE_CLASS_MAPPINGS = {\n    \"Example\" : Example,\n    \"Image Selector\" : ImageSelector,\n}\n\n# Optionally, you can rename the node in the `NODE_DISPLAY_NAME_MAPPINGS` dictionary.\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"Example\": \"Example Node\",\n    \"Image Selector\": \"Image Selector\",\n}\n```\n\n## Add some options\n\nThat node is maybe a bit boring, so we might add some options; a widget that allows you to choose the brightest image, or the reddest, bluest, or greenest. Edit your `INPUT_TYPES` to look like:\n\n```\n@classmethod    \ndef INPUT_TYPES(s):\n    return { \"required\":  { \"images\": (\"IMAGE\",), \n                            \"mode\": ([\"brightest\", \"reddest\", \"greenest\", \"bluest\"],)} }\n```\n\nThen update the main function. Weâ€™ll use a fairly naive definition of â€˜reddestâ€™ as being the average `R` value of the pixels divided by the average of all three colors. So:\n\n```\ndef choose_image(self, images, mode):\n    batch_size = images.shape[0]\n    brightness = list(torch.mean(image.flatten()).item() for image in images)\n    if (mode==\"brightest\"):\n        scores = brightness\n    else:\n        channel = 0 if mode==\"reddest\" else (1 if mode==\"greenest\" else 2)\n        absolute = list(torch.mean(image[:,:,channel].flatten()).item() for image in images)\n        scores = list( absolute[i]/(brightness[i]+1e-8) for i in range(batch_size) )\n    best = scores.index(max(scores))\n    result = images[best].unsqueeze(0)\n    return (result,)\n```\n\n## Tweak the UI\n\nMaybe weâ€™d like a bit of visual feedback, so letâ€™s send a little text message to be displayed.\n\n### Send a message from server\n\nThis requires two lines to be added to the Python code:\n\n```\nfrom server import PromptServer\n```\n\nand, at the end of the `choose_image` method, add a line to send a message to the front end (`send_sync` takes a message type, which should be unique, and a dictionary)\n\n```\nPromptServer.instance.send_sync(\"example.imageselector.textmessage\", {\"message\":f\"Picked image {best+1}\"})\nreturn (result,)\n```\n\n### Write a client extension\n\nTo add some Javascript to the client, create a subdirectory, `web/js` in your custom node directory, and modify the end of `__init__.py` to tell Comfy about it by exporting `WEB_DIRECTORY`:\n\n```\nWEB_DIRECTORY = \"./web/js\"\n__all__ = ['NODE_CLASS_MAPPINGS', 'WEB_DIRECTORY']\n```\n\nThe client extension is saved as a `.js` file in the `web/js` subdirectory, so create `image_selector/web/js/imageSelector.js` with the code below. (For more, see [client side coding](https://docs.comfy.org/custom-nodes/js/javascript_overview)).\n\n```\napp.registerExtension({\n\tname: \"example.imageselector\",\n    async setup() {\n        function messageHandler(event) { alert(event.detail.message); }\n        app.api.addEventListener(\"example.imageselector.textmessage\", messageHandler);\n    },\n})\n```\n\nAll weâ€™ve done is register an extension and add a listener for the message type we are sending in the `setup()` method. This reads the dictionary we sent (which is stored in `event.detail`). Stop the Comfy server, start it again, reload the webpage, and run your workflow.\n\n### The complete example\n\nThe complete example is available [here](https://gist.github.com/robinjhuang/fbf54b7715091c7b478724fc4dffbd03). You can download the example workflow [JSON file](https://github.com/Comfy-Org/docs/blob/main/public/workflow.json) or view it below:\n\n![Image Selector Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/firstnodeworkflow.png)"
},
{
  "url": "https://docs.comfy.org/registry/cicd",
  "markdown": "# Custom Node CI/CD - ComfyUI\n\n## Introduction\n\nWhen making changes to custom nodes, itâ€™s not uncommon to break things in Comfy or other custom nodes. It is often unrealistic to test on every operating system and different configurations of Pytorch.\n\n### Run Comfy Workflows using Github Actions\n\n[Comfy-Action](https://github.com/Comfy-Org/comfy-action) allows you to run a Comfy workflow.json file on Github Actions. It supports downloading models, custom nodes, and runs on Linux/Mac/Windows.\n\n### Results\n\nOutput files are uploaded to the [CI/CD Dashboard](https://comfyci.org/) and can be viewed as a last step before committing new changes or publishing new versions of the custom node. ![ComfyCI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyci.png)"
},
{
  "url": "https://docs.comfy.org/registry/publishing",
  "markdown": "# Publishing Nodes - ComfyUI\n\n## Set up a Registry Account\n\nFollow the steps below to set up a registry account and publish your first node.\n\n### Watch a Tutorial\n\n### Create a Publisher\n\nA publisher is an identity that can publish custom nodes to the registry. Every custom node needs to include a publisher identifier in the pyproject.toml file. Go to [Comfy Registry](https://registry.comfy.org/), and create a publisher account. Your publisher id is globally unique, and cannot be changed later because it is used in the URL of your custom node. Your publisher id is found after the `@` symbol on your profile page. ![Hero Dark](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/publisherid.png)\n\n### Create an API Key for publishing\n\nGo [here](https://registry.comfy.org/nodes) and click on the publisher you want to create an API key for. This will be used to publish a custom node via the CLI. ![Create key for Specific Publisher](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-1.png) Name the API key and save it somewhere safe. If you lose it, youâ€™ll have to create a new key. ![Create API Key](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-2.png)\n\n### Add Metadata\n\nThis command will generate the following metadata:\n\n```\n# pyproject.toml\n[project]\nname = \"\" # Unique identifier for your node. Immutable after creation.\ndescription = \"\"\nversion = \"1.0.0\" # Custom Node version. Must be semantically versioned.\nlicense = { file = \"LICENSE.txt\" }\ndependencies  = [] # Filled in from requirements.txt\n\n[project.urls]\nRepository = \"https://github.com/...\"\n\n[tool.comfy]\nPublisherId = \"\" # TODO (fill in Publisher ID from Comfy Registry Website).\nDisplayName = \"\" # Display name for the Custom Node. Can be changed later.\nIcon = \"https://example.com/icon.png\" # SVG, PNG, JPG or GIF (MAX. 800x400px)\n```\n\nAdd this file to your repository. Check the [specifications](https://docs.comfy.org/registry/specifications) for more information on the pyproject.toml file.\n\n## Publish to the Registry\n\n### Option 1: Comfy CLI\n\nRun the command below to manually publish your node to the registry.\n\nYouâ€™ll be prompted for the API key.\n\n```\nAPI Key for publisher '<publisher id>': ****************************************************\n\n...Version 1.0.0 Published. \nSee it here: https://registry.comfy.org/publisherId/your-node\n```\n\n### Option 2: Github Actions\n\nAutomatically publish your node through github actions."
},
{
  "url": "https://docs.comfy.org/registry/standards",
  "markdown": "# Standards - ComfyUI\n\n## Base Standards\n\nCustom nodes must provide valuable functionality to the ComfyUI community Avoid:\n\n*   Excessive self-promotion\n*   Impersonation or misleading behavior\n*   Malicious behavior\n*   Self-promotion is permitted only within your designated settings menu section\n*   Top and side menus should contain only useful functionality\n\n### 2\\. Node Compatibility\n\nDo not interfere with other custom nodesâ€™ operations (installation, updates, removal)\n\n*   For dependencies on other custom nodes:\n    *   Display clear warnings when dependent functionality is used\n    *   Provide example workflows demonstrating required nodes\n\n### 3\\. Legal Compliance\n\nMust comply with all applicable laws and regulations\n\n### 5\\. Quality Requirements\n\nNodes must be fully functional, well documented, and actively maintained.\n\n### 6\\. Fork Guidelines\n\nForked nodes must:\n\n*   Have clearly distinct names from original\n*   Provide significant differences in functionality or code\n\nBelow are standards that must be met to publish custom nodes to the registry.\n\n## Security Standards\n\nCustom nodes should be secure. We will start working with custom nodes that violate these standards to be rewritten. If there is some major functionality that should be exposed by core, please request it in the [rfcs repo](https://github.com/comfy-org/rfcs).\n\n### eval/exec Calls\n\n#### Policy\n\nThe use of `eval` and `exec` functions is prohibited in custom nodes due to security concerns.\n\n#### Reasoning\n\nThese functions can enable arbitrary code execution, creating potential Remote Code Execution (RCE) vulnerabilities when processing user inputs. Workflows containing nodes that pass user inputs into `eval` or `exec` could be exploited for various cyberattacks, including:\n\n*   Keylogging\n*   Ransomware\n*   Other malicious code execution\n\n### subprocess for pip install\n\n#### Policy\n\nRuntime package installation through subprocess calls is not permitted.\n\n#### Reasoning\n\n*   First item ComfyUI manager will ship with ComfyUI and lets the user install dependencies\n*   Centralized dependency management improves security and user experience\n*   Helps prevent potential supply chain attacks\n*   Eliminates need for multiple ComfyUI reloads\n\n### Code Obfuscation\n\n#### Policy\n\nCode obfuscation is prohibited in custom nodes.\n\n#### Reasoning\n\nObfuscated code:\n\n*   Impossible to review and likely to be malicious"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/image-to-image",
  "markdown": "# ComfyUI Image to Image Workflow\n\n## What is Image to Image\n\nImage to Image is a workflow in ComfyUI that allows users to input an image and generate a new image based on it. Image to Image can be used in scenarios such as:\n\n*   Converting original image styles, like transforming realistic photos into artistic styles\n*   Converting line art into realistic images\n*   Image restoration\n*   Colorizing old photos\n*   â€¦ and other scenarios\n\nTo explain it with an analogy: Itâ€™s like asking an artist to create a specific piece based on your reference image. If you carefully compare this tutorial with the [Text to Image](https://docs.comfy.org/tutorials/basic/text-to-image) tutorial, youâ€™ll notice that the Image to Image process is very similar to Text to Image, just with an additional input reference image as a condition. In Text to Image, we let the artist (image model) create freely based on our prompts, while in Image to Image, we let the artist create based on both our reference image and prompts.\n\n## ComfyUI Image to Image Workflow Example Guide\n\n### Model Installation\n\nDownload the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder.\n\n### Image to Image Workflow and Input Image\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![Image to Image Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image_to_image.png) \n\nDownload the image below and we will use it as the input image: ![Example Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/input.jpeg) \n\n### Complete the Workflow Step by Step\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![ComfyUI Image to Image Workflow - Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image-to-image-02-guide.jpg)\n\n1.  Ensure `Load Checkpoint` loads **v1-5-pruned-emaonly-fp16.safetensors**\n2.  Upload the input image to the `Load Image` node\n3.  Click `Queue` or press `Ctrl/Cmd + Enter` to generate\n\n## Key Points of Image to Image Workflow\n\nThe key to the Image to Image workflow lies in the `denoise` parameter in the `KSampler` node, which should be **less than 1** If youâ€™ve adjusted the `denoise` parameter and generated images, youâ€™ll notice:\n\n*   The smaller the `denoise` value, the smaller the difference between the generated image and the reference image\n*   The larger the `denoise` value, the larger the difference between the generated image and the reference image\n\nThis is because `denoise` determines the strength of noise added to the latent space image after converting the reference image. If `denoise` is 1, the latent space image will become completely random noise, making it the same as the latent space generated by the `empty latent image` node, losing all characteristics of the reference image. For the corresponding principles, please refer to the principle explanation in the [Text to Image](https://docs.comfy.org/tutorials/basic/text-to-image) tutorial.\n\n## Try It Yourself\n\n1.  Try modifying the `denoise` parameter in the **KSampler** node, gradually changing it from 1 to 0, and observe the changes in the generated images\n2.  Replace with your own prompts and reference images to generate your own image effects"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/inpaint",
  "markdown": "# ComfyUI Inpainting Workflow - ComfyUI\n\nThis article will introduce the concept of inpainting in AI image generation and guide you through creating an inpainting workflow in ComfyUI. Weâ€™ll cover:\n\n*   Using inpainting workflows to modify images\n*   Using the ComfyUI mask editor to draw masks\n*   `VAE Encoder (for Inpainting)` node\n\n## About Inpainting\n\nIn AI image generation, we often encounter situations where weâ€™re satisfied with the overall image but there are elements we donâ€™t want or that contain errors. Simply regenerating might produce a completely different image, so using inpainting to fix specific parts becomes very useful. Itâ€™s like having an **artist (AI model)** paint a picture, but weâ€™re still not satisfied with the specific details. We need to tell the artist **which areas to adjust (mask)**, and then let them **repaint (inpaint)** according to our requirements. Common inpainting scenarios include:\n\n*   **Defect Repair:** Removing unwanted objects, fixing incorrect AI-generated body parts, etc.\n*   **Detail Optimization:** Precisely adjusting local elements (like modifying clothing textures, adjusting facial expressions)\n*   And other scenarios\n\n### Model and Resource Preparation\n\n#### 1\\. Model Installation\n\nDownload the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder:\n\n#### 2\\. Inpainting Asset\n\nPlease download the following image which weâ€™ll use as input: ![ComfyUI Inpainting Input Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/input.png)\n\n#### 3\\. Inpainting Workflow\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![ComfyUI Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)\n\n### ComfyUI Inpainting Workflow Example Explanation\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![ComfyUI Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_workflow.png)\n\n1.  Ensure `Load Checkpoint` loads `512-inpainting-ema.safetensors`\n2.  Upload the input image to the `Load Image` node\n3.  Click `Queue` or use `Ctrl + Enter` to generate\n\n![Inpainting Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png) For comparison, hereâ€™s the result using the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model: ![SD1.5 Inpainting Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png) You will find that the results generated by the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) model have better inpainting effects and more natural transitions. This is because this model is specifically designed for inpainting, which helps us better control the generation area, resulting in improved inpainting effects. Do you remember the analogy weâ€™ve been using? Different models are like artists with varying abilities, but each artist has their own limits. Choosing the right model can help you achieve better generation results. You can try these approaches to achieve better results:\n\n1.  Modify positive and negative prompts with more specific descriptions\n2.  Try multiple runs using different seeds in the `KSampler` for different generation results\n3.  After learning about the mask editor in this tutorial, you can re-inpaint the generated results to achieve satisfactory outcomes.\n\nNext, weâ€™ll learn about using the **Mask Editor**. While our input image already includes an `alpha` transparency channel (the area we want to edit), so manual mask drawing isnâ€™t necessary, youâ€™ll often use the Mask Editor to create masks in practical applications.\n\n### Using the Mask Editor\n\nFirst right-click the `Save Image` node and select `Copy(Clipspace)`: ![Copy Image to Clipboard](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png) Then right-click the **Load Image** node and select `Paste(Clipspace)`: ![Paste Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png) Right-click the **Load Image** node again and select `Open in MaskEditor`: ![Open Mask Editor](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg) ![Mask Editor Demo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint-maskeditor.gif)\n\n1.  Adjust brush parameters on the right panel\n2.  Use eraser to correct mistakes\n3.  Click `Save` when finished\n\nThe drawn content will be used as a Mask input to the VAE Encoder (for Inpainting) node for encoding Then try adjusting your prompts and generating again until you achieve satisfactory results.\n\n## VAE Encoder (for Inpainting) Node\n\nComparing this workflow with [Text-to-Image](https://docs.comfy.org/tutorials/basic/text-to-image) and [Image-to-Image](https://docs.comfy.org/tutorials/basic/image-to-image), youâ€™ll notice the main differences are in the VAE sectionâ€™s conditional inputs. In this workflow, we use the **VAE Encoder (for Inpainting)** node, specifically designed for inpainting to help us better control the generation area and achieve better results. ![VAE Encoder (for Inpainting) Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg) **Input Types**\n\n| Parameter Name | Function |\n| --- | --- |\n| `pixels` | Input image to be encoded into latent space. |\n| `vae` | VAE model used to encode the image from pixel space to latent space. |\n| `mask` | Image mask specifying which areas need modification. |\n| `grow_mask_by` | Pixel value to expand the original mask outward, ensuring a transition area around the mask to avoid hard edges between inpainted and original areas. |\n\n**Output Types**\n\n| Parameter Name | Function |\n| --- | --- |\n| `latent` | Image encoded into latent space by the VAE. |"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/outpaint",
  "markdown": "# ComfyUI Outpainting Workflow Example - ComfyUI\n\nThis guide will introduce you to the concept of outpainting in AI image generation and how to create an outpainting workflow in ComfyUI. We will cover:\n\n*   Using outpainting workflow to extend an image\n*   Understanding and using outpainting-related nodes in ComfyUI\n*   Mastering the basic outpainting process\n\n## About Outpainting\n\nIn AI image generation, we often encounter situations where an existing image has good composition but the canvas area is too small, requiring us to extend the canvas to get a larger scene. This is where outpainting comes in. Basically, it requires similar content to [Inpainting](https://docs.comfy.org/tutorials/basic/inpaint), but we use different nodes to **build the mask**. Outpainting applications include:\n\n*   **Scene Extension:** Expand the scene range of the original image to show a more complete environment\n*   **Composition Adjustment:** Optimize the overall composition by extending the canvas\n*   **Content Addition:** Add more related scene elements to the original image\n\n### Preparation\n\n#### 1\\. Model Installation\n\nDownload the following model file and save it to `ComfyUI/models/checkpoints` directory:\n\n*   [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)\n\n#### 2\\. Input Image\n\nPrepare an image you want to extend. In this example, we will use the following image: ![ComfyUI Outpainting Input Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/input.png)\n\n#### 3\\. Outpainting Workflow\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![ComfyUI Outpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpaint.png)\n\n### Outpainting Workflow Usage Explanation\n\n![ComfyUI Outpainting Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpainting_workflow.jpg) The key steps of the outpainting workflow are as follows:\n\n1.  Load the locally installed model file in the `Load Checkpoint` node\n2.  Click the `Upload` button in the `Load Image` node to upload your image\n3.  Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute the image generation\n\nIn this workflow, we mainly use the `Pad Image for outpainting` node to control the direction and range of image extension. This is actually an [Inpaint](https://docs.comfy.org/tutorials/basic/inpaint) workflow, but we use different nodes to build the mask.\n\n### Pad Image for outpainting Node\n\n![Pad Image for outpainting Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/pad_image_for_outpainting.jpg) This node accepts an input image and outputs an extended image with a corresponding mask, where the mask is built based on the node parameters.\n\n#### Input Parameters\n\n| Parameter Name | Function |\n| --- | --- |\n| `image` | Input image |\n| `left` | Left padding amount |\n| `top` | Top padding amount |\n| `right` | Right padding amount |\n| `bottom` | Bottom padding amount |\n| `feathering` | Controls the smoothness of the transition between the original image and the added padding, higher values create smoother transitions |\n\n#### Output Parameters\n\n| Parameter Name | Function |\n| --- | --- |\n| `image` | Output `image` represents the padded image |\n| `mask` | Output `mask` indicates the original image area and the added padding area |\n\n#### Node Output Content\n\nAfter processing by the `Pad Image for outpainting` node, the output image and mask preview are as follows: ![Pad Image for outpainting Node Results](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg) You can see the corresponding output results:\n\n*   The `Image` output is the extended image\n*   The `Mask` output is the mask marking the extension areas"
},
{
  "url": "https://docs.comfy.org/index",
  "markdown": "# ComfyUI Official Documentation - ComfyUI\n\nGet Started\n\nComfyUI Official Documentation"
},
{
  "url": "https://docs.comfy.org/tutorials/basic/lora",
  "markdown": "# ComfyUI LoRA Example - ComfyUI\n\n**LoRA (Low-Rank Adaptation)** is an efficient technique for fine-tuning large generative models like Stable Diffusion. It introduces trainable low-rank matrices to the pre-trained model, adjusting only a portion of parameters rather than retraining the entire model, thus achieving optimization for specific tasks at a lower computational cost. Compared to base models like SD1.5, LoRA models are smaller and easier to train. ![LoRA Model vs Base Model Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/compare.png) The image above compares generation with the same parameters using [dreamshaper\\_8](https://civitai.com/models/4384?modelVersionId=128713) directly versus using the [blindbox\\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA model. As you can see, by using a LoRA model, we can generate images in different styles without adjusting the base model. We will demonstrate how to use a LoRA model. All LoRA variants: Lycoris, loha, lokr, locon, etcâ€¦ are used in the same way. In this example, we will learn how to load and use a LoRA model in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), covering the following topics:\n\n1.  Installing a LoRA model\n2.  Generating images using a LoRA model\n3.  A simple introduction to the `Load LoRA` node\n\n## Required Model Installation\n\nDownload the [dreamshaper\\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder. Download the [blindbox\\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.\n\n## LoRA Workflow File\n\nDownload the image below and **drag it into ComfyUI** to load the workflow. ![ComfyUI Workflow - LoRA](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/lora.png) \n\n## Complete the Workflow Step by Step\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![ComfyUI Workflow - LoRA Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/flow_diagram.png)\n\n1.  Ensure `Load Checkpoint` loads `dreamshaper_8.safetensors`\n2.  Ensure `Load LoRA` loads `blindbox_V1Mix.safetensors`\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n\n## Load LoRA Node Introduction\n\n![Load LoRA Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_lora.jpg) Models in the `ComfyUI\\models\\loras` folder will be detected by ComfyUI and can be loaded using this node.\n\n### Input Types\n\n| Parameter Name | Function |\n| --- | --- |\n| `model` | Connect to the base model |\n| `clip` | Connect to the CLIP model |\n| `lora_name` | Select the LoRA model to load and use |\n| `strength_model` | Affects how strongly the LoRA influences the model weights; higher values make the LoRA style stronger |\n| `strength_clip` | Affects how strongly the LoRA influences the CLIP text embeddings |\n\n### Output Types\n\n| Parameter Name | Function |\n| --- | --- |\n| `model` | Outputs the model with LoRA adjustments applied |\n| `clip` | Outputs the CLIP model with LoRA adjustments applied |\n\nThis node supports chain connections, allowing multiple `Load LoRA` nodes to be linked in series to apply multiple LoRA models. For more details, please refer to [ComfyUI Multiple LoRAs Example](https://docs.comfy.org/tutorials/basic/multiple-loras) ![LoRA Node Chain Connection](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)\n\n## Try It Yourself\n\n1.  Try modifying the prompt or adjusting different parameters of the `Load LoRA` node, such as `strength_model`, to observe changes in the generated images and become familiar with the `Load LoRA` node.\n2.  Visit [CivitAI](https://civitai.com/models) to download other kinds of LoRA models and try using them."
},
{
  "url": "https://docs.comfy.org/tutorials/basic/multiple-loras",
  "markdown": "# ComfyUI Multiple LoRAs Example - ComfyUI\n\nIn our [ComfyUI LoRA Example](https://docs.comfy.org/tutorials/basic/lora), we introduced how to load and use a single LoRA model, mentioning the nodeâ€™s chain connection capability. ![LoRA Node Chaining](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png) This tutorial demonstrates chaining multiple `Load LoRA` nodes to apply two LoRA models simultaneously: [blindbox\\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) and [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856). The comparison below shows individual effects of these LoRAs using identical parameters: ![Single LoRA Effects Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/compare.png) By chaining multiple LoRA models, we achieve a blended style in the final output: ![Multi-LoRA Application Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)\n\n## Model Installation\n\nDownload the [dreamshaper\\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder. Download the [blindbox\\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder. Download the [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.\n\n## Multi-LoRA Workflow\n\nDownload the image below and **drag it into ComfyUI** to load the workflow: ![ComfyUI Workflow - Multiple LoRAs](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png) \n\n## Complete the Workflow Step by Step\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![Multi-LoRA Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/flow_diagram.png)\n\n1.  Ensure `Load Checkpoint` loads **dreamshaper\\_8.safetensors**\n2.  Ensure first `Load LoRA` loads **blindbox\\_V1Mix.safetensors**\n3.  Ensure second `Load LoRA` loads **MoXinV1.safetensors**\n4.  Click `Queue` or press `Ctrl/Cmd + Enter` to generate\n\n## Try It Yourself\n\n1.  Adjust `strength_model` values in both `Load LoRA` nodes to control each LoRAâ€™s influence\n2.  Explore [CivitAI](https://civitai.com/models) for additional LoRAs and create custom combinations"
},
{
  "url": "https://docs.comfy.org/interface/settings/server-config",
  "markdown": "# Server Config - ComfyUI\n\n## Network\n\n### Host: The IP address to listen on\n\n*   **Function**: Sets the IP address the server binds to. Default `127.0.0.1` means only local access is allowed. If you need LAN access, you can set it to `0.0.0.0`\n\n### Port: The port to listen on\n\n**Function**: The port number the server listens on. Desktop version defaults to port 8000, Web version typically uses port 8188\n\n### TLS Key File: Path to TLS key file for HTTPS\n\n**Function**: The private key file path required for HTTPS encryption, used to establish secure connections\n\n### TLS Certificate File: Path to TLS certificate file for HTTPS\n\n**Function**: The certificate file path required for HTTPS encryption, used in conjunction with the private key\n\n### Enable CORS header: Use â€\\*â€ for all origins or specify domain\n\n**Function**: Cross-Origin Resource Sharing settings, allowing web browsers to access the server from different domains\n\n### Maximum upload size (MB)\n\n**Function**: Limits the maximum size of single file uploads, in MB, default 100MB. Affects upload limits for images, models and other files\n\n## CUDA\n\n### CUDA device index to use\n\n**Function**: Specifies which NVIDIA graphics card to use. 0 represents the first graphics card, 1 represents the second, and so on. Important for multi-GPU systems\n\n### Use CUDA malloc for memory allocation\n\n**Function**: Controls whether to use CUDAâ€™s memory allocator. Can improve memory management efficiency in certain situations\n\n## Inference\n\n### Global floating point precision\n\n**Function**: Sets the numerical precision for model calculations. FP16 saves VRAM but may affect quality, FP32 is more precise but uses more VRAM\n\n### UNET precision\n\n**Options**:\n\n*   `auto`: Automatically selects the most suitable precision\n*   `fp64`: 64-bit floating point precision, highest precision but largest VRAM usage\n*   `fp32`: 32-bit floating point precision, standard precision\n*   `fp16`: 16-bit floating point precision, can save VRAM\n*   `bf16`: 16-bit brain floating point precision, between fp16 and fp32\n*   `fp8_e4m3fn`: 8-bit floating point precision (e4m3), minimal VRAM usage\n*   `fp8_e5m2`: 8-bit floating point precision (e5m2), minimal VRAM usage\n\n**Function**: Specifically controls the computational precision of the UNET core component of diffusion models. Higher precision can provide better image generation quality but uses more VRAM. Lower precision can significantly save VRAM but may affect the quality of generated results.\n\n### VAE precision\n\n**Options and Recommendations**:\n\n*   `auto`: Automatically selects the most suitable precision, recommended for users with 8-12GB VRAM\n*   `fp16`: 16-bit floating point precision, recommended for users with 6GB or less VRAM, can save VRAM but may affect quality\n*   `fp32`: 32-bit floating point precision, recommended for users with 16GB or more VRAM who pursue the best quality\n*   `bf16`: 16-bit brain floating point precision, recommended for newer graphics cards that support this format, can achieve better performance balance\n\n**Function**: Controls the computational precision of the Variational Autoencoder (VAE), affecting the quality and speed of image encoding/decoding. Higher precision can provide better image reconstruction quality but uses more VRAM. Lower precision can save VRAM but may affect image detail restoration.\n\n### Run VAE on CPU\n\n**Function**: Forces VAE to run on CPU, can save VRAM but will reduce processing speed\n\n### Text Encoder precision\n\n**Options**:\n\n*   `auto`: Automatically selects the most suitable precision\n*   `fp8_e4m3fn`: 8-bit floating point precision (e4m3), minimal VRAM usage\n*   `fp8_e5m2`: 8-bit floating point precision (e5m2), minimal VRAM usage\n*   `fp16`: 16-bit floating point precision, can save VRAM\n*   `fp32`: 32-bit floating point precision, standard precision\n\n**Function**: Controls the computational precision of the text prompt encoder, affecting the accuracy of text understanding and VRAM usage. Higher precision can provide more accurate text understanding but uses more VRAM. Lower precision can save VRAM but may affect prompt parsing effectiveness.\n\n## Memory\n\n### Force channels-last memory format\n\n**Function**: Changes the data arrangement in memory, may improve performance on certain hardware\n\n### DirectML device index\n\n**Function**: Specifies the device when using DirectML acceleration on Windows, mainly for AMD graphics cards\n\n### Disable IPEX optimization\n\n**Function**: Disables Intel CPU optimization, mainly affects Intel processor performance\n\n### VRAM management mode\n\n**Options**:\n\n*   `auto`: Automatically manages VRAM, allocating VRAM based on model size and requirements\n*   `lowvram`: Low VRAM mode, uses minimal VRAM, may affect generation quality\n*   `normalvram`: Standard VRAM mode, balances VRAM usage and performance\n*   `highvram`: High VRAM mode, uses more VRAM for better performance\n*   `novram`: No VRAM usage, runs entirely on system memory\n*   `cpu`: CPU-only mode, doesnâ€™t use graphics card\n\n**Function**: Controls VRAM usage strategy, such as automatic management, low VRAM mode, etc.\n\n### Reserved VRAM (GB)\n\n**Function**: Amount of VRAM reserved for the operating system and other programs, prevents system freezing\n\n### Disable smart memory management\n\n**Function**: Disables automatic memory optimization, forces models to move to system memory to free VRAM\n\n## Preview\n\n### Method used for latent previews\n\n**Options**:\n\n*   `none`: No preview images displayed, only shows progress bar during generation\n*   `auto`: Automatically selects the most suitable preview method, dynamically adjusts based on system performance and VRAM\n*   `latent2rgb`: Directly converts latent space data to RGB images for preview, faster but average quality\n*   `taesd`: Uses lightweight TAESD model for preview, balances speed and quality\n\n**Function**: Controls how to preview intermediate results during generation. Different preview methods affect preview quality and performance consumption. Choosing the right preview method can find a balance between preview effects and system resource usage.\n\n### Size of preview images\n\n**Function**: Sets the resolution of preview images, affects preview clarity and performance. Larger sizes provide higher preview quality but also consume more VRAM\n\n## Cache\n\n### Use classic cache system\n\n**Function**: Uses traditional caching strategy, more conservative but stable\n\n### Use LRU caching with a maximum of N node results cached\n\n**Function**: Uses Least Recently Used (LRU) algorithm caching system, can cache a specified number of node computation results **Description**:\n\n*   Set a specific number to control maximum cache count, such as 10, 50, 100, etc.\n*   Caching can avoid repeated computation of the same node operations, improving workflow execution speed\n*   When cache reaches the limit, automatically clears the least recently used results\n*   Cached results occupy system memory (RAM/VRAM), larger values use more memory\n\n**Usage Recommendations**:\n\n*   Default value is null, meaning LRU caching is not enabled\n*   Set appropriate cache count based on system memory capacity and usage requirements\n*   Recommended for workflows that frequently reuse the same node configurations\n*   If system memory is sufficient, larger values can be set for better performance improvement\n\n## Attention\n\n### Cross attention method\n\n**Options**:\n\n*   `auto`: Automatically selects the most suitable attention computation method\n*   `split`: Block-wise attention computation, can save VRAM but slower speed\n*   `quad`: Uses quad attention algorithm, balances speed and VRAM usage\n*   `pytorch`: Uses PyTorch native attention computation, faster but higher VRAM usage\n\n**Function**: Controls the specific algorithm used when the model computes attention. Different algorithms make different trade-offs between generation quality, speed, and VRAM usage. Usually recommended to use auto for automatic selection.\n\n### Force attention upcast\n\n**Function**: Forces high-precision attention computation, improves quality but increases VRAM usage\n\n### Prevent attention upcast\n\n**Function**: Disables high-precision attention computation, saves VRAM\n\n## General\n\n### Disable xFormers optimization\n\n**Function**: Disables the optimization features of the xFormers library. xFormers is a library specifically designed to optimize the attention mechanisms of Transformer models, typically improving computational efficiency, reducing memory usage, and accelerating inference speed. Disabling this optimization will:\n\n*   Fall back to standard attention computation methods\n*   May increase memory usage and computation time\n*   Provide a more stable runtime environment in certain situations\n\n**Use Cases**:\n\n*   When encountering compatibility issues related to xFormers\n*   When more precise computation results are needed (some optimizations may affect numerical precision)\n*   When debugging or troubleshooting requires using standard implementations\n\n### Default hashing function for model files\n\n**Options**:\n\n*   `sha256`: Uses SHA-256 algorithm for hash verification, high security but slower computation\n*   `sha1`: Uses SHA-1 algorithm, faster but slightly lower security\n*   `sha512`: Uses SHA-512 algorithm, provides highest security but slowest computation\n*   `md5`: Uses MD5 algorithm, fastest but lowest security\n\n**Function**: Sets the hash algorithm for model file verification, used to verify file integrity. Different hash algorithms have different trade-offs between computation speed and security. Usually recommended to use sha256 as the default option, which achieves a good balance between security and performance.\n\n### Make pytorch use slower deterministic algorithms when it can\n\n**Function**: Forces PyTorch to use deterministic algorithms when possible to improve result reproducibility. **Description**:\n\n*   When enabled, PyTorch will prioritize deterministic algorithms over faster non-deterministic algorithms\n*   Same inputs will produce same outputs, helpful for debugging and result verification\n*   Deterministic algorithms typically run slower than non-deterministic algorithms\n*   Even with this setting enabled, completely identical image results cannot be guaranteed in all situations\n\n**Use Cases**:\n\n*   Scientific research requiring strict result reproducibility\n*   Debugging processes requiring stable output results\n*   Production environments requiring result consistency\n\n### Enable some untested and potentially quality deteriorating optimizations\n\n**Function**: Enables experimental optimizations that may improve speed but could potentially affect generation quality\n\n### Donâ€™t print server output to console\n\n**Function**: Prevents displaying server runtime information in the console, keeping the interface clean. **Description**:\n\n*   When enabled, ComfyUI server logs and runtime information will not be displayed\n*   Can reduce console information interference, making the interface cleaner\n*   May slightly improve system performance when thereâ€™s heavy log output\n*   Default is disabled (false), meaning server output is displayed by default\n\n**Use Cases**:\n\n*   Production environments where debugging information is not needed\n*   When wanting to keep the console interface clean\n*   When the system runs stably and log monitoring is not required\n\n**Note**: Itâ€™s recommended to keep this option disabled during development and debugging to promptly view server runtime status and error information.\n\n### Disable saving prompt metadata in files\n\n**Function**: Does not save workflow information in generated images, reducing file size, but also means the loss of corresponding workflow information, preventing you from using workflow output files to reproduce the corresponding generation results\n\n### Disable loading all custom nodes\n\n**Function**: Prevents loading all third-party extension nodes, typically used when troubleshooting issues to locate whether errors are caused by third-party extension nodes\n\n### Logging verbosity level\n\n**Function**: Controls the verbosity level of log output, used for debugging and monitoring system runtime status. **Options**:\n\n*   `CRITICAL`: Only outputs critical error information that may cause the program to stop running\n*   `ERROR`: Outputs error information indicating some functions cannot work properly\n*   `WARNING`: Outputs warning information indicating possible issues that donâ€™t affect main functionality\n*   `INFO`: Outputs general information including system runtime status and important operation records\n*   `DEBUG`: Outputs the most detailed debugging information including system internal runtime details\n\n**Description**:\n\n*   Log levels increase in verbosity from top to bottom\n*   Each level includes all log information from higher levels\n*   Recommended to set to INFO level for normal use\n*   Can be set to DEBUG level when troubleshooting for more information\n*   Can be set to WARNING or ERROR level in production environments to reduce log volume\n\n## Directories\n\n### Input directory\n\n**Function**: Sets the default storage path for input files (such as images, models)\n\n### Output directory\n\n**Function**: Sets the save path for generation results"
},
{
  "url": "https://docs.comfy.org/registry/specifications",
  "markdown": "# pyproject.toml - ComfyUI\n\n## Specifications\n\nThe `pyproject.toml` file contains two main sections for ComfyUI custom nodes: `[project]` and `[tool.comfy]`. Below are the specifications for each section.\n\n## \\[project\\] Section\n\n### name (required)\n\nThe node id uniquely identifies the custom node and will be used in URLs from the registry. Users can install the node by referencing this name:\n\n```\ncomfy node install <node-id>\n```\n\n**Requirements:**\n\n*   Must be less than 100 characters\n*   Can only contain alphanumeric characters, hyphens, underscores, and periods\n*   Cannot have consecutive special characters\n*   Cannot start with a number or special character\n*   Case-insensitive comparison\n\n**Best Practices:**\n\n*   Use a short, descriptive name\n*   Donâ€™t include â€œComfyUIâ€ in the name\n*   Make it memorable and easy to type\n\n**Examples:**\n\n```\nname = \"image-processor\"      # âœ… Good: Simple and clear\nname = \"super-resolution\"     # âœ… Good: Describes functionality\nname = \"ComfyUI-enhancer\"    # âŒ Bad: Includes ComfyUI\nname = \"123-tool\"            # âŒ Bad: Starts with number\n```\n\nSee the official [python documentation](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#name) for more details.\n\n### version (required)\n\nUses [semantic versioning](https://semver.org/) with a three-digit version number X.Y.Z:\n\n*   X (**MAJOR**): Breaking changes\n*   Y (**MINOR**): New features (backwards compatible)\n*   Z (**PATCH**): Bug fixes\n\n**Examples:**\n\n```\nversion = \"1.0.0\"    # Initial release\nversion = \"1.1.0\"    # Added new features\nversion = \"1.1.1\"    # Bug fix\nversion = \"2.0.0\"    # Breaking changes\n```\n\n### license (optional)\n\nSpecifies the license for your custom node. Can be specified in two ways:\n\n1.  **File Reference:**\n\n```\nlicense = { file = \"LICENSE\" }     # âœ… Points to LICENSE file\nlicense = { file = \"LICENSE.txt\" } # âœ… Points to LICENSE.txt\nlicense = \"LICENSE\"                # âŒ Incorrect format\n```\n\n2.  **License Name:**\n\n```\nlicense = { text = \"MIT License\" }  # âœ… Correct format\nlicense = { text = \"Apache-2.0\" }   # âœ… Correct format\nlicense = \"MIT LICENSE\"             # âŒ Incorrect format\n```\n\nCommon licenses: [MIT](https://opensource.org/license/mit), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html), [Apache](https://www.apache.org/licenses/LICENSE-2.0)\n\n### description (recommended)\n\nA brief description of what your custom node does.\n\n```\ndescription = \"A super resolution node for enhancing image quality\"\n```\n\n### repository (required)\n\nLinks to related resources:\n\n```\n[project.urls]\nRepository = \"https://github.com/username/repository\"\n```\n\n### urls (recommended)\n\nLinks to related resources:\n\n```\n[project.urls]\nDocumentation = \"https://github.com/username/repository/wiki\"\n\"Bug Tracker\" = \"https://github.com/username/repository/issues\"\n```\n\n### requires-python (recommended)\n\nSpecifies the Python versions that your node supports:\n\n```\nrequires-python = \">=3.8\"        # Python 3.8 or higher\nrequires-python = \">=3.8,<3.11\"  # Python 3.8 up to (but not including) 3.11\n```\n\n### Frontend Version Compatibility (optional)\n\nIf your node has specific requirements for which ComfyUI frontend versions it supports, you can specify this using the `comfyui-frontend-package` dependency. This package is published on [PyPI](https://pypi.org/project/comfyui-frontend-package/). For example, use this field when:\n\n*   Your custom node uses frontend APIs that were introduced in a specific version\n*   Youâ€™ve identified incompatibilities between your node and certain frontend versions\n*   Your node requires specific UI features only available in newer frontend versions\n\n```\n[project]\ndependencies = [\n    \"comfyui-frontend-package>=1.20.0\"       # Requires frontend 1.20.0 or newer\n    \"comfyui-frontend-package<=1.21.6\"       # Restricts to frontend versions up to 1.21.6\n    \"comfyui-frontend-package>=1.19,<1.22\"   # Works with frontend 1.19 to 1.21.x\n    \"comfyui-frontend-package~=1.20.0\"       # Compatible with 1.20.x but not 1.21.0\n    \"comfyui-frontend-package!=1.21.3\"       # Works with any version except 1.21.3\n]\n```\n\n### classifiers (recommended)\n\nUse classifiers to specify operating system compatibility and GPU accelerators. This information is used to help users find the right node for their system.\n\n```\n[project]\nclassifiers = [\n    # For OS-independent nodes (works on all operating systems)\n    \"Operating System :: OS Independent\",\n\n    # OR for OS-specific nodes, specify the supported systems:\n    \"Operating System :: Microsoft :: Windows\",  # Windows specific\n    \"Operating System :: POSIX :: Linux\",  # Linux specific\n    \"Operating System :: MacOS\",  # macOS specific\n    \n    # GPU Accelerator support\n    \"Environment :: GPU :: NVIDIA CUDA\",    # NVIDIA CUDA support\n    \"Environment :: GPU :: AMD ROCm\",       # AMD ROCm support\n    \"Environment :: GPU :: Intel Arc\",      # Intel Arc support\n    \"Environment :: NPU :: Huawei Ascend\",  # Huawei Ascend support\n    \"Environment :: GPU :: Apple Metal\",    # Apple Metal support\n]\n```\n\n### PublisherId (required)\n\nYour unique publisher identifier, typically matching your GitHub username. **Examples:**\n\n```\nPublisherId = \"john-doe\"        # âœ… Matches GitHub username\nPublisherId = \"image-wizard\"    # âœ… Unique identifier\n```\n\n### DisplayName (optional)\n\nA user-friendly name for your custom node.\n\n```\nDisplayName = \"Super Resolution Node\"\n```\n\n### Icon (optional)\n\nURL to your custom nodeâ€™s icon that will be displayed on the ComfyUI Registry and ComfyUI-Manager. **Requirements:**\n\n*   File types: SVG, PNG, JPG, or GIF\n*   Maximum resolution: 400px Ã— 400px\n*   Aspect ratio should be square\n\n```\nIcon = \"https://raw.githubusercontent.com/username/repo/main/icon.png\"\n```\n\nURL to a larger banner image that will be displayed on the ComfyUI Registry and ComfyUI-Manager. **Requirements:**\n\n*   File types: SVG, PNG, JPG, or GIF\n*   Aspect ratio: 21:9\n\n```\nBanner = \"https://raw.githubusercontent.com/username/repo/main/banner.png\"\n```\n\n### requires-comfyui (optional)\n\nSpecifies which version of ComfyUI your node is compatible with. This helps users ensure they have the correct version of ComfyUI installed. **Supported operators:** `<`, `>`, `<=`, `>=`, `~=`, `<>`, `!=` and ranges\n\n```\nrequires-comfyui = \">=1.0.0\"        # ComfyUI 1.0.0 or higher\nrequires-comfyui = \">=1.0.0,<2.0.0\"  # ComfyUI 1.0.0 up to (but not including) 2.0.0\nrequires-comfyui = \"~=1.0.0\"         # Compatible release: version 1.0.0 or newer, but not version 2.0.0\nrequires-comfyui = \"!=1.2.3\"         # Any version except 1.2.3\nrequires-comfyui = \">0.1.3,<1.0.0\"   # Greater than 0.1.3 and less than 1.0.0\n```\n\n### includes (optional)\n\nSpecifies whether to force include certain specific folders. For some situations, such as custom nodes in frontend projects, the final packaged output folder might be included in .gitignore. In such cases, we need to force include it for registry use.\n\n## Complete Example\n\n```\n[project]\nname = \"super-resolution-node\"\nversion = \"1.0.0\"\ndescription = \"Enhance image quality using advanced super resolution techniques\"\nlicense = { file = \"LICENSE\" }\nrequires-python = \">=3.8\"\ndependencies = [\n    \"comfyui-frontend-package<=1.21.6\"  # Frontend version compatibility\n]\nclassifiers = [\n    \"Operating System :: OS Independent\"  # Works on all operating systems\n]\ndynamic = [\"dependencies\"]\n\n[tool.setuptools.dynamic]\ndependencies = {file = [\"requirements.txt\"]}\n\n[project.urls]\nRepository = \"https://github.com/username/super-resolution-node\"\nDocumentation = \"https://github.com/username/super-resolution-node/wiki\"\n\"Bug Tracker\" = \"https://github.com/username/super-resolution-node/issues\"\n\n[tool.comfy]\nPublisherId = \"image-wizard\"\nDisplayName = \"Super Resolution Node\"\nIcon = \"https://raw.githubusercontent.com/username/super-resolution-node/main/icon.png\"\nBanner = \"https://raw.githubusercontent.com/username/super-resolution-node/main/banner.png\"\nrequires-comfyui = \">=1.0.0\"  # ComfyUI version compatibility\n```\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/installation/update_comfyui",
  "markdown": "# How to Update ComfyUI - ComfyUI\n\nWhile weâ€™ve covered ComfyUI updates across different installation methods in various sections, this comprehensive guide consolidates all update procedures to help users clearly understand how to update ComfyUI.\n\nComfyUI Portable provides convenient batch scripts for easy updates.\n\n### Update Script Location\n\nIn the `update` folder within your portable installation directory, youâ€™ll find the following update scripts:\n\n```\nComfyUI_windows_portable\nâ””â”€ ðŸ“‚update\n   â”œâ”€â”€ update.py\n   â”œâ”€â”€ update_comfyui.bat                           // Update to latest development version\n   â”œâ”€â”€ update_comfyui_stable.bat                    // Update to latest stable version\n   â””â”€â”€ update_comfyui_and_python_dependencies.bat   // Update dependencies (for troubleshooting)\n```\n\n## ComfyUI Version Types\n\nDepending on your installation method, ComfyUI offers several installation versions. The links below contain update instructions for each version.\n\n## What Needs to Be Updated When Updating ComfyUI?\n\nComfyUI updates primarily consist of two components:\n\n1.  Update ComfyUIâ€™s core code\n2.  Update ComfyUIâ€™s core dependencies, including necessary Python dependencies and ComfyUI functional dependency packages\n\n**Core Code**: New nodes, new model support, new features, etc. **Core Dependencies**: Mainly includes ComfyUIâ€™s frontend functionality, workflow templates, node help documentation, etc.\n\n```\ncomfyui-frontend-package   # ComfyUI frontend functionality\ncomfyui-workflow-templates # ComfyUI workflow templates  \ncomfyui-embedded-docs      # ComfyUI node help documentation\n```\n\nThese three core dependencies are maintained in separate repositories:\n\n*   [ComfyUI\\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend/) - Frontend interface and interactive features\n*   [workflow\\_templates](https://github.com/Comfy-Org/workflow_templates) - Pre-built workflow templates\n*   [comfyui-embedded-docs](https://github.com/Comfy-Org/embedded-docs) - Node help documentation\n\nItâ€™s important to understand the difference between development (nightly) and stable (release) versions:\n\n*   **Development version (nightly)**: Latest commit code, giving you access to the newest features, but may contain potential issues\n*   **Stable version (release)**: Built on stable releases, usually lags behind development versions but offers higher stability. We support stable versions after features are tested and stabilized\n\nMany users often find themselves on release versions or desktop versions during updates, but discover that needed features are only available in development versions. In such cases, check if your local `ComfyUI/requirements.txt` matches the [nightly version dependencies](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) to determine if all dependencies support the latest features.\n\n## Common Update Issues\n\n### Missing or Outdated Frontend, Workflow Templates, Node After Updates\n\nUsers often only use the `git pull` command to update ComfyUI code but **neglect core dependency updates**, leading to the following issues:\n\n*   Missing or abnormal frontend functionality\n*   Cannot find newly added workflow templates\n*   Outdated or missing node help documentation\n*   New features lack corresponding frontend support\n\nAfter using the `git pull` command, use the corresponding ComfyUI environment to use `pip install -r requirements.txt` to update dependencies.\n\n### How to Properly Update Core Dependencies\n\n**Recommended Method**: Use the `ComfyUI_windows_portable\\update\\update_comfyui.bat` batch script, which will update both ComfyUI code and all Python dependencies.**Manual Dependency Update**: If you need to manually update dependencies, use the following command:\n\n```\n# Open command line in portable version directory\n.\\python_embeded\\python.exe -m pip install -r ComfyUI\\requirements.txt\n```\n\n### Core Dependency Update Troubleshooting\n\nIf core dependency updates fail, follow these troubleshooting steps:\n\n### Why Canâ€™t I Find New Features After Updating?\n\nThis is one of the most common issues:\n\n*   If youâ€™re using the **Desktop version**, features may lag behind because the desktop version is built on stable releases\n*   Ensure youâ€™re using the **development version (nightly)**, not the **stable version (release)**\n\nAdditionally, ensure that corresponding dependencies have been successfully updated during the update process. If issues persist after updating, refer to the [Dependency Update Troubleshooting](#dependency-update-troubleshooting) section to diagnose problems.\n\n### How to Switch Between Development (Nightly) and Stable (Release) Versions?\n\nDifferences between versions:\n\n*   **Characteristics**: Contains the latest commit code\n*   **Advantages**: Experience the latest features and improvements first\n*   **Risks**: May contain undiscovered bugs or unstable factors\n*   **Suitable for**: Developers, testers, users wanting to experience the latest features\n\nUse `update_comfyui.bat` instead of `update_comfyui_stable.bat`:\n\n```\n# Development version (latest features)\ndouble-click: update_comfyui.bat\n\n# Stable version\ndouble-click: update_comfyui_stable.bat\n```\n\n### What to Do When Errors Occur After Updates?\n\n1.  **Check Dependencies**: Run `pip install -r requirements.txt` to ensure all dependencies are updated\n2.  **Check Custom Nodes**: Some custom nodes may be incompatible with new versions\n3.  **Roll Back Version**: If issues are severe, you can roll back to a previous stable version\n\nIf problems occur, refer to our troubleshooting page for solutions.\n\n[\n\n## Troubleshooting\n\nLearn how to troubleshoot ComfyUI issues\n\n\n\n](https://docs.comfy.org/troubleshooting/overview)\n\n### How to Stay Updated on New Features?\n\n*   **GitHub Releases**: Check [ComfyUI Releases](https://github.com/comfyanonymous/ComfyUI/releases) for stable version updates\n*   **GitHub Commits**: View [latest commits](https://github.com/comfyanonymous/ComfyUI/commits/master) to understand development progress\n*   **Community Discussion**: Follow our [blog](https://blog.comfy.org/) and [Twitter](https://x.com/comfyui) for the latest updates"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Funpublish-delete-a-specific-version-of-a-node",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fretrieve-permissions-the-user-has-for-a-given-publisher-1",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fcreate-a-new-personal-access-token",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/ClipTextEncodeSdxlRefiner",
  "markdown": "# ClipTextEncodeSdxlRefiner - ComfyUI Built-in Node Documentation\n\nThis node is specifically designed for the SDXL Refiner model to convert text prompts into conditioning information by incorporating aesthetic scores and dimensional information to enhance the conditions for generation tasks, thereby improving the final refinement effect. It acts like a professional art director, not only conveying your creative intent but also injecting precise aesthetic standards and specification requirements into the work.\n\n## About SDXL Refiner\n\nSDXL Refiner is a specialized refinement model that focuses on enhancing image details and quality based on the SDXL base model. This process is like having an art retoucher:\n\n1.  First, it receives preliminary images or text descriptions generated by the base model\n2.  Then, it guides the refinement process through precise aesthetic scoring and dimensional parameters\n3.  Finally, it focuses on processing high-frequency image details to improve overall quality\n\nRefiner can be used in two ways:\n\n*   As a standalone refinement step for post-processing images generated by the base model\n*   As part of an expert integration system, taking over processing during the low-noise phase of generation\n\n## Inputs\n\n| Parameter Name | Data Type | Input Type | Default Value | Value Range | Description |\n| --- | --- | --- | --- | --- | --- |\n| `clip` | CLIP | Required | \\-  | \\-  | CLIP model instance used for text tokenization and encoding, the core component for converting text into model-understandable format |\n| `ascore` | FLOAT | Optional | 6.0 | 0.0-1000.0 | Controls the visual quality and aesthetics of generated images, similar to setting quality standards for artwork:  <br>\\- High scores(7.5-8.5): Pursues more refined, detail-rich effects  <br>\\- Medium scores(6.0-7.0): Balanced quality control  <br>\\- Low scores(2.0-3.0): Suitable for negative prompts |\n| `width` | INT | Required | 1024 | 64-16384 | Specifies output image width (pixels), must be multiple of 8. SDXL performs best when total pixel count is close to 1024Ã—1024 (about 1M pixels) |\n| `height` | INT | Required | 1024 | 64-16384 | Specifies output image height (pixels), must be multiple of 8. SDXL performs best when total pixel count is close to 1024Ã—1024 (about 1M pixels) |\n| `text` | STRING | Required | \\-  | \\-  | Text prompt description, supports multi-line input and dynamic prompt syntax. In Refiner, text prompts should focus more on describing desired visual quality and detail characteristics |\n\n## Outputs\n\n| Output Name | Data Type | Description |\n| --- | --- | --- |\n| `CONDITIONING` | CONDITIONING | Refined conditional output containing integrated encoding of text semantics, aesthetic standards, and dimensional information, specifically for guiding SDXL Refiner model in precise image refinement |\n\n## Notes\n\n1.  This node is specifically optimized for the SDXL Refiner model and differs from regular CLIPTextEncode nodes\n2.  An aesthetic score of 7.5 is recommended as the baseline, which is the standard setting used in SDXL training\n3.  All dimensional parameters must be multiples of 8, and total pixel count close to 1024Ã—1024 (about 1M pixels) is recommended\n4.  The Refiner model focuses on enhancing image details and quality, so text prompts should emphasize desired visual effects rather than scene content\n5.  In practical use, Refiner is typically used in the later stages of generation (approximately the last 20% of steps), focusing on detail optimization"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fapi-nodes%2Fupload-files",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fretrieves-a-list-of-nodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/basic/upscale",
  "markdown": "# ComfyUI Image Upscale Workflow - ComfyUI\n\n## What is Image Upscaling?\n\nImage Upscaling is the process of converting low-resolution images to high-resolution using algorithms. Unlike traditional interpolation methods, AI upscaling models (like ESRGAN) can intelligently reconstruct details while maintaining image quality. For instance, the default SD1.5 model often struggles with large-size image generation. To achieve high-resolution results,we typically generate smaller images first and then use upscaling techniques. This article covers one of many upscaling methods in ComfyUI. In this tutorial, weâ€™ll guide you through:\n\n1.  Downloading and installing upscaling models\n2.  Performing basic image upscaling\n3.  Combining text-to-image workflows with upscaling\n\n## Upscaling Workflow\n\n### Model Installation\n\nRequired ESRGAN models download:\n\n### Workflow and Assets\n\nDownload and drag the following image into ComfyUI to load the basic upscaling workflow: ![Upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_workflow.png) \n\nUse this image in smaller size as input: ![Upscale-input](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale-input.jpg) \n\n### Complete the Workflow Step by Step\n\nFollow the steps in the diagram below to ensure the workflow runs correctly. ![Upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_simple_workflow.jpg)\n\n1.  Ensure `Load Upscale Model` loads `4x-ESRGAN.pth`\n2.  Upload the input image to the `Load Image` node\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n\nThe core components are the `Load Upscale Model` and `Upscale Image (Using Model)` nodes, which receive an image input and upscale it using the selected model.\n\n## Text-to-Image Combined Workflow\n\nAfter mastering basic upscaling, we can combine it with the [text-to-image](https://docs.comfy.org/tutorials/basic/text-to-image) workflow. For text-to-image basics, refer to the [text-to-image tutorial](https://docs.comfy.org/tutorials/basic/text-to-image). Download and drag this image into ComfyUI to load the combined workflow: ![Text-to-image upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/esrgan_example.png) This workflow connects the text-to-image output image directly to the upscaling nodes for final processing.\n\n## Additional Tips\n\n1.  **Chained Upscaling**: Combine multiple upscale nodes (e.g., 2x â†’ 4x) for ultra-high magnification\n2.  **Hybrid Workflow**: Connect upscale nodes after generation for â€œgenerate+enhanceâ€ pipelines\n3.  **Comparative Testing**: Different models perform better on specific image types - test multiple options"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fcreate-node-translations",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fretrieve-a-specific-node-by-id",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image",
  "markdown": "# Flux 1.1 Pro Ultra Image API Node ComfyUI Official Workflow Examples\n\nFLUX 1.1 Pro Ultra is a high-performance AI image generation tool by BlackForestLabs, featuring ultra-high resolution and efficient generation capabilities. It supports up to 4MP resolution (4x the standard version) while keeping single image generation time under 10 seconds - 2.5x faster than similar high-resolution models. The tool offers two core modes:\n\n*   **Ultra Mode**: Designed for high-resolution needs, perfect for advertising and e-commerce where detail magnification is important. It accurately reflects prompts while maintaining generation speed.\n*   **Raw Mode**: Focuses on natural realism, optimizing skin tones, lighting, and landscape details. Reduces the â€œAI lookâ€ and is ideal for photography and realistic style creation.\n\nWe now support the Flux 1.1 Pro Ultra Image node in ComfyUI. This guide will cover:\n\n*   Flux 1.1 Pro Text-to-Image\n*   Flux 1.1 Pro Image-to-Image (Remix)\n\n## Flux 1.1 Pro Ultra Image Node Documentation\n\nCheck the following documentation for detailed node parameter settings:\n\n*   [Flux 1.1 Pro Ultra Image](https://docs.comfy.org/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg)\n\n## Flux 1.1 \\[pro\\] Text-to-Image Tutorial\n\n### 1\\. Download Workflow File\n\nDownload and drag the following file into ComfyUI to load the workflow: ![Flux 1.1 pro Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_t2i.png)\n\n### 2\\. Complete the Workflow Steps\n\n![Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg) Follow the numbered steps to complete the basic workflow:\n\n1.  (Optional) Modify the prompt in the `Flux 1.1 [pro] Ultra Image` node\n2.  (Optional) Set `raw` parameter to `false` for more realistic output\n3.  Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image\n4.  After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\n## Flux 1.1\\[pro\\] Image-to-Image Tutorial\n\nWhen adding an `image_prompt` to the node input, the output will blend features from the input image (Remix). The `image_prompt_strength` value affects the blend ratio: higher values make the output more similar to the input image.\n\n### 1\\. Download Workflow File\n\nDownload and drag the following file into ComfyUI, or right-click the purple node in the Text-to-Image workflow and set `mode` to `always` to enable `image_prompt` input: ![Flux 1.1 pro Image-to-Image Remix](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_i2i.png) Weâ€™ll use this image as input: ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png) \n\n### 2\\. Complete the Workflow Steps\n\n![Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg) Follow these numbered steps:\n\n1.  Click **Upload** on the `Load Image` node to upload your input image\n2.  (Optional) Adjust `image_prompt_strength` in `Flux 1.1 [pro] Ultra Image` to change the blend ratio\n3.  Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image\n4.  After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\nHereâ€™s a comparison of outputs with different `image_prompt_strength` values: ![Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg)"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference",
  "markdown": "# Luma Reference - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Luma Reference Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-reference.jpg) The Luma Reference node allows you to set reference images and weights to guide the creation process of Luma image generation nodes, making the generated images closer to specific features of the reference images.\n\n## Node Function\n\nThis node works as a helper tool for Luma generation nodes, allowing users to provide reference images to influence generation results. It enables users to set the weight of reference images to control how much they affect the final result. Multiple Luma Reference nodes can be chained together, with a maximum of 4 working simultaneously according to API requirements.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | Image | \\-  | Input image used as reference |\n| weight | Float | 1.0 | Controls the strength of the reference imageâ€™s influence (0-1) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| luma\\_ref | LUMA\\_REF | Reference object containing image and weight |\n\n## Usage Example\n\n[\n\n## Luma Text to Image Workflow Example\n\nLuma Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)\n\n## How It Works\n\nThe Luma Reference node receives image input and allows setting a weight value. The node doesnâ€™t directly generate or modify images but creates a reference object containing image data and weight information, which is then passed to Luma generation nodes. During the generation process, Luma AI analyzes the features of the reference image and incorporates these features into the generation results based on the set weight. Higher weight values mean the generated image will be closer to the reference imageâ€™s features, while lower weight values indicate the reference image will only slightly influence the final result.\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\n\nclass LumaReferenceNode(ComfyNodeABC):\n    \"\"\"\n    Holds an image and weight for use with Luma Generate Image node.\n    \"\"\"\n\n    RETURN_TYPES = (LumaIO.LUMA_REF,)\n    RETURN_NAMES = (\"luma_ref\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_luma_reference\"\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Image to use as reference.\",\n                    },\n                ),\n                \"weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of image reference.\",\n                    },\n                ),\n            },\n            \"optional\": {\"luma_ref\": (LumaIO.LUMA_REF,)},\n        }\n\n    def create_luma_reference(\n        self, image: torch.Tensor, weight: float, luma_ref: LumaReferenceChain = None\n    ):\n        if luma_ref is not None:\n            luma_ref = luma_ref.clone()\n        else:\n            luma_ref = LumaReferenceChain()\n        luma_ref.add(LumaReference(image=image, weight=round(weight, 2)))\n        return (luma_ref,)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3",
  "markdown": "# Ideogram V3 - ComfyUI Native Node Documentation\n\n![ComfyUI Native Ideogram V3 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg) This node connects to the Ideogram V3 API to perform image generation tasks. Currently, this node supports two image generation modes:\n\n*   **Text-to-Image Mode** - Generate new images from text prompts\n*   **Inpainting Mode** - Regenerate specific areas by providing an original image and mask\n\n### Text-to-Image Mode\n\nThis is the default mode, activated when no image or mask inputs are provided. Simply provide a prompt and the desired parameters:\n\n1.  Describe the image you want in the prompt field\n2.  Select an appropriate aspect ratio or resolution\n3.  Adjust other parameters like magic prompt, seed, and rendering quality\n4.  Run the node to generate the image\n\n### Inpainting Mode\n\n**Important Note**: This mode requires both image and mask inputs. If only one is provided, the node will throw an error.\n\n1.  Connect the original image to the `image` input port\n2.  Create a mask with the same dimensions as the original image, where white areas represent parts to be regenerated\n3.  Connect the mask to the `mask` input port\n4.  Describe what you want to generate in the masked area in the prompt\n5.  Run the node to perform local editing\n\n## Parameter Descriptions\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the content to generate |\n| aspect\\_ratio | combo | â€1:1â€ | Image aspect ratio (text-to-image mode only) |\n| resolution | combo | â€Autoâ€ | Image resolution, overrides aspect ratio when set |\n| magic\\_prompt\\_option | combo | â€AUTOâ€ | Magic prompt enhancement: AUTO, ON, or OFF |\n| seed | int | 0   | Random seed value, 0 for random generation |\n| num\\_images | int | 1   | Number of images to generate (1-8) |\n| rendering\\_speed | combo | â€BALANCEDâ€ | Rendering speed: BALANCED, TURBO, or QUALITY |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| image | image | Input image for inpainting mode (**must be provided with mask**) |\n| mask | mask | Mask for inpainting, white areas will be replaced (**must be provided with image**) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image |\n\n## How It Works\n\nThe Ideogram V3 node uses state-of-the-art AI models to process user input, capable of understanding complex design intentions and text layout requirements. It supports two main modes:\n\n1.  **Generation Mode**: Creates new images from text prompts\n2.  **Edit Mode**: Uses original image + mask combination, replacing only the areas specified by the mask\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass IdeogramV3(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V3 model.\n\n    Supports both regular image generation from text prompts and image editing with mask.\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation or editing\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V3_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation. Ignored if resolution is not set to Auto.\",\n                    },\n                ),\n                \"resolution\": (\n                    IO.COMBO,\n                    {\n                        \"options\": V3_RESOLUTIONS,\n                        \"default\": \"Auto\",\n                        \"tooltip\": \"The resolution for image generation. If not set to Auto, this overrides the aspect_ratio setting.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n                \"rendering_speed\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"BALANCED\", \"TURBO\", \"QUALITY\"],\n                        \"default\": \"BALANCED\",\n                        \"tooltip\": \"Controls the trade-off between generation speed and quality\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v3\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        image=None,\n        mask=None,\n        resolution=\"Auto\",\n        aspect_ratio=\"1:1\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        num_images=1,\n        rendering_speed=\"BALANCED\",\n        auth_token=None,\n    ):\n        # Check if both image and mask are provided for editing mode\n        if image is not None and mask is not None:\n            # Edit mode\n            path = \"/proxy/ideogram/ideogram-v3/edit\"\n\n            # Process image and mask\n            input_tensor = image.squeeze().cpu()\n\n            # Validate mask dimensions match image\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n\n            # Process image\n            img_np = (input_tensor.numpy() * 255).astype(np.uint8)\n            img = Image.fromarray(img_np)\n            img_byte_arr = io.BytesIO()\n            img.save(img_byte_arr, format=\"PNG\")\n            img_byte_arr.seek(0)\n            img_binary = img_byte_arr\n            img_binary.name = \"image.png\"\n\n            # Process mask - white areas will be replaced\n            mask_np = (mask.squeeze().cpu().numpy() * 255).astype(np.uint8)\n            mask_img = Image.fromarray(mask_np)\n            mask_byte_arr = io.BytesIO()\n            mask_img.save(mask_byte_arr, format=\"PNG\")\n            mask_byte_arr.seek(0)\n            mask_binary = mask_byte_arr\n            mask_binary.name = \"mask.png\"\n\n            # Create edit request\n            edit_request = IdeogramV3EditRequest(\n                prompt=prompt,\n                rendering_speed=rendering_speed,\n            )\n\n            # Add optional parameters\n            if magic_prompt_option != \"AUTO\":\n                edit_request.magic_prompt = magic_prompt_option\n            if seed != 0:\n                edit_request.seed = seed\n            if num_images > 1:\n                edit_request.num_images = num_images\n\n            # Execute the operation for edit mode\n            operation = SynchronousOperation(\n                endpoint=ApiEndpoint(\n                    path=path,\n                    method=HttpMethod.POST,\n                    request_model=IdeogramV3EditRequest,\n                    response_model=IdeogramGenerateResponse,\n                ),\n                request=edit_request,\n                files={\n                    \"image\": img_binary,\n                    \"mask\": mask_binary,\n                },\n                content_type=\"multipart/form-data\",\n                auth_token=auth_token,\n            )\n\n        elif image is not None or mask is not None:\n            # If only one of image or mask is provided, raise an error\n            raise Exception(\"Ideogram V3 image editing requires both an image AND a mask\")\n        else:\n            # Generation mode\n            path = \"/proxy/ideogram/ideogram-v3/generate\"\n\n            # Create generation request\n            gen_request = IdeogramV3Request(\n                prompt=prompt,\n                rendering_speed=rendering_speed,\n            )\n\n            # Handle resolution vs aspect ratio\n            if resolution != \"Auto\":\n                gen_request.resolution = resolution\n            elif aspect_ratio != \"1:1\":\n                v3_aspect = V3_RATIO_MAP.get(aspect_ratio)\n                if v3_aspect:\n                    gen_request.aspect_ratio = v3_aspect\n\n            # Add optional parameters\n            if magic_prompt_option != \"AUTO\":\n                gen_request.magic_prompt = magic_prompt_option\n            if seed != 0:\n                gen_request.seed = seed\n            if num_images > 1:\n                gen_request.num_images = num_images\n\n            # Execute the operation for generation mode\n            operation = SynchronousOperation(\n                endpoint=ApiEndpoint(\n                    path=path,\n                    method=HttpMethod.POST,\n                    request_model=IdeogramV3Request,\n                    response_model=IdeogramGenerateResponse,\n                ),\n                request=gen_request,\n                auth_token=auth_token,\n            )\n\n        # Execute the operation and process response\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra",
  "markdown": "# Stability Stable Image Ultra - ComfyUI Native Node Documentation\n\n![ComfyUI Native Stability Stable Image Ultra Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg) The Stability Stable Image Ultra node uses Stability AIâ€™s Stable Diffusion Ultra API to generate high-quality images. It supports both text-to-image and image-to-image generation, creating detailed and artistic visuals from text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text description of what you want to generate. Better results come from clear, descriptive prompts that specify elements, colors and themes. You can control word importance using `(word:weight)` format, where weight is 0-1. For example: `The sky was (blue:0.3) and (green:0.8)` makes the sky more green than blue. |\n| aspect\\_ratio | select | â€1:1â€ | Width to height ratio of output image |\n| style\\_preset | select | â€Noneâ€ | Optional preset style for generated image |\n| seed | integer | 0   | Random seed for noise generation (0-4294967294) |\n\n### Optional Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image for image-to-image generation |\n| negative\\_prompt | string | \"\"  | Describes what you donâ€™t want in the output image. This is an advanced feature |\n| image\\_denoise | float | 0.5 | Denoising strength for input image (0.0-1.0). 0.0 keeps input image unchanged, 1.0 is like having no input image |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image |\n\n## Notes\n\n*   image\\_denoise has no effect when no input image is provided\n*   No preset style is applied when style\\_preset is â€œNoneâ€\n*   For image-to-image, input images are converted to the proper format before API submission\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass StabilityStableImageUltraNode:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/stability\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines\" +\n                                    \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines\" +\n                                    \"elements, colors, and subjects will lead to better results. \" +\n                                    \"To control the weight of a given word use the format `(word:weight)`,\" +\n                                    \"where `word` is the word you'd like to control the weight of and `weight`\" +\n                                    \"is a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\" +\n                                    \"would convey a sky that was blue and green, but more green than blue.\"\n                    },\n                ),\n                \"aspect_ratio\": ([x.value for x in StabilityAspectRatio],\n                    {\n                        \"default\": StabilityAspectRatio.ratio_1_1,\n                        \"tooltip\": \"Aspect ratio of generated image.\",\n                    },\n                ),\n                \"style_preset\": (get_stability_style_presets(),\n                    {\n                        \"tooltip\": \"Optional desired style of generated image.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 4294967294,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (IO.IMAGE,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"A blurb of text describing what you do not wish to see in the output image. This is an advanced feature.\"\n                    },\n                ),\n                \"image_denoise\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(self, prompt: str, aspect_ratio: str, style_preset: str, seed: int,\n                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,\n                 auth_token=None):\n        # prepare image binary if image present\n        image_binary = None\n        if image is not None:\n            image_binary = tensor_to_bytesio(image, 1504 * 1504).read()\n        else:\n            image_denoise = None\n\n        if not negative_prompt:\n            negative_prompt = None\n        if style_preset == \"None\":\n            style_preset = None\n\n        files = {\n            \"image\": image_binary\n        }\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/stability/v2beta/stable-image/generate/ultra\",\n                method=HttpMethod.POST,\n                request_model=StabilityStableUltraRequest,\n                response_model=StabilityStableUltraResponse,\n            ),\n            request=StabilityStableUltraRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                aspect_ratio=aspect_ratio,\n                seed=seed,\n                strength=image_denoise,\n                style_preset=style_preset,\n            ),\n            files=files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.finish_reason != \"SUCCESS\":\n            raise Exception(f\"Stable Image Ultra generation failed: {response_api.finish_reason}.\")\n\n        image_data = base64.b64decode(response_api.image)\n        returned_image = bytesio_to_image_tensor(BytesIO(image_data))\n\n        return (returned_image,)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background",
  "markdown": "# Recraft Replace Background - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Replace Background Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg) The Recraft Replace Background node uses Recraftâ€™s API to intelligently detect subjects in images and generate new backgrounds based on text prompts.\n\n```\n\nclass RecraftReplaceBackgroundNode:\n    \"\"\"\n    Replace background on image, based on provided prompt.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt: str,\n        n: int,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n        )\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/replaceBackground\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg",
  "markdown": "# Save SVG - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Save SVG Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/save-svg.jpg) The Save SVG node allows you to save SVG data from Recraft vector generation nodes as usable files in the filesystem. This is an essential component for handling and exporting vector graphics.\n\n## Node Function\n\nThis node receives SVG vector data and saves it as standard SVG files in the filesystem. It supports automatic file naming and save path specification, allowing vector graphics to be opened and edited in other software.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| svg | SVG | \\-  | SVG vector data to save |\n| filename\\_prefix | string | â€recraftâ€ | Prefix for the filename |\n| output\\_dir | string | \\-  | Output directory, defaults to ComfyUI output folder at `ComfyUI/output/svg/` |\n| index | integer | \\-1 | Save index, -1 means save all generated SVGs |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| SVG | SVG | Passes through the input SVG data |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\nclass SaveSVGNode:\n    \"\"\"\n    Save SVG files on disk.\n    \"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n\n    RETURN_TYPES = ()\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"save_svg\"\n    CATEGORY = \"api node/image/Recraft\"\n    OUTPUT_NODE = True\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"svg\": (RecraftIO.SVG,),\n                \"filename_prefix\": (\"STRING\", {\"default\": \"svg/ComfyUI\", \"tooltip\": \"The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes.\"})\n            },\n            \"hidden\": {\n                \"prompt\": \"PROMPT\",\n                \"extra_pnginfo\": \"EXTRA_PNGINFO\"\n            }\n        }\n\n    def save_svg(self, svg: SVG, filename_prefix=\"svg/ComfyUI\", prompt=None, extra_pnginfo=None):\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n        results = list()\n\n        # Prepare metadata JSON\n        metadata_dict = {}\n        if prompt is not None:\n            metadata_dict[\"prompt\"] = prompt\n        if extra_pnginfo is not None:\n            metadata_dict.update(extra_pnginfo)\n\n        # Convert metadata to JSON string\n        metadata_json = json.dumps(metadata_dict, indent=2) if metadata_dict else None\n\n        for batch_number, svg_bytes in enumerate(svg.data):\n            filename_with_batch_num = filename.replace(\"%batch_num%\", str(batch_number))\n            file = f\"{filename_with_batch_num}_{counter:05}_.svg\"\n\n            # Read SVG content\n            svg_bytes.seek(0)\n            svg_content = svg_bytes.read().decode('utf-8')\n\n            # Inject metadata if available\n            if metadata_json:\n                # Create metadata element with CDATA section\n                metadata_element = f\"\"\"  <metadata>\n    <![CDATA[\n{metadata_json}\n    ]]>\n  </metadata>\n\"\"\"\n                # Insert metadata after opening svg tag using regex\n                import re\n                svg_content = re.sub(r'(<svg[^>]*>)', r'\\1\\n' + metadata_element, svg_content)\n\n            # Write the modified SVG to file\n            with open(os.path.join(full_output_folder, file), 'wb') as svg_file:\n                svg_file.write(svg_content.encode('utf-8'))\n\n            results.append({\n                \"filename\": file,\n                \"subfolder\": subfolder,\n                \"type\": self.type\n            })\n            counter += 1\n        return { \"ui\": { \"images\": results } }\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image",
  "markdown": "# Luma Text to Image - ComfyUI Native Node Documentation\n\n![ComfyUI Native Luma Text to Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg) The Luma Text to Image node allows you to create highly realistic and artistic images from text descriptions using Luma AIâ€™s advanced image generation capabilities.\n\n## Node Function\n\nThis node connects to Luma AIâ€™s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail representation, particularly excelling at producing photorealistic content and artistic style images.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | String | \"\"  | Text prompt describing the content to generate |\n| model | Select | \\-  | Choose which generation model to use |\n| aspect\\_ratio | Select | 16:9 | Set the output imageâ€™s aspect ratio |\n| seed | Integer | 0   | Seed value to determine if the node should re-run, but actual results are independent of the seed |\n| style\\_image\\_weight | Float | 1.0 | Style image weight, range 0.0-1.0, only applies when style\\_image is provided, higher means stronger style reference |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| image\\_luma\\_ref | LUMA\\_REF | Luma reference node connection to influence generation with input images; up to 4 images |\n| style\\_image | Image | Style reference image; only 1 image will be used |\n| character\\_image | Image | Character reference images; can be a batch of multiple, up to 4 images |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | Image | Generated image result |\n\n## Usage Example\n\n[\n\n## Luma Text to Image Usage Example\n\nDetailed guide for Luma Text to Image workflow\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)\n\n## How It Works\n\nThe Luma Text to Image node analyzes the text prompt provided by the user and creates corresponding images through Luma AIâ€™s generation models. This process uses deep learning technology to understand text descriptions and convert them into visual representations. Users can fine-tune the generation process by adjusting various parameters, including resolution, guidance scale, and negative prompts. Additionally, the node supports using reference images and concept guidance to further influence the generation results, allowing creators to more precisely achieve their creative vision.\n\n## Source Code\n\n\\[Node Source Code (Updated on 2025-05-03)\\]\n\n```\n\nclass LumaImageGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously based on prompt and aspect ratio.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaImageModel],),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in LumaAspectRatio],\n                    {\n                        \"default\": LumaAspectRatio.ratio_16_9,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n                \"style_image_weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of style image. Ignored if no style_image provided.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image_luma_ref\": (\n                    LumaIO.LUMA_REF,\n                    {\n                        \"tooltip\": \"Luma Reference node connection to influence generation with input images; up to 4 images can be considered.\"\n                    },\n                ),\n                \"style_image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"Style reference image; only 1 image will be used.\"},\n                ),\n                \"character_image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Character reference images; can be a batch of multiple, up to 4 images can be considered.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        aspect_ratio: str,\n        seed,\n        style_image_weight: float,\n        image_luma_ref: LumaReferenceChain = None,\n        style_image: torch.Tensor = None,\n        character_image: torch.Tensor = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # handle image_luma_ref\n        api_image_ref = None\n        if image_luma_ref is not None:\n            api_image_ref = self._convert_luma_refs(\n                image_luma_ref, max_refs=4, auth_token=auth_token\n            )\n        # handle style_luma_ref\n        api_style_ref = None\n        if style_image is not None:\n            api_style_ref = self._convert_style_image(\n                style_image, weight=style_image_weight, auth_token=auth_token\n            )\n        # handle character_ref images\n        character_ref = None\n        if character_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                character_image, max_images=4, auth_token=auth_token\n            )\n            character_ref = LumaCharacterRef(\n                identity0=LumaImageIdentity(images=download_urls)\n            )\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations/image\",\n                method=HttpMethod.POST,\n                request_model=LumaImageGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaImageGenerationRequest(\n                prompt=prompt,\n                model=model,\n                aspect_ratio=aspect_ratio,\n                image_ref=api_image_ref,\n                style_ref=api_style_ref,\n                character_ref=character_ref,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        img_response = requests.get(response_poll.assets.image)\n        img = process_image_response(img_response)\n        return (img,)\n\n    def _convert_luma_refs(\n        self, luma_ref: LumaReferenceChain, max_refs: int, auth_token=None\n    ):\n        luma_urls = []\n        ref_count = 0\n        for ref in luma_ref.refs:\n            download_urls = upload_images_to_comfyapi(\n                ref.image, max_images=1, auth_token=auth_token\n            )\n            luma_urls.append(download_urls[0])\n            ref_count += 1\n            if ref_count >= max_refs:\n                break\n        return luma_ref.create_api_model(download_urls=luma_urls, max_refs=max_refs)\n\n    def _convert_style_image(\n        self, style_image: torch.Tensor, weight: float, auth_token=None\n    ):\n        chain = LumaReferenceChain(\n            first_ref=LumaReference(image=style_image, weight=weight)\n        )\n        return self._convert_luma_refs(chain, max_refs=1, auth_token=auth_token)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1",
  "markdown": "# OpenAI GPT Image 1 - ComfyUI Native Node Documentation\n\n![ComfyUI Native OpenAI GPT Image 1 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg) This node connects to OpenAIâ€™s GPT Image 1 API, allowing users to generate images through detailed text prompts. Unlike traditional DALLÂ·E models, GPT Image 1 leverages GPT-4â€™s language understanding capabilities to handle more complex prompts and generate images that better match user intent.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing what to generate |\n| quality | selection | â€lowâ€ | Image quality level, options: â€œlowâ€, â€œmediumâ€, â€œhighâ€ |\n| size | selection | â€autoâ€ | Output image size, options: â€œautoâ€, â€œ1024x1024â€, â€œ1024x1536â€, â€œ1536x1024â€ |\n\n### Image Editing Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| image | image | Input image for editing, supports multiple images |\n| mask | mask | Optional mask to specify areas to modify (single image only) |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| background | selection | Background options: â€œopaqueâ€ or â€œtransparentâ€ |\n| seed | integer | Random seed (not yet implemented in backend) |\n| n   | integer | Number of images to generate, range 1-8 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image result |\n\n## How It Works\n\nThe OpenAI GPT Image 1 node combines GPT-4â€™s language understanding with image generation. It analyzes the text prompt to understand its meaning and intent, then generates matching images. In image editing mode, the node can modify existing images. Using a mask allows precise control over which areas to change. Note that mask input only works with single image input. Users can control the output by adjusting parameters like quality level, size, background handling, and number of generations.\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass OpenAIGPTImage1(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's GPT Image 1 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are shortâ€‘lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for GPT Image 1\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"quality\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"low\", \"medium\", \"high\"],\n                        \"default\": \"low\",\n                        \"tooltip\": \"Image quality, affects cost and generation time.\",\n                    },\n                ),\n                \"background\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"opaque\", \"transparent\"],\n                        \"default\": \"opaque\",\n                        \"tooltip\": \"Return image with or without background\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"auto\", \"1024x1024\", \"1024x1536\", \"1536x1024\"],\n                        \"default\": \"auto\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"How many images to generate\",\n                    },\n                ),\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        quality=\"low\",\n        background=\"opaque\",\n        image=None,\n        mask=None,\n        n=1,\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"gpt-image-1\"\n        path = \"/proxy/openai/images/generations\"\n        content_type=\"application/json\"\n        request_class = OpenAIImageGenerationRequest\n        img_binaries = []\n        mask_binary = None\n        files = []\n\n        if image is not None:\n            path = \"/proxy/openai/images/edits\"\n            request_class = OpenAIImageEditRequest\n            content_type =\"multipart/form-data\"\n\n            batch_size = image.shape[0]\n\n            for i in range(batch_size):\n                single_image = image[i : i + 1]\n                scaled_image = downscale_image_tensor(single_image).squeeze()\n\n                image_np = (scaled_image.numpy() * 255).astype(np.uint8)\n                img = Image.fromarray(image_np)\n                img_byte_arr = io.BytesIO()\n                img.save(img_byte_arr, format=\"PNG\")\n                img_byte_arr.seek(0)\n                img_binary = img_byte_arr\n                img_binary.name = f\"image_{i}.png\"\n\n                img_binaries.append(img_binary)\n                if batch_size == 1:\n                    files.append((\"image\", img_binary))\n                else:\n                    files.append((\"image[]\", img_binary))\n\n        if mask is not None:\n            if image.shape[0] != 1:\n                raise Exception(\"Cannot use a mask with multiple image\")\n            if image is None:\n                raise Exception(\"Cannot use a mask without an input image\")\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n            batch, height, width = mask.shape\n            rgba_mask = torch.zeros(height, width, 4, device=\"cpu\")\n            rgba_mask[:, :, 3] = 1 - mask.squeeze().cpu()\n\n            scaled_mask = downscale_image_tensor(rgba_mask.unsqueeze(0)).squeeze()\n\n            mask_np = (scaled_mask.numpy() * 255).astype(np.uint8)\n            mask_img = Image.fromarray(mask_np)\n            mask_img_byte_arr = io.BytesIO()\n            mask_img.save(mask_img_byte_arr, format=\"PNG\")\n            mask_img_byte_arr.seek(0)\n            mask_binary = mask_img_byte_arr\n            mask_binary.name = \"mask.png\"\n            files.append((\"mask\", mask_binary))\n\n        # Build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=path,\n                method=HttpMethod.POST,\n                request_model=request_class,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=request_class(\n                model=model,\n                prompt=prompt,\n                quality=quality,\n                background=background,\n                n=n,\n                seed=seed,\n                size=size,\n            ),\n            files=files if files else None,\n            content_type=content_type,\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3",
  "markdown": "# OpenAI DALLÂ·E 3 - ComfyUI Native Node Documentation\n\n![ComfyUI Native OpenAI DALLÂ·E 3 Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg) This node connects to OpenAIâ€™s DALLÂ·E 3 API, allowing users to generate high-quality images through detailed text prompts. DALLÂ·E 3 is OpenAIâ€™s image generation model that offers significantly improved image quality, more accurate prompt understanding, and better detail rendering compared to previous versions.\n\n```\n\nclass OpenAIDalle3(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's DALLÂ·E 3 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are shortâ€‘lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for DALLÂ·E\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"quality\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"standard\", \"hd\"],\n                        \"default\": \"standard\",\n                        \"tooltip\": \"Image quality\",\n                    },\n                ),\n                \"style\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"natural\", \"vivid\"],\n                        \"default\": \"natural\",\n                        \"tooltip\": \"Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"1024x1024\", \"1024x1792\", \"1792x1024\"],\n                        \"default\": \"1024x1024\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        style=\"natural\",\n        quality=\"standard\",\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"dall-e-3\"\n\n        # build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/openai/images/generations\",\n                method=HttpMethod.POST,\n                request_model=OpenAIImageGenerationRequest,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=OpenAIImageGenerationRequest(\n                model=model,\n                prompt=prompt,\n                quality=quality,\n                size=size,\n                style=style,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale",
  "markdown": "# Recraft Creative Upscale - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Creative Upscale Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg) The Recraft Creative Upscale node uses Recraftâ€™s API to increase image resolution while creatively enhancing and enriching image details.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image to be creatively upscaled |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | High-resolution image after creative upscaling |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftCreativeUpscaleNode(RecraftCrispUpscaleNode):\n    \"\"\"\n    Upscale image synchronously.\n    Enhances a given raster image using â€˜creative upscaleâ€™ tool, boosting resolution with a focus on refining small details and faces.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_PATH = \"/proxy/recraft/images/creativeUpscale\"\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector",
  "markdown": "# Recraft Text to Vector - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Text to Vector Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg) The Recraft Text to Vector node lets you create high-quality vector graphics (SVG format) from text descriptions using Recraftâ€™s API. Itâ€™s perfect for making logos, icons, and infinitely scalable illustrations.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text description of the vector graphic to generate |\n| substyle | select | \\-  | Vector style subtype |\n| size | select | 1024x1024 | Canvas size for the vector output |\n| n   | integer | 1   | Number of results to generate (1-6) |\n| seed | integer | 0   | Random seed value |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| negative\\_prompt | string | Elements to exclude from generation |\n| recraft\\_controls | Recraft Controls | Additional control parameters (colors, etc.) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| SVG | vector | Generated SVG vector graphic, connect to SaveSVG node to save |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass RecraftTextToVectorNode:\n    \"\"\"\n    Generates SVG synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.SVG,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"substyle\": (get_v3_substyles(RecraftStyleV3.vector_illustration),),\n                \"size\": (\n                    [res.value for res in RecraftImageSize],\n                    {\n                        \"default\": RecraftImageSize.res_1024x1024,\n                        \"tooltip\": \"The size of the generated image.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        substyle: str,\n        size: str,\n        n: int,\n        seed,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # create RecraftStyle so strings will be formatted properly (i.e. \"None\" will become None)\n        recraft_style = RecraftStyle(RecraftStyleV3.vector_illustration, substyle=substyle)\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/recraft/image_generation\",\n                method=HttpMethod.POST,\n                request_model=RecraftImageGenerationRequest,\n                response_model=RecraftImageGenerationResponse,\n            ),\n            request=RecraftImageGenerationRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                model=RecraftModel.recraftv3,\n                size=size,\n                n=n,\n                style=recraft_style.style,\n                substyle=recraft_style.substyle,\n                controls=controls_api,\n            ),\n            auth_token=auth_token,\n        )\n        response: RecraftImageGenerationResponse = operation.execute()\n        svg_data = []\n        for data in response.data:\n            svg_data.append(download_url_to_bytesio(data.url, timeout=1024))\n\n        return (SVG(svg_data),)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image",
  "markdown": "# Recraft Style - Realistic Image - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Style - Realistic Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg) The Recraft Style - Realistic Image node helps set up realistic photo styles for Recraft image generation, with various substyle options to control the visual characteristics of generated images.\n\n## Node Function\n\nThis node creates a style configuration object that guides Recraftâ€™s image generation process towards realistic photo effects.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| substyle | select | None | Specific substyle of realistic photo (Required) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style config object to connect to Recraft generation nodes |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass RecraftStyleV3RealisticImageNode:\n    \"\"\"\n    Select realistic_image style and optional substyle.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.STYLEV3,)\n    RETURN_NAMES = (\"recraft_style\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_style\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_STYLE = RecraftStyleV3.realistic_image\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"substyle\": (get_v3_substyles(s.RECRAFT_STYLE),),\n            }\n        }\n\n    def create_style(self, substyle: str):\n        if substyle == \"None\":\n            substyle = None\n        return (RecraftStyle(self.RECRAFT_STYLE, substyle),)\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image",
  "markdown": "# Recraft Image to Image - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Image to Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg) The Recraft Image to Image node uses Recraftâ€™s API to generate new images based on a reference image and text prompts.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Reference image input |\n| prompt | string | \"\"  | Text description for the generated image |\n| n   | integer | 1   | Number of images to generate (1-6) |\n| seed | integer | 0   | Random seed value |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style settings for generated images |\n| negative\\_prompt | string | Elements to avoid in generated images |\n| recraft\\_controls | Recraft Controls | Additional controls like colors |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image result |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass RecraftImageToImageNode:\n    \"\"\"\n    Modify image based on prompt and strength.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"strength\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity.\"\n                    }\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt: str,\n        n: int,\n        strength: float,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            strength=round(strength, 2),\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n            controls=controls_api,\n            random_seed=seed,\n        )\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/imageToImage\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale",
  "markdown": "# Recraft Crisp Upscale - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Crisp Upscale Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg) The Recraft Crisp Upscale node uses Recraftâ€™s API to improve image resolution and clarity.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image to be upscaled |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Upscaled and enhanced output image |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftCrispUpscaleNode:\n    \"\"\"\n    Upscale image synchronously.\n    Enhances a given raster image using â€˜crisp upscaleâ€™ tool, increasing image resolution, making the image sharper and cleaner.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_PATH = \"/proxy/recraft/images/crispUpscale\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=self.RECRAFT_PATH,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image",
  "markdown": "# Recraft Text to Image - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Recraft Text to Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg) The Recraft Text to Image node lets you generate high-quality images from text prompts by directly connecting to Recraft AIâ€™s image generation API to create images in various styles.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text description for the image |\n| size | select | 1024x1024 | Output image size |\n| n   | int | 1   | Number of images (1-6) |\n| seed | int | 0   | Random seed value |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Image style setting, default is â€œrealistic photoâ€ |\n| negative\\_prompt | string | Elements to exclude from generation |\n| recraft\\_controls | Recraft Controls | Additional control parameters (colors, etc.) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Generated image(s) |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftTextToImageNode:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"size\": (\n                    [res.value for res in RecraftImageSize],\n                    {\n                        \"default\": RecraftImageSize.res_1024x1024,\n                        \"tooltip\": \"The size of the generated image.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        size: str,\n        n: int,\n        seed,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/recraft/image_generation\",\n                method=HttpMethod.POST,\n                request_model=RecraftImageGenerationRequest,\n                response_model=RecraftImageGenerationResponse,\n            ),\n            request=RecraftImageGenerationRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                model=RecraftModel.recraftv3,\n                size=size,\n                n=n,\n                style=recraft_style.style,\n                substyle=recraft_style.substyle,\n                style_id=recraft_style.style_id,\n                controls=controls_api,\n            ),\n            auth_token=auth_token,\n        )\n        response: RecraftImageGenerationResponse = operation.execute()\n        images = []\n        for data in response.data:\n            image = bytesio_to_image_tensor(\n                download_url_to_bytesio(data.url, timeout=1024)\n            )\n            if len(image.shape) < 4:\n                image = image.unsqueeze(0)\n            images.append(image)\n        output_image = torch.cat(images, dim=0)\n\n        return (output_image,)\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting",
  "markdown": "# Recraft Image Inpainting - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Image Inpainting Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg) The Recraft Image Inpainting node lets you modify specific areas of an image while keeping the rest unchanged. By providing an image, mask and text prompt, you can generate new content to fill the selected areas.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image to modify |\n| mask | mask | \\-  | Black and white mask defining areas to change |\n| prompt | string | \"\"  | Text describing what to generate in masked area |\n| n   | integer | 1   | Number of results to generate (1-6) |\n| seed | integer | 0   | Random seed value |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style settings for generated content |\n| negative\\_prompt | string | Elements to avoid in generated content |\n| recraft\\_controls | Recraft Controls | Additional controls like colors |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Modified image result |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftImageInpaintingNode:\n    \"\"\"\n    Modify image based on prompt and mask.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"mask\": (IO.MASK, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        mask: torch.Tensor,\n        prompt: str,\n        n: int,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n            random_seed=seed,\n        )\n\n        # prepare mask tensor\n        _, H, W, _ = image.shape\n        mask = mask.unsqueeze(-1)\n        mask = mask.movedim(-1,1)\n        mask = common_upscale(mask, width=W, height=H, upscale_method=\"nearest-exact\", crop=\"disabled\")\n        mask = mask.movedim(1,-1)\n        mask = (mask > 0.5).float()\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                mask=mask[i:i+1],\n                path=\"/proxy/recraft/images/inpaint\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n```"
},
{
  "url": "https://docs.comfy.org/specs/nodedef_json",
  "markdown": "# Node Definition JSON - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyNodeDefV2\",\n  \"definitions\": {\n    \"ComfyNodeDefV2\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"inputs\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"anyOf\": [\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": \"number\"\n                        }\n                      }\n                    ]\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"min\": {\n                    \"type\": \"number\"\n                  },\n                  \"max\": {\n                    \"type\": \"number\"\n                  },\n                  \"step\": {\n                    \"type\": \"number\"\n                  },\n                  \"display\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"slider\",\n                      \"number\",\n                      \"knob\"\n                    ]\n                  },\n                  \"control_after_generate\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"INT\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": \"number\"\n                        }\n                      }\n                    ]\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"min\": {\n                    \"type\": \"number\"\n                  },\n                  \"max\": {\n                    \"type\": \"number\"\n                  },\n                  \"step\": {\n                    \"type\": \"number\"\n                  },\n                  \"display\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"slider\",\n                      \"number\",\n                      \"knob\"\n                    ]\n                  },\n                  \"round\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"boolean\",\n                        \"const\": false\n                      }\n                    ]\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"FLOAT\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"label_on\": {\n                    \"type\": \"string\"\n                  },\n                  \"label_off\": {\n                    \"type\": \"string\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"BOOLEAN\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"type\": \"string\"\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"multiline\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"dynamicPrompts\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"defaultVal\": {\n                    \"type\": \"string\"\n                  },\n                  \"placeholder\": {\n                    \"type\": \"string\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"STRING\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {},\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"control_after_generate\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"image_upload\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"image_folder\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"input\",\n                      \"output\",\n                      \"temp\"\n                    ]\n                  },\n                  \"allow_batch\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"video_upload\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"remote\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"route\": {\n                        \"anyOf\": [\n                          {\n                            \"type\": \"string\",\n                            \"format\": \"uri\"\n                          },\n                          {\n                            \"type\": \"string\",\n                            \"pattern\": \"^\\\\/\"\n                          }\n                        ]\n                      },\n                      \"refresh\": {\n                        \"anyOf\": [\n                          {\n                            \"type\": \"number\",\n                            \"minimum\": -9007199254740991,\n                            \"maximum\": 9007199254740991\n                          },\n                          {\n                            \"type\": \"number\",\n                            \"maximum\": 9007199254740991,\n                            \"minimum\": -9007199254740991\n                          }\n                        ]\n                      },\n                      \"response_key\": {\n                        \"type\": \"string\"\n                      },\n                      \"query_params\": {\n                        \"type\": \"object\",\n                        \"additionalProperties\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"refresh_button\": {\n                        \"type\": \"boolean\"\n                      },\n                      \"control_after_refresh\": {\n                        \"type\": \"string\",\n                        \"enum\": [\n                          \"first\",\n                          \"last\"\n                        ]\n                      },\n                      \"timeout\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0\n                      },\n                      \"max_retries\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0\n                      }\n                    },\n                    \"required\": [\n                      \"route\"\n                    ],\n                    \"additionalProperties\": false\n                  },\n                  \"options\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": [\n                        \"string\",\n                        \"number\"\n                      ]\n                    }\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"COMBO\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {},\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              }\n            ]\n          }\n        },\n        \"outputs\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"index\": {\n                \"type\": \"number\"\n              },\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"is_list\": {\n                \"type\": \"boolean\"\n              },\n              \"options\": {\n                \"type\": \"array\"\n              },\n              \"tooltip\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"index\",\n              \"name\",\n              \"type\",\n              \"is_list\"\n            ],\n            \"additionalProperties\": false\n          }\n        },\n        \"hidden\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {}\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"display_name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"category\": {\n          \"type\": \"string\"\n        },\n        \"output_node\": {\n          \"type\": \"boolean\"\n        },\n        \"python_module\": {\n          \"type\": \"string\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\"\n        },\n        \"experimental\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"inputs\",\n        \"outputs\",\n        \"name\",\n        \"display_name\",\n        \"description\",\n        \"category\",\n        \"output_node\",\n        \"python_module\"\n      ],\n      \"additionalProperties\": false\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image",
  "markdown": "# Recraft Vectorize Image - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Recraft Vectorize Image Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg) The Recraft Vectorize Image node uses Recraftâ€™s API to convert raster images (like photos, PNGs or JPEGs) into vector SVG format.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | Image | \\-  | Input image to be converted to vector |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| SVG | Vector | Converted SVG vector graphic, needs to be connected to SaveSVG node to save |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass RecraftVectorizeImageNode:\n    \"\"\"\n    Generates SVG synchronously from an input image.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.SVG,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        svgs = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/vectorize\",\n                auth_token=auth_token,\n            )\n            svgs.append(SVG(sub_bytes))\n            pbar.update(1)\n\n        return (SVG.combine_all(svgs), )\n\n```"
},
{
  "url": "https://docs.comfy.org/specs/nodedef_json_1_0",
  "markdown": "# Node Definition JSON 1.0 - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyNodeDefV1\",\n  \"definitions\": {\n    \"ComfyNodeDefV1\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"input\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"required\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"INT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"FLOAT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"round\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"boolean\",\n                                    \"const\": false\n                                  }\n                                ]\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"BOOLEAN\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"label_on\": {\n                                \"type\": \"string\"\n                              },\n                              \"label_off\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"STRING\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"string\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"multiline\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"dynamicPrompts\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultVal\": {\n                                \"type\": \"string\"\n                              },\n                              \"placeholder\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": [\n                            \"string\",\n                            \"number\"\n                          ]\n                        }\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"COMBO\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            \"optional\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"INT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"FLOAT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"round\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"boolean\",\n                                    \"const\": false\n                                  }\n                                ]\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"BOOLEAN\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"label_on\": {\n                                \"type\": \"string\"\n                              },\n                              \"label_off\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"STRING\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"string\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"multiline\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"dynamicPrompts\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultVal\": {\n                                \"type\": \"string\"\n                              },\n                              \"placeholder\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": [\n                            \"string\",\n                            \"number\"\n                          ]\n                        }\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"COMBO\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            \"hidden\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {}\n            }\n          },\n          \"additionalProperties\": false\n        },\n        \"output\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"anyOf\": [\n              {\n                \"type\": \"string\"\n              },\n              {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": [\n                    \"string\",\n                    \"number\"\n                  ]\n                }\n              }\n            ]\n          }\n        },\n        \"output_is_list\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"output_name\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"output_tooltips\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"display_name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"category\": {\n          \"type\": \"string\"\n        },\n        \"output_node\": {\n          \"type\": \"boolean\"\n        },\n        \"python_module\": {\n          \"type\": \"string\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\"\n        },\n        \"experimental\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"display_name\",\n        \"description\",\n        \"category\",\n        \"output_node\",\n        \"python_module\"\n      ],\n      \"additionalProperties\": false\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/specs/workflow_json_0.4",
  "markdown": "# Workflow JSON 0.4 - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyWorkflow0_4\",\n  \"definitions\": {\n    \"ComfyWorkflow0_4\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"last_node_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"last_link_id\": {\n          \"type\": \"number\"\n        },\n        \"nodes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"size\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"flags\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"collapsed\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"pinned\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"allow_interaction\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"horizontal\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"skip_repeated_outputs\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"order\": {\n                \"type\": \"number\"\n              },\n              \"mode\": {\n                \"type\": \"number\"\n              },\n              \"inputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"link\": {\n                      \"type\": [\n                        \"number\",\n                        \"null\"\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"outputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"links\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        {\n                          \"type\": \"null\"\n                        }\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"properties\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"Node name for S&R\": {\n                    \"type\": \"string\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"widgets_values\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\"\n                  },\n                  {\n                    \"type\": \"object\",\n                    \"additionalProperties\": {}\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"bgcolor\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"type\",\n              \"pos\",\n              \"size\",\n              \"flags\",\n              \"order\",\n              \"mode\",\n              \"properties\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"links\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"array\",\n            \"minItems\": 6,\n            \"maxItems\": 6,\n            \"items\": [\n              {\n                \"type\": \"number\"\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"string\"\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    }\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              }\n            ]\n          }\n        },\n        \"groups\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"title\": {\n                \"type\": \"string\"\n              },\n              \"bounding\": {\n                \"type\": \"array\",\n                \"minItems\": 4,\n                \"maxItems\": 4,\n                \"items\": [\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"font_size\": {\n                \"type\": \"number\"\n              },\n              \"locked\": {\n                \"type\": \"boolean\"\n              }\n            },\n            \"required\": [\n              \"title\",\n              \"bounding\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"config\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"links_ontop\": {\n                      \"type\": \"boolean\"\n                    },\n                    \"align_to_grid\": {\n                      \"type\": \"boolean\"\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"extra\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"ds\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"scale\": {\n                          \"type\": \"number\"\n                        },\n                        \"offset\": {\n                          \"anyOf\": [\n                            {\n                              \"type\": \"object\",\n                              \"properties\": {\n                                \"0\": {\n                                  \"type\": \"number\"\n                                },\n                                \"1\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              \"required\": [\n                                \"0\",\n                                \"1\"\n                              ],\n                              \"additionalProperties\": true\n                            },\n                            {\n                              \"type\": \"array\",\n                              \"minItems\": 2,\n                              \"maxItems\": 2,\n                              \"items\": [\n                                {\n                                  \"type\": \"number\"\n                                },\n                                {\n                                  \"type\": \"number\"\n                                }\n                              ]\n                            }\n                          ]\n                        }\n                      },\n                      \"required\": [\n                        \"scale\",\n                        \"offset\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"info\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"name\": {\n                          \"type\": \"string\"\n                        },\n                        \"author\": {\n                          \"type\": \"string\"\n                        },\n                        \"description\": {\n                          \"type\": \"string\"\n                        },\n                        \"version\": {\n                          \"type\": \"string\"\n                        },\n                        \"created\": {\n                          \"type\": \"string\"\n                        },\n                        \"modified\": {\n                          \"type\": \"string\"\n                        },\n                        \"software\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"required\": [\n                        \"name\",\n                        \"author\",\n                        \"description\",\n                        \"version\",\n                        \"created\",\n                        \"modified\",\n                        \"software\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"linkExtensions\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"parentId\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    },\n                    \"reroutes\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          },\n                          \"pos\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"0\": {\n                                    \"type\": \"number\"\n                                  },\n                                  \"1\": {\n                                    \"type\": \"number\"\n                                  }\n                                },\n                                \"required\": [\n                                  \"0\",\n                                  \"1\"\n                                ],\n                                \"additionalProperties\": true\n                              },\n                              {\n                                \"type\": \"array\",\n                                \"minItems\": 2,\n                                \"maxItems\": 2,\n                                \"items\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"number\"\n                                  }\n                                ]\n                              }\n                            ]\n                          },\n                          \"linkIds\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              {\n                                \"type\": \"null\"\n                              }\n                            ]\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"pos\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"version\": {\n          \"type\": \"number\"\n        },\n        \"models\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"url\": {\n                \"type\": \"string\",\n                \"format\": \"uri\"\n              },\n              \"hash\": {\n                \"type\": \"string\"\n              },\n              \"hash_type\": {\n                \"type\": \"string\"\n              },\n              \"directory\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"name\",\n              \"url\",\n              \"directory\"\n            ],\n            \"additionalProperties\": false\n          }\n        }\n      },\n      \"required\": [\n        \"last_node_id\",\n        \"last_link_id\",\n        \"nodes\",\n        \"links\",\n        \"version\"\n      ],\n      \"additionalProperties\": true\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration",
  "markdown": "# Recraft Style - Digital Illustration - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Style Digital Illustration Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg) This node creates a style configuration object that guides Recraftâ€™s image generation process towards a digital illustration look.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| substyle | select | None | Specific substyle of digital illustration |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style config object to connect to Recraft generation nodes |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftStyleV3DigitalIllustrationNode(RecraftStyleV3RealisticImageNode):\n    \"\"\"\n    Select digital_illustration style and optional substyle.\n    \"\"\"\n\n    RECRAFT_STYLE = RecraftStyleV3.digital_illustration\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster",
  "markdown": "# Recraft Style - Logo Raster - ComfyUI Built-in Node Documentation\n\n![ComfyUI Built-in Recraft Style - Logo Raster Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg) This node creates a style configuration object that guides Recraftâ€™s image generation process toward professional logo design effects. By selecting different substyles, you can define the design style, complexity and use cases of the generated logo.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| substyle | Selection | \\-  | Specific substyle for logo raster (Required) |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | Style configuration object, connects to Recraft generation node |\n\n## Usage Example\n\n[\n\n## Recraft Text to Image Workflow Example\n\nRecraft Text to Image Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\nclass RecraftStyleV3LogoRasterNode(RecraftStyleV3RealisticImageNode):\n    \"\"\"\n    Select vector_illustration style and optional substyle.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"substyle\": (get_v3_substyles(s.RECRAFT_STYLE, include_none=False),),\n            }\n        }\n\n    RECRAFT_STYLE = RecraftStyleV3.logo_raster\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background",
  "markdown": "# Recraft Remove Background - ComfyUI Native Node Documentation\n\n![ComfyUI Native Recraft Remove Background Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg) The Recraft Remove Background node uses Recraftâ€™s API to intelligently detect and remove image backgrounds, creating images with transparent backgrounds and corresponding alpha masks.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image to remove background from |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| IMAGE | image | Image with background removed (with alpha channel) |\n| MASK | mask | Mask of the main subject (white areas are preserved) |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\nclass RecraftRemoveBackgroundNode:\n    \"\"\"\n    Remove background from image, and return processed image and mask.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE, IO.MASK)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/removeBackground\",\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        # use alpha channel as masks, in B,H,W format\n        masks_tensor = images_tensor[:,:,:,-1:].squeeze(-1)\n        return (images_tensor, masks_tensor)\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/3d/hunyuan3D-2",
  "markdown": "# ComfyUI Hunyuan3D-2 Examples - ComfyUI\n\n## Hunyuan3D 2.0 Introduction\n\n  ![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif) ![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif) [Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model released by Tencent, capable of generating high-fidelity 3D models with high-resolution texture maps through text or images. Hunyuan3D 2.0 adopts a two-stage generation approach, first generating a geometry model without textures, then synthesizing high-resolution texture maps. This effectively separates the complexity of shape and texture generation. Below are the two core components of Hunyuan3D 2.0:\n\n1.  **Geometry Generation Model (Hunyuan3D-DiT)**: Based on a flow diffusion Transformer architecture, it generates untextured geometric models that precisely match input conditions.\n2.  **Texture Generation Model (Hunyuan3D-Paint)**: Combines geometric conditions and multi-view diffusion techniques to add high-resolution textures to models, supporting PBR materials.\n\n**Key Advantages**\n\n*   **High-Precision Generation**: Sharp geometric structures, rich texture colors, support for PBR material generation, achieving near-realistic lighting effects.\n*   **Diverse Usage Methods**: Provides code calls, Blender plugins, Gradio applications, and online experience through the official website, suitable for different user needs.\n*   **Lightweight and Compatibility**: The Hunyuan3D-2mini model requires only 5GB VRAM, the standard version needs 6GB VRAM for shape generation, and the complete process (shape + texture) requires only 12GB VRAM.\n\nRecently (March 18, 2025), Hunyuan3D 2.0 also introduced a multi-view shape generation model (Hunyuan3D-2mv), which supports generating more detailed geometric structures from inputs at different angles. This example includes three workflows:\n\n*   Using Hunyuan3D-2mv with multiple view inputs to generate 3D models\n*   Using Hunyuan3D-2mv-turbo with multiple view inputs to generate 3D models\n*   Using Hunyuan3D-2 with a single view input to generate 3D models\n\n## ComfyUI Hunyuan3D-2mv Workflow Example\n\nIn the Hunyuan3D-2mv workflow, weâ€™ll use multi-view images to generate a 3D model. Note that multiple view images are not mandatory in this workflow - you can use only the `front` view image to generate a 3D model.\n\n### 1\\. Workflow\n\nPlease download the images below and drag into ComfyUI to load the workflow. ![Hunyuan3D-2mv workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/hunyuan-3d-multiview-elf.webp) Download the images below we will use them as input images.\n\n![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/front.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/left.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/back.png)\n\n### 2\\. Manual Model Installation\n\nDownload the model below and save it to the corresponding ComfyUI folder\n\n*   hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv.safetensors`\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ hunyuan3d-dit-v2-mv.safetensors  // renamed file\n```\n\n### 3\\. Steps to Run the Workflow\n\n![ComfyUI hunyuan3d_2mv](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg)\n\n1.  Ensure that the Image Only Checkpoint Loader(img2vid model) has loaded our downloaded and renamed `hunyuan3d-dit-v2-mv.safetensors` model\n2.  Load the corresponding view images in each of the `Load Image` nodes\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\nIf you need to add more views, make sure to load other view images in the `Hunyuan3Dv2ConditioningMultiView` node, and ensure that you load the corresponding view images in the `Load Image` nodes.\n\n## Hunyuan3D-2mv-turbo Workflow\n\nIn the Hunyuan3D-2mv-turbo workflow, weâ€™ll use the Hunyuan3D-2mv-turbo model to generate 3D models. This model is a step distillation version of Hunyuan3D-2mv, allowing for faster 3D model generation. In this version of the workflow, we set `cfg` to 1.0 and add a `flux guidance` node to control the `distilled cfg` generation.\n\n### 1\\. Workflow\n\nPlease download the images below and drag into ComfyUI to load the workflow. ![Hunyuan3D-2mv-turbo workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/hunyuan-3d-turbo.webp) Download the images below we will use them as input images.\n\n![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/front.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/right.png)\n\n### 2\\. Manual Model Installation\n\nDownload the model below and save it to the corresponding ComfyUI folder\n\n*   hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv-turbo.safetensors`\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ hunyuan3d-dit-v2-mv-turbo.safetensors  // renamed file\n```\n\n### 3\\. Steps to Run the Workflow\n\n![ComfyUI hunyuan3d_2mv_turbo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg)\n\n1.  Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2-mv-turbo.safetensors` model\n2.  Load the corresponding view images in each of the `Load Image` nodes\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Hunyuan3D-2 Single View Workflow\n\nIn the Hunyuan3D-2 workflow, weâ€™ll use the Hunyuan3D-2 model to generate 3D models. This model is not a multi-view model. In this workflow, we use the `Hunyuan3Dv2Conditioning` node instead of the `Hunyuan3Dv2ConditioningMultiView` node.\n\n### 1\\. Workflow\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Hunyuan3D-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d-non-multiview-train.webp) Download the image below we will use it as input image. ![ComfyUI Hunyuan 3D 2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan_3d_v2_non_multiview_train.png) \n\n### 2\\. Manual Model Installation\n\nDownload the model below and save it to the corresponding ComfyUI folder\n\n*   hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2.safetensors`\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ hunyuan3d-dit-v2.safetensors  // renamed file\n```\n\n### 3\\. Steps to Run the Workflow\n\n![ComfyUI hunyuan3d_2](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg)\n\n1.  Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2.safetensors` model\n2.  Load the image in the `Load Image` node\n3.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\nBelow are ComfyUI community resources related to Hunyuan3D-2\n\n*   [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)\n*   [Kijai/Hunyuan3D-2\\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)\n*   [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)\n\n## Hunyuan3D 2.0 Open-Source Model Series\n\nCurrently, Hunyuan3D 2.0 has open-sourced multiple models covering the complete 3D generation process. You can visit [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) for more information. **Hunyuan3D-2mini Series**\n\n| Model | Description | Date | Parameters | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-mini | Mini Image to Shape Model | 2025-03-18 | 0.6B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini) |\n\n**Hunyuan3D-2mv Series**\n\n| Model | Description | Date | Parameters | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-mv-Fast | Guidance Distillation Version, can halve DIT inference time | 2025-03-18 | 1.1B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast) |\n| Hunyuan3D-DiT-v2-mv | Multi-view Image to Shape Model, suitable for 3D creation requiring multiple angles to understand the scene | 2025-03-18 | 1.1B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv) |\n\n**Hunyuan3D-2 Series**\n\n| Model | Description | Date | Parameters | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-0-Fast | Guidance Distillation Model | 2025-02-03 | 1.1B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast) |\n| Hunyuan3D-DiT-v2-0 | Image to Shape Model | 2025-01-21 | 1.1B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0) |\n| Hunyuan3D-Paint-v2-0 | Texture Generation Model | 2025-01-21 | 1.3B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0) |\n| Hunyuan3D-Delight-v2-0 | Image Delight Model | 2025-01-21 | 1.3B | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0) |"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image",
  "markdown": "# Recraft Text to Image API Node ComfyUI Official Example\n\nThe [Recraft Text to Image](https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image) node allows you to create high-quality images in various styles using Recraft AIâ€™s image generation technology based on text descriptions. In this guide, weâ€™ll show you how to set up a text-to-image workflow using this node.\n\n### 1\\. Download the Workflow File\n\nThe workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow. ![Recraft Text to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/recraft/t2i/recraft_t2i.png)\n\n### 2\\. Follow the Steps to Run the Workflow\n\n![Recraft Text to Image Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg) Follow these numbered steps to run the basic workflow:\n\n1.  (Optional) Change the `Recraft Color RGB` in the `Color` node to your desired color\n2.  (Optional) Modify the `Recraft Style` node to control the visual style, such as digital art, realistic photo, or logo design. This group includes other style nodes you can enable as needed\n3.  (Optional) Edit the `prompt` parameter in the `Recraft Text to Image` node. You can also change the `size` parameter\n4.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n5.  After the API returns the result, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory\n\n> (Optional) Weâ€™ve included a **Convert to SVG** group in the workflow. Since the `Recraft Vectorize Image` node in this group consumes additional credits, enable it only when you need to convert the generated image to SVG format\n\n### 3\\. Additional Notes\n\n*   **Recraft Style**: Offers various preset styles like realistic photos, digital art, and logo designs\n*   **Seed Parameter**: Only used to determine if the node should run again, the actual generation result is not affected by the seed value\n\nCheck the following documentation for detailed parameter settings of the nodes\n\n[\n\n## Recraft Text to Image Node Documentation\n\nDocumentation for the Recraft Text to Image API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)[\n\n## Recraft Style Node Documentation\n\nDocumentation for the Recraft Style - Realistic Image API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)[\n\n## Recraft Controls Node Documentation\n\nDocumentation for the Recraft Controls API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/google/gemini",
  "markdown": "# Google Gemini API Node ComfyUI Official Example\n\nGoogle Gemini is a powerful AI model developed by Google, supporting conversational and text generation functions. Currently, ComfyUI has integrated the Google Gemini API, allowing you to directly use the related nodes in ComfyUI to complete conversational functions. In this guide, we will walk you through completing the corresponding conversational functionality.\n\n## Google Gemini Chat Workflow\n\n### 1\\. Workflow File Download\n\nPlease download the Json file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/google/api_google_gemini.json)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![OpenAI Chat Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg)\n\nYou can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Load Image` node, load the image you need AI to interpret\n2.  (Optional) If needed, you can modify the prompt in `Google Gemini` to have AI execute specific tasks\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the conversation.\n4.  After waiting for the API to return results, you can view the corresponding AI returned content in the `Preview Any` node.\n\n### 3\\. Additional Notes\n\n*   Currently, the file input node `Gemini Input Files` requires files to be uploaded to the `ComfyUI/input/` directory first. This node is being improved, and we will modify the template after updates\n*   The workflow provides an example using `Batch Images` for input. If you have multiple images that need AI interpretation, you can refer to the step diagram and use right-click to set the corresponding node mode to `Always` to enable it"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/ideogram/ideogram-v3",
  "markdown": "# ComfyUI Ideogram 3.0 API Node Official Examples\n\nIdeogram 3.0 is a powerful text-to-image model by Ideogram, known for its photorealistic quality, accurate text rendering, and consistent style control. The [Ideogram V3](https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3) node currently supports two modes:\n\n*   Text-to-Image mode\n*   Image Editing mode (when both image and mask inputs are provided)\n\n## Ideogram 3.0 Node Documentation\n\nCheck the following documentation for detailed node parameter settings:\n\n*   [Ideogram V3](https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)\n\n## Ideogram 3.0 API Node Text-to-Image Mode\n\nWhen using [Ideogram V3](https://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3) without image and mask inputs, the node operates in Text-to-Image mode.\n\n### 1\\. Download Workflow File\n\nDownload and drag the following file into ComfyUI to load the workflow: ![Ideogram 3.0 ComfyUI Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/ideogram/v3/ideogram_v3_t2i.png)\n\n### 2\\. Complete the Workflow Steps\n\n![Ideogram 3.0 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg) Follow the numbered steps to complete the basic workflow:\n\n1.  Enter your image description in the `prompt` field of the `Ideogram V3` node\n2.  Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image\n3.  After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\n## Ideogram 3.0 API Node Image Editing Mode\n\n\\[To be updated\\]"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image",
  "markdown": "# Luma Image to Image API Node ComfyUI Official Example\n\nThe [Luma Image to Image](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image) node allows you to modify existing images based on text prompts using Luma AI technology, while preserving certain features and structures from the original image. In this guide, weâ€™ll show you how to set up an image-to-image workflow using this node.\n\n## Luma Image to Image Node Documentation\n\nCheck the following documentation for detailed node parameter settings:\n\n[\n\n## Luma Image to Image Node Documentation\n\nLuma Image to Image API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)\n\n### 1\\. Download Workflow File\n\nDownload and drag the following image into ComfyUI to load the workflow (workflow information is included in the image metadata): ![Luma Image to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/luma_i2i.png) Download this image to use as input: ![Luma Image to Image Workflow Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/input.png)\n\n### 2\\. Complete the Workflow Steps\n\n![Luma Image to Image Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg) Follow these numbered steps:\n\n1.  Click **Upload** on the `Load Image` node to upload your input image\n2.  (Optional) Modify the workflow prompts\n3.  (Optional) Adjust `image_weight` to change input image influence (lower values stay closer to original)\n4.  Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image\n5.  After API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\n### 3\\. Results with Different `image_weight` Values\n\n![Weight Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/i2i_image_weight.jpg)"
},
{
  "url": "https://docs.comfy.org/installation/desktop/macos",
  "markdown": "# MacOS Desktop Version - ComfyUI\n\n**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files. ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop).\n\nThis tutorial will guide you through the software installation process and explain related configuration details.\n\n## ComfyUI Desktop (MacOS) Download\n\nPlease click the button below to download the installation package for MacOS **ComfyUI Desktop**\n\n[\n\nDownload for MacOS\n\n](https://download.comfy.org/mac/dmg/arm64)\n\n## Install via Homebrew\n\nComfyUI Desktop can also be installed via [Homebrew](https://brew.sh/):\n\n## ComfyUI Desktop Installation Steps\n\nDouble-click the downloaded installation package file. As shown in the image, drag the **ComfyUI** application into the **Applications** folder following the arrow ![ComfyUI Installation Package](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0.png) If your folder shows as below with a prohibition sign on the icon after opening the installation package, it means your current system version is not compatible with **ComfyUI Desktop** ![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0-1.png) Then find the **ComfyUI icon** in **Launchpad** and click it to enter ComfyUI initialization settings ![ComfyUI Lanchpad](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-1.jpg) \n\n## ComfyUI Desktop Initialization Process\n\n## First Image Generation\n\nAfter successful installation, you can refer to the section below to start your ComfyUI journey~\n\n[\n\n## First Image Generation\n\nThis tutorial will guide you through your first model installation and text-to-image generation\n\n\n\n](https://docs.comfy.org/get_started/first_generation)\n\n## How to Update ComfyUI Desktop\n\nCurrently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings ![ComfyUI Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg) You can also choose to manually check for available updates in the `Menu` â€”> `Help` â€”> `Check for Updates` ![ComfyUI Desktop Check for Updates](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_check_for_updates.jpg)\n\nIf you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:\n\n*   You have multiple ComfyUI instances and want them to share model files to save disk space\n*   You have different types of GUI programs (such as WebUI) and want them to use the same model files\n*   Model files cannot be recognized or found\n\nWe provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file\n\n### Open Config File\n\nFor the ComfyUI version such as [portable](https://docs.comfy.org/installation/comfyui_portable_windows) and [manual](https://docs.comfy.org/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nCopy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUIâ€™s root directory at `ComfyUI/extra_model_paths.yaml`. You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)\n\nIf the file does not exist, you can create it yourself with any text editor.\n\n### Example Structure\n\nSuppose you want to add the following model paths to ComfyUI:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ lora/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\nThen you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\nor\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\nOr you can refer to the default [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect. Below is the original config example:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\nFor example, if your WebUI is located at `D:\\stable-diffusion-webui\\`, you can modify the corresponding configuration to\n\n```\na111:\n    base_path: D:\\stable-diffusion-webui\\\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n```\n\nBesides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI\n\nBelow is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## Desktop Python Environment\n\nThe desktop installation will create a Python virtual environment in your chosen installation directory, typically a hidden `.venv` folder. If you need to handle dependencies for ComfyUI plugins, youâ€™ll need to do so within this environment. Using the system command line directly risks installing dependencies to the system environment, so please follow the instructions below to activate the appropriate environment.\n\n### How to use the Desktop Python environment?\n\nYou can use the built-in terminal in the desktop app to access the Python environment. ![ComfyUI Desktop Terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_terminal.jpg) \n\n1.  Click the icon in the menu bar to open the bottom panel\n2.  Click `Terminal` to open the terminal\n3.  If you want to check the Python installation location for the corresponding environment, you can use the following command\n\n```\n  python -c \"import sys; print(sys.executable)\"\n```\n\n## How to Uninstall ComfyUI Desktop\n\nFor **ComfyUI Desktop**, you can directly delete **ComfyUI** from the **Applications** folder If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:\n\n```\n~/Library/Application Support/ComfyUI\n```\n\nThe above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:\n\n*   models files\n*   custom nodes\n*   input/output directories\n\n## Troubleshooting\n\n### â€‹Error identificationâ€‹\n\nIf installation fails, you should see the following screen ![ComfyUI Installation Failed](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg) It is recommended to take these steps to find the error cause:\n\n1.  Click `Show Terminal` to view error output\n2.  Click `Open Logs` to view installation logs\n3.  Visit official forum to search for error reports\n4.  Click `Reinstall` to try reinstalling\n\nBefore submitting feedback, itâ€™s recommended to provide the **error output** and **log files** to tools like **GPT** ![ComfyUI Installation Failed - Error Log](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg) ![ComfyUI Installation Failed - GPT Feedback](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg) As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.\n\n### Feedback Installation Failure\n\nIf you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:\n\n*   Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)\n*   Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)\n\nWhen submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:\n\n1.  Log Files\n\n| Filename | Description | Location |\n| --- | --- | --- |\n| main.log | Contains logs related to desktop application and server startup from the Electron process |     |\n| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output |     |\n\n![ComfyUI Log Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)\n\n2.  Configuration Files\n\n| Filename | Description | Location |\n| --- | --- | --- |\n| extra\\_model\\_paths.yaml | Contains additional paths where ComfyUI will search for models and custom nodes |     |\n| config.json | Contains application configuration. This file should not be edited directly |     |\n\n![ComfyUI Config Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)"
},
{
  "url": "https://docs.comfy.org/installation/desktop/linux",
  "markdown": "# Linux Desktop Version - ComfyUI\n\nWhen Linux desktop packages become available, you can configure external model paths:\n\n## Adding External Model Paths\n\nIf you have models stored in other locations on your computer outside the ComfyUI installation directory, you can add them to ComfyUI by configuring the `extra_models_config.yaml` file. For ComfyUI Desktop, this file is located at:\n\n*   On Windows: `C:\\Users\\<YOUR_USERNAME>\\AppData\\Roaming\\ComfyUI\\extra_models_config.yaml`\n*   On macOS: `~/Library/Application Support/ComfyUI/extra_models_config.yaml`\n*   On Linux: `~/.config/ComfyUI/extra_models_config.yaml`\n\nFor detailed instructions, see [Models documentation](https://docs.comfy.org/development/core-concepts/models#adding-external-model-paths)"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image",
  "markdown": "# Stability AI Stable Diffusion 3.5 API Node ComfyUI Official Example\n\nThe [Stability AI Stable Diffusion 3.5 Image](https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image) node allows you to use Stability AIâ€™s Stable Diffusion 3.5 model to create high-quality, detail-rich image content through text prompts or reference images. In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.\n\n## Stability AI Stable Diffusion 3.5 Text-to-Image Workflow\n\n### 1\\. Workflow File Download\n\nThe image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow. ![Stability AI Stable Diffusion 3.5 Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_diffusion_3-5-t2i.png)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Stability AI Stable Diffusion 3.5 Text-to-Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg) You can follow the numbered steps in the image to complete the basic text-to-image workflow:\n\n1.  (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to input your desired image description. More detailed prompts often result in better image quality.\n2.  (Optional) Select the `model` parameter to choose which SD 3.5 model version to use.\n3.  (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics, such as â€œcinematicâ€ or â€œanimeâ€. Select â€œNoneâ€ to not apply any specific style.\n4.  (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you donâ€™t want to appear in the generated image.\n5.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.\n6.  After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.\n\n### 3\\. Additional Notes\n\n*   **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions lead to better results. Can include elements like scene, subject, colors, lighting, and style.\n*   **CFG Scale**: Controls how closely the generator follows the prompt. Higher values make the image more closely match the prompt description, but too high may result in oversaturated or unnatural results.\n*   **Style Preset**: Offers various preset styles for quickly defining the overall style of the image.\n*   **Negative Prompt**: Used to specify elements you donâ€™t want to appear in the generated image.\n*   **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during creation.\n*   Currently the `Load Image` node is in â€œBypassâ€ mode. To enable it, refer to the step guide and right-click the node to set â€œModeâ€ to â€œAlwaysâ€ to enable input, switching to image-to-image mode.\n*   `image_denoise` has no effect when there is no input image.\n\n## Stability AI Stable Diffusion 3.5 Image-to-Image Workflow\n\n### 1\\. Workflow File Download\n\nThe image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow. ![Stability AI Stable Diffusion 3.5 Image-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/stable_diffusion_3_5-i2i.png) Download the image below to use as input !\\[Stability AI Stable Diffusion 3.5 Image-to-Image Workflow Input Image\\](![Stability AI Stable Diffusion 3.5 å›¾ç”Ÿå›¾å·¥ä½œæµè¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/input.jpg)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Stability AI Stable Diffusion 3.5 Image-to-Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg) You can follow the numbered steps in the image to complete the image-to-image workflow:\n\n1.  Load a reference image through the `Load Image` node, which will serve as the basis for generation.\n2.  (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to describe elements you want to change or enhance in the reference image.\n3.  (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics.\n4.  (Optional|Important) Adjust the `image_denoise` parameter (range 0.0-1.0) to control how much the original image is modified:\n    *   Values closer to 0.0 make the generated image more similar to the input reference image (at 0.0, itâ€™s basically identical to the original)\n    *   Values closer to 1.0 make the generated image more like pure text-to-image generation (at 1.0, itâ€™s as if no reference image was provided)\n5.  (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you donâ€™t want to appear in the generated image.\n6.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.\n7.  After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.\n\n### 3\\. Additional Notes\n\nThe image below shows a comparison of results with and without input image using the same parameter settings: ![Stability AI Stable Diffusion 3.5 With/Without Image Input Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg) **Image Denoise**: This parameter determines how much of the original imageâ€™s features are preserved during generation. Itâ€™s the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths: ![Stability AI Stable Diffusion 3.5 Image-to-Image Denoise Strength Explanation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg)\n\n*   **Reference Image Selection**: Choosing images with clear subjects and good composition usually yields better results.\n*   **Prompt Tips**: In image-to-image mode, prompts should focus more on elements you want to change or enhance, rather than describing everything already present in the image.\n*   **Mode Switching**: When an input image is provided, the node automatically switches from text-to-image mode to image-to-image mode, and aspect ratio parameters are ignored.\n\nYou can refer to the documentation below to understand detailed parameter settings for the corresponding node\n\n[\n\n## Stability Stable Diffusion 3.5 Image Node Documentation\n\nStability Stable Diffusion 3.5 Image API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fapi-reference%2Fregistry%2Fdelete-a-publisher",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video",
  "markdown": "# Kling Text to Video - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg) The Kling Text to Video node connects to Klingâ€™s API service to generate videos from text descriptions. Users simply provide descriptive text to create corresponding video content.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | String | \"\"  | Text prompt describing desired video content |\n| negative\\_prompt | String | \"\"  | Elements to avoid in the video |\n| cfg\\_scale | Float | 7.0 | Controls how closely to follow the prompt |\n| model\\_name | Select | â€kling-v2-masterâ€ | Video generation model to use |\n| aspect\\_ratio | Select | AspectRatio enum | Output video aspect ratio |\n| duration | Select | Duration enum | Length of generated video |\n| mode | Select | Mode enum | Video generation mode |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n| Kling ID | String | Task identifier |\n| Duration (sec) | String | Video length in seconds |\n\n## How It Works\n\nThe node sends text prompts to Klingâ€™s API server, which processes and returns the generated video. The process includes initial request and status polling. When complete, the node downloads and outputs the video. Users can control the generation by adjusting parameters like negative prompts, configuration scale, and video properties. The system validates prompt length to ensure API compliance.\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\nclass KlingTextToVideoNode(KlingNodeBase):\n    \"\"\"Kling Text to Video Node\"\"\"\n\n    @staticmethod\n    def poll_for_task_status(task_id: str, auth_token: str) -> KlingText2VideoResponse:\n        \"\"\"Polls the Kling API endpoint until the task reaches a terminal state.\"\"\"\n        polling_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"{PATH_TEXT_TO_VIDEO}/{task_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=KlingText2VideoResponse,\n            ),\n            completed_statuses=[\n                TaskStatus.succeed.value,\n            ],\n            failed_statuses=[TaskStatus.failed.value],\n            status_extractor=lambda response: (\n                response.data.task_status.value\n                if response.data and response.data.task_status\n                else None\n            ),\n            auth_token=auth_token,\n        )\n        return polling_operation.execute()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"negative_prompt\", multiline=True\n                ),\n                \"model_name\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"model_name\",\n                    enum_type=ModelName,\n                    default=\"kling-v2-master\",\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingText2VideoRequest, \"cfg_scale\"\n                ),\n                \"mode\": model_field_to_node_input(\n                    IO.COMBO, KlingText2VideoRequest, \"mode\", enum_type=Mode\n                ),\n                \"duration\": model_field_to_node_input(\n                    IO.COMBO, KlingText2VideoRequest, \"duration\", enum_type=Duration\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (\"VIDEO\", \"STRING\", \"STRING\")\n    RETURN_NAMES = (\"VIDEO\", \"Kling ID\", \"Duration (sec)\")\n    DESCRIPTION = \"Kling Text to Video Node\"\n\n    def api_call(\n        self,\n        prompt: str,\n        negative_prompt: str,\n        model_name: str,\n        cfg_scale: float,\n        mode: str,\n        duration: int,\n        aspect_ratio: str,\n        camera_control: Optional[CameraControl] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile, str, str]:\n        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_T2V)\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_TEXT_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=KlingText2VideoRequest,\n                response_model=KlingText2VideoResponse,\n            ),\n            request=KlingText2VideoRequest(\n                prompt=prompt if prompt else None,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                duration=Duration(duration),\n                mode=Mode(mode),\n                model_name=ModelName(model_name),\n                cfg_scale=cfg_scale,\n                aspect_ratio=AspectRatio(aspect_ratio),\n                camera_control=camera_control,\n            ),\n            auth_token=auth_token,\n        )\n\n        initial_response = initial_operation.execute()\n        if not is_valid_initial_response(initial_response):\n            error_msg = f\"Kling initial request failed. Code: {initial_response.code}, Message: {initial_response.message}, Data: {initial_response.data}\"\n            logging.error(error_msg)\n            raise KlingApiError(error_msg)\n\n        task_id = initial_response.data.task_id\n        final_response = self.poll_for_task_status(task_id, auth_token)\n        if not is_valid_video_response(final_response):\n            error_msg = (\n                f\"Kling task {task_id} succeeded but no video data found in response.\"\n            )\n            logging.error(error_msg)\n            raise KlingApiError(error_msg)\n\n        video = final_response.data.task_result.videos[0]\n        logging.debug(\"Kling task %s succeeded. Video URL: %s\", task_id, video.url)\n        return (\n            download_url_to_video_output(video.url),\n            str(video.id),\n            str(video.duration),\n        )\n\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video",
  "markdown": "# Kling Start-End Frame to Video - ComfyUI Built-in Node\n\n![ComfyUI Built-in Kling Start-End Frame to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg) The Kling Start-End Frame to Video node lets you create smooth video transitions between two images. It automatically generates all the intermediate frames to produce a fluid transformation.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| start\\_frame | Image | Starting image for the video |\n| end\\_frame | Image | Ending image for the video |\n| prompt | String | Text describing video content and transition |\n| negative\\_prompt | String | Elements to avoid in the video |\n| cfg\\_scale | Float | Controls how closely to follow the prompt |\n| aspect\\_ratio | Select | Output video aspect ratio |\n| mode | Select | Video generation settings (mode/duration/model) |\n\n### Mode Options\n\nAvailable mode combinations:\n\n*   standard mode / 5s duration / kling-v1\n*   standard mode / 5s duration / kling-v1-5\n*   pro mode / 5s duration / kling-v1\n*   pro mode / 5s duration / kling-v1-5\n*   pro mode / 5s duration / kling-v1-6\n*   pro mode / 10s duration / kling-v1-5\n*   pro mode / 10s duration / kling-v1-6\n\nDefault: â€œpro mode / 5s duration / kling-v1â€\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## How It Works\n\nThe node analyzes the start and end frames to create a smooth transition sequence between them. It sends the images and parameters to Klingâ€™s API server, which generates all necessary intermediate frames for a fluid transformation. The transition style and content can be guided using prompts, while negative prompts help avoid unwanted elements.\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-03)\\]\n\n```\n\n\nclass KlingStartEndFrameNode(KlingImage2VideoNode):\n    \"\"\"\n    Kling First Last Frame Node. This node allows creation of a video from a first and last frame. It calls the normal image to video endpoint, but only allows the subset of input options that support the `image_tail` request field.\n    \"\"\"\n\n    @staticmethod\n    def get_mode_string_mapping() -> dict[str, tuple[str, str, str]]:\n        \"\"\"\n        Returns a mapping of mode strings to their corresponding (mode, duration, model_name) tuples.\n        Only includes config combos that support the `image_tail` request field.\n        \"\"\"\n        return {\n            \"standard mode / 5s duration / kling-v1\": (\"std\", \"5\", \"kling-v1\"),\n            \"standard mode / 5s duration / kling-v1-5\": (\"std\", \"5\", \"kling-v1-5\"),\n            \"pro mode / 5s duration / kling-v1\": (\"pro\", \"5\", \"kling-v1\"),\n            \"pro mode / 5s duration / kling-v1-5\": (\"pro\", \"5\", \"kling-v1-5\"),\n            \"pro mode / 5s duration / kling-v1-6\": (\"pro\", \"5\", \"kling-v1-6\"),\n            \"pro mode / 10s duration / kling-v1-5\": (\"pro\", \"10\", \"kling-v1-5\"),\n            \"pro mode / 10s duration / kling-v1-6\": (\"pro\", \"10\", \"kling-v1-6\"),\n        }\n\n    @classmethod\n    def INPUT_TYPES(s):\n        modes = list(KlingStartEndFrameNode.get_mode_string_mapping().keys())\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"end_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image_tail\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"mode\": (\n                    modes,\n                    {\n                        \"default\": modes[2],\n                        \"tooltip\": \"The configuration to use for the video generation following the format: mode / duration / model_name.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Generate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last.\"\n\n    def parse_inputs_from_mode(self, mode: str) -> tuple[str, str, str]:\n        \"\"\"Parses the mode input into a tuple of (model_name, duration, mode).\"\"\"\n        return KlingStartEndFrameNode.get_mode_string_mapping()[mode]\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        end_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        mode: str,\n        auth_token: Optional[str] = None,\n    ):\n        mode, duration, model_name = self.parse_inputs_from_mode(mode)\n        return super().api_call(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model_name=model_name,\n            start_frame=start_frame,\n            cfg_scale=cfg_scale,\n            mode=mode,\n            aspect_ratio=aspect_ratio,\n            duration=duration,\n            end_frame=end_frame,\n            auth_token=auth_token,\n        )\n\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2",
  "markdown": "# OpenAI DALLÂ·E 2 Node - ComfyUI\n\n![OpenAI DALLÂ·E 2 node screenshot](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-dall-e-2.jpg) OpenAI DALLÂ·E 2 is part of the ComfyUI API Nodes series, allowing users to generate images through OpenAIâ€™s **DALLÂ·E 2** model. This node supports:\n\n*   Text-to-image generation\n*   Image editing functionality (inpainting through masks)\n\n## Node Overview\n\nThe **OpenAI DALLÂ·E 2** node generates images synchronously through OpenAIâ€™s image generation API. It receives text prompts and returns images that match the description.\n\n## Parameter Description\n\n### Required Parameters\n\n| Parameter | Description |\n| --- | --- |\n| `prompt` | Text prompt describing the image content you want to generate |\n\n### Widget Parameters\n\n| Parameter | Description | Options/Range | Default Value |\n| --- | --- | --- | --- |\n| `seed` | Seed value for image generation (currently not implemented in the backend) | 0 to 2^31-1 | 0   |\n| `size` | Output image dimensions | â€256x256â€, â€œ512x512â€, â€œ1024x1024\" | \"1024x1024â€ |\n| `n` | Number of images to generate | 1 to 8 | 1   |\n\n### Optional Parameters\n\n| Parameter | Description | Options/Range | Default Value |\n| --- | --- | --- | --- |\n| `image` | Optional reference image for image editing | Any image input | None |\n| `mask` | Optional mask for local inpainting | Mask input | None |\n\n## Usage Method\n\n## Workflow Examples\n\nThis API node currently supports two workflows:\n\n*   Text to Image\n*   Inpainting\n\n### Text to Image Example\n\nThe image below contains a simple text-to-image workflow. Please download the corresponding image and drag it into ComfyUI to load the workflow. ![ComfyUI openai-dall-e-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/text2image.png) The corresponding example is very simple ![ComfyUI openai-dall-e-2 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg) You only need to load the `OpenAI DALLÂ·E 2` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.\n\n### Inpainting Workflow\n\nDALLÂ·E 2 supports image editing functionality, allowing you to use a mask to specify the area to be replaced. Below is a simple inpainting workflow example:\n\n#### 1\\. Workflow File Download\n\nDownload the image below and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI openai-dall-e-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/inpainting.png) We will use the image below as input: ![ComfyUI openai-dall-e-2 workflow input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/input.jpg) \n\n#### 2\\. Workflow File Usage Instructions\n\n![ComfyUI openai-dall-e-2 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg) Since this workflow is relatively simple, if you want to manually implement the corresponding workflow yourself, you can follow the steps below:\n\n1.  Use the `Load Image` node to load the image\n2.  Right-click on the load image node and select `MaskEditor`\n3.  In the mask editor, use the brush to draw the area you want to redraw\n4.  Connect the loaded image to the `image` input of the **OpenAI DALLÂ·E 2** node\n5.  Connect the mask to the `mask` input of the **OpenAI DALLÂ·E 2** node\n6.  Edit the prompt in the `prompt` node\n7.  Run the workflow\n\n**Notes**\n\n*   If you want to use the image editing functionality, you must provide both an image and a mask (both are required)\n*   The mask and image must be the same size\n*   When inputting large images, the node will automatically resize the image to an appropriate size\n*   The URLs returned by the API are valid for a short period, please save the results promptly\n*   Each generation consumes credits, charged according to image size and quantity\n\n## FAQs"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video",
  "markdown": "# Luma Text to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native Luma Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg) The Luma Text to Video node lets you create high-quality, smooth videos from text descriptions using Luma AIâ€™s video generation technology.\n\n## Node Function\n\nThis node connects to Luma AIâ€™s text-to-video API, allowing users to generate dynamic video content from detailed text prompts.\n\n## Parameters\n\n### Basic Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt | string | \"\"  | Text prompt describing the video content to generate |\n| model | select | \\-  | Video generation model to use |\n| aspect\\_ratio | select | â€ratio\\_16\\_9â€ | Video aspect ratio |\n| resolution | select | â€res\\_540pâ€ | Video resolution |\n| duration | select | \\-  | Video length options |\n| loop | boolean | False | Whether to loop the video |\n| seed | integer | 0   | Seed value for node rerun, results are nondeterministic |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPTS | Camera concepts to control motion via Luma Concepts node |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Usage Example\n\n[\n\n## Luma Text to Video Workflow Example\n\nLuma Text to Video Workflow Example\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass LumaTextToVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaVideoModel],),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in LumaAspectRatio],\n                    {\n                        \"default\": LumaAspectRatio.ratio_16_9,\n                    },\n                ),\n                \"resolution\": (\n                    [resolution.value for resolution in LumaVideoOutputResolution],\n                    {\n                        \"default\": LumaVideoOutputResolution.res_540p,\n                    },\n                ),\n                \"duration\": ([dur.value for dur in LumaVideoModelOutputDuration],),\n                \"loop\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        aspect_ratio: str,\n        resolution: str,\n        duration: str,\n        loop: bool,\n        seed,\n        luma_concepts: LumaConceptChain = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        duration = duration if model != LumaVideoModel.ray_1_6 else None\n        resolution = resolution if model != LumaVideoModel.ray_1_6 else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations\",\n                method=HttpMethod.POST,\n                request_model=LumaGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaGenerationRequest(\n                prompt=prompt,\n                model=model,\n                resolution=resolution,\n                aspect_ratio=aspect_ratio,\n                duration=duration,\n                loop=loop,\n                concepts=luma_concepts.create_api_model() if luma_concepts else None,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.assets.video)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3",
  "markdown": "# OpenAI DALLÂ·E 3 Node - ComfyUI\n\nOpenAI DALLÂ·E 3 is part of the ComfyUI API Nodes series, allowing users to generate images through OpenAIâ€™s **DALLÂ·E 3** model. This node supports text-to-image generation functionality. ![OpenAI DALLÂ·E 3 node screenshot](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg)\n\n## Node Overview\n\nDALLÂ·E 3 is OpenAIâ€™s latest image generation model, capable of creating detailed and high-quality images based on text prompts. Through this node in ComfyUI, you can directly access DALLÂ·E 3â€™s generation capabilities without leaving the ComfyUI interface. The **OpenAI DALLÂ·E 3** node generates images synchronously through OpenAIâ€™s image generation API. It receives text prompts and returns images that match the description.\n\n## Parameter Details\n\n### Required Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| prompt | Text | Text prompt for generating images. Supports multi-line input, can describe in detail the image content you want to generate. |\n\n### Widget Parameters\n\n| Parameter | Type | Options | Default Value | Description |\n| --- | --- | --- | --- | --- |\n| seed | Integer | 0-2147483647 | 0   | Random seed used to control the generation result |\n| quality | Option | standard, hd | standard | Image quality setting. The â€œhdâ€ option generates higher quality images but may require more computational resources |\n| style | Option | natural, vivid | natural | Image style. â€œVividâ€ tends to generate hyperrealistic and dramatic images, while â€œnaturalâ€ produces more natural, less exaggerated images |\n| size | Option | 1024x1024, 1024x1792, 1792x1024 | 1024x1024 | Size of the generated image. You can choose square or rectangular images in different orientations |\n\n## Usage Examples\n\nYou can download the image below and drag it into ComfyUI to load the corresponding workflow ![ComfyUI openai-dall-e-3 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png) Since the corresponding workflow is very simple, you can also directly add the **OpenAI DALLÂ·E 3** node in ComfyUI, input the description of the image you want to generate, and then run the workflow ![ComfyUI openai-dall-e-3 workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg)\n\n1.  Add the **OpenAI DALLÂ·E 3** node in ComfyUI\n2.  Enter the description of the image you want to generate in the prompt text box\n3.  Adjust optional parameters as needed (quality, style, size, etc.)\n4.  Run the workflow to generate the image\n\n## FAQs"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1",
  "markdown": "# OpenAI GPT-Image-1 Node - ComfyUI\n\n![OpenAI GPT-Image-1 Node Screenshot](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-gpt-image-1.jpg) OpenAI GPT-Image-1 is part of the ComfyUI API nodes series that allows users to generate images through OpenAIâ€™s **GPT-Image-1** model. This is the same model used for image generation in ChatGPT 4o. This node supports:\n\n*   Text-to-image generation\n*   Image editing functionality (inpainting through masks)\n\n## Node Overview\n\nThe **OpenAI GPT-Image-1** node synchronously generates images through OpenAIâ€™s image generation API. It receives text prompts and returns images matching the description. GPT-Image-1 is OpenAIâ€™s most advanced image generation model currently available, capable of creating highly detailed and realistic images.\n\n## Parameter Description\n\n### Required Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| `prompt` | Text | Text prompt describing the image content you want to generate |\n\n### Widget Parameters\n\n| Parameter | Type | Options | Default | Description |\n| --- | --- | --- | --- | --- |\n| `seed` | Integer | 0-2147483647 | 0   | Random seed used to control generation results |\n| `quality` | Option | low, medium, high | low | Image quality setting, affects cost and generation time |\n| `background` | Option | opaque, transparent | opaque | Whether the returned image has a background |\n| `size` | Option | auto, 1024x1024, 1024x1536, 1536x1024 | auto | Size of the generated image |\n| `n` | Integer | 1-8 | 1   | Number of images to generate |\n\n### Optional Parameters\n\n| Parameter | Type | Options | Default | Description |\n| --- | --- | --- | --- | --- |\n| `image` | Image | Any image input | None | Optional reference image for image editing |\n| `mask` | Mask | Mask input | None | Optional mask for inpainting (white areas will be replaced) |\n\n## Usage Examples\n\n### Text-to-Image Example\n\nThe image below contains a simple text-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI openai-gpt-image-1 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/text2image.png) The corresponding workflow is very simple: ![ComfyUI openai-gpt-image-1 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg) You only need to load the `OpenAI GPT-Image-1` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.\n\n### Image-to-Image Example\n\nThe image below contains a simple image-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI openai-gpt-image-1 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/image2image.png) We will use the image below as input: ![ComfyUI openai-gpt-image-1 workflow input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/input.webp) In this workflow, we use the `OpenAI GPT-Image-1` node to generate images and the `Load Image` node to load the input image, then connect it to the `image` input of the `OpenAI GPT-Image-1` node. ![ComfyUI openai-gpt-image-1 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg)\n\n### Multiple Image Input Example\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow. ![Multiple Image Input Example](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/multiple_image_input.png) Use the hat image below as an additional input image. ![Hat](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/hat.webp) The corresponding workflow is shown in the image below: ![Multiple Image Input Example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png) The `Batch Images` node is used to load multiple images into the `OpenAI GPT-Image-1` node.\n\n### Inpainting Workflow\n\nGPT-Image-1 also supports image editing functionality, allowing you to specify areas to replace using a mask. Below is a simple inpainting workflow example: Download the image below and drag it into ComfyUI to load the corresponding workflow. We will continue to use the input image from the image-to-image workflow section. ![ComfyUI openai-gpt-image-1 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/inpaint.png) The corresponding workflow is shown in the image ![ComfyUI openai-gpt-image-1 workflow example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg) Compared to the image-to-image workflow, we use the MaskEditor in the `Load Image` node through the right-click menu to draw a mask, then connect it to the `mask` input of the `OpenAI GPT-Image-1` node to complete the workflow. **Notes**\n\n*   The mask and image must be the same size\n*   When inputting large images, the node will automatically resize the image to an appropriate size\n\n## FAQs"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/runway/video-generation",
  "markdown": "# Runway API Node Video Generation ComfyUI Official Example\n\nRunway is a company focused on generative AI, providing powerful video generation capabilities. Currently, ComfyUI has integrated the Runway API, allowing you to directly use the related nodes in ComfyUI for video generation. Currently, ComfyUI natively integrates the following Runway video generation models:\n\n*   Runway Gen3a turbo\n*   Runway Gen4 turbo\n*   Runway First Last Frame to video\n\n## Gen3a turbo Image-to-Video Workflow\n\n### 1\\. Workflow File Download\n\nThe video below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/runway_image_to_video_gen3a_turbo.json)\n\nDownload the image below as input image ![ComfyUI Runway gen3a turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/steampunk.png)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway gen3a turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg) You can refer to the numbers in the image to complete the basic image-to-video workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  In the `Runway Gen3a turbo` node, set the `prompt` to describe video content, modify the `duration` parameter to set video length, modify the `ratio` parameter to set video aspect ratio\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.\n4.  After waiting for the API to return results, you can view the generated video in the `Save Video` node (right-click to save). The corresponding video will also be saved to the `ComfyUI/output/` directory.\n\n## Gen4 turbo Image-to-Video Workflow\n\n### 1\\. Workflow File Download\n\nThe video below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/runway_gen4_turo_image_to_video.json)\n\nDownload the image below as input image ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/godfather.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway gen4 turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg) You can refer to the numbers in the image to complete the basic image-to-video workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  In the `Runway Gen4 turbo` node, set the `prompt` to describe video content, modify the `duration` parameter to set video length, modify the `ratio` parameter to set video aspect ratio\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.\n4.  After waiting for the API to return results, you can view the generated video in the `Save Video` node (right-click to save). The corresponding video will also be saved to the `ComfyUI/output/` directory.\n\n## First-Last Frame Video Generation Workflow\n\n### 1\\. Workflow File Download\n\nThe video below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/runway_first_last_frame.json)\n\nDownload the images below as input images ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/first.jpg) ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/last.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway gen4 turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg) You can refer to the numbers in the image to complete the basic first-last frame to video workflow execution:\n\n1.  In the `Load Image` node, load the starting frame\n2.  In the `Load Image` node, load the ending frame\n3.  In the `Runway First-Last-Frame to Video` node, set the `prompt` to describe video content, modify the `duration` parameter to set video length, modify the `ratio` parameter to set video aspect ratio\n4.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.\n5.  After waiting for the API to return results, you can view the generated video in the `Save Video` node (right-click to save). The corresponding video will also be saved to the `ComfyUI/output/` directory."
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template",
  "markdown": "# PixVerse Template - ComfyUI Native Node Documentation\n\n![ComfyUI Native PixVerse Template Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg) The PixVerse Template node lets you choose from predefined video generation templates to control the style and effects of PixVerse video generation nodes. This helper node connects to PixVerse video generation nodes, allowing users to quickly apply preset video styles without manually adjusting complex parameter combinations.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| template | Select | Choose a template from available video presets |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| pixverse\\_template | PixverseIO.TEMPLATE | Configuration object containing the selected template ID |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\n\nclass PixverseTemplateNode:\n    \"\"\"\n    Select template for Pixverse Video generation.\n    \"\"\"\n\n    RETURN_TYPES = (PixverseIO.TEMPLATE,)\n    RETURN_NAMES = (\"pixverse_template\",)\n    FUNCTION = \"create_template\"\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"template\": (list(pixverse_templates.keys()), ),\n            }\n        }\n\n    def create_template(self, template: str):\n        template_id = pixverse_templates.get(template, None)\n        if template_id is None:\n            raise Exception(f\"Template '{template}' is not recognized.\")\n        # just return the integer\n        return (template_id,)\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video",
  "markdown": "# Luma Image to Video API Node ComfyUI Official Example\n\nThe [Luma Image to Video](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video) node allows you to convert static images into smooth, dynamic videos using Luma AIâ€™s advanced technology, bringing life and motion to your images. In this guide, weâ€™ll show you how to set up a workflow for image-to-video conversion using this node.\n\n## Luma Image to Video Node Documentation\n\nCheck out the following documentation to learn more about the nodeâ€™s parameters:\n\n[\n\nLuma Image to Video API node documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)[\n\n## Luma Concepts Node Docs\n\nLuma Concepts API node documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)\n\n## Image to Video Workflow with Luma API Node\n\nThe Luma Image to Video node requires at least one image input (`first_image` or `last_image`) along with text prompts to determine the videoâ€™s motion effects. In this guide, weâ€™ve created an example using `first_image` and `luma_concepts` to showcase Luma AIâ€™s video generation capabilities.\n\n### 1\\. Download the Workflow\n\nThe workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow. ![Luma Image to Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/luma_i2v.mp4) Download the following image to use as input: ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/input.png)\n\n### 2\\. Follow the Workflow Steps\n\n![Luma Image to Video Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg) Follow these basic steps to run the workflow:\n\n1.  Upload your input image in the `first_image` node\n2.  (Optional) Write prompts in the Luma Image to Video node to describe how you want the image animated\n3.  (Optional) Modify the `Luma Concepts` node to control camera movement for professional cinematography\n4.  Click `Run` or use `Ctrl(cmd) + Enter` to generate the video\n5.  Once the API returns results, view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory\n\n### 3\\. Additional Notes\n\n*   **Image Input Requirements**: At least one of `first_image` or `last_image` is required, with a maximum of 1 image per input\n*   **Luma Concepts**: Controls camera movement for professional video effects\n*   **Seed Parameter**: Only determines if the node should rerun, doesnâ€™t affect generation results\n*   **Enable Input Nodes**: Right-click on purple â€œBypassâ€ mode nodes and set â€œmodeâ€ to â€œalwaysâ€ to enable inputs\n*   **Model Selection**: Different video generation models have unique characteristics, adjustable via the model parameter\n*   **Resolution and Duration**: Adjust output video resolution and length using resolution and duration parameters"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video",
  "markdown": "# MiniMax Image to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native MiniMax Image to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg) The MiniMax Image to Video node uses MiniMaxâ€™s API to generate videos from input images and text prompts.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| image | image | \\-  | Input image used as the first frame of video |\n| prompt\\_text | string | \"\"  | Text prompt to guide video generation |\n| model | select | â€I2V-01â€ | Available models: â€œI2V-01-Directorâ€, â€œI2V-01â€, â€œI2V-01-liveâ€ |\n\n### Optional Parameters\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| seed | integer | Random seed for noise generation |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | video | Generated video |\n\n## Source Code\n\n\\[Node source code (Updated on 2025-05-03)\\]\n\n```\n\nclass MinimaxImageToVideoNode(MinimaxTextToVideoNode):\n    \"\"\"\n    Generates videos synchronously based on an image and prompt, and optional parameters using Minimax's API.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Image to use as first frame of video generation\"\n                    },\n                ),\n                \"prompt_text\": (\n                    \"STRING\",\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt to guide the video generation\",\n                    },\n                ),\n                \"model\": (\n                    [\n                        \"I2V-01-Director\",\n                        \"I2V-01\",\n                        \"I2V-01-live\",\n                    ],\n                    {\n                        \"default\": \"I2V-01\",\n                        \"tooltip\": \"Model to use for video generation\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from an image and prompts using Minimax's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Minimax\"\n    API_NODE = True\n    OUTPUT_NODE = True\n```"
},
{
  "url": "https://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video",
  "markdown": "# Pika 2.2 Text to Video - ComfyUI Native Node Documentation\n\n![ComfyUI Native Pika 2.2 Text to Video Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg) The Pika 2.2 Text to Video node uses Pikaâ€™s 2.2 API to create videos from text descriptions. It connects to Pikaâ€™s text-to-video API, allowing users to generate videos using text prompts with various control parameters.\n\n## Parameters\n\n### Required Parameters\n\n| Parameter | Type | Default | Description |\n| --- | --- | --- | --- |\n| prompt\\_text | String | \"\"  | Text prompt describing the video content |\n| negative\\_prompt | String | \"\"  | Elements to exclude from the video |\n| seed | Integer | 0   | Random seed for generation |\n| resolution | Select | â€1080pâ€ | Output video resolution |\n| duration | Select | â€5sâ€ | Length of generated video |\n| aspect\\_ratio | Float | 1.7777777777777777 | Video aspect ratio, range 0.4-2.5, step 0.001 |\n\n### Output\n\n| Output | Type | Description |\n| --- | --- | --- |\n| VIDEO | Video | Generated video |\n\n## Source Code\n\n\\[Node Source Code (Updated 2025-05-05)\\]\n\n```\n\nclass PikaTextToVideoNodeV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Text to Video Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                **cls.get_base_inputs_types(PikaBodyGenerate22T2vGenerate22T2vPost),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.FLOAT,\n                    PikaBodyGenerate22T2vGenerate22T2vPost,\n                    \"aspectRatio\",\n                    step=0.001,\n                    min=0.4,\n                    max=2.5,\n                    default=1.7777777777777777,\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Sends a text prompt to the Pika API v2.2 to generate a video.\"\n\n    def api_call(\n        self,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        aspect_ratio: float,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika 2.2 Text to Video.\"\"\"\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_TEXT_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22T2vGenerate22T2vPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=PikaBodyGenerate22T2vGenerate22T2vPost(\n                promptText=prompt_text,\n                negativePrompt=negative_prompt,\n                seed=seed,\n                resolution=resolution,\n                duration=duration,\n                aspectRatio=aspect_ratio,\n            ),\n            auth_token=auth_token,\n            content_type=\"application/x-www-form-urlencoded\",\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fbuilt-in-nodes%2Fapi-node%2Fvideo%2Fkwai_vgi%2Fkling-camera-control-i2v",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video",
  "markdown": "# Luma Text to Video API Node ComfyUI Official Guide\n\nThe [Luma Text to Video](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video) node allows you to create high-quality, smooth videos from text descriptions using Luma AIâ€™s innovative video generation technology. In this guide, weâ€™ll show you how to set up a text-to-video workflow using this node.\n\n## Luma Text to Video Node Documentation\n\nCheck out the following documentation to learn more about the node parameters:\n\n[\n\nDocumentation for the Luma Text to Video API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)[\n\n## Luma Concepts Node Docs\n\nDocumentation for the Luma Concepts API node\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)\n\n## Text to Video Workflow with Luma API Node\n\nThe Luma Text to Video node requires text prompts to describe the video content. In this guide, weâ€™ve created examples using `prompt` and `luma_concepts` to showcase Luma AIâ€™s excellent video generation capabilities.\n\n### 1\\. Download the Workflow\n\nThe workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow. ![Luma Text to Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2v/luma_t2v.mp4)\n\n### 2\\. Follow the Steps\n\n![Luma Text to Video Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg) Follow these basic steps to run the workflow:\n\n1.  Write your prompt in the `Luma Text to Video` node to describe the video content you want\n2.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the video\n3.  After the API returns results, you can view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory\n\n> (Optional) Modify the `Luma Concepts` node to control camera movements and add professional cinematography\n\n### 3\\. Additional Notes\n\n*   **Writing Prompts**: Describe scenes, subjects, actions, and mood in detail for best results\n*   **Luma Concepts**: Mainly used for camera control to create professional video shots\n*   **Seed Parameter**: Only determines if the node should rerun, doesnâ€™t affect generation results\n*   **Model Selection**: Different video models have different features, adjustable via the model parameter\n*   **Resolution and Duration**: Adjust output video resolution and length using these parameters\n*   **Ray 1.6 Model Note**: Duration and resolution parameters donâ€™t work when using the Ray 1.6 model\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/interface/settings/comfy-desktop",
  "markdown": "# Comfy Desktop General Settings - ComfyUI\n\n### Window Style\n\n**Function**: Controls the title bar style of the application window\n\n### Automatically check for updates\n\n**Function**: Automatically checks for ComfyUI Desktop updates and will remind you to update when updates are available\n\n### Send anonymous usage metrics\n\n**Function**: Sends anonymous usage statistics to help improve the software. Changes to this setting require a restart to take effect\n\n## UV\n\nThis section is mainly for users in China, because many of the original mirrors used by Desktop are outside of China, so access may not be friendly for domestic users. You can set your own mirror sources here to improve access speed and ensure that corresponding packages can be accessed and downloaded normally.\n\n### Python Install Mirror\n\n**Function**:\n\n*   Managed Python installation packages are downloaded from the Astral python-build-standalone project\n*   Can set mirror URL to use different Python installation sources\n*   The provided URL will replace the default GitHub download address\n*   Supports using file:// protocol to read distribution packages from local directories  \n    **Validation**: Automatically checks mirror reachability\n\n### Pypi Install Mirror\n\n**Function**: Default pip package installation mirror source\n\n### Torch Install Mirror\n\n**Function**: PyTorch-specific pip installation mirror source"
},
{
  "url": "https://docs.comfy.org/interface/settings/3d",
  "markdown": "# ComfyUI 3D Settings - ComfyUI\n\nThis section of settings is mainly used to control the initialization settings of 3D-related components in ComfyUI, including camera, lighting, scene, etc. When creating new 3D components, they will be initialized according to these settings. After creation, these settings can still be adjusted individually.\n\n## Camera\n\n### Initial Camera Type\n\n*   **Options**:\n    *   `perspective`\n    *   `orthographic`\n*   **Function**: Controls whether the default camera is perspective or orthographic when creating new 3D components. This default setting can still be switched individually for each component after creation\n\n![æ‘„åƒæœºç±»åž‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/camera_type.jpg)\n\n## Light\n\nThe light settings in this section are used to set the default lighting settings for 3D components. The corresponding settings in the 3D settings in ComfyUI can also be modified. ![light](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/light.jpg)\n\n### Light Adjustment Increment\n\n*   **Default Value**: 0.5\n*   **Function**: Controls the step size when adjusting light intensity in 3D scenes. Smaller step values allow for finer light adjustments, while larger values make each adjustment more noticeable\n\n### Light Intensity Minimum\n\n*   **Default Value**: 1\n*   **Function**: Sets the minimum light intensity value allowed in 3D scenes. This defines the lowest brightness that can be set when adjusting the lighting of any 3D control\n\n### Light Intensity Maximum\n\n*   **Default Value**: 10\n*   **Function**: Sets the maximum light intensity value allowed in 3D scenes. This defines the upper limit of brightness that can be set when adjusting the lighting of any 3D control\n\n### Initial Light Intensity\n\n*   **Default Value**: 3\n*   **Function**: Sets the default brightness level of lights in 3D scenes. This value determines the intensity with which lights illuminate objects when creating new 3D controls, but each control can be adjusted individually after creation\n\n## Scene\n\n### Initial Background Color\n\n*   **Function**: Controls the default background color of 3D scenes. This setting determines the background appearance when creating new 3D components, but each component can be adjusted individually after creation\n*   **Default Value**: `282828` (dark gray)\n\nChange the background color, which can also be adjusted in the canvas. ![Initial Background Color vs Modified](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/background_color.jpg) \n\n### Initial Preview Visibility\n\n*   **Function**: Controls whether the preview screen is displayed by default when creating new 3D components. This default setting can still be toggled individually for each component after creation\n*   **Default Value**: true (enabled)\n\n![Preview vs hide preview](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/hide_preview.jpg)\n\n### Initial Grid Visibility\n\n*   **Function**: Controls whether the grid is displayed by default when creating new 3D components. This default setting can still be toggled individually for each component after creation\n*   **Default Value**: true (enabled)\n\nHide or show the grid on initialization ![Show gird vs hide gird](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/hide_grid.jpg)"
},
{
  "url": "https://docs.comfy.org/interface/settings/comfy",
  "markdown": "# Comfy Settings - ComfyUI\n\n## API Nodes\n\n### Show API node pricing badge\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display pricing badges on API nodes, helping users identify the usage cost of API nodes\n\n![Show API node pricing badge](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/api_node_pricing_badge.jpg)\n\n> For more information about API nodes, please refer to [API Nodes](https://docs.comfy.org/tutorials/api-nodes/overview)\n\n## Dev Mode\n\n### Enable dev mode options (API save, etc.)\n\n*   **Default Value**: Disabled\n*   **Function**: Enables development mode options (such as API save, etc.)\n\n## Edit Token Weight\n\n### Ctrl+up/down precision\n\n*   **Default Value**: 0.01\n*   **Function**: When using CLIPTextEncode type nodes or text input node widgets, use Ctrl+up/down to quickly adjust weights. This option changes the weight value for each adjustment\n\n## Locale\n\n### Language\n\n*   **Options**: English, ä¸­æ–‡ (Chinese),æ—¥æœ¬èªž (Japanese), í•œêµ­ì–´ (Korean), Ð ÑƒÑÑÐºÐ¸Ð¹ (Russian), EspaÃ±ol (Spanish), FranÃ§ais (French)\n*   **Default Value**: Auto-detect browser language\n*   **Function**: Modify the display language of ComfyUI interface\n\n*   **Default Value**: Top\n*   **Function**: Select menu interface and position, currently only supports Top, Bottom, Disabled\n\nThe menu interface will be displayed at the top of the workspace ![Top Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/UseNewMenu_top.jpg) \n\n## Model Library\n\nModel Library refers to the model management function in the ComfyUI sidebar menu. You can use this function to view models in your `ComfyUI/models` and additionally configured model folders. ![Model Library](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/sidepanel/model_library.jpg)\n\n### What name to display in the model library tree view\n\n*   **Default Value**: title\n*   **Function**: Select the name format to display in the model library tree view, currently only supports filename and title\n\n### Automatically load all model folders\n\n*   **Default Value**: Disabled\n*   **Function**: Whether to automatically detect model files in all folders when clicking the model library. Enabling may cause loading delays (requires traversing all folders). When disabled, files in the corresponding folder will only be loaded when clicking the folder name.\n\n## Node\n\nDuring the iteration process of ComfyUI, we will adjust some nodes and enable some nodes. These nodes may undergo major changes or be removed in future versions. However, to ensure compatibility, deprecated nodes have not been removed. You can use the settings below to enable whether to display **experimental nodes** and **deprecated nodes**.\n\n### Show deprecated nodes in search\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to display deprecated nodes in search. Deprecated nodes are hidden by default in the UI, but remain effective in existing workflows.\n\n![Show deprecated nodes in search](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/depr_node.jpg)\n\n### Show experimental nodes in search\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display experimental nodes in search. Experimental nodes are some new feature support, but are not fully stable and may change or be removed in future versions.\n\n![Show experimental nodes in search](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/beta_node.jpg)\n\n## Node Search Box\n\n### Number of nodes suggestions\n\n*   **Default Value**: 5\n*   **Function**: Used to modify the number of recommended nodes in the related node context menu. The larger the value, the more related recommended nodes are displayed.\n\n![Number of nodes suggestions](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_suggestions.jpg)\n\n### Show node frequency in search results\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to display node usage frequency in search results\n\n![Show node frequency in search results](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_frequency.png)\n\n### Show node id name in search results\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to display node ID names in search results\n\n![Show node id name in search results](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_id_name.jpg)\n\n### Show node category in search results\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display node categories in search results, helping users understand node classification information\n\n### Node preview\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display node previews in search results, making it convenient for you to quickly preview nodes\n\n### Node search box implementation\n\n*   **Default Value**: default\n*   **Function**: Select the implementation method of the node search box (experimental feature). If you select `litegraph (legacy)`, it will switch to the early ComfyUI search box\n\n### Widget control mode\n\n*   **Options**: before, after\n*   **Function**: Controls whether the timing of node widget value updates is before or after workflow execution, such as updating seed values\n\n### Textarea widget spellcheck\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether text area widgets enable spellcheck, providing spellcheck functionality during text input. This functionality is implemented through the browserâ€™s spellcheck attribute\n\n## Queue\n\n### Queue history size\n\n*   **Default Value**: 100\n*   **Function**: Controls the queue history size recorded in the sidebar queue history panel. The larger the value, the more queue history is recorded. When the number is large, loading the page will also consume more memory\n\n## Queue Button\n\n### Batch count limit\n\n*   **Default Value**: 100\n*   **Function**: Sets the maximum number of tasks added to the queue in a single click, preventing accidentally adding too many tasks to the queue\n\n## Validation\n\n### Validate node definitions (slow)\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to validate all node definitions at startup (slow). Only recommended for node developers. When enabled, the system will use Zod schemas to strictly validate each node definition. This functionality will consume more memory and time\n*   **Error Handling**: Failed node definitions will be skipped and warning information will be output to the console\n\n### Validate workflows\n\n*   **Default Value**: Enabled\n*   **Function**: Ensures the structural and connection correctness of workflows. If enabled, the system will call `useWorkflowValidation().validateWorkflow()` to validate workflow data\n*   **Validation Process**: The validation process includes two steps:\n    *   Schema validation: Use Zod schemas to validate workflow structure\n    *   Link repair: Check and repair connection issues between nodes\n*   **Error Handling**: When validation fails, error prompts will be displayed, but workflow loading will not be blocked\n\n## Window\n\n### Show confirmation when closing window\n\n*   **Default Value**: Enabled\n*   **Function**: When there are modified but unsaved workflows, controls whether to display confirmation when closing the browser window or tab, preventing accidental window closure that leads to loss of unsaved workflows\n\n## Workflow\n\n### Persist workflow state and restore on page (re)load\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to restore workflow state on page (re)load, maintaining workflow content after page refresh\n\n### Auto Save\n\n*   **Default Value**: off\n*   **Function**: Controls the auto-save behavior of workflows, automatically saving workflow changes to avoid data loss\n\n### Auto Save Delay (ms)\n\n*   **Default Value**: 1000\n*   **Function**: Sets the delay time for auto-save, only effective when auto-save is set to â€œafter delayâ€\n\n### Show confirmation when deleting workflows\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display a confirmation dialog when deleting workflows in the sidebar, preventing accidental deletion of important workflows\n\n### Save and restore canvas position and zoom level in workflows\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to save and restore canvas position and zoom level in workflows, restoring the previous view state when reopening workflows\n\n### Opened workflows position\n\n*   **Options**: Sidebar, Topbar, Topbar (Second Row)\n*   **Default Value**: Topbar\n*   **Function**: Controls the display position of opened workflow tabs, currently only supports Sidebar, Topbar, Topbar (Second Row)\n\n### Prompt for filename when saving workflow\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to prompt for filename input when saving workflows, allowing users to customize workflow filenames\n\n### Sort node IDs when saving workflow\n\n*   **Default Value**: Disabled\n*   **Function**: Determines whether to sort node IDs when saving workflows, making workflow file format more standardized and convenient for version control\n\n### Show missing nodes warning\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display warnings for missing nodes in workflows, helping users identify unavailable nodes in workflows\n\n### Show missing models warning\n\n*   **Default Value**: Enabled\n*   **Function**: We support adding model link information to widget values in workflow files for prompts when loading model files. When enabled, if you donâ€™t have the corresponding model files locally, warnings for missing models in workflows will be displayed\n\n### Require confirmation when clearing workflow\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display a confirmation dialog when clearing workflows, preventing accidental clearing of workflow content\n\n### Save node IDs to workflow\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to save node IDs when saving workflows, making workflow file format more standardized and convenient for version control"
},
{
  "url": "https://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1",
  "markdown": "# ComfyUI ACE-Step Native Example - ComfyUI\n\nACE-Step is an open-source foundational music generation model jointly developed by Chinese team StepFun and ACE Studio, aimed at providing music creators with efficient, flexible and high-quality music generation and editing tools. The model is released under the [Apache-2.0](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-license) license and is free for commercial use. As a powerful music generation foundation, ACE-Step provides rich extensibility. Through fine-tuning techniques like LoRA and ControlNet, developers can customize the model according to their actual needs. Whether itâ€™s audio editing, vocal synthesis, accompaniment production, voice cloning or style transfer applications, ACE-Step provides stable and reliable technical support. This flexible architecture greatly simplifies the development process of music AI applications, allowing more creators to quickly apply AI technology to music creation. Currently, ACE-Step has released related training code, including LoRA model training, and the corresponding ControlNet training code will be released in the future. You can visit their [Github](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-roadmap) to learn more details.\n\n## ACE-Step ComfyUI Text-to-Audio Generation Workflow Example\n\n### 1\\. Download Workflow and Related Models\n\nClick the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information. Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_t2m.json)\n\nYou can also manually download [ace\\_step\\_v1\\_3.5b.safetensors](https://huggingface.co/Comfy-Org/ACE-Step_ComfyUI_repackaged/blob/main/all_in_one/ace_step_v1_3.5b.safetensors) and save it to the `ComfyUI/models/checkpoints` folder\n\n### 2\\. Complete the Workflow Step by Step\n\n![Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg)\n\n1.  Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model\n2.  (Optional) In the `EmptyAceStepLatentAudio` node, you can set the duration of the music to be generated\n3.  (Optional) In the `LatentOperationTonemapReinhard` node, you can adjust the `multiplier` to control the volume of the vocals (higher numbers result in more prominent vocals)\n4.  (Optional) Input corresponding music styles etc. in the `tags` field of `TextEncodeAceStepAudio`\n5.  (Optional) Input corresponding lyrics in the `lyrics` field of `TextEncodeAceStepAudio`\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation\n7.  After the workflow completes,, you can preview the generated audio in the `Save Audio` node. You can click to play and listen to it, and the audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).\n\n## ACE-Step ComfyUI Audio-to-Audio Workflow\n\nSimilar to image-to-image workflows, you can input a piece of music and use the workflow below to resample and generate music. You can also adjust the difference from the original audio by controlling the `denoise` parameter in the `Ksampler`.\n\n### 1\\. Download Workflow File\n\nClick the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_m2m_editing.json)\n\nDownload the following audio file as the input audio:\n\n[\n\nDownload Example Audio File for Input\n\n](https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/audio/ace-step/input.mp3)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ACE-Step Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg)\n\n1.  Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model\n2.  Upload the provided audio file in the `LoadAudio` node\n3.  (Optional) Input corresponding music styles and lyrics in the `tags` and `lyrics` fields of `TextEncodeAceStepAudio`. Providing lyrics is very important for audio editing\n4.  (Optional) Modify the `denoise` parameter in the `Ksampler` node to adjust the noise added during sampling to control similarity with the original audio (smaller values result in more similarity to the original audio; setting it to `1.00` is approximately equivalent to having no audio input)\n5.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation\n6.  After the workflow completes, you can preview the generated audio in the `Save Audio` node. You can click to play and listen to it, and the audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).\n\nYou can also implement the lyrics modification and editing functionality from the ACE-Step project page, modifying the original lyrics to change the audio effect.\n\n### 3\\. Additional Workflow Notes\n\n1.  In the example workflow, you can change the `tags` in `TextEncodeAceStepAudio` from `male voice` to `female voice` to generate female vocals.\n2.  You can also modify the `lyrics` in `TextEncodeAceStepAudio` to change the lyrics and thus the generated audio. Refer to the examples on the ACE-Step project page for more details.\n\n## ACE-Step Prompt Guide\n\nACE currently uses two types of prompts: `tags` and `lyrics`.\n\n*   `tags`: Mainly used to describe music styles, scenes, etc. Similar to prompts we use for other generations, they primarily describe the overall style and requirements of the audio, separated by English commas\n*   `lyrics`: Mainly used to describe lyrics, supporting lyric structure tags such as \\[verse\\], \\[chorus\\], and \\[bridge\\] to distinguish different parts of the lyrics. You can also input instrument names for purely instrumental music\n\nYou can find rich examples of `tags` and `lyrics` on the [ACE-Step model homepage](https://ace-step.github.io/). You can refer to these examples to try corresponding prompts. This documentâ€™s prompt guide is organized based on the project to help you quickly try combinations to achieve your desired effect.\n\n### Tags (prompt)\n\n#### Mainstream Music Styles\n\nUse short tag combinations to generate specific music styles\n\n*   electronic\n*   rock\n*   pop\n*   funk\n*   soul\n*   cyberpunk\n*   Acid jazz\n*   electro\n*   em (electronic music)\n*   soft electric drums\n*   melodic\n\n#### Scene Types\n\nCombine specific usage scenarios and atmospheres to generate music that matches the corresponding mood\n\n*   background music for parties\n*   radio broadcasts\n*   workout playlists\n\n#### Instrumental Elements\n\n*   saxophone\n*   jazz\n*   piano, violin\n\n#### Vocal Types\n\n*   female voice\n*   male voice\n*   clean vocals\n\n#### Professional Terms\n\nUse some professional terms commonly used in music to precisely control music effects\n\n*   110 bpm (beats per minute is 110)\n*   fast tempo\n*   slow tempo\n*   loops\n*   fills\n*   acoustic guitar\n*   electric bass\n\n### Lyrics\n\n#### Lyric Structure Tags\n\n*   \\[outro\\]\n*   \\[verse\\]\n*   \\[chorus\\]\n*   \\[bridge\\]\n\n#### Multilingual Support\n\n*   ACE-Step V1 supports multiple languages. When used, ACE-Step converts different languages into English letters and then generates music.\n*   In ComfyUI, we havenâ€™t fully implemented the conversion of all languages to English letters. Currently, only [Japanese hiragana and katakana characters](https://github.com/comfyanonymous/ComfyUI/commit/5d3cc85e13833aeb6ef9242cdae243083e30c6fc) are implemented. So if you need to use multiple languages for music generation, you need to first convert the corresponding language to English letters, and then input the language code abbreviation at the beginning of the `lyrics`, such as Chinese `[zh]`, Korean `[ko]`, etc.\n\nFor example:\n\n```\n[verse]\n\n[zh]wo3zou3guo4shen1ye4de5jie1dao4\n[zh]leng3feng1chui1luan4si1nian4de5piao4liang4wai4tao4\n[zh]ni3de5wei1xiao4xiang4xing1guang1hen3xuan4yao4\n[zh]zhao4liang4le5wo3gu1du2de5mei3fen1mei3miao3\n\n[chorus]\n\n[verse]â€‹\n[ko]hamkke si-kkeuleo-un sesang-ui sodong-eul pihaeâ€‹\n[ko]honja ogsang-eseo dalbich-ui eolyeompus-ileul balabodaâ€‹\n[ko]niga salang-eun lideum-i ganghan eum-ag gatdago malhaess-eoâ€‹\n[ko]han ta han tamada ma-eum-ui ondoga eolmana heojeonhanji ijge hae\n\n[bridge]\n[es]cantar mi anhelo por ti sin ocultar\n[es]como poesÃ­a y pintura, lleno de anhelo indescifrable\n[es]tu sombra es tan terca como el viento, inborrable\n[es]persiguiÃ©ndote en vuelo, brilla como cruzar una mar de nubes\n\n[chorus]\n[fr]que tu sois le vent qui souffle sur ma main\n[fr]un contact chaud comme la douce pluie printaniÃ¨re\n[fr]que tu sois le vent qui s'entoure de mon corps\n[fr]un amour profond qui ne s'Ã©loignera jamais\n```\n\nCurrently, ACE-Step supports 19 languages, but the following ten languages have better support:\n\n*   English\n*   Chinese: \\[zh\\]\n*   Russian: \\[ru\\]\n*   Spanish: \\[es\\]\n*   Japanese: \\[ja\\]\n*   German: \\[de\\]\n*   French: \\[fr\\]\n*   Portuguese: \\[pt\\]\n*   Italian: \\[it\\]\n*   Korean: \\[ko\\]\n\n*   [Project Page](https://ace-step.github.io/)\n*   [Hugging Face](https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B)\n*   [GitHub](https://github.com/ace-step/ACE-Step)\n*   [Training Scripts](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-train)"
},
{
  "url": "https://docs.comfy.org/interface/settings/lite-graph",
  "markdown": "# ComfyUI LiteGraph (Canvas) Settings - ComfyUI\n\nLiteGraph is the underlying graphics rendering engine of ComfyUI. The settings in this category mainly control the behavior and appearance of graphical interfaces such as canvas, nodes, and links.\n\n### Show selection toolbox\n\n*   **Default Value**: Enabled\n*   **Function**: The selection toolbox is a floating quick action toolbar that appears on nodes after they are selected, providing common quick operations such as partial execution, pinning, deletion, color modification, etc.\n\n![Show selection toolbox](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/selection-toolbox.jpg)\n\n### Low quality rendering zoom threshold\n\n*   **Default Value**: 0.6\n*   **Range**: 0.1 - 1.0\n*   **Function**: When rendering the interface, especially when the workflow is particularly complex and the entire canvas is particularly large, the frontend rendering of corresponding elements will consume a lot of memory and cause lag. By lowering this threshold, you can control elements to enter low quality rendering mode when scaled to a specific percentage, thereby reducing memory consumption. The corresponding different rendering modes are shown below\n\n![Low quality rendering](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/render-mode.jpg)\n\n### Maximum FPS\n\n*   **Default Value**: 0 (use screen refresh rate)\n*   **Range**: 0 - 120\n*   **Function**: Limits the rendering frame rate of the canvas. 0 means using the screen refresh rate. Higher FPS will make the canvas rendering smoother, but will also consume more performance. Too low values will cause more obvious stuttering.\n\n### Always snap to grid\n\n*   **Default Value**: Disabled\n*   **Function**: When this option is not enabled, you can hold the `Shift` key to align node edges with the grid. When enabled, node edges will automatically align with the grid without holding the `Shift` key.\n\n### Snap to grid size\n\n*   **Range**: 1 - 500\n*   **Function**: When auto-snap is enabled or when moving nodes while holding the `Shift` key, this parameter determines the grid size for snapping. The default value is 10, and you can adjust it according to your needs.\n\n### Enable fast-zoom shortcut (Ctrl + Shift + Drag)\n\n*   **Default Value**: Enabled\n*   **Function**: Enables the `Ctrl + Shift + Left Mouse Button Drag` fast zoom function, providing a faster zoom operation method\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display the canvas menu in the bottom right corner\n\nThe canvas menu is located in the bottom right corner of the entire ComfyUI interface, containing operations such as canvas zooming, temporarily hiding all connections, quickly scaling the workflow to fit the canvas, etc., as shown in the image below ![Show graph canvas menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/canvas_menu.jpg)\n\n### Canvas zoom speed\n\n*   **Default Value**: 1.1\n*   **Range**: 1.01 - 2.5\n*   **Function**: Controls the speed of canvas zooming, adjusts the sensitivity of mouse wheel zooming\n\n### Show canvas info on bottom left corner (fps, etc.)\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to display canvas information in the bottom left corner, showing performance metrics like FPS\n\n![Canvas info](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/canvas-info.jpg)\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to scale node combo widget menus (lists) when zoomed in, allowing users to select node combo widgets\n\n## Graph\n\n### Link Render Mode\n\n*   **Default Value**: 2 (Spline)\n*   **Options**: Straight, Linear, Spline, Hidden\n*   **Function**: Sets the rendering style of connections, controlling the visual style of links between nodes\n\n![Link Render Mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/link-render-mode.jpg)\n\n## Group\n\nThis section of settings is mainly related to node group functionality ![Node Group](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-group.png)\n\n### Double click group title to edit\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether you can double-click the node title to edit it, allowing users to rename nodes, marked as part `1` in the image\n\n### Group selected nodes padding\n\n*   **Default Value**: 10\n*   **Range**: 0 - 100\n*   **Function**: Sets the inner padding when grouping selected nodes, controlling the spacing between the group frame and nodes, marked as the arrow annotation part `2` in the image\n\n## Link\n\n### Link midpoint markers\n\n*   **Default Value**: Circle\n*   **Options**: None, Circle, Arrow\n*   **Function**: Sets the marker style at link midpoints, displaying direction indicators at link midpoints\n\n![Link midpoint markers](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/link-midpoint.jpg)\n\n## Link Release\n\nThis menu section currently mainly controls related operations when link connections are released. The current two related operations are: **A node recommendation list related to the current input/output will appear after release**\n\n**A search box will be launched after release**\n\n### Action on link release (Shift)\n\n*   **Default Value**: search box\n*   **Options**: context menu, search box, no action\n*   **Function**: Sets the action when releasing links while holding the Shift key, special behavior when releasing links while holding Shift\n\n### Action on link release (No modifier)\n\n*   **Default Value**: context menu\n*   **Options**: context menu, search box, no action\n*   **Function**: Sets the default action when releasing links, controls the behavior after dragging and releasing links\n\n## Node\n\n### Always shrink new nodes\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether to automatically shrink when creating new nodes, so nodes can always display the minimum size, but may cause some text display to be truncated when adding, requiring manual adjustment of node size\n\n### Enable DOM element clipping (enabling may reduce performance)\n\n*   **Default Value**: Enabled\n*   **Function**: Enables DOM element clipping (may affect performance), optimizes rendering but may reduce performance\n\n### Middle-click creates a new Reroute node\n\n*   **Default Value**: Enabled\n*   **Function**: Creates a new reroute node when middle-clicking, quickly creates reroute nodes for organizing connections\n\n### Keep all links when deleting nodes\n\n*   **Default Value**: Enabled\n*   **Function**: Automatically bypasses connections when deleting intermediate nodes, attempts to reconnect input and output links when deleting nodes\n\n### Snap highlights node\n\n*   **Default Value**: Enabled\n*   **Function**: Highlights nodes when dragging links to them, provides visual feedback, shows connectable nodes. When enabled, the effect is as shown in the image below, the corresponding side of the link will display highlighted style\n\n![Snap highlights node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/highlights-node.jpg)\n\n### Auto snap link to node slot\n\n*   **Default Value**: Enabled\n*   **Function**: Automatically snaps to available slots when dragging links to nodes, simplifies connection operations, automatically finds suitable input slots\n\n### Enable Tooltips\n\n*   **Default Value**: Enabled\n*   **Function**: Some node information will contain tooltips, including parameter descriptions, etc. When enabled, these tooltips will be displayed when hovering the mouse, as shown in the image below\n\n![Enable Tooltips](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/tooltips.jpg)\n\n### Tooltip Delay\n\n*   **Default Value**: 500\n*   **Function**: Controls the delay time for tooltips, in milliseconds. Setting to 0 means displaying tooltips immediately\n\n### Node life cycle badge mode\n\n*   **Default Value**: Show all\n*   **Function**: Controls the display of node lifecycle markers, showing node status information\n\n### Node ID badge mode\n\n*   **Default Value**: Show all\n*   **Function**: Controls the display of node ID markers, showing node unique identifiers\n\n![Node ID badge mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-id-badge.jpg)\n\n### Node source badge mode\n\n*   **Options**:\n    *   None\n    *   Hide built-in\n    *   Show all\n*   **Function**: Controls the display mode of node source markers, showing node source information. The corresponding display effect is shown in the image below. If show all is selected, it will display labels for both custom nodes and built-in nodes, making it convenient for you to determine the corresponding node source. The corresponding fox logo represents ComfyUI built-in nodes\n\n![Node source badge mode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-source-badge.jpg)\n\n### Double click node title to edit\n\n*   **Default Value**: Enabled\n*   **Function**: Controls whether you can double-click the node title to edit it, allowing users to rename nodes\n\n### Float widget rounding decimal places \\[0 = auto\\]\n\n*   **Default Value**: 0 (auto)\n*   **Range**: 0 - 6\n*   **Function**: Sets the decimal places for float widget rounding, 0 means auto, requires page reload\n\n### Disable default float widget rounding\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to disable default float widget rounding, requires page reload, cannot be disabled when the node backend has set rounding\n\n### Disable node widget sliders\n\n*   **Default Value**: Disabled\n*   **Function**: Controls whether to disable slider controls in node widgets, forcing text input instead of sliders\n\n### Preview image format\n\n*   **Default Value**: Empty string (use original format)\n*   **Function**: Sets the format for preview images in image widgets, converts to lightweight formats like webp, jpeg, etc.\n\n### Show width Ã— height below the image preview\n\n*   **Default Value**: Enabled\n*   **Function**: Displays width Ã— height information below image previews, showing image dimension information\n\n![Show width Ã— height below the image preview](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/show-size.jpg)\n\n## Pointer\n\n### Enable trackpad gestures\n\n*   **Default Value**: Enabled\n*   **Function**: This setting enables trackpad mode for the canvas, allowing two-finger pinch zoom and drag.\n\n### Double click interval (maximum)\n\n*   **Default Value**: 300\n*   **Function**: Maximum time (milliseconds) between two clicks of a double-click. Increasing this value helps solve issues where double-clicks are sometimes not recognized.\n\n### Pointer click drift delay\n\n*   **Default Value**: 150\n*   **Function**: Maximum time (milliseconds) to ignore pointer movement after pressing the pointer button. Helps prevent accidental mouse movement when clicking.\n\n### Pointer click drift (maximum distance)\n\n*   **Default Value**: 6\n*   **Function**: If the pointer moves more than this distance while holding the button, it is considered a drag (rather than a click). Helps prevent accidental mouse movement when clicking.\n\n## Reroute\n\n### Reroute spline offset\n\n*   **Default Value**: 20\n*   **Function**: Used to determine the smoothness of curves on both sides of reroute nodes. Larger values make curves smoother, smaller values make curves sharper.\n\n![Reroute spline offset](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/reroute-spline-offset.jpg)"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/moonvalley/moonvalley-video-generation",
  "markdown": "# Moonvalley API Node Official ComfyUI Example\n\nMoonvalley Marey Realism v1.5 is an AI video generation model designed for cinematic-level creation. The model is **trained entirely with commercially licensed content**, ensuring **copyright compliance and commercial safety**.\n\n## Product Highlights\n\n*   Exceptional prompt comprehension: Accurately interprets complex prompt instructions.\n*   Native 1080p HD quality: The training dataset is based on **1080P** videos, resulting in fine and detailed output.\n*   Realistic physics and dynamic performance: Precisely simulates physical motion models and natural dynamics, delivering professional-grade realism.\n*   Complex scene layering and advanced lighting effects: Supports foreground, midground, and background layering in complex scenes, with intelligent spatial relationship understanding.\n*   Production-level control features such as motion and pose transfer: Automatically generates realistic lighting for composite scenes.\n\nCurrently, Moonvalley-related API nodes are natively supported in ComfyUI. You can use the corresponding text-to-video, image-to-video, and video-to-video capabilities directly in ComfyUI.\n\n## Moonvalley Text-to-Video Workflow\n\n### 1\\. Download the Workflow File\n\n[\n\nDownload the workflow file in JSON format\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_text_to_video.json)\n\n### 2\\. Follow the Steps to Run the Workflow\n\n![Text-to-Video Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg)\n\n1.  Enter the positive prompt (content you want to appear in the video)\n2.  Enter the negative prompt (content you do not want to appear in the video)\n3.  Modify the video output resolution\n4.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to start video generation\n5.  After the API returns the result, you can view the generated video in the `Save Video` node. The video will also be saved in the `ComfyUI/output/` directory\n\n## Moonvalley Image-to-Video Workflow\n\n### 1\\. Download the Workflow File\n\n[\n\nDownload the workflow file in JSON format\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video.json)\n\nDownload the image below as the input image ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video_input.webp)\n\n### 2\\. Follow the Steps to Run the Workflow\n\n![Image-to-Video Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg)\n\n1.  Load the input image in the `Load Image` node\n2.  Enter the positive prompt (content you want to appear in the video)\n3.  Enter the negative prompt (content you do not want to appear in the video)\n4.  Modify the video output resolution\n5.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to start video generation\n6.  After the API returns the result, you can view the generated video in the `Save Video` node. The video will also be saved in the `ComfyUI/output/` directory\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/interface/shortcuts",
  "markdown": "# ComfyUI Keyboard Shortcuts and Custom Settings\n\nCurrently, ComfyUI supports custom keyboard shortcuts. You can set the shortcuts by clicking on `Settings (gear icon)` â€”> `Keybinding`. ![ComfyUI Shortcut Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/keybinding.jpg) In the corresponding menu, you can see all the current shortcut settings for ComfyUI. Click the `edit icon` before the corresponding command to customize the shortcut. Below is the current list of shortcuts for ComfyUI, which you can customize as needed.\n\n| Shortcut | Command |\n| --- | --- |\n| Ctrl + Enter | Queue prompt |\n| Ctrl + Shift + Enter | Queue prompt (Front) |\n| Ctrl + Alt + Enter | Interrupt |\n| Ctrl + Z / Ctrl + Y | Undo/Redo |\n| Ctrl + S | Save workflow |\n| Ctrl + O | Load workflow |\n| Ctrl + A | Select all nodes |\n| Alt + C | Collapse/uncollapse selected nodes |\n| Ctrl + M | Mute/unmute selected nodes |\n| Ctrl + B | Bypass/unbypass selected nodes |\n| Delete  <br>Backspace | Delete selected nodes |\n| Backspace | Clear workflow |\n| Space | Move canvas when holding and moving cursor |\n| Ctrl + Click  <br>Shift + Click | Add clicked node to selection |\n| Ctrl + C/Ctrl + V | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes) |\n| Ctrl + C/Ctrl + Shift + V | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |\n| Shift + Drag | Move multiple selected nodes at the same time |\n| Ctrl + G | Add frame to selected nodes |\n| Ctrl + , | Show settings dialog |\n| Alt + = | Zoom in (canvas) |\n| Alt + - | Zoom out (canvas) |\n| .   | Fit view to selected nodes |\n| P   | Pin/unpin selected items |\n| Q   | Toggle queue sidebar |\n| W   | Toggle workflow sidebar |\n| N   | Toggle node library sidebar |\n| M   | Toggle model library sidebar |\n| Ctrl + \\` | Toggle log bottom panel |\n| F   | Toggle focus mode (full screen) |\n| R   | Refresh node definitions |\n| Double-Click LMB | Quick search for nodes to add |"
},
{
  "url": "https://docs.comfy.org/interface/settings/extension",
  "markdown": "# Extension Settings - ComfyUI\n\nThe Extension settings panel is a special management panel in the ComfyUI frontend settings system, specifically used to manage the enable/disable status of frontend extension plugins. Unlike Custom Nodes, this panel is only used to manage frontend extensions registered by custom nodes, not to disable custom nodes themselves. ![extension](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/extension/extension.jpg) These frontend extension plugins are used to enhance the ComfyUI experience, such as providing shortcuts, settings, UI components, menu items, and other features. Extension status changes require a page reload to take effect:\n\n## Extension Settings Panel Features\n\n### 1\\. Extension List Management\n\nDisplays all registered extensions, including:\n\n*   Extension Name\n*   Core extension identification (displays â€œCoreâ€ label)\n*   Enable/disable status\n\n### 2\\. Search Functionality\n\nProvides a search box to quickly find specific extensions:\n\n### 3\\. Enable/Disable Control\n\nEach extension has an independent toggle switch:\n\n### 4\\. Batch Operations\n\nProvides right-click menu for batch operations:\n\n*   Enable All extensions\n*   Disable All extensions\n*   Disable 3rd Party extensions (keep core extensions)\n\n## Notes\n\n*   Extension status changes require a page reload to take effect\n*   Some core extensions cannot be disabled\n*   The system will automatically disable known problematic extensions\n*   Extension settings are automatically saved to the user configuration file\n\nThis Extension settings panel is essentially a â€œfrontend plugin managerâ€ that allows users to flexibly control ComfyUIâ€™s functional modules."
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter",
  "markdown": "# ComfyUI Depth T2I Adapter Usage Example\n\n## Introduction to T2I Adapter\n\n[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) is a lightweight adapter developed by [Tencent ARC Lab](https://github.com/TencentARC) designed to enhance the structural, color, and style control capabilities of text-to-image generation models (such as Stable Diffusion). It works by aligning external conditions (such as edge detection maps, depth maps, sketches, or color reference images) with the modelâ€™s internal features, achieving high-precision control without modifying the original model structure. With only about 77M parameters (approximately 300MB in size), its inference speed is about 3 times faster than [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly), and it supports multiple condition combinations (such as sketch + color grid). Application scenarios include line art to image conversion, color style transfer, multi-element scene generation, and more.\n\n### Comparison Between T2I Adapter and ControlNet\n\nAlthough their functions are similar, there are notable differences in implementation and application:\n\n1.  **Lightweight Design**: T2I Adapter has fewer parameters and a smaller memory footprint\n2.  **Inference Speed**: T2I Adapter is typically about 3 times faster than ControlNet\n3.  **Control Precision**: ControlNet offers more precise control in certain scenarios, while T2I Adapter is more suitable for lightweight control\n4.  **Multi-condition Combination**: T2I Adapter shows more significant resource advantages when combining multiple conditions\n\n### Main Types of T2I Adapter\n\nT2I Adapter provides various types to control different aspects:\n\n*   **Depth**: Controls the spatial structure and depth relationships in images\n*   **Line Art (Canny/Sketch)**: Controls image edges and lines\n*   **Keypose**: Controls character poses and actions\n*   **Segmentation (Seg)**: Controls scene layout through semantic segmentation\n*   **Color**: Controls the overall color scheme of images\n\nIn ComfyUI, using T2I Adapter is similar to [ControlNet](https://docs.comfy.org/tutorials/controlnet/controlnet) in terms of interface and workflow. In this example, we will demonstrate how to use a depth T2I Adapter to control an interior scene. ![ComfyUI Depth T2I Adapter Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)\n\n## Value of Depth T2I Adapter Applications\n\nDepth maps have several important applications in image generation:\n\n1.  **Spatial Layout Control**: Accurately describes three-dimensional spatial structures, suitable for interior design and architectural visualization\n2.  **Object Positioning**: Controls the relative position and size of objects in a scene, suitable for product showcases and scene construction\n3.  **Perspective Relationships**: Maintains reasonable perspective and proportions, suitable for landscape and urban scene generation\n4.  **Light and Shadow Layout**: Natural light and shadow distribution based on depth information, enhancing realism\n\nWe will use interior design as an example to demonstrate how to use the depth T2I Adapter, but these techniques are applicable to other scenarios as well.\n\n## ComfyUI Depth T2I Adapter Workflow Example Explanation\n\n### 1\\. Depth T2I Adapter Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Depth T2I Adapter](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)\n\nPlease download the image below, which we will use as input: ![ComfyUI Interior Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)\n\n### 2\\. Model Installation\n\n*   [interiordesignsuperm\\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [t2iadapter\\_depth\\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ interiordesignsuperm_v2.safetensors\nâ”‚   â””â”€â”€ controlnet/\nâ”‚       â””â”€â”€ t2iadapter_depth_sd15v2.pth\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - Depth T2I Adapter Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg)\n\n1.  Ensure that `Load Checkpoint` can load **interiordesignsuperm\\_v2.safetensors**\n2.  Ensure that `Load ControlNet` can load **t2iadapter\\_depth\\_sd15v2.pth**\n3.  Click `Upload` in the `Load Image` node to upload the input image provided earlier\n4.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n## General Tips for Using T2I Adapter\n\n### Input Image Quality Optimization\n\nRegardless of the application scenario, high-quality input images are key to successfully using T2I Adapter:\n\n1.  **Moderate Contrast**: Control images (such as depth maps, line art) should have clear contrast, but not excessively extreme\n2.  **Clear Boundaries**: Ensure that major structures and element boundaries are clearly distinguishable in the control image\n3.  **Noise Control**: Try to avoid excessive noise in control images, especially for depth maps and line art\n4.  **Reasonable Layout**: Control images should have a reasonable spatial layout and element distribution\n\n## Characteristics of T2I Adapter Usage\n\nOne major advantage of T2I Adapter is its ability to easily combine multiple conditions for complex control effects:\n\n1.  **Depth + Edge**: Control spatial layout while maintaining clear structural edges, suitable for architecture and interior design\n2.  **Line Art + Color**: Control shapes while specifying color schemes, suitable for character design and illustrations\n3.  **Pose + Segmentation**: Control character actions while defining scene areas, suitable for complex narrative scenes\n\nMixing different T2I Adapters, or combining them with other control methods (such as ControlNet, regional prompts, etc.), can further expand creative possibilities. To achieve mixing, simply chain multiple `Apply ControlNet` nodes together in the same way as described in [Mixing ControlNet](https://docs.comfy.org/tutorials/controlnet/mixing-controlnets)."
},
{
  "url": "https://docs.comfy.org/interface/settings/about",
  "markdown": "# About Page - ComfyUI\n\nThe About page is an information display panel in the ComfyUI settings system, used to show application version information, related links, and system statistics. These settings can provide us with critical information when you submit feedback or report issues. ![about](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/settings-about.jpg)\n\n### Version Information Badges\n\nThe About page displays the following core version information:\n\n*   **ComfyUI Version**: Shows the backend ComfyUI version number, linked to the official GitHub repository\n*   **ComfyUI\\_frontend Version**: Shows the frontend interface version number, linked to the frontend GitHub repository\n*   **Discord Community**: Provides a link to the ComfyOrg Discord server\n*   **Official Website**: Links to the ComfyOrg official website\n\n### Custom Node Badges\n\nIf custom nodes are installed, the About page will also display additional badge information provided by custom nodes. These badges are registered by each custom node through the `aboutPageBadges` property.\n\n### System Info\n\nThe bottom of the page displays detailed system statistics, including:\n\n*   Hardware configuration information\n*   Software environment information\n*   System performance data\n\n## Extension Developer Guide\n\nExtension developers can add custom badges to the About page by adding the `aboutPageBadges` property to their extension configuration:\n\n```\napp.registerExtension({\n  name: 'MyExtension',\n  aboutPageBadges: [\n    {\n      label: 'My Extension v1.0.0',\n      url: 'https://github.com/myuser/myextension',\n      icon: 'pi pi-github'\n    }\n  ]\n})\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/depth-controlnet",
  "markdown": "# ComfyUI Depth ControlNet Usage Example\n\n## Introduction to Depth Maps and Depth ControlNet\n\nA depth map is a special type of image that uses grayscale values to represent the distance between objects in a scene and the observer or camera. In a depth map, the grayscale value is inversely proportional to distance: brighter areas (closer to white) indicate objects that are closer, while darker areas (closer to black) indicate objects that are farther away. ![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png) Depth ControlNet is a ControlNet model specifically trained to understand and utilize depth map information. It helps AI correctly interpret spatial relationships, ensuring that generated images conform to the spatial structure specified by the depth map, thereby enabling precise control over three-dimensional spatial layouts.\n\n### Application Scenarios for Depth Maps with ControlNet\n\nDepth maps have numerous applications in various scenarios:\n\n1.  **Portrait Scenes**: Control the spatial relationship between subjects and backgrounds, avoiding distortion in critical areas such as faces\n2.  **Landscape Scenes**: Control the hierarchical relationships between foreground, middle ground, and background\n3.  **Architectural Scenes**: Control the spatial structure and perspective relationships of buildings\n4.  **Product Showcase**: Control the separation and spatial positioning of products against their backgrounds\n\nIn this example, we will use a depth map to generate an architectural visualization scene.\n\n## ComfyUI ControlNet Workflow Example Explanation\n\n### 1\\. ControlNet Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![Depth Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_controlnet.png)\n\nPlease download the image below, which we will use as input: ![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)\n\n### 2\\. Model Installation\n\n*   [architecturerealmix\\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [control\\_v11f1p\\_sd15\\_depth\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ architecturerealmix_v11.safetensors\nâ”‚   â””â”€â”€ controlnet/\nâ”‚       â””â”€â”€ control_v11f1p_sd15_depth_fp16.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - Depth ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth.jpg)\n\n1.  Ensure that `Load Checkpoint` can load **architecturerealmix\\_v11.safetensors**\n2.  Ensure that `Load ControlNet` can load **control\\_v11f1p\\_sd15\\_depth\\_fp16.safetensors**\n3.  Click `Upload` in the `Load Image` node to upload the depth image provided earlier\n4.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n## Combining Depth Control with Other Techniques\n\nBased on different creative needs, you can combine Depth ControlNet with other types of ControlNet to achieve better results:\n\n1.  **Depth + Lineart**: Maintain spatial relationships while reinforcing outlines, suitable for architecture, products, and character design\n2.  **Depth + Pose**: Control character posture while maintaining correct spatial relationships, suitable for character scenes\n\nFor more information on using multiple ControlNet models together, please refer to the [Mixing ControlNet](https://docs.comfy.org/tutorials/controlnet/mixing-controlnets) example."
},
{
  "url": "https://docs.comfy.org/interface/appearance",
  "markdown": "# Customizing ComfyUI Appearance - ComfyUI\n\nComfyUI offers flexible appearance customization options that allow you to personalize the interface to your preferences.\n\n## Color Palette System\n\nThe primary way to customize ComfyUIâ€™s appearance is through the built-in color palette system. ![Color Palette](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/color-palette.jpg) This allows you to:\n\n1.  Switch ComfyUI themes\n2.  Export the currently selected theme as JSON format\n3.  Load custom theme configuration from JSON file\n4.  Delete custom theme configuration\n\n### How to Customize Color Themes\n\nThe color palette allows you to modify many specific properties. Here are some of the most commonly customized elements, with colors represented in hexadecimal format:\n\n```\n{\n  \"id\": \"dark\",                     // Must be unique, cannot be the same as other theme IDs\n  \"name\": \"Dark (Default)\",         // Theme name, displayed in theme selector\n  \"colors\": {\n    \"node_slot\": {                  // Node connection slot color configuration\n      \"CLIP\": \"#FFD500\",            // CLIP model connection slot color\n      \"CLIP_VISION\": \"#A8DADC\",     // CLIP Vision model connection slot color\n      \"CLIP_VISION_OUTPUT\": \"#ad7452\", // CLIP Vision output connection slot color\n      \"CONDITIONING\": \"#FFA931\",     // Conditioning control connection slot color\n      \"CONTROL_NET\": \"#6EE7B7\",     // ControlNet model connection slot color\n      \"IMAGE\": \"#64B5F6\",           // Image data connection slot color\n      \"LATENT\": \"#FF9CF9\",          // Latent space connection slot color\n      \"MASK\": \"#81C784\",            // Mask data connection slot color\n      \"MODEL\": \"#B39DDB\",           // Model connection slot color\n      \"STYLE_MODEL\": \"#C2FFAE\",     // Style model connection slot color\n      \"VAE\": \"#FF6E6E\",             // VAE model connection slot color\n      \"NOISE\": \"#B0B0B0\",           // Noise data connection slot color\n      \"GUIDER\": \"#66FFFF\",          // Guider connection slot color\n      \"SAMPLER\": \"#ECB4B4\",         // Sampler connection slot color\n      \"SIGMAS\": \"#CDFFCD\",          // Sigmas data connection slot color\n      \"TAESD\": \"#DCC274\"            // TAESD model connection slot color\n    },\n    \"litegraph_base\": {             // LiteGraph base interface configuration\n      \"BACKGROUND_IMAGE\": \"\",        // Background image, default is empty\n      \"CLEAR_BACKGROUND_COLOR\": \"#222\", // Main canvas background color\n      \"NODE_TITLE_COLOR\": \"#999\",    // Node title text color\n      \"NODE_SELECTED_TITLE_COLOR\": \"#FFF\", // Selected node title color\n      \"NODE_TEXT_SIZE\": 14,          // Node text size\n      \"NODE_TEXT_COLOR\": \"#AAA\",     // Node text color\n      \"NODE_TEXT_HIGHLIGHT_COLOR\": \"#FFF\", // Node text highlight color\n      \"NODE_SUBTEXT_SIZE\": 12,       // Node subtext size\n      \"NODE_DEFAULT_COLOR\": \"#333\",   // Node default color\n      \"NODE_DEFAULT_BGCOLOR\": \"#353535\", // Node default background color\n      \"NODE_DEFAULT_BOXCOLOR\": \"#666\", // Node default border color\n      \"NODE_DEFAULT_SHAPE\": 2,        // Node default shape\n      \"NODE_BOX_OUTLINE_COLOR\": \"#FFF\", // Node border outline color\n      \"NODE_BYPASS_BGCOLOR\": \"#FF00FF\", // Node bypass background color\n      \"NODE_ERROR_COLOUR\": \"#E00\",    // Node error state color\n      \"DEFAULT_SHADOW_COLOR\": \"rgba(0,0,0,0.5)\", // Default shadow color\n      \"DEFAULT_GROUP_FONT\": 24,       // Group default font size\n      \"WIDGET_BGCOLOR\": \"#222\",       // Widget background color\n      \"WIDGET_OUTLINE_COLOR\": \"#666\", // Widget outline color\n      \"WIDGET_TEXT_COLOR\": \"#DDD\",    // Widget text color\n      \"WIDGET_SECONDARY_TEXT_COLOR\": \"#999\", // Widget secondary text color\n      \"WIDGET_DISABLED_TEXT_COLOR\": \"#666\", // Widget disabled state text color\n      \"LINK_COLOR\": \"#9A9\",          // Connection line color\n      \"EVENT_LINK_COLOR\": \"#A86\",    // Event connection line color\n      \"CONNECTING_LINK_COLOR\": \"#AFA\", // Connecting line color\n      \"BADGE_FG_COLOR\": \"#FFF\",      // Badge foreground color\n      \"BADGE_BG_COLOR\": \"#0F1F0F\"    // Badge background color\n    },\n    \"comfy_base\": {                  // ComfyUI base interface configuration\n      \"fg-color\": \"#fff\",            // Foreground color\n      \"bg-color\": \"#202020\",         // Background color\n      \"comfy-menu-bg\": \"#353535\",    // Menu background color\n      \"comfy-menu-secondary-bg\": \"#303030\", // Secondary menu background color\n      \"comfy-input-bg\": \"#222\",      // Input field background color\n      \"input-text\": \"#ddd\",          // Input text color\n      \"descrip-text\": \"#999\",        // Description text color\n      \"drag-text\": \"#ccc\",           // Drag text color\n      \"error-text\": \"#ff4444\",       // Error text color\n      \"border-color\": \"#4e4e4e\",     // Border color\n      \"tr-even-bg-color\": \"#222\",    // Table even row background color\n      \"tr-odd-bg-color\": \"#353535\",  // Table odd row background color\n      \"content-bg\": \"#4e4e4e\",       // Content area background color\n      \"content-fg\": \"#fff\",          // Content area foreground color\n      \"content-hover-bg\": \"#222\",    // Content area hover background color\n      \"content-hover-fg\": \"#fff\",    // Content area hover foreground color\n      \"bar-shadow\": \"rgba(16, 16, 16, 0.5) 0 0 0.5rem\" // Bar shadow effect\n    }\n  }\n}\n```\n\n## Canvas\n\n### Background Image\n\n*   **Requirements**: ComfyUI frontend version 1.20.5 or newer\n*   **Function**: Set a custom background image for the canvas to provide a more personalized workspace. You can upload images or use web images to set the background for the canvas.\n\n![Set Background Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/set-as-bg.jpg)\n\n## Node\n\n### Node Opacity\n\n*   **Function**: Set the opacity of nodes, where 0 represents completely transparent and 1 represents completely opaque.\n\n![Node Opacity](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/node-opacity.jpg)\n\n### Textarea Widget Font Size\n\n*   **Range**: 8 - 24\n*   **Function**: Set the font size in textarea widgets. Adjusts the text display size in text input boxes to improve readability.\n\n![Textarea Widget Font Size](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/textarea-font-size.jpg)\n\n*   **Function**: When enabled, the sidebar width will be unified to a consistent width when switching between different sidebars. If disabled, different sidebars can maintain their custom widths when switching.\n\n*   **Function**: Control the size of the sidebar, can be set to normal or small.\n\n*   **Function**: Control whether the sidebar is displayed on the left or right side of the interface, allowing users to adjust the sidebar position according to their usage habits.\n\n## Tree Explorer\n\n### Tree Explorer Item Padding\n\n*   **Function**: Set the padding of items in the tree explorer (sidebar panel), adjusting the spacing between items in the tree structure.\n\n![Tree Explorer Item Padding](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/padding.jpg)\n\n## Advanced Customization with user.css\n\nFor cases where the color palette doesnâ€™t provide enough control, you can use custom CSS via a user.css file. This method is recommended for advanced users who need to customize elements that arenâ€™t available in the color palette system.\n\n### Requirements\n\n*   ComfyUI frontend version 1.20.5 or newer\n\n### Setting Up user.css\n\n1.  Create a file named `user.css` in your ComfyUI user directory (same location as your workflows and settings - see location details below)\n2.  Add your custom CSS rules to this file\n3.  Restart ComfyUI or refresh the page to apply changes\n\n### User Directory Location\n\nThe ComfyUI user directory is where your personal settings, workflows, and customizations are stored. The location depends on your installation type:\n\n```\nC:\\Users\\<your username>\\AppData\\Roaming\\ComfyUI\\user\n```\n\nThis location contains your workflows, settings, and other user-specific files. After finding the above folder location, please copy the corresponding CSS file to the corresponding user directory, such as the default user folder being `ComfyUI/user/default`, then restart ComfyUI or refresh the page to apply changes.\n\n### user.css Examples and Related Instructions\n\nThe `user.css` file is loaded early in the application startup process. So you may need to use `!important` in your CSS rules to ensure they override the default styles. **user.css Customization Examples**\n\n```\n/* Increase font size in inputs and menus for better readability */\n.comfy-multiline-input, .litecontextmenu .litemenu-entry {\n    font-size: 20px !important;\n}\n\n/* Make context menu entries larger for easier selection */\n.litegraph .litemenu-entry,\n.litemenu-title {\n  font-size: 24px !important; \n}\n\n/* Custom styling for specific elements not available in the color palette */\n.comfy-menu {\n    border: 1px solid rgb(126, 179, 189) !important;\n    border-radius: 0px 0px 0px 10px !important;\n    backdrop-filter: blur(2px);\n}\n```\n\n**Best Practices**\n\n1.  **Start with the color palette** for most customizations\n2.  Use **user.css only when necessary** for elements not covered by the color palette\n3.  **Export your theme** before making significant changes so you can revert if needed\n4.  **Share your themes** with the community to inspire others\n\n**Troubleshooting**\n\n*   If your color palette changes donâ€™t appear, try refreshing the page\n*   If CSS customizations donâ€™t work, check that youâ€™re using frontend version 1.20.5+\n*   Try adding `!important` to user.css rules that arenâ€™t being applied\n*   Keep backups of your customizations to easily restore them\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass",
  "markdown": "# ComfyUI Pose ControlNet Usage Example\n\n## Introduction to OpenPose\n\n[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is an open-source real-time multi-person pose estimation system developed by Carnegie Mellon University (CMU), representing a significant breakthrough in the field of computer vision. The system can simultaneously detect multiple people in an image, capturing:\n\n*   **Body skeleton**: 18 keypoints, including head, shoulders, elbows, wrists, hips, knees, and ankles\n*   **Facial expressions**: 70 facial keypoints for capturing micro-expressions and facial contours\n*   **Hand details**: 21 hand keypoints for precisely expressing finger positions and gestures\n*   **Foot posture**: 6 foot keypoints, recording standing postures and movement details\n\n![OpenPose Example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/openpose_example.jpg) In AI image generation, skeleton structure maps generated by OpenPose serve as conditional inputs for ControlNet, enabling precise control over the posture, actions, and expressions of generated characters. This allows us to generate realistic human figures with expected poses and actions, greatly improving the controllability and practical value of AI-generated content. Particularly for early Stable Diffusion 1.5 series models, skeletal maps generated by OpenPose can effectively prevent issues with distorted character actions, limbs, and expressions.\n\n### 1\\. Pose ControlNet Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)\n\nPlease download the image below, which we will use as input: ![ComfyUI Pose Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)\n\n### 2\\. Manual Model Installation\n\n*   [control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)\n*   [majicmixRealistic\\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model&format=SafeTensor&size=pruned&fp=fp16)\n*   [japaneseStyleRealistic\\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model&format=SafeTensor&size=pruned&fp=fp16)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ majicmixRealistic_v7.safetensors\nâ”‚   â”‚   â””â”€â”€ japaneseStyleRealistic_v20.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ vae-ft-mse-840000-ema-pruned.safetensors\nâ”‚   â””â”€â”€ controlnet/\nâ”‚       â””â”€â”€ control_v11p_sd15_openpose_fp16.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - Pose ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg) Follow these steps according to the numbered markers in the image:\n\n1.  Ensure that `Load Checkpoint` can load **majicmixRealistic\\_v7.safetensors**\n2.  Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**\n3.  Ensure that `Load ControlNet Model` can load **control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors**\n4.  Click the select button in the `Load Image` node to upload the pose input image provided earlier, or use your own OpenPose skeleton map\n5.  Ensure that `Load Checkpoint` can load **japaneseStyleRealistic\\_v20.safetensors**\n6.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n## Explanation of the Pose ControlNet 2-Pass Workflow\n\nThis workflow uses a two-pass image generation approach, dividing the image creation process into two phases:\n\n### First Phase: Basic Pose Image Generation\n\nIn the first phase, the **majicmixRealistic\\_v7** model is combined with Pose ControlNet to generate an initial character pose image:\n\n1.  First, load the majicmixRealistic\\_v7 model via the `Load Checkpoint` node\n2.  Load the pose control model through the `Load ControlNet Model` node\n3.  The input pose image is fed into the `Apply ControlNet` node and combined with positive and negative prompt conditions\n4.  The first `KSampler` node (typically using 20-30 steps) generates a basic character pose image\n5.  The pixel-space image for the first phase is obtained through `VAE Decode`\n\nThis phase primarily focuses on correct character posture, pose, and basic structure, ensuring that the generated character conforms to the input skeletal pose.\n\n### Second Phase: Style Optimization and Detail Enhancement\n\nIn the second phase, the output image from the first phase is used as a reference, with the **japaneseStyleRealistic\\_v20** model performing stylization and detail enhancement:\n\n1.  The image generated in the first phase creates a larger resolution latent space through the `Upscale latent` node\n2.  The second `Load Checkpoint` loads the japaneseStyleRealistic\\_v20 model, which focuses on details and style\n3.  The second `KSampler` node uses a lower `denoise` strength (typically 0.4-0.6) for refinement, preserving the basic structure from the first phase\n4.  Finally, a higher quality, larger resolution image is output through the second `VAE Decode` and `Save Image` nodes\n\nThis phase primarily focuses on style consistency, detail richness, and enhancing overall image quality.\n\n## Advantages of 2-Pass Image Generation\n\nCompared to single-pass generation, the two-pass image generation method offers the following advantages:\n\n1.  **Higher Resolution**: Two-pass processing can generate high-resolution images beyond the capabilities of single-pass generation\n2.  **Style Blending**: Can combine advantages of different models, such as using a realistic model in the first phase and a stylized model in the second phase\n3.  **Better Details**: The second phase can focus on optimizing details without having to worry about overall structure\n4.  **Precise Control**: Once pose control is completed in the first phase, the second phase can focus on refining style and details\n5.  **Reduced GPU Load**: Generating in two passes allows for high-quality large images with limited GPU resources"
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/mixing-controlnets",
  "markdown": "# ComfyUI Mixing ControlNet Examples - ComfyUI\n\nIn AI image generation, a single control condition often fails to meet the requirements of complex scenes. Mixing multiple ControlNets allows you to control different regions or aspects of an image simultaneously, achieving more precise control over image generation. In certain scenarios, mixing ControlNets can leverage the characteristics of different control conditions to achieve more refined conditional control:\n\n1.  **Scene Complexity**: Complex scenes require multiple control conditions working together\n2.  **Fine-grained Control**: By adjusting the strength parameter of each ControlNet, you can precisely control the degree of influence for each part\n3.  **Complementary Effects**: Different types of ControlNets can complement each other, compensating for the limitations of single controls\n4.  **Creative Expression**: Combining different controls can produce unique creative effects\n\n### How to Mix ControlNets\n\nWhen mixing multiple ControlNets, each ControlNet influences the image generation process according to its applied area. ComfyUI enables multiple ControlNet conditions to be applied sequentially in a layered manner through chain connections in the `Apply ControlNet` node: ![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)\n\n## ComfyUI ControlNet Regional Division Mixing Example\n\nIn this example, we will use a combination of **Pose ControlNet** and **Scribble ControlNet** to generate a scene containing multiple elements: a character on the left controlled by Pose ControlNet and a cat on a scooter on the right controlled by Scribble ControlNet.\n\n### 1\\. ControlNet Mixing Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Mixing ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets.png) \n\nInput pose image (controls the character pose on the left): ![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input.png) Input scribble image (controls the cat and scooter on the right): ![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input_scribble.png)\n\n### 2\\. Manual Model Installation\n\n*   [awpainting\\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)\n*   [control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ awpainting_v14.safetensors\nâ”‚   â”œâ”€â”€ controlnet/\nâ”‚   â”‚   â””â”€â”€ control_v11p_sd15_scribble_fp16.safetensors\nâ”‚   â”‚   â””â”€â”€ control_v11p_sd15_openpose_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ vae-ft-mse-840000-ema-pruned.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - Mixing ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg) Follow these steps according to the numbered markers in the image:\n\n1.  Ensure that `Load Checkpoint` can load **awpainting\\_v14.safetensors**\n2.  Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**\n\nFirst ControlNet group using the Openpose model: 3. Ensure that `Load ControlNet Model` loads **control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors** 4. Click `Upload` in the `Load Image` node to upload the pose image provided earlier Second ControlNet group using the Scribble model: 5. Ensure that `Load ControlNet Model` loads **control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors** 6. Click `Upload` in the `Load Image` node to upload the scribble image provided earlier 7. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n## Workflow Explanation\n\n#### Strength Balance\n\nWhen controlling different regions of an image, balancing the strength parameters is particularly important:\n\n*   If the ControlNet strength for one region is significantly higher than another, it may cause that regionâ€™s control effect to overpower and suppress the other region\n*   Itâ€™s recommended to set similar strength values for ControlNets controlling different regions, for example, both set to 1.0\n\n#### Prompt Techniques\n\nFor regional division mixing, the prompt needs to include descriptions of both regions:\n\n```\n\"A woman in red dress, a cat riding a scooter, detailed background, high quality\"\n```\n\nSuch a prompt covers both the character and the cat on the scooter, ensuring the model pays attention to both control regions.\n\n## Multi-dimensional Control Applications for a Single Subject\n\nIn addition to the regional division mixing shown in this example, another common mixing approach is to apply multi-dimensional control to the same subject. For example:\n\n*   **Pose + Depth**: Control character posture and spatial sense\n*   **Pose + Canny**: Control character posture and edge details\n*   **Pose + Reference**: Control character posture while referencing a specific style\n\nIn this type of application, reference images for multiple ControlNets should be aligned to the same subject, and their strengths should be adjusted to ensure proper balance. By combining different types of ControlNets and specifying their control regions, you can achieve precise control over elements in your image."
},
{
  "url": "https://docs.comfy.org/tutorials/flux/flux-1-controlnet",
  "markdown": "# ComfyUI Flux.1 ControlNet Examples - ComfyUI\n\n ![Flux.1 Canny Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-canny-controlnet.png) ![Flux.1 Depth Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-depth-controlnet.png)\n\n## FLUX.1 ControlNet Model Introduction\n\nFLUX.1 Canny and Depth are two powerful models from the [FLUX.1 Tools](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/). This toolkit is designed to add control and guidance capabilities to FLUX.1, enabling users to modify and recreate real or generated images. **FLUX.1-Depth-dev** and **FLUX.1-Canny-dev** are both 12B parameter Rectified Flow Transformer models that can generate images based on text descriptions while maintaining the structural features of the input image. The Depth version maintains the spatial structure of the source image through depth map extraction techniques, while the Canny version uses edge detection techniques to preserve the structural features of the source image, allowing users to choose the appropriate control method based on different needs. Both models have the following features:\n\n*   Top-tier output quality and detail representation\n*   Excellent prompt following ability while maintaining consistency with the original image\n*   Trained using guided distillation techniques for improved efficiency\n*   Open weights for the research community\n*   API interfaces (pro version) and open-source weights (dev version)\n\nAdditionally, Black Forest Labs also provides **FLUX.1-Depth-dev-lora** and **FLUX.1-Canny-dev-lora** adapter versions extracted from the complete models. These can be applied to the FLUX.1 \\[dev\\] base model to provide similar functionality with smaller file size, especially suitable for resource-constrained environments. We will use the full version of **FLUX.1-Canny-dev** and **FLUX.1-Depth-dev-lora** to complete the workflow examples.\n\n## FLUX.1-Canny-dev Complete Version Workflow\n\n### 1\\. Workflow and Asset\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow ![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev.png) Please download the image below, which we will use as the input image ![ComfyUI Flux.1 Canny Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev-input.png)\n\n### 2\\. Manual Models Installation\n\nComplete model list:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) (Please ensure you have agreed to the corresponding repoâ€™s terms)\n\nFile storage location:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ t5xxl_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ flux1-canny-dev.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Flux.1 Canny Controlnet Step Process](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg)\n\n1.  Make sure `ae.safetensors` is loaded in the `Load VAE` node\n2.  Make sure `flux1-canny-dev.safetensors` is loaded in the `Load Diffusion Model` node\n3.  Make sure the following models are loaded in the `DualCLIPLoader` node:\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n4.  Upload the provided input image in the `Load Image` node\n5.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n### 4\\. Start Your Experimentation\n\nTry using the [FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) model to complete the Depth version of the workflow You can use the image below as input ![ComfyUI Indoor Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png) Or use the following custom nodes to complete image preprocessing:\n\n*   [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n*   [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n## FLUX.1-Depth-dev-lora Workflow\n\nThe LoRA version workflow builds on the complete version by adding the LoRA model. Compared to the [complete version of the Flux workflow](https://docs.comfy.org/tutorials/flux/flux-1-text-to-image), it adds nodes for loading and using the corresponding LoRA model.\n\n### 1\\. Workflow and Asset\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow ![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora.png) Please download the image below, which we will use as the input image ![ComfyUI Flux.1 Depth Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora-input.png)\n\n### 2\\. Manual Model Download\n\nComplete model list:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)\n*   [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)\n\nFile storage location:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ t5xxl_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â”œâ”€â”€ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ flux1-dev.safetensors\nâ”‚   â””â”€â”€ loras/\nâ”‚       â””â”€â”€ flux1-depth-dev-lora.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Flux.1 Depth Controlnet Step Process](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg)\n\n1.  Make sure `flux1-dev.safetensors` is loaded in the `Load Diffusion Model` node\n2.  Make sure `flux1-depth-dev-lora.safetensors` is loaded in the `LoraLoaderModelOnly` node\n3.  Make sure the following models are loaded in the `DualCLIPLoader` node:\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n4.  Upload the provided input image in the `Load Image` node\n5.  Make sure `ae.safetensors` is loaded in the `Load VAE` node\n6.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n### 4\\. Start Your Experimentation\n\nTry using the [FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) model to complete the Canny version of the workflow Use [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) or [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to complete image preprocessing\n\nXLab and InstantX + Shakker Labs have released Controlnets for Flux. **InstantX:**\n\n*   [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)\n*   [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)\n*   [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)\n\n**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections) Place these files in the `ComfyUI/models/controlnet` directory. You can visit [Flux Controlnet Example](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png) to get the corresponding workflow image, and use the image from [here](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png) as the input image."
},
{
  "url": "https://docs.comfy.org/tutorials/flux/flux-1-fill-dev",
  "markdown": "# ComfyUI Flux.1 fill dev Example\n\n![Flux.1 fill dev](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-fill-dev-demo.jpeg)\n\n## Introduction to Flux.1 fill dev Model\n\nFlux.1 fill dev is one of the core tools in the [FLUX.1 Tools suite](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/), specifically designed for image inpainting and outpainting. Key features of Flux.1 fill dev:\n\n*   Powerful image inpainting and outpainting capabilities, with results second only to the commercial version FLUX.1 Fill \\[pro\\].\n*   Excellent prompt understanding and following ability, precisely capturing user intent while maintaining high consistency with the original image.\n*   Advanced guided distillation training technology, making the model more efficient while maintaining high-quality output.\n*   Friendly licensing terms, with generated outputs usable for personal, scientific, and commercial purposes, please refer to the [FLUX.1 \\[dev\\] non-commercial license](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) for details.\n\nOpen Source Repository: [FLUX.1 \\[dev\\]](https://huggingface.co/black-forest-labs/FLUX.1-dev) This guide will demonstrate inpainting and outpainting workflows based on the Flux.1 fill dev model. If youâ€™re not familiar with inpainting and outpainting workflows, you can refer to [ComfyUI Layout Inpainting Example](https://docs.comfy.org/tutorials/basic/inpaint) and [ComfyUI Image Extension Example](https://docs.comfy.org/tutorials/basic/outpaint) for some related explanations.\n\nBefore we begin, letâ€™s complete the installation of the Flux.1 Fill dev model files. The inpainting and outpainting workflows will use exactly the same model files. If youâ€™ve previously used the full version of the [Flux.1 Text-to-Image workflow](https://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-fill-dev.safetensors** model file in this section. However, since downloading the corresponding model requires agreeing to the corresponding usage agreement, please visit the [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) page and make sure you have agreed to the corresponding agreement as shown in the image below. ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_fill_dev_agreement.jpg) Complete model list:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)\n\nFile storage location:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚    â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚    â””â”€â”€ t5xxl_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚    â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚        â””â”€â”€ flux1-fill-dev.safetensors\n```\n\n## Flux.1 Fill dev inpainting workflow\n\n### 1\\. Inpainting workflow and asset\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![ComfyUI Flux.1 inpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint.png) Please download the image below, we will use it as the input image ![ComfyUI Flux.1 inpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input.png) \n\n### 2\\. Steps to run the workflow\n\n![ComfyUI Flux.1 Fill dev Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_inpaint.jpg)\n\n1.  Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.\n2.  Ensure the `DualCLIPLoader` node has the following models loaded:\n    *   clip\\_name1: `t5xxl_fp16.safetensors`\n    *   clip\\_name2: `clip_l.safetensors`\n3.  Ensure the `Load VAE` node has `ae.safetensors` loaded.\n4.  Upload the input image provided in the document to the `Load Image` node; if youâ€™re using the version without a mask, remember to complete the mask drawing using the mask editor\n5.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Flux.1 Fill dev Outpainting Workflow\n\n### 1\\. Outpainting workflow and asset\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![ComfyUI Flux.1 outpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint.png) Please download the image below, we will use it as the input image ![ComfyUI Flux.1 outpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint_input.png) \n\n### 2\\. Steps to run the workflow\n\n![ComfyUI Flux.1 Fill dev Outpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_outpaint.jpg)\n\n1.  Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.\n2.  Ensure the `DualCLIPLoader` node has the following models loaded:\n    *   clip\\_name1: `t5xxl_fp16.safetensors`\n    *   clip\\_name2: `clip_l.safetensors`\n3.  Ensure the `Load VAE` node has `ae.safetensors` loaded.\n4.  Upload the input image provided in the document to the `Load Image` node\n5.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow"
},
{
  "url": "https://docs.comfy.org/interface/settings/mask-editor",
  "markdown": "# ComfyUI Mask Editor Settings - ComfyUI\n\n## Brush Adjustment\n\n### Brush adjustment speed multiplier\n\n*   **Function**: Controls the speed of brush size and hardness changes during adjustment\n*   **Description**: Higher values mean faster changes\n\n### Lock brush adjustment to dominant axis\n\n*   **Function**: When enabled, brush adjustment will only affect size or hardness based on the direction you move\n*   **Description**: This feature allows users to more precisely control brush property adjustments\n\n## New Editor\n\n### Use new mask editor\n\n*   **Function**: Switch to the new brush editor interface\n*   **Description**: Allows users to switch between new and old editor interfaces\n\nThe new version has a better UI interface and interaction, with more complete functionality ![new](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/maskeditor/new-mask-editor.jpg) \n\n#### 0 reactions\n\nOn this page\n\n*   [Brush Adjustment](#brush-adjustment)\n*   [Brush adjustment speed multiplier](#brush-adjustment-speed-multiplier)\n*   [Lock brush adjustment to dominant axis](#lock-brush-adjustment-to-dominant-axis)\n*   [New Editor](#new-editor)\n*   [Use new mask editor](#use-new-mask-editor)"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/overview",
  "markdown": "# API Nodes - ComfyUI\n\nAPI Nodes are ComfyUIâ€™s new way of calling closed-source models through API requests, providing ComfyUI users with access to external state-of-the-art AI models without complex API key setup.\n\n## What are API Nodes?\n\nAPI Nodes are a set of special nodes that connect to external API services, allowing you to use closed-source or third-party hosted AI models directly in your ComfyUI workflows. These nodes are designed to seamlessly integrate the capabilities of external models while maintaining the open-source nature of ComfyUIâ€™s core. Currently supported models include:\n\n*   **Black Forest Labs**: Flux 1.1\\[pro\\] Ultra, Flux .1\\[pro\\], Flux .1 Kontext Pro, Flux .1 Kontext Max\n*   **Google**: Veo2, Gemini 2.5 Pro, Gemini 2.5 Flash\n*   **Ideogram**: V3, V2, V1\n*   **Kling**: 2.0, 1.6, 1.5 & Various Effects\n*   **Luma**: Photon, Ray2, Ray1.6\n*   **MiniMax**: Text-to-Video, Image-to-Video\n*   **OpenAI**: o1, o1-pro, o3, gpt-4o, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, DALLÂ·E 2, DALLÂ·E 3, GPT-Image-1\n*   **PixVerse**: V4 & Effects\n*   **Pika**: 2.2\n*   **Recraft**: V3, V2 & Various Tools\n*   **Rodin**: 3D Generation\n*   **Stability AI**: Stable Image Ultra, Stable Diffusion 3.5 Large, Image Upscale\n*   **Tripo**: v1-4, v2.0, v2.5\n\n## Prerequisites for Using API Nodes\n\nTo use API Nodes, the following requirements must be met:\n\n### 1\\. ComfyUI Version Requirements\n\nPlease update your ComfyUI to the latest version, as we may add more API support in the future, and corresponding nodes will be updated, so please keep your ComfyUI up to date.\n\n### 2\\. Account and Credits Requirements\n\nYou need to be logged into your ComfyUI with a [Comfy account](https://docs.comfy.org/interface/user) and have a credit balance of [credits](https://docs.comfy.org/interface/credits) greater than 0. Log in via `Settings` -> `User`: ![ComfyUI User Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user.jpg) Go to `Settings` -> `Credits` to purchase credits ![Credits Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/purchase-1.jpg) Please refer to the corresponding documentation for account and credits to ensure this requirement:\n\n*   [Comfy account](https://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.\n*   [Credits](https://docs.comfy.org/interface/credits): After logging in, the settings interface will show a credits menu where you can purchase credits. We use a prepaid system, so there will be no unexpected charges.\n\n### 3\\. Network Environment Requirements\n\nAPI access requires that your current requests are based on a secure network environment. The current requirements for API access are as follows:\n\n*   The local network only allows access from `127.0.0.1` or `localhost`, and you can directly use the login function.\n*   If you are accessing from a local area network or a website that is not on the whitelist, please log in with an API Key. Please refer to [Log in with an API Key](https://docs.comfy.org/interface/user#logging-in-with-an-api-key).\n*   You should be able to access our API service normally (in some regions, you may need to use a proxy service).\n*   Access should be carried out in an `https` environment to ensure the security of the requests.\n\n### 4\\. Using the Corresponding Nodes\n\n**Add to Workflow**: Add the API node to your workflow just like you would with other nodes. **Run**: Set the parameters and then run the workflow. ![API Nodes](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/sidebar.jpg)\n\n## Log in with API Key on non-whitelisted websites\n\nCurrently, we have set up a whitelist to restrict the websites where you can log in to your ComfyUI account. If you need to log in to your ComfyUI account on some non-whitelisted websites, please refer to the account management section to learn how to log in using an API Key. In this case, the corresponding website does not need to be on our whitelist.\n\n[\n\n## Account Management\n\nLearn how to log in with ComfyUI API Key\n\n\n\n](https://docs.comfy.org/interface/user#logging-in-with-an-api-key)\n\n![Select Comfy API Key Login](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user/user-login-api-1.jpg)\n\n## Use ComfyUI API Key Integration to call paid model API nodes\n\nCurrently, we support accessing our services through ComfyUI API Key Integration to call paid model API nodes. Please refer to the API Key Integration section to learn how to use API Key Integration to call paid model API nodes.\n\n[\n\n## API Key Integration\n\nPlease refer to the API Key Integration section to learn how to use API Key Integration to call paid model API nodes\n\n\n\n](https://docs.comfy.org/development/comfyui-server/api-key-integration)\n\n## Advantages of API Nodes\n\nAPI Nodes provide several important advantages for ComfyUI users:\n\n*   **Access to closed-source models**: Use state-of-the-art AI models without having to deploy them yourself\n*   **Seamless integration**: API nodes are fully compatible with other ComfyUI nodes and can be combined to create complex workflows\n*   **Simplified experience**: No need to manage API keys or handle complex API requests\n*   **Controlled costs**: The prepaid system ensures you have complete control over your spending with no unexpected charges\n\n## Pricing\n\n[\n\n## API Node Pricing\n\nPlease refer to the pricing page for the corresponding API pricing\n\n\n\n](https://docs.comfy.org/tutorials/api-nodes/pricing)\n\n## About Open Source and Opt-in\n\nItâ€™s important to note that **API Nodes are completely optional**. ComfyUI will always remain fully open-source and free for local users. API nodes are designed as an â€œopt-inâ€ feature, providing convenience for those who want access to external SOTA (state-of-the-art) models.\n\n## Use Cases\n\nA powerful application of API Nodes is combining the output of external models with local nodes. For example:\n\n*   Using GPT-Image-1 to generate a base image, then transforming it into video with a local `wan` node\n*   Combining externally generated images with local upscaling or style transfer nodes\n*   Creating hybrid workflows that leverage the advantages of both closed-source and open-source models\n\nThis flexibility makes ComfyUI a truly universal generative AI interface, integrating various AI capabilities into a unified workflow, opening up more possibilities\n\n## FAQs"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/rodin/model-generation",
  "markdown": "# Rodin API Node Model Generation ComfyUI Official Example\n\nHyper3D Rodin (hyper3d.ai) is a platform focused on rapidly generating high-quality, production-ready 3D models and materials through artificial intelligence. ComfyUI has now natively integrated the corresponding Rodin model generation API, allowing you to conveniently use the related nodes in ComfyUI for model generation. Currently, ComfyUIâ€™s API nodes support the following Rodin model generation capabilities:\n\n*   Single-view model generation\n*   Multi-view model generation\n*   Model generation with different levels of detail\n\n## Single-view Model Generation Workflow\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/rodin_image_to_model.json)\n\nDownload the image below as input image ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/doll.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Rodin Image to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  (Optional) In `Rodin 3D Generate - Regular Generate` adjust the corresponding parameters\n    *   polygon\\_count: You can set different polygon counts, the higher the value, the smoother and more detailed the model\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/Rodin` directory\n4.  In the `Preview 3D` node, click to expand the menu\n5.  Select `Export` to directly export the corresponding model\n\n## Multi-view Model Generation Workflow\n\nThe corresponding `Rodin 3D Generate - Regular Generate` allows up to 5 image inputs\n\n### 1\\. Workflow File Download\n\nYou can modify the single-view workflow to a multi-view workflow, or directly download the workflow file below Download the file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/api_rodin_multiview_to_model.json)\n\nDownload the images below as input images ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/front.jpg) ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/back.jpg) ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/left.jpg)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Rodin Image to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg) You can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Load Image` node, load the provided input images\n2.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/Rodin` directory\n3.  In the `Preview 3D` node, click to expand the menu\n4.  Select `Export` to directly export the corresponding model\n\nCurrently, Rodin provides different types of model generation nodes in ComfyUI, since the corresponding input conditions are the same as the workflow introduced in this article, you can enable them as needed. In addition, we have provided corresponding nodes in the corresponding templates, you can also modify the corresponding node mode as needed to enable them ![Rodin Other Related Nodes](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/other_nodes.jpg)"
},
{
  "url": "https://docs.comfy.org/tutorials/image/hidream/hidream-e1",
  "markdown": "# ComfyUI Native HiDream-E1, E1.1 Workflow Example\n\n![HiDream-E1 Demo](https://raw.githubusercontent.com/HiDream-ai/HiDream-E1/refs/heads/main/assets/demo.jpg) HiDream-E1 is an interactive image editing large model officially open-sourced by HiDream-ai, built based on HiDream-I1. It allows you to edit images using natural language. The model is released under the [MIT License](https://github.com/HiDream-ai/HiDream-E1?tab=MIT-1-ov-file), supporting use in personal projects, scientific research, and commercial applications. In combination with the previously released [hidream-i1](https://docs.comfy.org/tutorials/image/hidream/hidream-i1), it enables **creative capabilities from image generation to editing**.\n\n| Name | Update Date | Inference Steps | Resolution | HuggingFace Repository |\n| --- | --- | --- | --- | --- |\n| HiDream-E1-Full | 2025-4-28 | 28  | 768x768 | ðŸ¤— [HiDream-E1-Full](https://huggingface.co/HiDream-ai/HiDream-E1-Full) |\n| HiDream-E1.1 | 2025-7-16 | 28  | Dynamic (1 Megapixel) | ðŸ¤— [HiDream-E1.1](https://huggingface.co/HiDream-ai/HiDream-E1-1) |\n\n[HiDream E1 - Github](https://github.com/HiDream-ai/HiDream-E1)\n\nAll the models involved in this guide can be found [here](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files). Except for the Diffusion model, E1 and E1.1 use the same models. The corresponding workflow files also include the relevant model information. You can choose to manually download and save the models, or follow the workflow prompts to download them after loading the workflow. It is recommended to use E1.1. This model requires a large amount of VRAM to run. Please refer to the relevant sections for specific VRAM requirements. **Diffusion Model** You do not need to download both models. Since E1.1 is an iterative version based on E1, our tests show that its quality and performance are significantly improved compared to E1.\n\n*   [hidream\\_e1\\_1\\_bf16.safetensors (Recommended)](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_1_bf16.safetensors) 34.2GB\n*   [hidream\\_e1\\_full\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_full_bf16.safetensors) 34.2GB\n\n**Text Encoder**:\n\n*   [clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors) 236.12MB\n*   [clip\\_g\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors) 1.29GB\n*   [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) 4.8GB\n*   [llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors) 8.46GB\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) 319.77MB\n\n> This is the VAE model for Flux. If you have used the Flux workflow before, you may have already downloaded this file.\n\nModel Save Location\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€â”€ clip_l_hidream.safetensors\nâ”‚   â”‚   â”œâ”€â”€â”€ clip_g_hidream.safetensors\nâ”‚   â”‚   â”œâ”€â”€â”€ t5xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â”‚   â””â”€â”€â”€ llama_3.1_8b_instruct_fp8_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ diffusion_models/\nâ”‚       â”œâ”€â”€ hidream_e1_1_bf16.safetensors\nâ”‚       â””â”€â”€ hidream_e1_full_bf16.safetensors\n```\n\nE1.1 is an updated version released on July 16, 2025. This version supports dynamic 1-megapixel resolution, and the workflow uses the `Scale Image to Total Pixels` node to dynamically adjust the input image to 1 million pixels.\n\n### 1\\. HiDream E1.1 Workflow and Related Materials\n\nDownload the image below and drag it into ComfyUI with the corresponding workflow and models loaded: ![HiDream E1.1 Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/hidream_e1_1.png) Download the image below as input: ![HiDream E1.1 Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/input.webp) \n\n### 2\\. Step-by-step Guide to Running the HiDream-e1 Workflow\n\n![hidream_e1_1_guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/hidream/hidream-e1-1-guide.jpg) Follow these steps to run the workflow:\n\n1.  Make sure the `Load Diffusion Model` node loads the `hidream_e1_1_bf16.safetensors` model.\n2.  Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly:\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node uses the `ae.safetensors` file.\n4.  In the `Load Image` node, load the provided input or your desired image.\n5.  In the `Empty Text Encoder(Positive)` node, enter **the modifications you want to make to the image**.\n6.  In the `Empty Text Encoder(Negative)` node, enter **the content you do not want to appear in the image**.\n7.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute image generation.\n\n### 3\\. Additional Notes on the Workflow\n\n*   Since HiDream E1.1 supports dynamic input with a total of 1 million pixels, the workflow uses `Scale Image to Total Pixels` to process and convert all input images, which may cause the aspect ratio to differ from the original input image.\n*   When using the fp16 version of the model, in actual tests, the full version ran out of memory on both A100 40GB and 4090D 24GB, so the workflow is set by default to use `fp8_e4m3fn_fast` for inference.\n\n## HiDream E1 ComfyUI Native Workflow Example\n\nE1 is a model released on April 28, 2025. This model only supports 768\\*768 resolution.\n\n### 1\\. HiDream-e1 workflow\n\nPlease download the image below and drag it into ComfyUI. The workflow already contains model download information, and after loading, it will prompt you to download the corresponding models. ![ComfyUI Native HiDream-e1 Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/hidream_e1_full.png) Download this image below as input: ![ComfyUI Native HiDream-e1 Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/input.webp) \n\n### 2\\. Complete the HiDream-e1 Workflow Step by Step\n\n![hidream_e1_full_step_guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg) Follow these steps to complete the workflow:\n\n1.  Make sure the `Load Diffusion Model` node has loaded the `hidream_e1_full_bf16.safetensors` model\n2.  Ensure that the four corresponding text encoders are correctly loaded in the `QuadrupleCLIPLoader`\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node is using the `ae.safetensors` file\n4.  Load the input image we downloaded earlier in the `Load Image` node\n5.  (Important) Enter **the prompt for how you want to modify the image** in the `Empty Text Encoder(Positive)` node\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n\n### Additional Notes on ComfyUI HiDream-e1 Workflow\n\n*   You may need to modify the prompt multiple times or generate multiple times to get better results\n*   This model has difficulty maintaining consistency when changing image styles, so try to make your prompts as complete as possible\n*   As the model supports a resolution of 768\\*768, in actual testing with other dimensions, the image performance is poor or even significantly different at other dimensions"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/pricing",
  "markdown": "# Pricing - ComfyUI\n\nBFLFlux 1.1 \\[pro\\] Ultra ImageNA$0.06BFLFlux.1 Canny Control ImageNA$0.05BFLFlux.1 Depth Control ImageNA$0.05BFLFlux.1 Expand ImageNA$0.05BFLFlux.1 Fill ImageNA$0.05BFLFlux.1 Kontext \\[max\\] ImageNA$0.08BFLFlux.1 Kontext \\[pro\\] ImageNA$0.04BFLFlux.1 Kontext \\[pro\\] ImageNA$0.05KlingKling Image Generationkling-v1-5, 1, image to image$0.028KlingKling Image Generationkling-v1-5, 1, text to image$0.014KlingKling Image Generationkling-v1, 1, image to image$0.0035KlingKling Image Generationkling-v1, 1, text to image$0.0035KlingKling Image Generationkling-v2, 1, text to image$0.014KlingKling Virtual Try OnNA$0.07KlingKling, Text to Video (Camera Control)NA$0.49KlingKling Dual Character Video, Effectskling-v1-5, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1-5, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1-5, std, 5$0.28KlingKling Dual Character Video, Effectskling-v1-5, std, 10$0.56KlingKling Dual Character Video, Effectskling-v1-6, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1-6, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1-6, std, 5$0.28KlingKling Dual Character Video, Effectskling-v1-6, std, 10$0.56KlingKling Dual Character Video, Effectskling-v1, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1, std, 5$0.14KlingKling Dual Character Video, Effectskling-v1, std, 10$0.28KlingKling Image to Videokling-v1-5, pro, 5$0.49KlingKling Image to Videokling-v1-5, pro, 10$0.98KlingKling Image to Videokling-v1-5, std, 5$0.28KlingKling Image to Videokling-v1-5, std, 10$0.56KlingKling Image to Videokling-v1-6, pro, 5$0.49KlingKling Image to Videokling-v1-6, pro, 10$0.98KlingKling Image to Videokling-v1-6, std, 5$0.28KlingKling Image to Videokling-v1-6, std, 10$0.56KlingKling Image to Videokling-v1, pro, 5$0.49KlingKling Image to Videokling-v1, pro, 10$0.98KlingKling Image to Videokling-v1, std, 5$0.14KlingKling Image to Videokling-v1, std, 10$0.28KlingKling Image to Videokling-v2-maser, pro, 5s$1.4KlingKling Image to Videokling-v2-maser, pro, 10s$2.8KlingKling Image to Videokling-v2-maser, std, 5s$1.4KlingKling Image to Videokling-v2-maser, std, 10s$2.8KlingKling Lip Sync Video with, Audio5s$0.07KlingKling Lip Sync Video with, Audio10s$0.14KlingKling Lip Sync Video with Text5s$0.07KlingKling Lip Sync Video with Text10s$0.14KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1$0.49KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1-5$0.49KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1-6$0.49KlingKling Start-End Frame to Videopro mode / 10s duration / kling-v1-5$0.98KlingKling Start-End Frame to Videopro mode / 10s duration / kling-v1-6$0.98KlingKling Start-End Frame to Videostandard mode / 5s duration / kling-v1$0.14KlingKling Text to Videopro mode / 5s duration / kling-v1$0.49KlingKling Text to Videopro mode / 5s duration / kling-v2-master$1.4KlingKling Text to Videopro mode / 10s duration / kling-v1$0.98KlingKling Text to Videopro mode / 10s duration / kling-v2-master$2.8KlingKling Text to Videostandard mode / 5s duration / kling-v1$0.14KlingKling Text to Videostandard mode / 5s duration / kling-v1-6$0.28KlingKling Text to Videostandard mode / 5s duration / kling-v2-master$1.4KlingKling Text to Videostandard mode / 10s duration / kling-v1$0.28KlingKling Text to Videostandard mode / 10s duration / kling-v1-6$0.56KlingKling Text to Videostandard mode / 10s duration / kling-v2-master$2.8KlingKling Video Effectsdizzydizzy/bloombloom, 5$0.49KlingKling Video Effectsfuzzyfuzzy/squish/expansion, 5$0.28KlingKling Video ExtendNA$0.28LumaLuma, Text to Imagephoto-flash-1$0.0019LumaLuma, Text to Imagephoto-flash-1$0.0019LumaLuma Image to Imagephoton-1$0.0073LumaLuma Image to Imagephoton-1$0.0073LumaLuma Image to Videoray-1-6, 4k, 5s$3.19LumaLuma Image to Videoray-1-6, 4k, 9s$5.73LumaLuma Image to Videoray-1-6, 540p, 5s$0.2LumaLuma Image to Videoray-1-6, 540p, 9s$0.36LumaLuma Image to Videoray-1-6, 720p, 5s$0.35LumaLuma Image to Videoray-1-6, 720p, 9s$0.64LumaLuma Image to Videoray-1-6, 1080p, 5s$0.8LumaLuma Image to Videoray-1-6, 1080p, 9s$1.43LumaLuma Image to Videoray-2, 4k, 5s$6.37LumaLuma Image to Videoray-2, 4k, 9s$11.47LumaLuma Image to Videoray-2, 540p, 5s$0.4LumaLuma Image to Videoray-2, 540p, 9s$0.72LumaLuma Image to Videoray-2, 720p, 5s$0.71LumaLuma Image to Videoray-2, 720p, 9s$1.27LumaLuma Image to Videoray-2, 1080p, 5s$1.59LumaLuma Image to Videoray-2, 1080p, 9s$2.87LumaLuma Image to Videoray-flash-2, 4k, 5s$2.19LumaLuma Image to Videoray-flash-2, 4k, 9s$3.94LumaLuma Image to Videoray-flash-2, 540p, 5s$0.14LumaLuma Image to Videoray-flash-2, 540p, 9s$0.25LumaLuma Image to Videoray-flash-2, 720p, 5s$0.24LumaLuma Image to Videoray-flash-2, 720p, 9s$0.44LumaLuma Image to Videoray-flash-2, 1080p, 5s$0.55LumaLuma Image to Videoray-flash-2, 1080p, 9s$0.99LumaLuma Text-to-videoray-1-6, 4k, 5s$3.19LumaLuma Text-to-videoray-1-6, 4k, 9s$5.73LumaLuma Text-to-videoray-1-6, 540p, 5s$0.2LumaLuma Text-to-videoray-1-6, 540p, 9s$0.36LumaLuma Text-to-videoray-1-6, 720p, 5s$0.35LumaLuma Text-to-videoray-1-6, 720p, 9s$0.64LumaLuma Text-to-videoray-1-6, 1080p, 5s$0.8LumaLuma Text-to-videoray-1-6, 1080p, 9s$1.43LumaLuma Text-to-videoray-2, 4k, 5s$6.37LumaLuma Text-to-videoray-2, 4k, 9s$11.47LumaLuma Text-to-videoray-2, 540p, 5s$0.4LumaLuma Text-to-videoray-2, 540p, 9s$0.72LumaLuma Text-to-videoray-2, 720p, 5s$0.71LumaLuma Text-to-videoray-2, 720p, 9s$1.27LumaLuma Text-to-videoray-2, 1080p, 5s$1.59LumaLuma Text-to-videoray-2, 1080p, 9s$2.87LumaLuma Text-to-videoray-flash-2, 4k, 5s$2.19LumaLuma Text-to-videoray-flash-2, 4k, 9s$3.94LumaLuma Text-to-videoray-flash-2, 540p, 5s$0.14LumaLuma Text-to-videoray-flash-2, 540p, 9s$0.25LumaLuma Text-to-videoray-flash-2, 720p, 5s$0.24LumaLuma Text-to-videoray-flash-2, 720p, 9s$0.44LumaLuma Text-to-videoray-flash-2, 1080p, 5s$0.55LumaLuma Text-to-videoray-flash-2, 1080p, 9s$0.99GoogleGoogle Veo2 Video Generation5$2.5GoogleGoogle Veo2 Video Generation8$4GoogleGoogle Geminigemini-2.5-flash-preview-04-171.25/1Minputtokens+1.25/1M input tokens + 10/1M output tokens (< 200K tokens)GoogleGoogle Geminigemini-2.5-pro-preview-05-060.16/1Minputtokens+0.16/1M input tokens + 0.6/1M output tokens + $1/1M input audio tokens (< 200K tokens)MinimaxMinimax, Text to Video6s clip$0.43MinimaxMinimax Hailuo-02768P 6s$0.28MinimaxMinimax Hailuo-02768P 10s$0.56MinimaxMinimax Hailuo-021080P 6s$0.49MinimaxMinimax Image to Video6s clip$0.43RecraftRecraft, Creative Upscale ImageNA$0.25RecraftRecraft, Crisp Upscale ImageNA$0.004RecraftRecraft, Image Inpainting1$0.04RecraftRecraft, Image to Image1$0.04RecraftRecraft, Remove BackgroundNA$0.01RecraftRecraft, Replace Background1$0.04RecraftRecraft, Text to Image1$0.04RecraftRecraft, Text to Vector1$0.08RecraftRecraft, Vectorize ImageNA$0.01IdeogramIdeogram, V11, false$0.06IdeogramIdeogram, V11, true$0.02IdeogramIdeogram V21, false$0.08IdeogramIdeogram V21, true$0.05IdeogramIdeogram V31, Balanced$0.06IdeogramIdeogram V31, Quality$0.09IdeogramIdeogram V31, Turbo$0.03RunwayRuway, Text to ImageNA$0.08RunwayRunway, First-Last-Frame to Video5s$0.25RunwayRunway, First-Last-Frame to Video10s$0.5RunwayRunway Image to Video (Gen3a, Turbo)5s$0.25RunwayRunway Image to Video (Gen3a, Turbo)10s$0.5RunwayRunway Image to Video (Gen4, Turbo)5s$0.25RunwayRunway Image to Video (Gen4, Turbo)10s$0.5OpenAIGPT-Image-1 - Actualinput image tokens10/1Mtokens+,inputtexttokens10 / 1M tokens +, input text tokens5 / 1M tokens +,output tokens$40 / 1M tokensOpenAIGPT-Image-1 (Approximate price)high, 1024x1024$0.167OpenAIGPT-Image-1 (Approximate price)high, 1024x1536$0.25OpenAIGPT-Image-1 (Approximate price)high, 1536x1024$0.25OpenAIGPT-Image-1 (Approximate price)low, 1024x1024$0.011OpenAIGPT-Image-1 (Approximate price)low, 1024x1536$0.016OpenAIGPT-Image-1 (Approximate price)low, 1536x1024$0.016OpenAIGPT-Image-1 (Approximate price)medium, 1024x1024$0.042OpenAIGPT-Image-1 (Approximate price)medium, 1024x1536$0.063OpenAIGPT-Image-1 (Approximate price)medium, 1536x1024$0.063OpenAIImage Generation (DALLÂ·E 2)size = 512 \\* 512$0.018OpenAIImage Generation (DALLÂ·E 2)size = 1024 \\* 1024$0.02OpenAIImage Generation (DALLÂ·E 2)size 256 \\* 256$0.016OpenAIImage Generation (DALLÂ·E 3 HD)size = 1024 \\* 1024, hd$0.08OpenAIImage Generation (DALLÂ·E 3 HD)size = 1024 \\* 1792, hd$0.12OpenAIImage Generation (DALLÂ·E 3 HD)size = 1792 \\* 1024, hd$0.12OpenAIImage Generation (DALLÂ·E 3 Std)size = 1024 \\* 1024,std$0.04OpenAIImage Generation (DALLÂ·E 3 Std)size = 1024 \\* 1792, std$0.08OpenAIImage Generation (DALLÂ·E 3 Std)size = 1792 \\* 1024, std$0.08PixversePixVerse, Text to Video360p fast 5s$0.9PixversePixVerse, Text to Video360p normal 5s$0.45PixversePixVerse, Text to Video360p normal 8s$0.9PixversePixVerse, Text to Video540p fast 5s$0.9PixversePixVerse, Text to Video540p normal 5s$0.45PixversePixVerse, Text to Video540p normal 8s$0.9PixversePixVerse, Text to Video720p fast 5s$1.2PixversePixVerse, Text to Video720p normal 5s$0.6PixversePixVerse, Text to Video720p normal 8s$1.2PixversePixVerse, Text to Video1080p normal 5s$1.2PixversePixVerse, Transition Video360p fast 5s$0.9PixversePixVerse, Transition Video360p normal 5s$0.45PixversePixVerse, Transition Video360p normal 8s$0.9PixversePixVerse, Transition Video540p fast 5s$0.9PixversePixVerse, Transition Video540p normal 5s$0.45PixversePixVerse, Transition Video540p normal 8s$0.9PixversePixVerse, Transition Video720p fast 5s$1.2PixversePixVerse, Transition Video720p normal 5s$0.6PixversePixVerse, Transition Video720p normal 8s$1.2PixversePixVerse, Transition Video1080p normal 5s$1.2PixversePixVerse,Image to Video360p fast 5s$0.9PixversePixVerse,Image to Video360p normal 5s$0.45PixversePixVerse,Image to Video360p normal 8s$0.9PixversePixVerse,Image to Video540p fast 5s$0.9PixversePixVerse,Image to Video540p normal 5s$0.45PixversePixVerse,Image to Video540p normal 8s$0.9PixversePixVerse,Image to Video720p fast 5s$1.2PixversePixVerse,Image to Video720p normal 5s$0.6PixversePixVerse,Image to Video720p normal 8s$1.2PixversePixVerse,Image to Video1080p normal 5s$1.2PikaPika, Scenes (Video Image Composition)720p, 5s$0.3PikaPika, Scenes (Video Image Composition)720p, 10s$0.4PikaPika, Scenes (Video Image Composition)1080p, 5s$0.5PikaPika, Scenes (Video Image Composition)1080p, 10s$1.5PikaPika, Start and End Frame to Video720p, 5s$0.2PikaPika, Start and End Frame to Video720p, 10s$0.25PikaPika, Start and End Frame to Video1080p, 5s$0.3PikaPika, Start and End Frame to Video1080p, 10s$1PikaPika, Text to Video720p, 5s$0.2PikaPika, Text to Video720p, 10s$0.6PikaPika, Text to Video1080p, 5s$0.45PikaPika, Text to Video1080p, 10s$1PikaPika,Image to Video720p, 5s$0.2PikaPika,Image to Video720p, 10s$0.6PikaPika,Image to Video1080p, 5s$0.45PikaPika,Image to Video1080p, 10s$1PikaPika Swaps, (Video Object Replacement)NA$0.3PikaPikadditios, (Video Object Insertion)NA$0.3PikaPikaffects, (Video Effects)NA$0.45MoonvalleyImage to video - 5sNA$1.5MoonvalleyText to video - 5sNA$1.5MoonvalleyVideo to video - 5sNA$2.25RodinRodin 3D, Generate - Regular GenerateNA$0.4RodinRodin 3D Generate - Detail, GenerateNA$0.4RodinRodin 3D Generate - Sketch, GenerateNA$0.4RodinRodin 3D Generate - Smooth, GenerateNA$0.4TripoTripo:, Text to Modelany style, false, any quality, false$0.15TripoTripo:, Text to Modelany style, false, any quality, true$0.2TripoTripo:, Text to Modelany style, true, detailed, false$0.35TripoTripo:, Text to Modelany style, true, detailed, true$0.4TripoTripo:, Text to Modelany style, true, standard, false$0.25TripoTripo:, Text to Modelany style, true, standard, true$0.3TripoTripo:, Text to Modelnone, false, any, quality, false$0.1TripoTripo:, Text to Modelnone, false, any quality, true$0.15TripoTripo:, Text to Modelnone, true, detailed, false$0.3TripoTripo:, Text to Modelnone, true, detailed, true$0.35TripoTripo:, Text to Modelnone, true, standard, false$0.2TripoTripo:, Text to Modelnone, true, standard, true$0.25TripoTripo:,Image to Model / Multiview to Modelany style, false, any quality, false$0.25TripoTripo:,Image to Model / Multiview to Modelany style, false, any quality, true$0.3TripoTripo:,Image to Model / Multiview to Modelany style, true, detailed, false$0.45TripoTripo:,Image to Model / Multiview to Modelany style, true, detailed, true$0.5TripoTripo:,Image to Model / Multiview to Modelany style, true, standard, false$0.35TripoTripo:,Image to Model / Multiview to Modelany style, true, standard, true$0.4TripoTripo:,Image to Model / Multiview to Modelnone, false, any, quality, false$0.2TripoTripo:,Image to Model / Multiview to Modelnone, false, any quality, true$0.25TripoTripo:,Image to Model / Multiview to Modelnone, true, detailed, false$0.4TripoTripo:,Image to Model / Multiview to Modelnone, true, detailed, true$0.45TripoTripo:,Image to Model / Multiview to Modelnone, true, standard, false$0.3TripoTripo:,Image to Model / Multiview to Modelnone, true, standard, true$0.35TripoTripo: Convert modelNA$0.1TripoTripo: Refine Draft modelNA$0.3TripoTripo: Retarget rigged modelNA$0.1TripoTripo: Rig modelNA$0.25TripoTripo: Texture modeldetailed$0.2TripoTripo: Texture modelstandard$0.1Stability AIStability, AI Stable Image UltraNA$0.08Stability AIStability AI Stable Diffusion, 3.5 Imagesd3.5-large$0.065Stability AIStability AI Stable Diffusion, 3.5 Imagesd3.5-medium$0.035Stability AIStability AI Upscale, ConservativeNA$0.25Stability AIStability AI Upscale CreativeNA$0.25Stability AIStability AI Upscale FastNA$0.01"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/server_overview",
  "markdown": "# Properties - ComfyUI\n\n### Simple Example\n\nHereâ€™s the code for the Invert Image Node, which gives an overview of the key concepts in custom node development.\n\n```\nclass InvertImageNode:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": { \"image_in\" : (\"IMAGE\", {}) },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"image_out\",)\n    CATEGORY = \"examples\"\n    FUNCTION = \"invert\"\n\n    def invert(self, image_in):\n        image_out = 1 - image_in\n        return (image_out,)\n```\n\n### Main properties\n\nEvery custom node is a Python class, with the following key properties:\n\n#### INPUT\\_TYPES\n\n`INPUT_TYPES`, as the name suggests, defines the inputs for the node. The method returns a `dict` which _must_ contain the key `required`, and _may_ also include the keys `optional` and/or `hidden`. The only difference between `required` and `optional` inputs is that `optional` inputs can be left unconnected. For more information on `hidden` inputs, see [Hidden Inputs](https://docs.comfy.org/custom-nodes/backend/more_on_inputs#hidden-inputs). Each key has, as its value, another `dict`, in which key-value pairs specify the names and types of the inputs. The types are defined by a `tuple`, the first element of which defines the data type, and the second element of which is a `dict` of additional parameters. Here we have just one required input, named `image_in`, of type `IMAGE`, with no additional parameters. Note that unlike the next few attributes, this `INPUT_TYPES` is a `@classmethod`. This is so that the options in dropdown widgets (like the name of the checkpoint to be loaded) can be computed by Comfy at run time. Weâ€™ll go into this more later.\n\n#### RETURN\\_TYPES\n\nA `tuple` of `str` defining the data types returned by the node. If the node has no outputs this must still be provided `RETURN_TYPES = ()`\n\n#### RETURN\\_NAMES\n\nThe names to be used to label the outputs. This is optional; if omitted, the names are simply the `RETURN_TYPES` in lowercase.\n\n#### CATEGORY\n\nWhere the node will be found in the ComfyUI **Add Node** menu. Submenus can be specified as a path, eg. `examples/trivial`.\n\n#### FUNCTION\n\nThe name of the Python function in the class that should be called when the node is executed. The function is called with named arguments. All `required` (and `hidden`) inputs will be included; `optional` inputs will be included only if they are connected, so you should provide default values for them in the function definition (or capture them with `**kwargs`). The function returns a tuple corresponding to the `RETURN_TYPES`. This is required even if nothing is returned (`return ()`). Again, if you only have one output, remember that trailing comma `return (image_out,)`!\n\nA great feature of Comfy is that it caches outputs, and only executes nodes that might produce a different result than the previous run. This can greatly speed up lots of workflows. In essence this works by identifying which nodes produce an output (these, notably the Image Preview and Save Image nodes, are always executed), and then working backwards to identify which nodes provide data that might have changed since the last run. Two optional features of a custom node assist in this process.\n\n#### OUTPUT\\_NODE\n\nBy default, a node is not considered an output. Set `OUTPUT_NODE = True` to specify that it is.\n\n#### IS\\_CHANGED\n\nBy default, Comfy considers that a node has changed if any of its inputs or widgets have changed. This is normally correct, but you may need to override this if, for instance, the node uses a random number (and does not specify a seed - itâ€™s best practice to have a seed input in this case so that the user can control reproducibility and avoid unnecessary execution), or loads an input that may have changed externally, or sometimes ignores inputs (so doesnâ€™t need to execute just because those inputs changed).\n\n`IS_CHANGED` is passed the same arguments as the main function defined by `FUNCTION`, and can return any Python object. This object is compared with the one returned in the previous run (if any) and the node will be considered to have changed if `is_changed != is_changed_old` (this code is in `execution.py` if you need to dig). Since `True == True`, a node that returns `True` to say it has changed will be considered not to have! Iâ€™m sure this would be changed in the Comfy code if it wasnâ€™t for the fact that it might break existing nodes to do so. To specify that your node should always be considered to have changed (which you should avoid if possible, since it stops Comfy optimising what gets run), `return float(\"NaN\")`. This returns a `NaN` value, which is not equal to anything, even another `NaN`. A good example of actually checking for changes is the code from the built-in LoadImage node, which loads the image and returns a hash\n\n```\n    @classmethod\n    def IS_CHANGED(s, image):\n        image_path = folder_paths.get_annotated_filepath(image)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n```\n\n### Other attributes\n\nThere are three other attributes that can be used to modify the default Comfy treatment of a node.\n\n#### INPUT\\_IS\\_LIST, OUTPUT\\_IS\\_LIST\n\nThese are used to control sequential processing of data, and are described [later](https://docs.comfy.org/custom-nodes/backend/lists).\n\n### VALIDATE\\_INPUTS\n\nIf a class method `VALIDATE_INPUTS` is defined, it will be called before the workflow begins execution. `VALIDATE_INPUTS` should return `True` if the inputs are valid, or a message (as a `str`) describing the error (which will prevent execution).\n\n#### Validating Constants\n\n`VALIDATE_INPUTS` is called with only the inputs that its signature requests (those returned by `inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args`). Any inputs which are received in this way will _not_ run through the default validation rules. For example, in the following snippet, the front-end will use the specified `min` and `max` values of the `foo` input, but the back-end will not enforce it.\n\n```\nclass CustomNode:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": { \"foo\" : (\"INT\", {\"min\": 0, \"max\": 10}) },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, foo):\n        # YOLO, anything goes!\n        return True\n```\n\nAdditionally, if the function takes a `**kwargs` input, it will receive _all_ available inputs and all of them will skip validation as if specified explicitly.\n\n#### Validating Types\n\nIf the `VALIDATE_INPUTS` method receives an argument named `input_types`, it will be passed a dictionary in which the key is the name of each input which is connected to an output from another node and the value is the type of that output. When this argument is present, all default validation of input types is skipped. Hereâ€™s an example making use of the fact that the front-end allows for the specification of multiple types:\n\n```\nclass AddNumbers:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"input1\" : (\"INT,FLOAT\", {\"min\": 0, \"max\": 1000})\n                \"input2\" : (\"INT,FLOAT\", {\"min\": 0, \"max\": 1000})\n            },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, input_types):\n        # The min and max of input1 and input2 are still validated because\n        # we didn't take `input1` or `input2` as arguments\n        if input_types[\"input1\"] not in (\"INT\", \"FLOAT\"):\n            return \"input1 must be an INT or FLOAT type\"\n        if input_types[\"input2\"] not in (\"INT\", \"FLOAT\"):\n            return \"input2 must be an INT or FLOAT type\"\n        return True\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/faq",
  "markdown": "# FAQs about API Nodes - ComfyUI\n\nThis article addresses common questions regarding the use of API nodes.\n\nWhy can't I find the API nodes?\n\nPlease update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.\n\nPlease note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.\n\nWhy can't I use / log in to the API Nodes?\n\nAPI access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:\n\n*   The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.\n*   Able to access our API service normally (a proxy service may be required in some regions).\n*   Your account does not have enough [credits](https://docs.comfy.org/interface/credits).\n\nWhy can't I use API node even after logging in, or why does it keep asking me to log in while using?\n\n*   Currently, only `127.0.0.1` or `localhost` access is supported.\n*   Ensure your account has enough credits.\n\nCan API Nodes be used for free?\n\nAPI Nodes require credits for API calls to closed-source models, so they do not support free usage.\n\nHow to purchase credits?\n\nPlease refer to the following documentation:\n\n1.  [Comfy Account](https://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.\n2.  [Credits](https://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` â†’ `Credits`. We use a prepaid system, so there will be no unexpected charges.\n3.  Complete the payment through Stripe.\n4.  Check if the credits have been updated. If not, try restarting or refreshing the page.\n\nAre unused credits refundable?\n\nCurrently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).\n\nCan credits go negative?\n\nCredits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.\n\nWhere can I check usage and expenses?\n\nPlease visit the [Credits](https://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.\n\nIs it possible to use my own API Key?\n\nCurrently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.\n\nDo credits expire?\n\nNo, your credits do not expire.\n\nCan credits be transferred or shared?\n\nNo, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.\n\nCan I use the same account on different devices?\n\nWe do not limit the number of devices that can log in; you can use your account anywhere you want.\n\nHow can I request for my account or information to be deleted??\n\nEmail a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/lifecycle",
  "markdown": "# Lifecycle - ComfyUI\n\n## How Comfy loads custom nodes\n\nWhen Comfy starts, it scans the directory `custom_nodes` for Python modules, and attempts to load them. If the module exports `NODE_CLASS_MAPPINGS`, it will be treated as a custom node.\n\n### **init**.py\n\n`__init__.py` is executed when Comfy attempts to import the module. For a module to be recognized as containing custom node definitions, it needs to export `NODE_CLASS_MAPPINGS`. If it does (and if nothing goes wrong in the import), the nodes defined in the module will be available in Comfy. If there is an error in your code, Comfy will continue, but will report the module as having failed to load. So check the Python console! A very simple `__init__.py` file would look like this:\n\n```\nfrom .python_file import MyCustomNode\nNODE_CLASS_MAPPINGS = { \"My Custom Node\" : MyCustomNode }\n__all__ = [\"NODE_CLASS_MAPPINGS\"]\n```\n\n#### NODE\\_CLASS\\_MAPPINGS\n\n`NODE_CLASS_MAPPINGS` must be a `dict` mapping custom node names (unique across the Comfy install) to the corresponding node class.\n\n#### NODE\\_DISPLAY\\_NAME\\_MAPPINGS\n\n`__init__.py` may also export `NODE_DISPLAY_NAME_MAPPINGS`, which maps the same unique name to a display name for the node. If `NODE_DISPLAY_NAME_MAPPINGS` is not provided, Comfy will use the unique name as the display name.\n\n#### WEB\\_DIRECTORY\n\nIf you are deploying client side code, you will also need to export the path, relative to the module, in which the JavaScript files are to be found. It is conventional to place these in a subdirectory of your custom node named `js`."
},
{
  "url": "https://docs.comfy.org/tutorials/video/hunyuan-video",
  "markdown": "# ComfyUI Hunyuan Video Examples - ComfyUI\n\nHunyuan Video series is developed and open-sourced by [Tencent](https://huggingface.co/tencent), featuring a hybrid architecture that supports both [Text-to-Video](https://github.com/Tencent/HunyuanVideo) and [Image-to-Video](https://github.com/Tencent/HunyuanVideo-I2V) generation with a parameter scale of 13B. Technical features:\n\n*   **Core Architecture:** Uses a DiT (Diffusion Transformer) architecture similar to Sora, effectively fusing text, image, and motion information to improve consistency, quality, and alignment between generated video frames. A unified full-attention mechanism enables multi-view camera transitions while ensuring subject consistency.\n*   **3D VAE:** The custom 3D VAE compresses videos into a compact latent space, making image-to-video generation more efficient.\n*   **Superior Image-Video-Text Alignment:** Utilizing MLLM text encoders that excel in both image and video generation, better following text instructions, capturing details, and performing complex reasoning.\n\nYou can learn more through the official repositories: [Hunyuan Video](https://github.com/Tencent/HunyuanVideo) and [Hunyuan Video-I2V](https://github.com/Tencent/HunyuanVideo-I2V). This guide will walk you through setting up both **Text-to-Video** and **Image-to-Video** workflows in ComfyUI.\n\n## Common Models for All Workflows\n\nThe following models are used in both Text-to-Video and Image-to-Video workflows. Please download and save them to the specified directories:\n\n*   [clip\\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)\n*   [llava\\_llama3\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)\n*   [hunyuan\\_video\\_vae\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)\n\nStorage location:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ llava_llama3_fp8_scaled.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ hunyuan_video_vae_bf16.safetensors\n```\n\n## Hunyuan Text-to-Video Workflow\n\nHunyuan Text-to-Video was open-sourced in December 2024, supporting 5-second short video generation through natural language descriptions in both Chinese and English.\n\n### 1\\. Workflow\n\nDownload the image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Hunyuan Text-to-Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp) \n\n### 2\\. Manual Models Installation\n\nDownload [hunyuan\\_video\\_t2v\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models` folder. Ensure you have all these model files in the correct locations:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors                       // Shared model\nâ”‚   â”‚   â””â”€â”€ llava_llama3_fp8_scaled.safetensors      // Shared model\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ hunyuan_video_vae_bf16.safetensors       // Shared model\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ hunyuan_video_t2v_720p_bf16.safetensors  // T2V model\n```\n\n### 3\\. Steps to Run the Workflow\n\n![ComfyUI Hunyuan Video T2V Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg)\n\n1.  Ensure the `DualCLIPLoader` node has loaded these models:\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_t2v_720p_bf16.safetensors`\n3.  Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`\n4.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Hunyuan Image-to-Video Workflow\n\nHunyuan Image-to-Video model was open-sourced on March 6, 2025, based on the HunyuanVideo framework. It transforms static images into smooth, high-quality videos and also provides LoRA training code to customize special video effects like hair growth, object transformation, etc. Currently, the Hunyuan Image-to-Video model has two versions:\n\n*   v1 â€œconcatâ€: Better motion fluidity but less adherence to the image guidance\n*   v2 â€œreplaceâ€: Updated the day after v1, with better image guidance but seemingly less dynamic compared to v1\n\nv1 â€œconcatâ€\n\n![HunyuanVideo v1](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp)\n\nv2 â€œreplaceâ€\n\n![HunyuanVideo v2](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp)\n\n### Shared Model for v1 and v2 Versions\n\nDownload the following file and save it to the `ComfyUI/models/clip_vision` directory:\n\n*   [llava\\_llama3\\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)\n\n### V1 â€œconcatâ€ Image-to-Video Workflow\n\n#### 1\\. Workflow and Asset\n\nDownload the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Hunyuan Image-to-Video v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp) Download the image below, which weâ€™ll use as the starting frame for the image-to-video generation: ![Starting Frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png) \n\n*   [hunyuan\\_video\\_image\\_to\\_video\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)\n\nEnsure you have all these model files in the correct locations:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ clip_vision/\nâ”‚   â”‚   â””â”€â”€ llava_llama3_vision.safetensors                     // I2V shared model\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors                                  // Shared model\nâ”‚   â”‚   â””â”€â”€ llava_llama3_fp8_scaled.safetensors                 // Shared model\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ hunyuan_video_vae_bf16.safetensors                  // Shared model\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 \"concat\" version model\n```\n\n#### 3\\. Steps to Run the Workflow\n\n![ComfyUI Hunyuan Video I2V v1 Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg)\n\n1.  Ensure that `DualCLIPLoader` has loaded these models:\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  Ensure that `Load CLIP Vision` has loaded `llava_llama3_vision.safetensors`\n3.  Ensure that `Load Image Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`\n4.  Ensure that `Load VAE` has loaded `vae_name: hunyuan_video_vae_bf16.safetensors`\n5.  Ensure that `Load Diffusion Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`\n6.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n### v2 â€œreplaceâ€ Image-to-Video Workflow\n\nThe v2 workflow is essentially the same as the v1 workflow. You just need to download the **replace** model and use it in the `Load Diffusion Model` node.\n\n#### 1\\. Workflow and Asset\n\nDownload the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - Hunyuan Image-to-Video v2](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp) Download the image below, which weâ€™ll use as the starting frame for the image-to-video generation: ![Starting Frame](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png) \n\n*   [hunyuan\\_video\\_v2\\_replace\\_image\\_to\\_video\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)\n\nEnsure you have all these model files in the correct locations:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ clip_vision/\nâ”‚   â”‚   â””â”€â”€ llava_llama3_vision.safetensors                                // I2V shared model\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors                                             // Shared model\nâ”‚   â”‚   â””â”€â”€ llava_llama3_fp8_scaled.safetensors                            // Shared model\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ hunyuan_video_vae_bf16.safetensors                             // Shared model\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 \"replace\" version model\n```\n\n#### 3\\. Steps to Run the Workflow\n\n![ComfyUI Hunyuan Video I2V v2 Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg)\n\n1.  Ensure the `DualCLIPLoader` node has loaded these models:\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  Ensure the `Load CLIP Vision` node has loaded `llava_llama3_vision.safetensors`\n3.  Ensure the `Load Image Model` node has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`\n4.  Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`\n5.  Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`\n6.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Try it yourself\n\nHere are some images and prompts we provide. Based on that content or make an adjustment to create your own video. ![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png)\n\n```\nFuturistic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/samurai.png)\n\n```\nSamurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/a_flying_car.png)\n\n```\nflying car fastly moving and flying through the city\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png)\n\n```\ncyberpunk car race in night city, dynamic, super fast, fast shot\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/fun-camera",
  "markdown": "# ComfyUI Wan2.1 Fun Camera Official Examples\n\n**Wan2.1 Fun Camera** is a video generation project launched by the Alibaba team, focusing on controlling video generation effects through camera motion. **Model Weights Download**:\n\n*   [14B Version](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-14B-Control-Camera)\n*   [1.3B Version](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-1.3B-Control-Camera)\n\n**Code Repository**: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun) **ComfyUI now natively supports the Wan2.1 Fun Camera model**.\n\n## Model Installation\n\nThese models only need to be installed once. Additionally, model download information is included in the corresponding workflow images, so you can choose your preferred way to download the models. All of the following models can be found at [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) **Diffusion Models** choose either 1.3B or 14B:\n\n*   [wan2.1\\_fun\\_camera\\_v1.1\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors)\n*   [wan2.1\\_fun\\_camera\\_v1.1\\_14B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_14B_bf16.safetensors)\n\nIf youâ€™ve used Wan2.1 related models before, you should already have the following models. If not, please download them: **Text Encoders** choose one:\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)\n\nFile Storage Location:\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚ â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚ â”‚   â”œâ”€â”€ wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors # 1.3B version\nâ”‚ â”‚   â””â”€â”€ wan2.1_fun_camera_v1.1_14B_bf16.safetensors # 14B version\nâ”‚ â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚ â”‚   â””â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚ â”œâ”€â”€ ðŸ“‚ vae/\nâ”‚ â”‚   â””â”€â”€ wan_2.1_vae.safetensors\nâ”‚ â””â”€â”€ ðŸ“‚ clip_vision/\nâ”‚     â””â”€â”€ clip_vision_h.safetensors\n```\n\n## ComfyUI Wan2.1 Fun Camera 1.3B Native Workflow Example\n\n#### 1.1 Workflow File\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow:\n\n[\n\nDownload Json Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B.json)\n\n#### 1.2 Input Image Download\n\nPlease download the image below, which we will use as the starting frame: ![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B_input.jpg)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Wan2.1 Fun Camera Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg)\n\n1.  Ensure the correct version of model file is loaded:\n    *   1.3B version: `wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors`\n    *   14B version: `wan2.1_fun_camera_v1.1_14B_bf16.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Load Image` node\n6.  Modify the Prompt if youâ€™re using your own input image\n7.  Set camera motion in the `WanCameraEmbedding` node\n8.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute generation\n\n## ComfyUI Wan2.1 Fun Camera 14B Workflow and Input Image\n\n[\n\nDownload Json Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B.json)\n\n**Input Image** ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B_input.jpg) \n\n## Performance Reference\n\n**1.3B Version**:\n\n*   512Ã—512 resolution on RTX 4090 takes about 72 seconds to generate 81 frames\n\n**14B Version**:\n\n*   RTX4090 24GB VRAM may experience insufficient memory when generating 512Ã—512 resolution, and memory issues have also occurred on A100 when using larger sizes"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/manager",
  "markdown": "# Publishing to the Manager - ComfyUI\n\n### Using ComfyUI Manager\n\nTo make your custom node available through **ComfyUI Manager** you need to save it as a git repository (generally at `github.com`) and then submit a Pull Request on the **ComfyUI Manager** git, in which you have edited `custom-node-list.json` to add your node. [More details](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#how-to-register-your-custom-node-into-comfyui-manager). When a user installs the node, **ComfyUI Manager** will:\n\n### ComfyUI Manager files\n\nAs indicated above, there are a number of files and scripts that **ComfyUI Manager** will use to manage the lifecycle of a custom node. These are all optional.\n\n*   `requirements.txt` - Python dependencies as mentioned above\n*   `install.py`, `uninstall.py` - executed when the custom node is installed or uninstalled\n*   `disable.py`, `enable.py` - executed when a custom node is disabled or re-enabled\n*   `node_list.json` - only required if the custom nodes pattern of NODE\\_CLASS\\_MAPPINGS is not conventional.\n\nSee the [ComfyUI Manager guide](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#custom-node-support-guide) for official details."
},
{
  "url": "https://docs.comfy.org/tutorials/image/cosmos/cosmos-predict2-t2i",
  "markdown": "# Cosmos Predict2 Text-to-Image ComfyUI Official Example\n\nCosmos-Predict2 is NVIDIAâ€™s next-generation physical world foundation model, specifically designed for high-quality visual generation and prediction tasks in physical AI scenarios. The model features exceptional physical accuracy, environmental interactivity, and detail reproduction capabilities, enabling realistic simulation of complex physical phenomena and dynamic scenes. Cosmos-Predict2 supports various generation methods including Text-to-Image (Text2Image) and Video-to-World (Video2World), and is widely used in industrial simulation, autonomous driving, urban planning, scientific research, and other fields. GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2) huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959) This guide will walk you through completing **text-to-image** workflow in ComfyUI. For the video generation section, please refer to the following part:\n\n[\n\n## Cosmos Predict2 Video Generation\n\nUsing Cosmos-Predict2 for video generation\n\n\n\n](https://docs.comfy.org/tutorials/video/cosmos/cosmos-predict2-video2world)\n\n## Cosmos Predict2 Video2World Workflow\n\nWhen testing the 2B version showed it uses around 16GB of VRAM.\n\n### 1\\. Workflow File\n\nPlease download the image below and drag it into ComfyUI to load the workflow. The workflow already has embedded model download links. ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/cosmos/predict2/cosmos_predict2_2B_t2i.png)\n\n### 2\\. Manual Model Installation\n\nIf the model download wasnâ€™t successful, you can try to download them manually by yourself in this section. **Diffusion model**\n\n*   [cosmos\\_predict2\\_2B\\_t2i.safetensors](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged/resolve/main/cosmos_predict2_2B_t2i.safetensors)\n\nFor other weights, please visit [Cosmos\\_Predict2\\_repackaged](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged) to download **Text encoder** [oldt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/resolve/main/text_encoders/oldt5_xxl_fp8_e4m3fn_scaled.safetensors) **VAE** [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors) File Storage Location\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€ cosmos_predict2_2B_t2i.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ oldt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚       â””â”€â”€  wan_2.1_vae.safetensors\n```\n\n### 3\\. Complete Workflow Step by Step\n\n![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/cosmos/cosmos_predict2_2B_t2i_step_guide.jpg) Please follow the steps in the image to run the workflow:\n\n1.  Ensure the `Load Diffusion Model` node has loaded `cosmos_predict2_2B_t2i.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `oldt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Set the image size in `EmptySD3LatentImage`\n5.  Modify the prompts in the `ClipTextEncode` node\n6.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to run the worklfow\n7.  Once generation is complete, the image will automatically save to the `ComfyUI/output/` directory. You can also preview it in the `save image` node.\n\n#### 0 reactions"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/lists",
  "markdown": "# Data lists - ComfyUI\n\n## Length one processing\n\nInternally, the Comfy server represents data flowing from one node to the next as a Python `list`, normally length 1, of the relevant datatype. In normal operation, when a node returns an output, each element in the output `tuple` is separately wrapped in a list (length 1); then when the next node is called, the data is unwrapped and passed to the main function.\n\n## List processing\n\nIn some circumstance, multiple data instances are processed in a single workflow, in which case the internal data will be a list containing the data instances. An example of this might be processing a series of images one at a time to avoid running out of VRAM, or handling images of different sizes. By default, Comfy will process the values in the list sequentially:\n\n*   if the inputs are `list`s of different lengths, the shorter ones are padded by repeating the last value\n*   the main method is called once for each value in the input lists\n*   the outputs are `list`s, each of which is the same length as the longest input\n\nThe relevant code can be found in the method `map_node_over_list` in `execution.py`. However, as Comfy wraps node outputs into a `list` of length one, if the `tuple` returned by a custom node contains a `list`, that `list` will be wrapped, and treated as a single piece of data. In order to tell Comfy that the list being returned should not be wrapped, but treated as a series of data for sequential processing, the node should provide a class attribute `OUTPUT_IS_LIST`, which is a `tuple[bool]`, of the same length as `RETURN_TYPES`, specifying which outputs which should be so treated. A node can also override the default input behaviour and receive the whole list in a single call. This is done by setting a class attribute `INPUT_IS_LIST` to `True`. Hereâ€™s a (lightly annotated) example from the built in nodes - `ImageRebatch` takes one or more batches of images (received as a list, because `INPUT_IS_LIST - True`) and rebatches them into batches of the requested size.\n\n```\n\nclass ImageRebatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"images\": (\"IMAGE\",),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}) }}\n    RETURN_TYPES = (\"IMAGE\",)\n    INPUT_IS_LIST = True\n    OUTPUT_IS_LIST = (True, )\n    FUNCTION = \"rebatch\"\n    CATEGORY = \"image/batch\"\n\n    def rebatch(self, images, batch_size):\n        batch_size = batch_size[0]    # everything comes as a list, so batch_size is list[int]\n\n        output_list = []\n        all_images = []\n        for img in images:                    # each img is a batch of images\n            for i in range(img.shape[0]):     # each i is a single image\n                all_images.append(img[i:i+1])\n\n        for i in range(0, len(all_images), batch_size): # take batch_size chunks and turn each into a new batch\n            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))  # will die horribly if the image batches had different width or height!\n\n        return (output_list,)\n```\n\n#### INPUT\\_IS\\_LIST"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/more_on_inputs",
  "markdown": "# Hidden and Flexible inputs - ComfyUI\n\nAlongside the `required` and `optional` inputs, which create corresponding inputs or widgets on the client-side, there are three `hidden` input options which allow the custom node to request certain information from the server. These are accessed by returning a value for `hidden` in the `INPUT_TYPES` `dict`, with the signature `dict[str,str]`, containing one or more of `PROMPT`, `EXTRA_PNGINFO`, or `UNIQUE_ID`\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": {...},\n        \"optional\": {...},\n        \"hidden\": {\n            \"unique_id\": \"UNIQUE_ID\",\n            \"prompt\": \"PROMPT\", \n            \"extra_pnginfo\": \"EXTRA_PNGINFO\",\n        }\n    }\n```\n\n### UNIQUE\\_ID\n\n`UNIQUE_ID` is the unique identifier of the node, and matches the `id` property of the node on the client side. It is commonly used in client-server communications (see [messages](https://docs.comfy.org/development/comfyui-server/comms_messages#getting-node-id)).\n\n### PROMPT\n\n`PROMPT` is the complete prompt sent by the client to the server. See [the prompt object](https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking#prompt) for a full description.\n\n`EXTRA_PNGINFO` is a dictionary that will be copied into the metadata of any `.png` files saved. Custom nodes can store additional information in this dictionary for saving (or as a way to communicate with a downstream node).\n\n### DYNPROMPT\n\n`DYNPROMPT` is an instance of `comfy_execution.graph.DynamicPrompt`. It differs from `PROMPT` in that it may mutate during the course of execution in response to [Node Expansion](https://docs.comfy.org/custom-nodes/backend/expansion).\n\n## Flexible inputs\n\n### Custom datatypes\n\nIf you want to pass data between your own custom nodes, you may find it helpful to define a custom datatype. This is (almost) as simple as just choosing a name for the datatype, which should be a unique string in upper case, such as `CHEESE`. You can then use `CHEESE` in your node `INPUT_TYPES` and `RETURN_TYPES`, and the Comfy client will only allow `CHEESE` outputs to connect to a `CHEESE` input. `CHEESE` can be any python object. The only point to note is that because the Comfy client doesnâ€™t know about `CHEESE` you need (unless you define a custom widget for `CHEESE`, which is a topic for another day), to force it to be an input rather than a widget. This can be done with the `forceInput` option in the input options dictionary:\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": { \"my_cheese\": (\"CHEESE\", {\"forceInput\":True}) }\n    }\n```\n\n### Wildcard inputs\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": { \"anything\": (\"*\",{})},\n    }\n\n@classmethod\ndef VALIDATE_INPUTS(s, input_types):\n    return True\n```\n\nThe frontend allows `*` to indicate that an input can be connected to any source. Because this is not officially supported by the backend, you can skip the backend validation of types by accepting a parameter named `input_types` in your `VALIDATE_INPUTS` function. (See [VALIDATE\\_INPUTS](https://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) for more information.) Itâ€™s up to the node to make sense of the data that is passed.\n\n### Dynamically created inputs\n\nIf inputs are dynamically created on the client side, they canâ€™t be defined in the Python source code. In order to access this data we need an `optional` dictionary that allows Comfy to pass data with arbitrary names. Since the Comfy server\n\n```\nclass ContainsAnyDict(dict):\n    def __contains__(self, key):\n        return True\n...\n\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": {},\n        \"optional\": ContainsAnyDict()\n    }\n...\n\ndef main_method(self, **kwargs):\n    # the dynamically created input data will be in the dictionary kwargs\n\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/tensors",
  "markdown": "# Working with torch.Tensor - ComfyUI\n\n## pytorch, tensors, and torch.Tensor\n\nAll the core number crunching in Comfy is done by [pytorch](https://pytorch.org/). If your custom nodes are going to get into the guts of stable diffusion you will need to become familiar with this library, which is way beyond the scope of this introduction. However, many custom nodes will need to manipulate images, latents and masks, each of which are represented internally as `torch.Tensor`, so youâ€™ll want to bookmark the [documentation for torch.Tensor](https://pytorch.org/docs/stable/tensors.html).\n\n### What is a Tensor?\n\n`torch.Tensor` represents a tensor, which is the mathematical generalization of a vector or matrix to any number of dimensions. A tensorâ€™s _rank_ is the number of dimensions it has (so a vector has _rank_ 1, a matrix _rank_ 2); its _shape_ describes the size of each dimension. So an RGB image (of height H and width W) might be thought of as three arrays (one for each color channel), each measuring H x W, which could be represented as a tensor with _shape_ `[H,W,3]`. In Comfy images almost always come in a batch (even if the batch only contains a single image). `torch` always places the batch dimension first, so Comfy images have _shape_ `[B,H,W,3]`, generally written as `[B,H,W,C]` where C stands for Channels.\n\n### squeeze, unsqueeze, and reshape\n\nIf a tensor has a dimension of size 1 (known as a collapsed dimension), it is equivalent to the same tensor with that dimension removed (a batch with 1 image is just an image). Removing such a collapsed dimension is referred to as squeezing, and inserting one is known as unsqueezing.\n\nTo represent the same data in a different shape is referred to as reshaping. This often requires you to know the underlying data structure, so handle with care!\n\n### Important notation\n\n`torch.Tensor` supports most Python slice notation, iteration, and other common list-like operations. A tensor also has a `.shape` attribute which returns its size as a `torch.Size` (which is a subclass of `tuple` and can be treated as such). There are some other important bits of notation youâ€™ll often see (several of these are less common standard Python notation, seen much more frequently when dealing with tensors)\n\n*   `torch.Tensor` supports the use of `None` in slice notation to indicate the insertion of a dimension of size 1.\n*   `:` is frequently used when slicing a tensor; this simply means â€˜keep the whole dimensionâ€™. Itâ€™s like using `a[start:end]` in Python, but omitting the start point and end point.\n*   `...` represents â€˜the whole of an unspecified number of dimensionsâ€™. So `a[0, ...]` would extract the first item from a batch regardless of the number of dimensions.\n*   in methods which require a shape to be passed, it is often passed as a `tuple` of the dimensions, in which a single dimension can be given the size `-1`, indicating that the size of this dimension should be calculated based on the total size of the data.\n\n```\n>>> a = torch.Tensor((1,2))\n>>> a.shape\ntorch.Size([2])\n>>> a[:,None].shape \ntorch.Size([2, 1])\n>>> a.reshape((1,-1)).shape\ntorch.Size([1, 2])\n```\n\n### Elementwise operations\n\nMany binary on `torch.Tensor` (including â€™+â€™, â€™-â€™, â€™\\*â€™, â€™/â€™ and â€™==â€™) are applied elementwise (independently applied to each element). The operands must be _either_ two tensors of the same shape, _or_ a tensor and a scalar. So:\n\n```\n>>> import torch\n>>> a = torch.Tensor((1,2))\n>>> b = torch.Tensor((3,2))\n>>> a*b\ntensor([3., 4.])\n>>> a/b\ntensor([0.3333, 1.0000])\n>>> a==b\ntensor([False,  True])\n>>> a==1\ntensor([ True, False])\n>>> c = torch.Tensor((3,2,1)) \n>>> a==c\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n```\n\n### Tensor truthiness\n\nYou may be familiar with the truthy value of a Python list as `True` for any non-empty list, and `False` for `None` or `[]`. By contrast A `torch.Tensor` (with more than one elements) does not have a defined truthy value. Instead you need to use `.all()` or `.any()` to combine the elementwise truthiness:\n\n```\n>>> a = torch.Tensor((1,2))\n>>> print(\"yes\" if a else \"no\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n>>> a.all()\ntensor(False)\n>>> a.any()\ntensor(True)\n```\n\nThis also means that you need to use `if a is not None:` not `if a:` to determine if a tensor variable has been set."
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_overview",
  "markdown": "# Javascript Extensions - ComfyUI\n\n## Extending the Comfy Client\n\nComfy can be modified through an extensions mechanism. To add an extension you need to:\n\n*   Export `WEB_DIRECTORY` from your Python module,\n*   Place one or more `.js` files into that directory,\n*   Use `app.registerExtension` to register your extension.\n\nThese three steps are below. Once you know how to add an extension, look through the [hooks](https://docs.comfy.org/custom-nodes/js/javascript_hooks) available to get your code called, a description of various [Comfy objects](https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking) you might need, or jump straight to some [example code snippets](https://docs.comfy.org/custom-nodes/js/javascript_examples).\n\n### Exporting `WEB_DIRECTORY`\n\nThe Comfy web client can be extended by creating a subdirectory in your custom node directory, conventionally called `js`, and exporting `WEB_DIRECTORY` - so your `__init_.py` will include something like:\n\n```\nWEB_DIRECTORY = \"./js\"\n__all__ = [\"NODE_CLASS_MAPPINGS\", \"NODE_DISPLAY_NAME_MAPPINGS\", \"WEB_DIRECTORY\"]\n```\n\n### Including `.js` files\n\n_Only_ `.js` files will be added to the webpage. Other resources (such as `.css` files) can be accessed at `extensions/custom_node_subfolder/the_file.css` and added programmatically.\n\n### Registering an extension\n\nThe basic structure of an extension follows is to import the main Comfy `app` object, and call `app.registerExtension`, passing a dictionary that contains a unique `name`, and one or more functions to be called by hooks in the Comfy code. A complete, trivial, and annoying, extension might look like this:\n\n```\nimport { app } from \"../../scripts/app.js\";\napp.registerExtension({ \n\tname: \"a.unique.name.for.a.useless.extension\",\n\tasync setup() { \n\t\talert(\"Setup complete!\")\n\t},\n})\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/backend/snippets",
  "markdown": "# Annotated Examples - ComfyUI\n\nA growing collection of fragments of example codeâ€¦\n\n## Images and Masks\n\n### Load an image\n\nLoad an image into a batch of size 1 (based on `LoadImage` source code in `nodes.py`)\n\n```\ni = Image.open(image_path)\ni = ImageOps.exif_transpose(i)\nif i.mode == 'I':\n    i = i.point(lambda i: i * (1 / 255))\nimage = i.convert(\"RGB\")\nimage = np.array(image).astype(np.float32) / 255.0\nimage = torch.from_numpy(image)[None,]\n```\n\n### Save an image batch\n\nSave a batch of images (based on `SaveImage` source code in `nodes.py`)\n\n```\nfor (batch_number, image) in enumerate(images):\n    i = 255. * image.cpu().numpy()\n    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n    filepath = # some path that takes the batch number into account\n    img.save(filepath)\n```\n\n### Invert a mask\n\nInverting a mask is a straightforward process. Since masks are normalised to the range \\[0,1\\]:\n\n### Convert a mask to Image shape\n\n```\n# We want [B,H,W,C] with C = 1\nif len(mask.shape)==2: # we have [H,W], so insert B and C as dimension 1\n    mask = mask[None,:,:,None]\nelif len(mask.shape)==3 and mask.shape[2]==1: # we have [H,W,C]\n    mask = mask[None,:,:,:]\nelif len(mask.shape)==3:                      # we have [B,H,W]\n    mask = mask[:,:,:,None]\n```\n\n### Using Masks as Transparency Layers\n\nWhen used for tasks like inpainting or segmentation, the MASKâ€™s values will eventually be rounded to the nearest integer so that they are binary â€” 0 indicating regions to be ignored and 1 indicating regions to be targeted. However, this doesnâ€™t happen until the MASK is passed to those nodes. This flexibility allows you to use MASKs as you would in digital photography contexts as a transparency layer:\n\n```\n# Invert mask back to original transparency layer\nmask = 1.0 - mask\n\n# Unsqueeze the `C` (channels) dimension\nmask = mask.unsqueeze(-1)\n\n# Concatenate (\"cat\") along the `C` dimension\nrgba_image = torch.cat((rgb_image, mask), dim=-1)\n```\n\n## Noise\n\n### Creating noise variations\n\nHereâ€™s an example of creating a noise object which mixes the noise from two sources. This could be used to create slight noise variations by varying `weight2`.\n\n```\nclass Noise_MixedNoise:\n    def __init__(self, nosie1, noise2, weight2):\n        self.noise1  = noise1\n        self.noise2  = noise2\n        self.weight2 = weight2\n\n    @property\n    def seed(self): return self.noise1.seed\n\n    def generate_noise(self, input_latent:torch.Tensor) -> torch.Tensor:\n        noise1 = self.noise1.generate_noise(input_latent)\n        noise2 = self.noise2.generate_noise(input_latent)\n        return noise1 * (1.0-self.weight2) + noise2 * (self.weight2)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/BasicScheduler",
  "markdown": "# BasicScheduler - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\n`BasicScheduler` èŠ‚ç‚¹æ—¨åœ¨æ ¹æ®æä¾›çš„è°ƒåº¦å™¨ã€æ¨¡åž‹å’ŒåŽ»å™ªå‚æ•°ä¸ºæ‰©æ•£æ¨¡åž‹è®¡ç®—ä¸€ç³»åˆ— sigma å€¼ã€‚å®ƒæ ¹æ®åŽ»å™ªå› å­åŠ¨æ€è°ƒæ•´æ€»æ­¥éª¤æ•°ï¼Œä»¥å¾®è°ƒæ‰©æ•£è¿‡ç¨‹ï¼Œåœ¨ä¸€äº›éœ€è¦ç²¾ç»†æŽ§åˆ¶çš„é«˜çº§çš„é‡‡æ ·è¿‡ç¨‹ï¼ˆæ¯”å¦‚åˆ†æ­¥é‡‡æ ·ï¼‰ç­‰ç­‰æä¾›äº†ç²¾ç»†çš„ä¸åŒé˜¶æ®µçš„â€œé…æ–¹â€\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | è¾“å…¥ç±»åž‹ | é»˜è®¤å€¼ | èŒƒå›´  | æ¯”å–»è¯´æ˜Ž | æŠ€æœ¯ä½œç”¨ |\n| --- | --- | --- | --- | --- | --- | --- |\n| `æ¨¡åž‹` | MODEL | Input | \\-  | \\-  | **ç”»å¸ƒç±»åž‹**ï¼šä¸åŒæè´¨çš„ç”»å¸ƒéœ€è¦ä¸åŒçš„é¢œæ–™é…æ–¹ | æ‰©æ•£æ¨¡åž‹å¯¹è±¡ï¼Œå†³å®šsigmaå€¼çš„è®¡ç®—åŸºç¡€ |\n| `è°ƒåº¦å™¨` | COMBO\\[STRING\\] | Widget | \\-  | 9ç§é€‰é¡¹ | **è°ƒè‰²æŠ€æ³•**ï¼šé€‰æ‹©é¢œæ–™æµ“åº¦å˜åŒ–çš„æ–¹å¼ | è°ƒåº¦ç®—æ³•ï¼ŒæŽ§åˆ¶å™ªå£°è¡°å‡æ¨¡å¼ |\n| `æ­¥æ•°` | INT | Widget | 20  | 1-10000 | **è°ƒè‰²æ¬¡æ•°**ï¼šè°ƒè‰²20æ¬¡ vs 50æ¬¡çš„ç²¾ç»†åº¦å·®å¼‚ | é‡‡æ ·æ­¥æ•°ï¼Œå½±å“ç”Ÿæˆè´¨é‡ä¸Žé€Ÿåº¦ |\n| `é™å™ª` | FLOAT | Widget | 1.0 | 0.0-1.0 | **åˆ›ä½œå¼ºåº¦**ï¼šä»Žå¾®è°ƒåˆ°é‡ç»˜çš„æŽ§åˆ¶åŠ›åº¦ | åŽ»å™ªå¼ºåº¦ï¼Œæ”¯æŒéƒ¨åˆ†é‡ç»˜åœºæ™¯ |\n\n### è°ƒåº¦å™¨ç±»åž‹è¯¦è§£\n\nåŸºäºŽæºç  `comfy.samplers.SCHEDULER_NAMES`ï¼Œæ”¯æŒä»¥ä¸‹9ç§è°ƒåº¦å™¨ï¼š\n\n| è°ƒåº¦å™¨åç§° | ç‰¹ç‚¹  | é€‚ç”¨åœºæ™¯ | å™ªå£°è¡°å‡ç‰¹æ€§ |\n| --- | --- | --- | --- |\n| **normal** | æ ‡å‡†çº¿æ€§è°ƒåº¦ | é€šç”¨åœºæ™¯ï¼Œå¹³è¡¡æ•ˆæžœ | å‡åŒ€é€’å‡ |\n| **karras** | å¹³æ»‘è¿‡æ¸¡è°ƒåº¦ | é«˜è´¨é‡ç”Ÿæˆï¼Œç»†èŠ‚ä¸°å¯Œ | å¹³æ»‘éžçº¿æ€§é€’å‡ |\n| **exponential** | æŒ‡æ•°é€’å‡è°ƒåº¦ | å¿«é€Ÿç”Ÿæˆï¼Œæ•ˆçŽ‡ä¼˜å…ˆ | æŒ‡æ•°åž‹å¿«é€Ÿé€’å‡ |\n| **sgm\\_uniform** | SGMå‡åŒ€è°ƒåº¦ | ç‰¹å®šæ¨¡åž‹ä¼˜åŒ– | SGMä¼˜åŒ–é€’å‡ |\n| **simple** | ç®€å•è°ƒåº¦ | å¿«é€Ÿæµ‹è¯•ï¼ŒåŸºç¡€åº”ç”¨ | ç®€åŒ–é€’å‡ |\n| **ddim\\_uniform** | DDIMå‡åŒ€è°ƒåº¦ | DDIMé‡‡æ ·ä¼˜åŒ– | DDIMç‰¹å®šé€’å‡ |\n| **beta** | Betaåˆ†å¸ƒè°ƒåº¦ | ç‰¹æ®Šåˆ†å¸ƒéœ€æ±‚ | Betaå‡½æ•°é€’å‡ |\n| **linear\\_quadratic** | çº¿æ€§äºŒæ¬¡è°ƒåº¦ | å¤æ‚åœºæ™¯ä¼˜åŒ– | äºŒæ¬¡å‡½æ•°é€’å‡ |\n| **kl\\_optimal** | KLæœ€ä¼˜è°ƒåº¦ | ç†è®ºæœ€ä¼˜åŒ– | KLæ•£åº¦ä¼˜åŒ–é€’å‡ |\n\n## è¾“å‡ºç»“æžœ\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | è¾“å‡ºç±»åž‹ | æ¯”å–»è¯´æ˜Ž | æŠ€æœ¯å«ä¹‰ |\n| --- | --- | --- | --- | --- |\n| `sigmas` | SIGMAS | Output | **è°ƒè‰²é…æ–¹è¡¨**ï¼šè¯¦ç»†çš„é¢œæ–™æµ“åº¦æ¸…å•ï¼Œä¾›ç”»å®¶é€æ­¥ä½¿ç”¨ | å™ªå£°æ°´å¹³åºåˆ—ï¼ŒæŒ‡å¯¼æ‰©æ•£æ¨¡åž‹çš„åŽ»å™ªè¿‡ç¨‹ |\n\n## èŠ‚ç‚¹è§’è‰²ï¼šç”»å®¶çš„è°ƒè‰²åŠ©æ‰‹\n\næƒ³è±¡ä½ æ˜¯ä¸€ä½ç”»å®¶ï¼Œæ­£åœ¨ä»Žä¸€å›¢æ··ä¹±çš„é¢œæ–™ï¼ˆå™ªå£°ï¼‰åˆ›ä½œæ¸…æ™°çš„å›¾åƒã€‚`BasicScheduler` å°±åƒæ˜¯çš„**ä¸“ä¸šè°ƒè‰²åŠ©æ‰‹**ï¼Œå®ƒçš„å·¥ä½œæ˜¯ä¸ºæ‚¨å‡†å¤‡ä¸€ç³»åˆ—ç²¾ç¡®çš„é¢œæ–™æµ“åº¦é…æ–¹ï¼š\n\n### å·¥ä½œæµç¨‹\n\n*   **ç¬¬1æ­¥**ï¼šä½¿ç”¨90%æµ“åº¦çš„é¢œæ–™ï¼ˆé«˜å™ªå£°æ°´å¹³ï¼‰\n*   **ç¬¬2æ­¥**ï¼šä½¿ç”¨80%æµ“åº¦çš„é¢œæ–™\n*   **ç¬¬3æ­¥**ï¼šä½¿ç”¨70%æµ“åº¦çš„é¢œæ–™\n*   **â€¦**\n*   **æœ€åŽä¸€æ­¥**ï¼šä½¿ç”¨0%æµ“åº¦ï¼ˆçº¯å‡€ç”»å¸ƒï¼Œæ— å™ªå£°ï¼‰\n\n### è°ƒè‰²åŠ©æ‰‹çš„ç‰¹æ®ŠæŠ€èƒ½\n\n**ä¸åŒçš„è°ƒè‰²æ–¹æ³•ï¼ˆschedulerï¼‰**ï¼š\n\n*   **â€œkarrasâ€è°ƒè‰²æ³•**ï¼šé¢œæ–™æµ“åº¦å˜åŒ–éžå¸¸å¹³æ»‘ï¼Œåƒä¸“ä¸šç”»å®¶çš„æ¸å˜æŠ€å·§\n*   **â€œexponentialâ€è°ƒè‰²æ³•**ï¼šé¢œæ–™æµ“åº¦å¿«é€Ÿé€’å‡ï¼Œé€‚åˆå¿«é€Ÿåˆ›ä½œ\n*   **â€œlinearâ€è°ƒè‰²æ³•**ï¼šé¢œæ–™æµ“åº¦å‡åŒ€é€’å‡ï¼Œç¨³å®šå¯æŽ§\n\n**ç²¾ç»†æŽ§åˆ¶ï¼ˆstepsï¼‰**ï¼š\n\n*   **20æ¬¡è°ƒè‰²**ï¼šå¿«é€Ÿä½œç”»ï¼Œæ•ˆçŽ‡ä¼˜å…ˆ\n*   **50æ¬¡è°ƒè‰²**ï¼šç²¾ç»†ä½œç”»ï¼Œè´¨é‡ä¼˜å…ˆ\n\n**åˆ›ä½œå¼ºåº¦ï¼ˆdenoiseï¼‰**ï¼š\n\n*   **1.0 = å…¨æ–°åˆ›ä½œ**ï¼šå®Œå…¨ä»Žç©ºç™½ç”»å¸ƒå¼€å§‹\n*   **0.5 = åŠæ”¹é€ **ï¼šä¿ç•™åŽŸç”»ä¸€åŠï¼Œæ”¹é€ ä¸€åŠ\n*   **0.2 = å¾®è°ƒä¼˜åŒ–**ï¼šåªå¯¹åŽŸç”»è¿›è¡Œç»†å¾®è°ƒæ•´\n\n### ä¸Žå…¶ä»–èŠ‚ç‚¹çš„é…åˆ\n\n`BasicScheduler`ï¼ˆè°ƒè‰²åŠ©æ‰‹ï¼‰â†’ å‡†å¤‡é…æ–¹ â†’ `SamplerCustom`ï¼ˆç”»å®¶ï¼‰â†’ å®žé™…ç»˜ç”» â†’ å®Œæˆä½œå“"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/Canny",
  "markdown": "# Canny - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\nä»Žç…§ç‰‡ä¸­æå–æ‰€æœ‰è¾¹ç¼˜çº¿æ¡ï¼Œå°±åƒç”¨é’¢ç¬”ä¸ºç…§ç‰‡æè¾¹ä¸€æ ·ï¼ŒæŠŠç‰©ä½“çš„è½®å»“å’Œç»†èŠ‚è¾¹ç•Œéƒ½ç”»å‡ºæ¥ã€‚\n\n## å·¥ä½œåŽŸç†\n\næƒ³è±¡æ‚¨æ˜¯ä¸€ä½ç”»å®¶ï¼Œè¦ç”¨é’¢ç¬”ä¸ºä¸€å¼ ç…§ç‰‡æè¾¹ã€‚CannyèŠ‚ç‚¹å°±åƒä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¸®æ‚¨å†³å®šå“ªäº›åœ°æ–¹éœ€è¦ç”»çº¿ï¼ˆè¾¹ç¼˜ï¼‰ï¼Œå“ªäº›åœ°æ–¹ä¸éœ€è¦ã€‚ è¿™ä¸ªè¿‡ç¨‹å°±åƒç­›é€‰å·¥ä½œï¼š\n\n*   **é«˜é˜ˆå€¼**æ˜¯â€å¿…é¡»ç”»çº¿çš„æ ‡å‡†â€ï¼šåªæœ‰éžå¸¸æ˜Žæ˜¾ã€æ¸…æ™°çš„è½®å»“çº¿æ‰ä¼šè¢«ç”»å‡ºæ¥ï¼Œæ¯”å¦‚äººç‰©è„¸éƒ¨è½®å»“ã€å»ºç­‘ç‰©è¾¹æ¡†\n*   **ä½Žé˜ˆå€¼**æ˜¯â€å®Œå…¨ä¸ç”»çº¿çš„æ ‡å‡†â€ï¼šå¤ªå¾®å¼±çš„è¾¹ç¼˜ä¼šè¢«å¿½ç•¥ï¼Œé¿å…ç”»å‡ºå™ªç‚¹å’Œæ— æ„ä¹‰çš„çº¿æ¡\n*   **ä¸­é—´åŒºåŸŸ**ï¼šä»‹äºŽä¸¤ä¸ªæ ‡å‡†ä¹‹é—´çš„è¾¹ç¼˜ï¼Œå¦‚æžœè¿žç€â€å¿…é¡»ç”»çš„çº¿â€å°±ä¸€èµ·ç”»å‡ºæ¥ï¼Œå¦‚æžœæ˜¯å­¤ç«‹çš„å°±ä¸ç”»\n\næœ€ç»ˆè¾“å‡ºä¸€å¼ é»‘ç™½å›¾åƒï¼Œç™½è‰²éƒ¨åˆ†æ˜¯æ£€æµ‹åˆ°çš„è¾¹ç¼˜çº¿æ¡ï¼Œé»‘è‰²éƒ¨åˆ†æ˜¯æ²¡æœ‰è¾¹ç¼˜çš„åŒºåŸŸã€‚\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | è¾“å…¥æ–¹å¼ | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | åŠŸèƒ½è¯´æ˜Ž |\n| --- | --- | --- | --- | --- | --- |\n| å›¾åƒ  | IMAGE | è¿žæŽ¥  | \\-  | \\-  | éœ€è¦æå–è¾¹ç¼˜çš„åŽŸå§‹ç…§ç‰‡ |\n| ä½Žé˜ˆå€¼ | FLOAT | æ‰‹åŠ¨è¾“å…¥ | 0.4 | 0.01-0.99 | ä½Žé˜ˆå€¼ï¼Œå†³å®šå¿½ç•¥å¤šå¼±çš„è¾¹ç¼˜ã€‚æ•°å€¼è¶Šå°ä¿ç•™çš„ç»†èŠ‚è¶Šå¤šï¼Œä½†å¯èƒ½äº§ç”Ÿå™ªç‚¹ |\n| é«˜é˜ˆå€¼ | FLOAT | æ‰‹åŠ¨è¾“å…¥ | 0.8 | 0.01-0.99 | é«˜é˜ˆå€¼ï¼Œå†³å®šä¿ç•™å¤šå¼ºçš„è¾¹ç¼˜ã€‚æ•°å€¼è¶Šå¤§åªä¿ç•™æœ€æ˜Žæ˜¾çš„è½®å»“çº¿ |\n\n## è¾“å‡º\n\n| è¾“å‡ºåç§° | æ•°æ®ç±»åž‹ | è¯´æ˜Ž  |\n| --- | --- | --- |\n| å›¾åƒ  | IMAGE | é»‘ç™½è¾¹ç¼˜å›¾åƒï¼Œç™½è‰²çº¿æ¡ä¸ºæ£€æµ‹åˆ°çš„è¾¹ç¼˜ï¼Œé»‘è‰²åŒºåŸŸä¸ºæ— è¾¹ç¼˜éƒ¨åˆ† |\n\n## å‚æ•°å¯¹æ¯”\n\n![åŽŸå›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/canny/input.webp) ![å‚æ•°å¯¹æ¯”](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/canny/compare.webp) **å¸¸è§é—®é¢˜ï¼š**\n\n*   è¾¹ç¼˜æ–­æ–­ç»­ç»­ï¼šå°è¯•é™ä½Žé«˜é˜ˆå€¼\n*   å‡ºçŽ°å¾ˆå¤šå™ªç‚¹ï¼šæé«˜ä½Žé˜ˆå€¼\n*   ä¸¢å¤±é‡è¦ç»†èŠ‚ï¼šé™ä½Žä½Žé˜ˆå€¼\n*   è¾¹ç¼˜å¤ªç²—ç³™ï¼šæ£€æŸ¥è¾“å…¥å›¾åƒè´¨é‡å’Œåˆ†è¾¨çŽ‡"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/CheckpointLoaderSimple",
  "markdown": "# CheckpointLoaderSimple - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\nè¿™æ˜¯ä¸€ä¸ªæ¨¡åž‹åŠ è½½å™¨èŠ‚ç‚¹ï¼Œç”¨äºŽä»ŽæŒ‡å®šä½ç½®åŠ è½½æ¨¡åž‹æ–‡ä»¶ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä¸»æ¨¡åž‹ã€æ–‡æœ¬ç¼–ç å™¨å’Œå›¾åƒç¼–è§£ç å™¨ã€‚ è¿™ä¸ªèŠ‚ç‚¹ä¼šè‡ªåŠ¨æ£€æµ‹`ComfyUI/models/checkpoints`æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ¨¡åž‹æ–‡ä»¶ï¼Œä»¥åŠä½ åœ¨extra\\_model\\_paths.yamlæ–‡ä»¶ä¸­é…ç½®çš„é¢å¤–è·¯å¾„ã€‚\n\n1.  **æ¨¡åž‹å…¼å®¹æ€§**ï¼šç¡®ä¿æ‰€é€‰æ¨¡åž‹ä¸Žä½ çš„å·¥ä½œæµç¨‹å…¼å®¹ï¼Œä¸åŒç±»åž‹çš„æ¨¡åž‹ï¼ˆå¦‚SD1.5ã€SDXLã€Fluxç­‰ï¼‰éœ€è¦é…åˆç›¸åº”çš„é‡‡æ ·å™¨å’Œå…¶ä»–èŠ‚ç‚¹\n2.  **æ–‡ä»¶ç®¡ç†**ï¼šå°†æ¨¡åž‹æ–‡ä»¶æ”¾åœ¨`ComfyUI/models/checkpoints`æ–‡ä»¶å¤¹ä¸­ï¼Œæˆ–é€šè¿‡extra\\_model\\_paths.yamlé…ç½®å…¶ä»–è·¯å¾„\n3.  **ç•Œé¢åˆ·æ–°**ï¼šå¦‚æžœåœ¨ComfyUIè¿è¡ŒæœŸé—´æ·»åŠ äº†æ–°çš„æ¨¡åž‹æ–‡ä»¶ï¼Œéœ€è¦åˆ·æ–°æµè§ˆå™¨ï¼ˆCtrl+Rï¼‰æ‰èƒ½åœ¨ä¸‹æ‹‰åˆ—è¡¨ä¸­çœ‹åˆ°æ–°æ–‡ä»¶\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | è¾“å…¥æ–¹å¼ | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | åŠŸèƒ½è¯´æ˜Ž |\n| --- | --- | --- | --- | --- | --- |\n| checkpointåç§° | STRING | ä¸‹æ‹‰é€‰æ‹© | null | checkpointsæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ¨¡åž‹æ–‡ä»¶ | é€‰æ‹©è¦åŠ è½½çš„æ£€æŸ¥ç‚¹æ¨¡åž‹æ–‡ä»¶åç§°ï¼Œå†³å®šäº†åŽç»­ç”Ÿå›¾ä½¿ç”¨çš„AIæ¨¡åž‹ |\n\n## è¾“å‡º\n\n| è¾“å‡ºåç§° | æ•°æ®ç±»åž‹ | è¯´æ˜Ž  |\n| --- | --- | --- |\n| æ¨¡åž‹  | MODEL | ç”¨äºŽå›¾åƒåŽ»å™ªç”Ÿæˆçš„ä¸»è¦æ‰©æ•£æ¨¡åž‹ï¼Œæ˜¯AIç»˜ç”»çš„æ ¸å¿ƒç»„ä»¶ |\n| CLIP | CLIP | ç”¨äºŽç¼–ç æ–‡æœ¬æç¤ºè¯çš„æ¨¡åž‹ï¼Œå°†æ–‡å­—æè¿°è½¬æ¢ä¸ºAIèƒ½ç†è§£çš„ä¿¡æ¯ |\n| VAE | VAE | ç”¨äºŽå›¾åƒç¼–è§£ç çš„æ¨¡åž‹ï¼Œè´Ÿè´£åœ¨åƒç´ ç©ºé—´å’Œæ½œåœ¨ç©ºé—´ä¹‹é—´è½¬æ¢ |"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipLoader",
  "markdown": "# åŠ è½½CLIP - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\nè¯¥èŠ‚ç‚¹ä¸»è¦ç”¨äºŽå•ç‹¬åŠ è½½ CLIP æ–‡æœ¬ç¼–ç å™¨æ¨¡åž‹ã€‚ æ”¯æŒæ£€æµ‹ä»¥ä¸‹è·¯å¾„çš„æ¨¡åž‹æ–‡ä»¶æ£€æµ‹ï¼š\n\n*   â€œComfyUI/models/text\\_encoders/â€\n*   â€œComfyUI/models/clip/â€\n\n> å¦‚æžœä½ æ˜¯åœ¨ ComfyUI å¯åŠ¨åŽæ‰ä¿å­˜çš„æ¨¡åž‹åˆ™éœ€è¦åˆ·æ–° ComfyUI å‰ç«¯æ¥èŽ·å–æœ€æ–°çš„æ¨¡åž‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨\n\næ”¯æŒçš„æ¨¡åž‹æ ¼å¼æœ‰:\n\n*   `.ckpt`\n*   `.pt`\n*   `.pt2`\n*   `.bin`\n*   `.pth`\n*   `.safetensors`\n*   `.pkl`\n*   `.sft`\n\næ›´å¤šæœ€æ–°æ¨¡åž‹æ–‡ä»¶åŠ è½½è¯¦æƒ…è¯·æŸ¥é˜…[folder\\_paths](https://github.com/comfyanonymous/ComfyUI/blob/master/folder_paths.py)\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | ä½œç”¨  |\n| --- | --- | --- |\n| `CLIPåç§°` | COMBO\\[STRING\\] | æŒ‡å®šè¦åŠ è½½çš„ CLIP æ¨¡åž‹çš„åç§°ã€‚æ­¤åç§°ç”¨äºŽåœ¨é¢„å®šä¹‰çš„ç›®å½•ç»“æž„å†…å®šä½æ¨¡åž‹æ–‡ä»¶ã€‚ |\n| `ç±»åž‹` | COMBO\\[STRING\\] | ç¡®å®šè¦åŠ è½½çš„ CLIP æ¨¡åž‹ç±»åž‹ï¼Œéšç€ ComfyUI æ”¯æŒçš„æ¨¡åž‹æ•°é‡å¢žåŠ è¿™é‡Œçš„ç±»åž‹ä¹Ÿä¼šæ–°å¢žå¯¹åº”çš„ç±»åž‹ï¼Œè¯·æŸ¥çœ‹[node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py)ä¸­æºç é‡Œå…³äºŽ`CLIPLoader` ç±»çš„ç›¸å…³å®šä¹‰ |\n| `è®¾å¤‡` | COMBO\\[STRING\\] | é€‰æ‹©åŠ è½½ CLIP æ¨¡åž‹çš„è®¾å¤‡ï¼Œ`default` å°†ä¼šå°†å¯¹åº”çš„æ¨¡åž‹åœ¨ GPU ä¸Šè¿è¡Œï¼Œå¦‚æžœé€‰æ‹©`CPU` å°†å¼ºåˆ¶åœ¨ CPUä¸Šè¿›è¡ŒåŠ è½½ |\n\n### ä¸åŒ`è®¾å¤‡`é€‰é¡¹çš„è¯´æ˜Ž\n\n**é€‰æ‹© â€œdefaultâ€ çš„æƒ…å†µ**ï¼š\n\n*   æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜\n*   å¸Œæœ›èŽ·å¾—æœ€ä½³æ€§èƒ½\n*   è®©ç³»ç»Ÿè‡ªåŠ¨ä¼˜åŒ–å†…å­˜ä½¿ç”¨\n\n**é€‰æ‹© â€œcpuâ€ çš„æƒ…å†µ**ï¼š\n\n*   GPU å†…å­˜ä¸è¶³\n*   éœ€è¦ä¸ºå…¶ä»–æ¨¡åž‹ï¼ˆå¦‚ UNetï¼‰ä¿ç•™ GPU å†…å­˜\n*   åœ¨ä½Ž VRAM çŽ¯å¢ƒä¸‹è¿è¡Œ\n*   è°ƒè¯•æˆ–ç‰¹æ®Šç”¨é€”éœ€è¦\n\n**æ€§èƒ½å½±å“** CPU è¿è¡Œä¼šæ¯” GPU è¿è¡Œæ…¢å¾ˆå¤šï¼Œä½†å¯ä»¥èŠ‚çœå®è´µçš„ GPU å†…å­˜ä¾›å…¶ä»–æ›´é‡è¦çš„æ¨¡åž‹ç»„ä»¶ä½¿ç”¨ã€‚åœ¨å†…å­˜å—é™çš„çŽ¯å¢ƒä¸­ï¼Œå°† CLIP æ¨¡åž‹æ”¾åœ¨ CPU ä¸Šæ˜¯ä¸€ä¸ªå¸¸è§çš„ä¼˜åŒ–ç­–ç•¥ã€‚\n\n### æ”¯æŒçš„æ­é…\n\n| æ¨¡åž‹ç±»åž‹ | å¯¹åº”ç¼–ç å™¨ |\n| --- | --- |\n| stable\\_diffusion | clip-l |\n| stable\\_cascade | clip-g |\n| sd3 | t5 xxl/ clip-g / clip-l |\n| stable\\_audio | t5 base |\n| mochi | t5 xxl |\n| cosmos | old t5 xxl |\n| lumina2 | gemma 2 2B |\n| wan | umt5 xxl |\n\næœªæ¥éšç€ ComfyUI çš„æ›´æ–°ï¼Œè¿™ä¸ªæ›´æ–°æ­é…å¯èƒ½ä¼šæ–°å¢žï¼Œè¯¦æƒ…è¯·å‚è€ƒ [node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py)ä¸­æºç é‡Œå…³äºŽ`CLIPLoader` ç±»çš„ç›¸å…³å®šä¹‰è¯´æ˜Ž\n\n## è¾“å‡º\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | ä½œç”¨  |\n| --- | --- | --- |\n| `clip` | CLIP | åŠ è½½çš„ CLIP æ¨¡åž‹ï¼Œå‡†å¤‡ç”¨äºŽä¸‹æ¸¸ä»»åŠ¡æˆ–è¿›ä¸€æ­¥å¤„ç†ã€‚ |\n\n## å…¶å®ƒæ‰©å±•\n\nCLIP æ¨¡åž‹åœ¨ ComfyUI ä¸­æ‰®æ¼”ç€æ–‡æœ¬ç¼–ç å™¨çš„æ ¸å¿ƒè§’è‰²ï¼Œè´Ÿè´£å°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸ºå¯ä¾›æ‰©æ•£æ¨¡åž‹ç†è§£çš„æ•°å€¼è¡¨ç¤ºï¼Œä½ å¯ä»¥æŠŠå®ƒç†è§£æˆç¿»è¯‘å®˜ï¼Œè´Ÿè´£å°†ä½ çš„æ–‡æœ¬ç¿»è¯‘æˆå¤§æ¨¡åž‹å¯ä»¥ç†è§£çš„è¯­è¨€ï¼Œå½“ç„¶ä¸åŒæ¨¡åž‹ä¹Ÿå­˜åœ¨ç€ â€œæ–¹è¨€â€ ï¼Œæ‰€ä»¥åœ¨ä¸åŒæž¶æž„çš„æ¨¡åž‹ä¹‹é—´éœ€è¦ä¸åŒçš„ CLIP æ¨¡åž‹æ¥å®Œæˆæ–‡æœ¬ç¼–ç çš„è¿™ä¸€è¿‡ç¨‹ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipMergeSimple",
  "markdown": "# CLIPèžåˆç®€æ˜“ - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\n`CLIPèžåˆç®€æ˜“` æ˜¯ä¸€ä¸ªé«˜çº§æ¨¡åž‹åˆå¹¶èŠ‚ç‚¹ï¼Œç”¨äºŽå°†ä¸¤ä¸ª CLIP æ–‡æœ¬ç¼–ç å™¨æ¨¡åž‹æŒ‰æŒ‡å®šæ¯”ä¾‹è¿›è¡Œåˆå¹¶. æ­¤èŠ‚ç‚¹ä¸“é—¨ç”¨äºŽæ ¹æ®æŒ‡å®šæ¯”ä¾‹åˆå¹¶ä¸¤ä¸ªCLIPæ¨¡åž‹ï¼Œæœ‰æ•ˆåœ°æ··åˆå®ƒä»¬çš„ç‰¹æ€§ã€‚å®ƒæœ‰é€‰æ‹©æ€§åœ°å°†ä¸€ä¸ªæ¨¡åž‹çš„è¡¥ä¸åº”ç”¨åˆ°å¦ä¸€ä¸ªæ¨¡åž‹ä¸Šï¼ŒæŽ’é™¤åƒä½ç½®IDå’Œå¯¹æ•°å°ºåº¦è¿™æ ·çš„ç‰¹å®šç»„ä»¶ï¼Œä»¥åˆ›å»ºä¸€ä¸ªç»“åˆäº†ä¸¤ä¸ªæºæ¨¡åž‹ç‰¹å¾çš„æ··åˆæ¨¡åž‹ã€‚\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | ä½œç”¨  |\n| --- | --- | --- |\n| `clip1` | CLIP | è¦åˆå¹¶çš„ç¬¬ä¸€ä¸ªCLIPæ¨¡åž‹ã€‚å®ƒä½œä¸ºåˆå¹¶è¿‡ç¨‹çš„åŸºç¡€æ¨¡åž‹ã€‚ |\n| `clip2` | CLIP | è¦åˆå¹¶çš„ç¬¬äºŒä¸ªCLIPæ¨¡åž‹ã€‚æ ¹æ®æŒ‡å®šçš„æ¯”ä¾‹ï¼Œé™¤ä½ç½®IDå’Œå¯¹æ•°å°ºåº¦å¤–ï¼Œå…¶å…³é”®è¡¥ä¸å°†åº”ç”¨äºŽç¬¬ä¸€ä¸ªæ¨¡åž‹ã€‚ |\n| `æ¯”ä¾‹` | FLOAT | å–å€¼èŒƒå›´ `0.0 - 1.0` ç¡®å®šä»Žç¬¬äºŒä¸ªæ¨¡åž‹èžåˆåˆ°ç¬¬ä¸€ä¸ªæ¨¡åž‹ä¸­çš„ç‰¹æ€§æ¯”ä¾‹ã€‚æ¯”ä¾‹ä¸º1.0æ„å‘³ç€å®Œå…¨é‡‡ç”¨ç¬¬äºŒä¸ªæ¨¡åž‹çš„ç‰¹æ€§ï¼Œè€Œ0.0åˆ™ä»…ä¿ç•™ç¬¬ä¸€ä¸ªæ¨¡åž‹çš„ç‰¹æ€§ã€‚ |\n\n## è¾“å‡º\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | ä½œç”¨  |\n| --- | --- | --- |\n| `clip` | CLIP | ç»“æžœåˆå¹¶çš„CLIPæ¨¡åž‹ï¼Œæ ¹æ®æŒ‡å®šçš„æ¯”ä¾‹æ•´åˆäº†ä¸¤ä¸ªè¾“å…¥æ¨¡åž‹çš„ç‰¹æ€§ã€‚ |\n\n## åˆå¹¶æœºåˆ¶è¯¦è§£\n\n### åˆå¹¶ç®—æ³•\n\nèŠ‚ç‚¹ä½¿ç”¨åŠ æƒå¹³å‡çš„æ–¹å¼åˆå¹¶ä¸¤ä¸ªæ¨¡åž‹ï¼š\n\n1.  **å…‹éš†åŸºç¡€æ¨¡åž‹**: é¦–å…ˆå…‹éš† `clip1` ä½œä¸ºåŸºç¡€æ¨¡åž‹\n2.  **èŽ·å–è¡¥ä¸**: ä»Ž `clip2` èŽ·å–æ‰€æœ‰é”®å€¼è¡¥ä¸ (key patches)\n3.  **è¿‡æ»¤ç‰¹æ®Šé”®**: è·³è¿‡ `.position_ids` å’Œ `.logit_scale` ç»“å°¾çš„é”®\n4.  **åº”ç”¨åŠ æƒåˆå¹¶**: ä½¿ç”¨å…¬å¼ `(1.0 - æ¯”ä¾‹) * clip1 + æ¯”ä¾‹ * clip2`\n\n### æ¯”ä¾‹ å‚æ•°è¯´æ˜Ž\n\n*   **æ¯”ä¾‹ = 0.0**: å®Œå…¨ä½¿ç”¨ clip1ï¼Œå¿½ç•¥ clip2\n*   **æ¯”ä¾‹ = 0.5**: ä¸¤ä¸ªæ¨¡åž‹å„å  50%\n*   **æ¯”ä¾‹ = 1.0**: å®Œå…¨ä½¿ç”¨ clip2ï¼Œå¿½ç•¥ clip1\n\n## ä½¿ç”¨åœºæ™¯\n\n1.  **æ¨¡åž‹é£Žæ ¼èžåˆ**: ç»“åˆä¸åŒè®­ç»ƒæ•°æ®çš„ CLIP æ¨¡åž‹ç‰¹ç‚¹\n2.  **æ€§èƒ½ä¼˜åŒ–**: å¹³è¡¡ä¸åŒæ¨¡åž‹çš„ä¼˜ç¼ºç‚¹\n3.  **å®žéªŒç ”ç©¶**: æŽ¢ç´¢ä¸åŒ CLIP ç¼–ç å™¨çš„ç»„åˆæ•ˆæžœ"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipSave",
  "markdown": "# ä¿å­˜CLIP - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\n`CLIPä¿å­˜` èŠ‚ç‚¹ç”¨äºŽå°† CLIP æ–‡æœ¬ç¼–ç å™¨æ¨¡åž‹ä¿å­˜ä¸º SafeTensors æ ¼å¼æ–‡ä»¶, è¯¥èŠ‚ç‚¹å±žäºŽé«˜çº§æ¨¡åž‹åˆå¹¶å·¥ä½œæµçš„ä¸€éƒ¨åˆ†ï¼Œé€šå¸¸ä¸Ž `CLIPMergeSimple`ã€`CLIPMergeAdd` ç­‰èŠ‚ç‚¹é…åˆä½¿ç”¨ã€‚ä¿å­˜çš„æ–‡ä»¶é‡‡ç”¨ SafeTensors æ ¼å¼ï¼Œç¡®ä¿å®‰å…¨æ€§å’Œå…¼å®¹æ€§ã€‚\n\n## è¾“å…¥\n\n| å‚æ•°å | ç±»åž‹  | æ˜¯å¦å¿…éœ€ | é»˜è®¤å€¼ | æè¿°  |\n| --- | --- | --- | --- | --- |\n| `clip` | CLIP | å¿…éœ€  | \\-  | è¦ä¿å­˜çš„ CLIP æ¨¡åž‹ |\n| `æ–‡ä»¶åå‰ç¼€` | STRING | å¿…éœ€  | `\"clip/ComfyUI\"` | ä¿å­˜æ–‡ä»¶çš„å‰ç¼€è·¯å¾„ |\n| `prompt` | PROMPT | éšè—å‚æ•° | \\-  | å·¥ä½œæµæç¤ºä¿¡æ¯ï¼ˆç”¨äºŽå…ƒæ•°æ®ï¼‰ |\n| `extra_pnginfo` | EXTRA\\_PNGINFO | éšè—å‚æ•° | \\-  | é¢å¤–çš„PNGä¿¡æ¯ï¼ˆç”¨äºŽå…ƒæ•°æ®ï¼‰ |\n\n## è¾“å‡º\n\nè¯¥èŠ‚ç‚¹æ²¡æœ‰å®šä¹‰è¾“å‡ºç±»åž‹, ä¼šå°†å¤„ç†åŽçš„æ–‡ä»¶ä¿å­˜åˆ° `ComfyUI/output/` æ–‡ä»¶å¤¹ä¸‹\n\n### å¤šæ–‡ä»¶ä¿å­˜ç­–ç•¥\n\nèŠ‚ç‚¹ä¼šæ ¹æ® CLIP æ¨¡åž‹ç±»åž‹åˆ†åˆ«ä¿å­˜ä¸åŒç»„ä»¶ï¼š\n\n| å‰ç¼€ç±»åž‹ | æ–‡ä»¶ååŽç¼€ | è¯´æ˜Ž  |\n| --- | --- | --- |\n| `clip_l.` | `_clip_l` | CLIP-L æ–‡æœ¬ç¼–ç å™¨ |\n| `clip_g.` | `_clip_g` | CLIP-G æ–‡æœ¬ç¼–ç å™¨ |\n| ç©ºå‰ç¼€ | æ— åŽç¼€ | å…¶ä»– CLIP ç»„ä»¶ |"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking",
  "markdown": "# Comfy Objects - ComfyUI\n\n## LiteGraph\n\nThe Comfy UI is built on top of [LiteGraph](https://github.com/jagenjo/litegraph.js). Much of the Comfy functionality is provided by LiteGraph, so if developing more complex nodes you will probably find it helpful to clone that repository and browse the documentation, which can be found at `doc/index.html`.\n\n## ComfyApp\n\nThe `app` object (always accessible by `import { app } from \"../../scripts/app.js\";`) represents the Comfy application running in the browser, and contains a number of useful properties and functions, some of which are listed below.\n\n### Properties\n\nImportant properties of `app` include (this is not an exhaustive list):\n\n| property | contents |\n| --- | --- |\n| `canvas` | An LGraphCanvas object, representing the current user interface. It contains some potentially interesting properties, such as `node_over` and `selected_nodes`. |\n| `canvasEl` | The DOM `<canvas>` element |\n| `graph` | A reference to the LGraph object describing the current graph |\n| `runningNodeId` | During execution, the node currently being executed |\n| `ui` | Provides access to some UI elements, such as the queue, menu, and dialogs |\n\n`canvas` (for graphical elements) and `graph` (for logical connections) are probably the ones you are most likely to want to access.\n\n### Functions\n\nAgain, there are many. A few significant ones are:\n\n| function | notes |\n| --- | --- |\n| graphToPrompt | Convert the graph into a prompt that can be sent to the Python server |\n| loadGraphData | Load a graph |\n| queuePrompt | Submit a prompt to the queue |\n| registerExtension | Youâ€™ve seen this one - used to add an extension |\n\n## LGraph\n\nThe `LGraph` object is part of the LiteGraph framework, and represents the current logical state of the graph (nodes and links). If you want to manipulate the graph, the LiteGraph documentation (at `doc/index.html` if you clone `https://github.com/jagenjo/litegraph.js`) describes the functions you will need. You can use `graph` to obtain details of nodes and links, for example:\n\n```\nconst ComfyNode_object_for_my_node = app.graph._nodes_by_id(my_node_id) \nComfyNode_object_for_my_node.inputs.forEach(input => {\n    const link_id = input.link;\n    if (link_id) {\n        const LLink_object = app.graph.links[link_id]\n        const id_of_upstream_node = LLink_object.origin_id\n        // etc\n    }\n});\n```\n\n## LLink\n\nThe `LLink` object, accessible through `graph.links`, represents a single link in the graph, from node `link.origin_id` output slot `link.origin_slot` to node `link.target_id` slot `link.target_slot`. It also has a string representing the data type, in `link.type`, and `link.id`. `LLink`s are created in the `connect` method of a `LGraphNode` (of which `ComfyNode` is a subclass).\n\n## ComfyNode\n\n`ComfyNode` is a subclass of `LGraphNode`, and the LiteGraph documentation is therefore helpful for more generic operations. However, Comfy has significantly extended the LiteGraph core behavior, and also does not make use of all LiteGraph functionality.\n\nA `ComfyNode` object represents a node in the current workflow. It has a number of important properties that you may wish to make use of, a very large number of functions that you may wish to use, or hijack to modify behavior. To get a more complete sense of the node object, you may find it helpful to insert the following code into your extension and place a breakpoint on the `console.log` command. When you then create a new node you can use your favorite debugger to interrogate the node.\n\n```\nasync nodeCreated(node) {\n    console.log(\"nodeCreated\")\n}\n```\n\n### Properties\n\n| property | contents |\n| --- | --- |\n| `bgcolor` | The background color of the node, or undefined for the default |\n| `comfyClass` | The Python class representing the node |\n| `flags` | A dictionary that may contain flags related to the state of the node. In particular, `flags.collapsed` is true for collapsed nodes. |\n| `graph` | A reference to the LGraph object |\n| `id` | A unique id |\n| `input_type` | A list of the input types (eg â€œSTRINGâ€, â€œMODELâ€, â€œCLIPâ€ etc). Generally matches the Python INPUT\\_TYPES |\n| `inputs` | A list of inputs (discussed below) |\n| `mode` | Normally 0, set to 2 if the node is muted and 4 if the node is bypassed. Values of 1 and 3 are not used by Comfy |\n| `order` | The nodeâ€™s position in the execution order. Set by `LGraph.computeExecutionOrder()` when the prompt is submitted |\n| `pos` | The \\[x,y\\] position of the node on the canvas |\n| `properties` | A dictionary containing `\"Node name for S&R\"`, used by LiteGraph |\n| `properties_info` | The type and default value of entries in `properties` |\n| `size` | The width and height of the node on the canvas |\n| `title` | Display Title |\n| `type` | The unique name (from Python) of the node class |\n| `widgets` | A list of widgets (discussed below) |\n| `widgets_values` | A list of the current values of widgets |\n\n### Functions\n\nThere are a very large number of functions (85, last time I counted). A selection are listed below. Most of these functions are unmodified from the LiteGraph core code.\n\n#### Inputs, Outputs, Widgets\n\n| function | notes |\n| --- | --- |\n| Inputs / Outputs | Most have output methods with the equivalent names: s/In/Out/ |\n| `addInput` | Create a new input, defined by name and type |\n| `addInputs` | Array version of `addInput` |\n| `findInputSlot` | Find the slot index from the input name |\n| `findInputSlotByType` | Find an input matching the type. Options to prefer, or only use, free slots |\n| `removeInput` | By slot index |\n| `getInputNode` | Get the node connected to this input. The output equivalent is `getOutputNodes` and returns a list |\n| `getInputLink` | Get the LLink connected to this input. No output equivalent |\n| Widgets |     |\n| `addWidget` | Add a standard Comfy widget |\n| `addCustomWidget` | Add a custom widget (defined in the `getComfyWidgets` hook) |\n| `addDOMWidget` | Add a widget defined by a DOM element |\n| `convertWidgetToInput` | Convert a widget to an input if allowed by `isConvertableWidget` (in `widgetInputs.js`) |\n\n#### Connections\n\n| function | notes |\n| --- | --- |\n| `connect` | Connect this nodeâ€™s output to another nodeâ€™s input |\n| `connectByType` | Connect output to another node by specifying the type - connects to first available matching slot |\n| `connectByTypeOutput` | Connect input to another node output by type |\n| `disconnectInput` | Remove any link into the input (specified by name or index) |\n| `disconnectOutput` | Disconnect an output from a specified nodeâ€™s input |\n| `onConnectionChange` | Called on each node. `side==1` if itâ€™s an input on this node |\n| `onConnectInput` | Called _before_ a connection is made. If this returns `false`, the connection is refused |\n\n#### Display\n\n| function | notes |\n| --- | --- |\n| `setDirtyCanvas` | Specify that the foreground (nodes) and/or background (links and images) need to be redrawn |\n| `onDrawBackground` | Called with a `CanvasRenderingContext2D` object to draw the background. Used by Comfy to render images |\n| `onDrawForeground` | Called with a `CanvasRenderingContext2D` object to draw the node. |\n| `getTitle` | The title to be displayed. |\n| `collapse` | Toggles the collapsed state of the node. |\n\n#### Other\n\n| function | notes |\n| --- | --- |\n| `changeMode` | Use to set the node to bypassed (`mode == 4`) or not (`mode == 0`) |\n\nInputs and Widgets represent the two ways that data can be fed into a node. In general a widget can be converted to an input, but not all inputs can be converted to a widget (as many datatypes canâ€™t be entered through a UI element). `node.inputs` is a list of the current inputs (colored dots on the left hand side of the node), specifying their `.name`, `.type`, and `.link` (a reference to the connected `LLink` in `app.graph.links`). If an input is a widget which has been converted, it also holds a reference to the, now inactive, widget in `.widget`. `node.widgets` is a list of all widgets, whether or not they have been converted to an input. A widget has:\n\n| property/function | notes |\n| --- | --- |\n| `callback` | A function called when the widget value is changed |\n| `last_y` | The vertical position of the widget in the node |\n| `name` | The (unique within a node) widget name |\n| `options` | As specified in the Python code (such as default, min, and max) |\n| `type` | The name of the widget type (see below) in lowercase |\n| `value` | The current widget value. This is a property with `get` and `set` methods |\n\n### Widget Types\n\n`app.widgets` is a dictionary of currently registered widget types, keyed in the UPPER CASE version of the name of the type. Build in Comfy widgets types include the self explanatory `BOOLEAN`, `INT`, and `FLOAT`, as well as `STRING` (which comes in two flavours, single line and multiline), `COMBO` for dropdown selection from a list, and `IMAGEUPLOAD`, used in Load Image nodes. Custom widget types can be added by providing a `getCustomWidgets` method in your extension.\n\n### Linked widgets\n\nWidgets can also be linked - the built in behavior of `seed` and `control_after_generate`, for example. A linked widget has `.type = 'base_widget_type:base_widget_name'`; so `control_after_generate` may have type `int:seed`.\n\n## Prompt\n\nWhen you press the `Queue Prompt` button in Comfy, the `app.graphToPrompt()` method is called to convert the current graph into a prompt that can be sent to the server. `app.graphToPrompt` returns an object (referred to herein as `prompt`) with two properties, `output` and `workflow`.\n\n### output\n\n`prompt.output` maps from the `node_id` of each node in the graph to an object with two properties.\n\n*   `prompt.output[node_id].class_type`, the unique name of the custom node class, as defined in the Python code\n*   `prompt.output[node_id].inputs`, which contains the value of each input (or widget) as a map from the input name to:\n    *   the selected value, if it is a widget, or\n    *   an array containing (`upstream_node_id`, `upstream_node_output_slot`) if there is a link connected to the input, or\n    *   undefined, if it is a widget that has been converted to an input and is not connected\n    *   other unconnected inputs are not included in `.inputs`\n\n### workflow\n\n`prompt.workflow` contains the following properties:\n\n*   `config` - a dictionary of additional configuration options (empty by default)\n*   `extra` - a dictionary containing extra information about the workflow. By default it contains:\n    *   `extra.ds` - describes the current view of the graph (`scale` and `offset`)\n*   `groups` - all groups in the workflow\n*   `last_link_id` - the id of the last link added\n*   `last_node_id` - the id of the last node added\n*   `links` - a list of all links in the graph. Each entry is an array of five integers and one string:\n    *   (`link_id`, `upstream_node_id`, `upstream_node_output_slot`, `downstream_node_id`, `downstream_node_input_slot`, `data type`)\n*   `nodes` - a list of all nodes in the graph. Each entry is a map of a subset of the properties of the node as described [above](#comfynode)\n    *   The following properties are included: `flags`, `id`, `inputs`, `mode`, `order`, `pos`, `properties`, `size`, `type`, `widgets_values`\n    *   In addition, unless a node has no outputs, there is an `outputs` property, which is a list of the outputs of the node, each of which contains:\n        *   `name` - the name of the output\n        *   `type` - the data type of the output\n        *   `links` - a list of the `link_id` of all links from this output (if there are no connections, may be an empty list, or null),\n        *   `shape` - the shape used to draw the output (default 3 for a dot)\n        *   `slot_index` - the slot number of the output\n*   `version` - the LiteGraph version number (at time of writing, `0.4`)"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_settings",
  "markdown": "# Settings - ComfyUI\n\nYou can provide a settings object to ComfyUI that will show up when the user opens the ComfyUI settings panel.\n\n## Basic operation\n\n### Add a setting\n\n```\nimport { app } from \"../../scripts/app.js\";\n\napp.registerExtension({\n    name: \"My Extension\",\n    settings: [\n        {\n            id: \"example.boolean\",\n            name: \"Example boolean setting\",\n            type: \"boolean\",\n            defaultValue: false,\n        },\n    ],\n});\n```\n\nThe `id` must be unique across all extensions and will be used to fetch values. If you do not [provide a category](#categories), then the `id` will be split by `.` to determine where it appears in the settings panel.\n\n*   If your `id` doesnâ€™t contain any `.` then it will appear in the â€œOtherâ€ category and your `id` will be used as the section heading.\n*   If your `id` contains at least one `.` then the leftmost part will be used as the setting category and the second part will be used as the section heading. Any further parts are ignored.\n\n### Read a setting\n\n```\nimport { app } from \"../../scripts/app.js\";\n\nif (app.extensionManager.setting.get('example.boolean')) {\n    console.log(\"Setting is enabled.\");\n} else {\n    console.log(\"Setting is disabled.\");\n}\n```\n\n### React to changes\n\nThe `onChange()` event handler will be called as soon as the user changes the setting in the settings panel. This will also be called when the extension is registered, on every page load.\n\n```\n{\n    id: \"example.boolean\",\n    name: \"Example boolean setting\",\n    type: \"boolean\",\n    defaultValue: false,\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Write a setting\n\n```\nimport { app } from \"../../scripts/app.js\";\n\ntry {\n    await app.extensionManager.setting.set(\"example.boolean\", true);\n} catch (error) {\n    console.error(`Error changing setting: ${error}`);\n}\n```\n\nThe setting types are based on [PrimeVue](https://primevue.org/) components. Props described in the PrimeVue documentation can be defined for ComfyUI settings by adding them in an `attrs` field. For instance, this adds increment/decrement buttons to a number input:\n\n```\n{\n    id: \"example.number\",\n    name: \"Example number setting\",\n    type: \"number\",\n    defaultValue: 0,\n    attrs: {\n        showButtons: true,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n## Types\n\n### Boolean\n\nThis shows an on/off toggle. Based on the [ToggleSwitch PrimeVue component](https://primevue.org/toggleswitch/).\n\n```\n{\n    id: \"example.boolean\",\n    name: \"Example boolean setting\",\n    type: \"boolean\",\n    defaultValue: false,\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Text\n\nThis is freeform text. Based on the [InputText PrimeVue component](https://primevue.org/inputtext/).\n\n```\n{\n    id: \"example.text\",\n    name: \"Example text setting\",\n    type: \"text\",\n    defaultValue: \"Foo\",\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Number\n\nThis for entering numbers. To allow decimal places, set the `maxFractionDigits` attribute to a number greater than zero. Based on the [InputNumber PrimeVue component](https://primevue.org/inputnumber/).\n\n```\n{\n    id: \"example.number\",\n    name: \"Example number setting\",\n    type: \"number\",\n    defaultValue: 42,\n    attrs: {\n        showButtons: true,\n        maxFractionDigits: 1,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Slider\n\nThis lets the user enter a number directly or via a slider. Based on the [Slider PrimeVue component](https://primevue.org/slider/). Ranges are not supported.\n\n```\n{\n    id: \"example.slider\",\n    name: \"Example slider setting\",\n    type: \"slider\",\n    attrs: {\n        min: -10,\n        max: 10,\n        step: 0.5,\n    },\n    defaultValue: 0,\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Combo\n\nThis lets the user pick from a drop-down list of values. You can provide options either as a plain string or as an object with `text` and `value` fields. If you only provide a plain string, then it will be used for both. You can let the user enter freeform text by supplying the `editable: true` attribute, or search by supplying the `filter: true` attribute. Based on the [Select PrimeVue component](https://primevue.org/select/). Groups are not supported.\n\n```\n{\n    id: \"example.combo\",\n    name: \"Example combo setting\",\n    type: \"combo\",\n    defaultValue: \"first\",\n    options: [\n        { text: \"My first option\", value: \"first\" },\n        \"My second option\",\n    ],\n    attrs: {\n        editable: true,\n        filter: true,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Color\n\nThis lets the user select a color from a color picker or type in a hex reference. Note that the format requires six full hex digits - three digit shorthand does not work. Based on the [ColorPicker PrimeVue component](https://primevue.org/colorpicker/).\n\n```\n{\n    id: \"example.color\",\n    name: \"Example color setting\",\n    type: \"color\",\n    defaultValue: \"ff0000\",\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Image\n\nThis lets the user upload an image. The setting will be saved as a [data URL](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data). Based on the [FileUpload PrimeVue component](https://primevue.org/fileupload/).\n\n```\n{\n    id: \"example.image\",\n    name: \"Example image setting\",\n    type: \"image\",\n    onChange: (newVal, oldVal) => {\n        console.log(`Setting was changed from ${oldVal} to ${newVal}`);\n    },\n}\n```\n\n### Hidden\n\nHidden settings arenâ€™t displayed in the settings panel, but you can read and write to them from your code.\n\n```\n{\n    id: \"example.hidden\",\n    name: \"Example hidden setting\",\n    type: \"hidden\",\n}\n```\n\n## Other\n\n### Categories\n\nYou can specify the categorisation of your setting separately to the `id`. This means you can change the categorisation and naming without changing the `id` and losing the values that have already been set by users.\n\n```\n{\n    id: \"example.boolean\",\n    name: \"Example boolean setting\",\n    type: \"boolean\",\n    defaultValue: false,\n    category: [\"Category name\", \"Section heading\", \"Setting label\"],\n}\n```\n\n### Tooltips\n\nYou can add extra contextual help with the `tooltip` field. This adds a small â„¹ï¸Ž icon after the field name that will show the help text when the user hovers over it.\n\n```\n{\n    id: \"example.boolean\",\n    name: \"Example boolean setting\",\n    type: \"boolean\",\n    defaultValue: false,\n    tooltip: \"This is some helpful information\",\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncodeFlux",
  "markdown": "# CLIPæ–‡æœ¬ç¼–ç Flux - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\n`CLIPæ–‡æœ¬ç¼–ç Flux` æ˜¯ ComfyUI ä¸­ä¸“ä¸º Flux æž¶æž„è®¾è®¡çš„é«˜çº§æ–‡æœ¬ç¼–ç èŠ‚ç‚¹ã€‚å®ƒé‡‡ç”¨åŒæ–‡æœ¬ç¼–ç å™¨ï¼ˆCLIP-L ä¸Ž T5XXLï¼‰ååŒæœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†ç»“æž„åŒ–å…³é”®è¯å’Œè¯¦ç»†è‡ªç„¶è¯­è¨€æè¿°ï¼Œä¸º Flux æ¨¡åž‹æä¾›æ›´ç²¾å‡†ã€æ›´ä¸°å¯Œçš„æ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆè´¨é‡ã€‚ è¯¥èŠ‚ç‚¹åŸºäºŽåŒç¼–ç å™¨åä½œæœºåˆ¶ï¼š\n\n1.  `clip_l` è¾“å…¥ä¼šè¢« CLIP-L ç¼–ç å™¨å¤„ç†ï¼Œæå–é£Žæ ¼ã€ä¸»é¢˜ç­‰å…³é”®è¯ç‰¹å¾ï¼Œé€‚åˆç®€æ´æè¿°ã€‚\n2.  `t5xxl` è¾“å…¥ç”± T5XXL ç¼–ç å™¨å¤„ç†ï¼Œæ“…é•¿ç†è§£å¤æ‚ã€ç»†è‡´çš„è‡ªç„¶è¯­è¨€åœºæ™¯æè¿°ã€‚\n3.  ä¸¤è·¯ç¼–ç ç»“æžœèžåˆåŽï¼Œç»“åˆâ€å¼•å¯¼â€å‚æ•°ï¼Œç”Ÿæˆç»Ÿä¸€çš„æ¡ä»¶åµŒå…¥ï¼ˆCONDITIONINGï¼‰ï¼Œç”¨äºŽä¸‹æ¸¸çš„ Flux é‡‡æ ·å™¨èŠ‚ç‚¹ï¼ŒæŽ§åˆ¶ç”Ÿæˆå†…å®¹ä¸Žæ–‡æœ¬æè¿°çš„å¥‘åˆåº¦ã€‚\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | è¾“å…¥æ–¹å¼ | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | åŠŸèƒ½è¯´æ˜Ž |\n| --- | --- | --- | --- | --- | --- |\n| `clip` | CLIP | èŠ‚ç‚¹è¾“å…¥ | æ—    | \\-  | å¿…é¡»æ˜¯æ”¯æŒ Flux æž¶æž„çš„ CLIP æ¨¡åž‹ï¼ŒåŒ…å« CLIP-L å’Œ T5XXL ä¸¤ä¸ªç¼–ç å™¨ |\n| `clip_l` | STRING | æ–‡æœ¬æ¡† | æ—    | æœ€å¤š77ä¸ªtoken | é€‚åˆè¾“å…¥ç®€æ´çš„å…³é”®è¯æè¿°ï¼Œå¦‚é£Žæ ¼ã€ä¸»é¢˜ç­‰ |\n| `t5xxl` | STRING | æ–‡æœ¬æ¡† | æ—    | è¿‘ä¹Žæ— é™åˆ¶ | é€‚åˆè¾“å…¥è¯¦ç»†çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œè¡¨è¾¾å¤æ‚åœºæ™¯å’Œç»†èŠ‚ |\n| `å¼•å¯¼` | FLOAT | æ»‘å—  | 3.5 | 0.0 - 100.0 | æŽ§åˆ¶æ–‡æœ¬æ¡ä»¶å¯¹ç”Ÿæˆè¿‡ç¨‹çš„å½±å“å¼ºåº¦ï¼Œæ•°å€¼è¶Šå¤§è¶Šä¸¥æ ¼éµå¾ªæ–‡æœ¬æè¿° |\n\n## è¾“å‡º\n\n| è¾“å‡ºåç§° | æ•°æ®ç±»åž‹ | è¯´æ˜Ž  |\n| --- | --- | --- |\n| `æ¡ä»¶` | CONDITIONING | åŒ…å«åŒç¼–ç å™¨å¤„ç†åŽçš„æ¡ä»¶åµŒå…¥å’Œå¼•å¯¼å‚æ•°ï¼Œç”¨äºŽæ¡ä»¶å›¾åƒç”Ÿæˆ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n### æç¤ºè¯ç¤ºä¾‹\n\n*   **clip\\_l è¾“å…¥æ¡†**ï¼ˆå…³é”®è¯é£Žæ ¼ï¼‰ï¼š\n    *   ä½¿ç”¨ç»“æž„åŒ–ã€ç®€æ´çš„å…³é”®è¯ç»„åˆ\n    *   ç¤ºä¾‹ï¼š`masterpiece, best quality, portrait, oil painting, dramatic lighting`\n    *   é‡ç‚¹æè¿°é£Žæ ¼ã€è´¨é‡ã€ä¸»é¢˜ç­‰æ ¸å¿ƒå…ƒç´ \n*   **t5xxl è¾“å…¥æ¡†**ï¼ˆè‡ªç„¶è¯­è¨€æè¿°ï¼‰ï¼š\n    *   ä½¿ç”¨å®Œæ•´ã€æµç•…çš„åœºæ™¯æè¿°\n    *   ç¤ºä¾‹ï¼š`A highly detailed portrait in oil painting style, featuring dramatic chiaroscuro lighting that creates deep shadows and bright highlights, emphasizing the subject's features with renaissance-inspired composition.`\n    *   é‡ç‚¹æè¿°åœºæ™¯ç»†èŠ‚ã€ç©ºé—´å…³ç³»ã€å…‰å½±æ•ˆæžœ\n\n### æ³¨æ„äº‹é¡¹\n\n1.  ç¡®ä¿ä½¿ç”¨å…¼å®¹çš„ Flux æž¶æž„ CLIP æ¨¡åž‹\n2.  å»ºè®®åŒæ—¶å¡«å†™ clip\\_l å’Œ t5xxlï¼Œä»¥å‘æŒ¥åŒç¼–ç å™¨ä¼˜åŠ¿\n3.  æ³¨æ„ clip\\_l çš„è¯å…ƒæ•°é‡é™åˆ¶ï¼ˆ77ä¸ªtokenï¼‰\n4.  æ ¹æ®ç”Ÿæˆæ•ˆæžœè°ƒæ•´â€å¼•å¯¼â€å‚æ•°"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncodeSdxl",
  "markdown": "# CLIPæ–‡æœ¬ç¼–ç SDXL - ComfyUIå†…ç½®èŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\næ­¤èŠ‚ç‚¹è®¾è®¡ä½¿ç”¨ç‰¹åˆ«ä¸ºSDXLæž¶æž„å®šåˆ¶çš„CLIPæ¨¡åž‹å¯¹æ–‡æœ¬è¾“å…¥è¿›è¡Œç¼–ç ã€‚å®ƒä½¿ç”¨åŒç¼–ç å™¨ç³»ç»Ÿï¼ˆCLIP-Lå’ŒCLIP-Gï¼‰æ¥å¤„ç†æ–‡æœ¬æè¿°ï¼Œä»Žè€Œç”Ÿæˆæ›´å‡†ç¡®çš„å›¾åƒã€‚\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | ä½œç”¨  |\n| --- | --- | --- |\n| `clip` | `CLIP` | ç”¨äºŽç¼–ç æ–‡æœ¬çš„CLIPæ¨¡åž‹å®žä¾‹ã€‚ |\n| `å®½åº¦` | `INT` | æŒ‡å®šå›¾åƒçš„å®½åº¦ï¼ˆä»¥åƒç´ ä¸ºå•ä½ï¼‰ï¼Œé»˜è®¤1024ã€‚ |\n| `é«˜åº¦` | `INT` | æŒ‡å®šå›¾åƒçš„é«˜åº¦ï¼ˆä»¥åƒç´ ä¸ºå•ä½ï¼‰ï¼Œé»˜è®¤1024ã€‚ |\n| `è£å‰ªå®½` | `INT` | è£å‰ªåŒºåŸŸçš„å®½åº¦ï¼ˆä»¥åƒç´ ä¸ºå•ä½ï¼‰ï¼Œé»˜è®¤0ã€‚ |\n| `è£å‰ªé«˜` | `INT` | è£å‰ªåŒºåŸŸçš„é«˜åº¦ï¼ˆä»¥åƒç´ ä¸ºå•ä½ï¼‰ï¼Œé»˜è®¤0ã€‚ |\n| `ç›®æ ‡å®½åº¦` | `INT` | è¾“å‡ºå›¾åƒçš„ç›®æ ‡å®½åº¦ï¼Œé»˜è®¤1024ã€‚ |\n| `ç›®æ ‡é«˜åº¦` | `INT` | è¾“å‡ºå›¾åƒçš„ç›®æ ‡é«˜åº¦ï¼Œé»˜è®¤1024ã€‚ |\n| `text_g` | `STRING` | å…¨å±€æ–‡æœ¬æè¿°ï¼Œç”¨äºŽæ•´ä½“åœºæ™¯æè¿°ã€‚ |\n| `text_l` | `STRING` | å±€éƒ¨æ–‡æœ¬æè¿°ï¼Œç”¨äºŽç»†èŠ‚æè¿°ã€‚ |\n\n## è¾“å‡º\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | ä½œç”¨  |\n| --- | --- | --- |\n| `æ¡ä»¶` | `CONDITIONING` | åŒ…å«ç¼–ç åŽçš„æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆæ‰€éœ€çš„æ¡ä»¶ä¿¡æ¯ã€‚ |"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncodeHunyuanDit",
  "markdown": "# CLIPæ–‡æœ¬ç¼–ç æ··å…ƒDiT - ComfyUIå†…ç½®èŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\n`CLIPæ–‡æœ¬ç¼–ç æ··å…ƒDiT` èŠ‚ç‚¹çš„ä¸»è¦åŠŸèƒ½æ˜¯å°†è¾“å…¥çš„æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡åž‹å¯ä»¥ç†è§£çš„å½¢å¼ã€‚æ˜¯ä¸€ä¸ªé«˜çº§æ¡ä»¶åŒ–èŠ‚ç‚¹ï¼Œä¸“é—¨ç”¨äºŽ HunyuanDiT æ¨¡åž‹çš„åŒæ–‡æœ¬ç¼–ç å™¨æž¶æž„ã€‚ ä¸»è¦ä½œç”¨å®ƒå°±åƒä¸€ä¸ªç¿»è¯‘å™¨ï¼Œå¯ä»¥å°†æˆ‘ä»¬çš„æ–‡å­—æè¿°è½¬æ¢æˆ AI æ¨¡åž‹èƒ½ç†è§£çš„â€æœºå™¨è¯­è¨€â€ã€‚å…¶ä¸­ `bert` å’Œ `mt5xl` åå¥½ä¸åŒç±»åž‹çš„æç¤ºè¯è¾“å…¥\n\n## è¾“å…¥\n\n| å‚æ•°  | æ•°æ®ç±»åž‹ | æè¿°  |\n| --- | --- | --- |\n| `clip` | CLIP | ä¸€ä¸ª CLIP æ¨¡åž‹å®žä¾‹ï¼Œç”¨äºŽæ–‡æœ¬çš„æ ‡è®°åŒ–å’Œç¼–ç ï¼Œæ˜¯ç”Ÿæˆæ¡ä»¶çš„æ ¸å¿ƒã€‚ |\n| `bert` | STRING | éœ€è¦ç¼–ç çš„æ–‡æœ¬è¾“å…¥ï¼Œåå¥½çŸ­è¯­ã€å…³é”®è¯ï¼Œæ”¯æŒå¤šè¡Œå’ŒåŠ¨æ€æç¤ºã€‚ |\n| `mt5xl` | STRING | å¦ä¸€ä¸ªéœ€è¦ç¼–ç çš„æ–‡æœ¬è¾“å…¥ï¼Œæ”¯æŒå¤šè¡Œå’ŒåŠ¨æ€æç¤ºï¼ˆå¤šè¯­è¨€ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨å®Œæ•´çš„å¥å­å’Œå¤æ‚çš„æè¿°ã€‚ |\n\n## è¾“å‡º\n\n| å‚æ•°  | æ•°æ®ç±»åž‹ | æè¿°  |\n| --- | --- | --- |\n| `æ¡ä»¶` | CONDITIONING | ç¼–ç åŽçš„æ¡ä»¶è¾“å‡ºï¼Œç”¨äºŽç”Ÿæˆä»»åŠ¡ä¸­çš„è¿›ä¸€æ­¥å¤„ç†ã€‚ |"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncodeSdxlRefiner",
  "markdown": "# CLIPæ–‡æœ¬ç¼–ç SDXLç²¾ç‚¼å™¨ - ComfyUIå†…ç½®èŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\næ­¤èŠ‚ç‚¹ä¸“é—¨ä¸º SDXL Refiner æ¨¡åž‹è®¾è®¡ï¼Œç”¨äºŽå°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸ºæ¡ä»¶ä¿¡æ¯ï¼Œé€šè¿‡çº³å…¥å®¡ç¾Žå¾—åˆ†å’Œç»´åº¦ä¿¡æ¯æ¥å¢žå¼ºç”Ÿæˆä»»åŠ¡çš„æ¡ä»¶ï¼Œä»Žè€Œæå‡æœ€ç»ˆçš„ç²¾ç‚¼æ•ˆæžœã€‚å®ƒå°±åƒæ˜¯ä¸€ä½ä¸“ä¸šçš„è‰ºæœ¯æŒ‡å¯¼ï¼Œä¸ä»…ä¼ è¾¾æ‚¨çš„åˆ›ä½œæ„å›¾ï¼Œè¿˜èƒ½ä¸ºä½œå“æ³¨å…¥ç²¾ç¡®çš„ç¾Žå­¦æ ‡å‡†å’Œè§„æ ¼è¦æ±‚ã€‚\n\n## å·¥ä½œåŽŸç†\n\nSDXL Refiner æ˜¯ä¸€ä¸ªä¸“é—¨çš„ç²¾ç‚¼æ¨¡åž‹ï¼Œå®ƒåœ¨ SDXL åŸºç¡€æ¨¡åž‹çš„åŸºç¡€ä¸Šï¼Œä¸“æ³¨äºŽæå‡å›¾åƒçš„ç»†èŠ‚å’Œè´¨é‡ã€‚è¿™ä¸ªè¿‡ç¨‹å°±åƒæ˜¯ä¸€ä½è‰ºæœ¯ä¿®é¥°å¸ˆï¼š\n\n1.  é¦–å…ˆï¼Œå®ƒæŽ¥æ”¶åŸºç¡€æ¨¡åž‹ç”Ÿæˆçš„åˆæ­¥å›¾åƒæˆ–æ–‡æœ¬æè¿°\n2.  ç„¶åŽï¼Œé€šè¿‡ç²¾ç¡®çš„ç¾Žå­¦è¯„åˆ†å’Œå°ºå¯¸å‚æ•°æ¥æŒ‡å¯¼ç²¾ç‚¼è¿‡ç¨‹\n3.  æœ€åŽï¼Œå®ƒä¸“æ³¨äºŽå¤„ç†å›¾åƒçš„é«˜é¢‘ç»†èŠ‚ï¼Œæå‡æ•´ä½“è´¨é‡\n\nRefiner å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼ä½¿ç”¨ï¼š\n\n*   ä½œä¸ºç‹¬ç«‹çš„ç²¾ç‚¼æ­¥éª¤ï¼Œå¯¹åŸºç¡€æ¨¡åž‹ç”Ÿæˆçš„å›¾åƒè¿›è¡ŒåŽæœŸå¤„ç†\n*   ä½œä¸ºä¸“å®¶é›†æˆç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹çš„ä½Žå™ªå£°é˜¶æ®µæŽ¥ç®¡å¤„ç†\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | è¾“å…¥æ–¹å¼ | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | åŠŸèƒ½è¯´æ˜Ž |\n| --- | --- | --- | --- | --- | --- |\n| `CLIP` | CLIP | å¿…éœ€  | \\-  | \\-  | ç”¨äºŽæ–‡æœ¬æ ‡è®°åŒ–å’Œç¼–ç çš„ CLIP æ¨¡åž‹å®žä¾‹ï¼Œæ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡åž‹å¯ç†è§£æ ¼å¼çš„æ ¸å¿ƒç»„ä»¶ |\n| `ç¾Žå­¦åˆ†æ•°` | FLOAT | å¯é€‰  | 6.0 | 0.0-1000.0 | æŽ§åˆ¶ç”Ÿæˆå›¾åƒçš„è§†è§‰è´¨é‡å’Œç¾Žè§‚ç¨‹åº¦ï¼Œç±»ä¼¼äºŽä¸ºè‰ºæœ¯ä½œå“è®¾å®šè´¨é‡æ ‡å‡†ï¼š  <br>\\- é«˜åˆ†å€¼(7.5-8.5)ï¼šè¿½æ±‚æ›´ç²¾ç¾Žã€ç»†èŠ‚ä¸°å¯Œçš„æ•ˆæžœ  <br>\\- ä¸­ç­‰åˆ†å€¼(6.0-7.0)ï¼šå¹³è¡¡çš„è´¨é‡æŽ§åˆ¶  <br>\\- ä½Žåˆ†å€¼(2.0-3.0)ï¼šé€‚ç”¨äºŽè´Ÿé¢æç¤º |\n| `å®½åº¦` | INT | å¿…éœ€  | 1024 | 64-16384 | æŒ‡å®šè¾“å‡ºå›¾åƒçš„å®½åº¦ï¼ˆåƒç´ ï¼‰ï¼Œéœ€è¦æ˜¯ 8 çš„å€æ•°ã€‚SDXL åœ¨æ€»åƒç´ é‡æŽ¥è¿‘ 1024Ã—1024 (çº¦100ä¸‡åƒç´ ) æ—¶æ•ˆæžœæœ€ä½³ |\n| `é«˜åº¦` | INT | å¿…éœ€  | 1024 | 64-16384 | æŒ‡å®šè¾“å‡ºå›¾åƒçš„é«˜åº¦ï¼ˆåƒç´ ï¼‰ï¼Œéœ€è¦æ˜¯ 8 çš„å€æ•°ã€‚SDXL åœ¨æ€»åƒç´ é‡æŽ¥è¿‘ 1024Ã—1024 (çº¦100ä¸‡åƒç´ ) æ—¶æ•ˆæžœæœ€ä½³ |\n| `text` | STRING | å¿…éœ€  | \\-  | \\-  | æ–‡æœ¬æç¤ºæè¿°ï¼Œæ”¯æŒå¤šè¡Œè¾“å…¥å’ŒåŠ¨æ€æç¤ºè¯­æ³•ã€‚åœ¨ Refiner ä¸­ï¼Œæ–‡æœ¬æç¤ºåº”æ›´æ³¨é‡æè¿°æœŸæœ›çš„è§†è§‰è´¨é‡å’Œç»†èŠ‚ç‰¹å¾ |\n\n## è¾“å‡º\n\n| è¾“å‡ºåç§° | æ•°æ®ç±»åž‹ | è¯´æ˜Ž  |\n| --- | --- | --- |\n| `æ¡ä»¶` | CONDITIONING | ç»è¿‡ç»†åŒ–çš„æ¡ä»¶è¾“å‡ºï¼ŒåŒ…å«äº†æ–‡æœ¬è¯­ä¹‰ã€ç¾Žå­¦æ ‡å‡†å’Œå°ºå¯¸ä¿¡æ¯çš„ç»¼åˆç¼–ç ï¼Œä¸“é—¨ç”¨äºŽæŒ‡å¯¼ SDXL Refiner æ¨¡åž‹è¿›è¡Œç²¾ç¡®çš„å›¾åƒç²¾ç‚¼ |\n\n## æ³¨æ„äº‹é¡¹\n\n1.  è¯¥èŠ‚ç‚¹ä¸“é—¨ä¸º SDXL Refiner æ¨¡åž‹ä¼˜åŒ–ï¼Œä¸Žæ™®é€šçš„ CLIPTextEncode èŠ‚ç‚¹æœ‰æ‰€ä¸åŒ\n2.  ç¾Žå­¦åˆ†æ•°å»ºè®®ä½¿ç”¨ 7.5 ä½œä¸ºåŸºå‡†å€¼ï¼Œè¿™æ˜¯ SDXL è®­ç»ƒæ—¶çš„æ ‡å‡†è®¾ç½®\n3.  æ‰€æœ‰å°ºå¯¸å‚æ•°å¿…é¡»æ˜¯ 8 çš„å€æ•°ï¼Œä¸”å»ºè®®æ€»åƒç´ é‡æŽ¥è¿‘ 1024Ã—1024ï¼ˆçº¦100ä¸‡åƒç´ ï¼‰\n4.  Refiner æ¨¡åž‹ä¸“æ³¨äºŽæå‡å›¾åƒç»†èŠ‚å’Œè´¨é‡ï¼Œå› æ­¤æ–‡æœ¬æç¤ºåº”è¯¥æ›´æ³¨é‡æè¿°æœŸæœ›çš„è§†è§‰æ•ˆæžœï¼Œè€Œä¸æ˜¯åœºæ™¯å†…å®¹\n5.  åœ¨å®žé™…ä½¿ç”¨ä¸­ï¼ŒRefiner é€šå¸¸ç”¨äºŽç”Ÿæˆè¿‡ç¨‹çš„åŽæœŸé˜¶æ®µï¼ˆçº¦æœ€åŽ20%çš„æ­¥éª¤ï¼‰ï¼Œä¸“æ³¨äºŽç»†èŠ‚ä¼˜åŒ–"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_toast",
  "markdown": "# Toast API - ComfyUI\n\nThe Toast API provides a way to display non-blocking notification messages to users. These are useful for providing feedback without interrupting workflow.\n\n## Basic Usage\n\n### Simple Toast\n\n```\n// Display a simple info toast\napp.extensionManager.toast.add({\n  severity: \"info\",\n  summary: \"Information\",\n  detail: \"Operation completed successfully\",\n  life: 3000\n});\n```\n\n### Toast Types\n\n```\n// Success toast\napp.extensionManager.toast.add({\n  severity: \"success\",\n  summary: \"Success\",\n  detail: \"Data saved successfully\",\n  life: 3000\n});\n\n// Warning toast\napp.extensionManager.toast.add({\n  severity: \"warn\",\n  summary: \"Warning\",\n  detail: \"This action may cause problems\",\n  life: 5000\n});\n\n// Error toast\napp.extensionManager.toast.add({\n  severity: \"error\",\n  summary: \"Error\",\n  detail: \"Failed to process request\",\n  life: 5000\n});\n```\n\n### Alert Helper\n\n```\n// Shorthand for creating an alert toast\napp.extensionManager.toast.addAlert(\"This is an important message\");\n```\n\n## API Reference\n\n### Toast Message\n\n```\napp.extensionManager.toast.add({\n  severity?: \"success\" | \"info\" | \"warn\" | \"error\" | \"secondary\" | \"contrast\", // Message severity level (default: \"info\")\n  summary?: string,         // Short title for the toast\n  detail?: any,             // Detailed message content\n  closable?: boolean,       // Whether user can close the toast (default: true)\n  life?: number,            // Duration in milliseconds before auto-closing\n  group?: string,           // Group identifier for managing related toasts\n  styleClass?: any,         // Style class of the message\n  contentStyleClass?: any   // Style class of the content\n});\n```\n\n### Alert Helper\n\n```\napp.extensionManager.toast.addAlert(message: string);\n```\n\n### Additional Methods\n\n```\n// Remove a specific toast\napp.extensionManager.toast.remove(toastMessage);\n\n// Remove all toasts\napp.extensionManager.toast.removeAll();\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_sidebar_tabs",
  "markdown": "# Sidebar Tabs - ComfyUI\n\nThe Sidebar Tabs API allows extensions to add custom tabs to the sidebar of the ComfyUI interface. This is useful for adding features that require persistent visibility and quick access.\n\n## Basic Usage\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"customSidebar\",\n  icon: \"pi pi-compass\",\n  title: \"Custom Tab\",\n  tooltip: \"My Custom Sidebar Tab\",\n  type: \"custom\",\n  render: (el) => {\n    el.innerHTML = '<div>This is my custom sidebar content</div>';\n  }\n});\n```\n\n## Tab Configuration\n\nEach tab requires several properties:\n\n```\n{\n  id: string,              // Unique identifier for the tab\n  icon: string,            // Icon class for the tab button\n  title: string,           // Title text for the tab\n  tooltip?: string,        // Tooltip text on hover (optional)\n  type: string,            // Tab type (usually \"custom\")\n  render: (element) => void // Function that populates the tab content\n}\n```\n\nThe `render` function receives a DOM element where you should insert your tabâ€™s content.\n\n## Icon Options\n\nSidebar tab icons can use various icon sets:\n\n*   PrimeVue icons: `pi pi-[icon-name]` (e.g., `pi pi-home`)\n*   Material Design icons: `mdi mdi-[icon-name]` (e.g., `mdi mdi-robot`)\n*   Font Awesome icons: `fa-[style] fa-[icon-name]` (e.g., `fa-solid fa-star`)\n\nEnsure the corresponding icon library is loaded before using these icons.\n\n## Stateful Tab Example\n\nYou can create tabs that maintain state:\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"statefulTab\",\n  icon: \"pi pi-list\",\n  title: \"Notes\",\n  type: \"custom\",\n  render: (el) => {\n    // Create elements\n    const container = document.createElement('div');\n    container.style.padding = '10px';\n    \n    const notepad = document.createElement('textarea');\n    notepad.style.width = '100%';\n    notepad.style.height = '200px';\n    notepad.style.marginBottom = '10px';\n    \n    // Load saved content if available\n    const savedContent = localStorage.getItem('comfyui-notes');\n    if (savedContent) {\n      notepad.value = savedContent;\n    }\n    \n    // Auto-save content\n    notepad.addEventListener('input', () => {\n      localStorage.setItem('comfyui-notes', notepad.value);\n    });\n    \n    // Assemble the UI\n    container.appendChild(notepad);\n    el.appendChild(container);\n  }\n});\n```\n\n## Using React Components\n\nYou can mount React components in sidebar tabs:\n\n```\n// Import React dependencies in your extension\nimport React from \"react\";\nimport ReactDOM from \"react-dom/client\";\n\n// Register sidebar tab with React content\napp.extensionManager.registerSidebarTab({\n  id: \"reactSidebar\",\n  icon: \"mdi mdi-react\",\n  title: \"React Tab\",\n  type: \"custom\",\n  render: (el) => {\n    const container = document.createElement(\"div\");\n    container.id = \"react-sidebar-container\";\n    el.appendChild(container);\n    \n    // Define a simple React component\n    function SidebarContent() {\n      const [count, setCount] = React.useState(0);\n      \n      return (\n        <div style={{ padding: \"10px\" }}>\n          <h3>React Sidebar</h3>\n          <p>Count: {count}</p>\n          <button onClick={() => setCount(count + 1)}>\n            Increment\n          </button>\n        </div>\n      );\n    }\n    \n    // Mount React component\n    ReactDOM.createRoot(container).render(\n      <React.StrictMode>\n        <SidebarContent />\n      </React.StrictMode>\n    );\n  }\n});\n```\n\nFor a real-world example of a React application integrated as a sidebar tab, check out the [ComfyUI-Copilot project on GitHub](https://github.com/AIDC-AI/ComfyUI-Copilot).\n\n## Dynamic Content Updates\n\nYou can update sidebar content in response to graph changes:\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"dynamicSidebar\",\n  icon: \"pi pi-chart-line\",\n  title: \"Stats\",\n  type: \"custom\",\n  render: (el) => {\n    const container = document.createElement('div');\n    container.style.padding = '10px';\n    el.appendChild(container);\n    \n    // Function to update stats\n    function updateStats() {\n      const stats = {\n        nodes: app.graph._nodes.length,\n        connections: Object.keys(app.graph.links).length\n      };\n      \n      container.innerHTML = `\n        <h3>Workflow Stats</h3>\n        <ul>\n          <li>Nodes: ${stats.nodes}</li>\n          <li>Connections: ${stats.connections}</li>\n        </ul>\n      `;\n    }\n    \n    // Initial update\n    updateStats();\n    \n    // Listen for graph changes\n    const api = app.api;\n    api.addEventListener(\"graphChanged\", updateStats);\n    \n    // Clean up listeners when tab is destroyed\n    return () => {\n      api.removeEventListener(\"graphChanged\", updateStats);\n    };\n  }\n});\n```"
},
{
  "url": "https://docs.comfy.org/custom-nodes/js/javascript_topbar_menu",
  "markdown": "# Topbar Menu - ComfyUI\n\nThe Topbar Menu API allows extensions to add custom menu items to the ComfyUIâ€™s top menu bar. This is useful for providing access to advanced features or less frequently used commands.\n\n## Basic Usage\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  // Define commands\n  commands: [\n    { \n      id: \"myCommand\", \n      label: \"My Command\", \n      function: () => { alert(\"Command executed!\"); } \n    }\n  ],\n  // Add commands to menu\n  menuCommands: [\n    { \n      path: [\"Extensions\", \"My Extension\"], \n      commands: [\"myCommand\"] \n    }\n  ]\n});\n```\n\nCommand definitions follow the same pattern as in the [Commands and Keybindings API](https://docs.comfy.org/custom-nodes/js/javascript_commands_keybindings). See that page for more detailed information about defining commands.\n\n## Command Configuration\n\nEach command requires an `id`, `label`, and `function`:\n\n```\n{\n  id: string,              // Unique identifier for the command\n  label: string,           // Display name for the command\n  function: () => void     // Function to execute when command is triggered\n}\n```\n\nThe `menuCommands` array defines where to place commands in the menu structure:\n\n```\n{\n  path: string[],          // Array representing menu hierarchy\n  commands: string[]       // Array of command IDs to add at this location\n}\n```\n\nThe `path` array specifies the menu hierarchy. For example, `[\"File\", \"Export\"]` would add commands to the â€œExportâ€ submenu under the â€œFileâ€ menu.\n\n```\napp.registerExtension({\n  name: \"MenuExamples\",\n  commands: [\n    { \n      id: \"saveAsImage\", \n      label: \"Save as Image\", \n      function: () => { \n        // Code to save canvas as image\n      } \n    },\n    { \n      id: \"exportWorkflow\", \n      label: \"Export Workflow\", \n      function: () => { \n        // Code to export workflow\n      } \n    }\n  ],\n  menuCommands: [\n    // Add to File menu\n    { \n      path: [\"File\"], \n      commands: [\"saveAsImage\", \"exportWorkflow\"] \n    }\n  ]\n});\n```\n\n```\napp.registerExtension({\n  name: \"SubmenuExample\",\n  commands: [\n    { \n      id: \"option1\", \n      label: \"Option 1\", \n      function: () => { console.log(\"Option 1\"); } \n    },\n    { \n      id: \"option2\", \n      label: \"Option 2\", \n      function: () => { console.log(\"Option 2\"); } \n    },\n    { \n      id: \"suboption1\", \n      label: \"Sub-option 1\", \n      function: () => { console.log(\"Sub-option 1\"); } \n    }\n  ],\n  menuCommands: [\n    // Create a nested menu structure\n    { \n      path: [\"Extensions\", \"My Tools\"], \n      commands: [\"option1\", \"option2\"] \n    },\n    { \n      path: [\"Extensions\", \"My Tools\", \"Advanced\"], \n      commands: [\"suboption1\"] \n    }\n  ]\n});\n```\n\nYou can add the same command to multiple menu locations:\n\n```\napp.registerExtension({\n  name: \"MultiLocationExample\",\n  commands: [\n    { \n      id: \"helpCommand\", \n      label: \"Get Help\", \n      function: () => { window.open(\"https://docs.example.com\", \"_blank\"); } \n    }\n  ],\n  menuCommands: [\n    // Add to Help menu\n    { \n      path: [\"Help\"], \n      commands: [\"helpCommand\"] \n    },\n    // Also add to Extensions menu\n    { \n      path: [\"Extensions\"], \n      commands: [\"helpCommand\"] \n    }\n  ]\n});\n```\n\nCommands can work with other ComfyUI APIs like settings. For more information about the Settings API, see the [Settings API](https://docs.comfy.org/custom-nodes/js/javascript_settings) documentation."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/bfl/flux-1-1-pro-ultra-image",
  "markdown": "# Flux 1.1 \\[pro\\] Ultra Image - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Flux 1.1 [pro] Ultra Image èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg) Flux 1.1 \\[pro\\] Ultra Image èŠ‚ç‚¹å…è®¸ä½ é€šè¿‡æ–‡æœ¬æç¤ºè¯ç”Ÿæˆè¶…é«˜åˆ†è¾¨çŽ‡çš„å›¾åƒï¼Œç›´æŽ¥è¿žæŽ¥ Black Forest Labs çš„æœ€æ–°å›¾åƒç”Ÿæˆ APIã€‚ æ­¤èŠ‚ç‚¹æ”¯æŒä¸¤ç§ä¸»è¦ä½¿ç”¨æ¨¡å¼ï¼š\n\n1.  **æ–‡ç”Ÿå›¾**ï¼šé€šè¿‡æ–‡æœ¬æç¤ºè¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼ˆä¸ä½¿ç”¨ä»»ä½•å›¾åƒæ”¶å…¥æ—¶ï¼‰\n2.  **å›¾ç”Ÿå›¾**ï¼šå°†çŽ°æœ‰å›¾åƒä¸Žæç¤ºè¯ç»“åˆï¼Œåˆ›å»ºèžåˆä¸¤è€…ç‰¹ç‚¹çš„æ–°å›¾åƒï¼ˆRemix æ¨¡å¼ï¼‰\n\næ­¤èŠ‚ç‚¹æ”¯æŒé€šè¿‡ API è°ƒç”¨ Ultra æ¨¡å¼ï¼Œèƒ½å¤Ÿç”Ÿæˆ 4 å€äºŽæ ‡å‡† Flux 1.1 \\[pro\\] åˆ†è¾¨çŽ‡çš„å›¾åƒï¼ˆé«˜è¾¾ 4MPï¼‰ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æç¤ºè¯éµå¾ªæ€§ï¼Œå¹¶ä¸”ä¿æŒä»… 10 ç§’çš„è¶…å¿«ç”Ÿæˆæ—¶é—´ã€‚ä¸Žå…¶ä»–é«˜åˆ†è¾¨çŽ‡æ¨¡åž‹ç›¸æ¯”ï¼Œç”Ÿæˆé€Ÿåº¦æé«˜äº† 2.5 å€ä»¥ä¸Šã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | ç”Ÿæˆå›¾åƒçš„æ–‡æœ¬æè¿° |\n| prompt\\_upsampling | å¸ƒå°”å€¼ | False | æ˜¯å¦ä½¿ç”¨æç¤ºè¯ä¸Šé‡‡æ ·æŠ€æœ¯å¢žå¼ºç»†èŠ‚ã€‚å¯ç”¨åŽä¼šè‡ªåŠ¨ä¿®æ”¹æç¤ºè¯ä»¥èŽ·å¾—æ›´å…·åˆ›é€ æ€§çš„ç”Ÿæˆï¼Œä½†ç»“æžœä¼šå˜å¾—ä¸ç¡®å®šï¼ˆç›¸åŒç§å­ä¸ä¼šäº§ç”Ÿå®Œå…¨ç›¸åŒçš„ç»“æžœï¼‰ |\n| seed | æ•´æ•°  | 0   | éšæœºç§å­å€¼ï¼ŒæŽ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ |\n| aspect\\_ratio | å­—ç¬¦ä¸² | â€16:9â€ | å›¾åƒçš„å®½é«˜æ¯”ï¼Œå¿…é¡»åœ¨ 1:4 åˆ° 4:1 ä¹‹é—´ |\n| raw | å¸ƒå°”å€¼ | False | è®¾ç½®ä¸º True æ—¶ï¼Œç”Ÿæˆæ›´å°‘å¤„ç†ç—•è¿¹ã€æ›´è‡ªç„¶çš„å›¾åƒ |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image\\_prompt | å›¾åƒ  | æ—    | å¯é€‰è¾“å…¥ï¼Œç”¨äºŽå›¾ç”Ÿå›¾ï¼ˆRemixï¼‰æ¨¡å¼ |\n| image\\_prompt\\_strength | æµ®ç‚¹æ•° | 0.1 | åœ¨æœ‰`image_prompt` è¾“å…¥æ—¶ç”Ÿæ•ˆï¼Œè°ƒèŠ‚æç¤ºè¯ä¸Žå›¾åƒæç¤ºä¹‹é—´çš„æ··åˆç¨‹åº¦ï¼Œå€¼è¶Šå¤§è¾“å‡ºè¶ŠæŽ¥è¿‘è¾“å…¥å›¾åƒï¼ŒèŒƒå›´ä¸º 0.0-1.0 |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | ç”Ÿæˆçš„é«˜åˆ†è¾¨çŽ‡å›¾åƒç»“æžœ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\nè¯·è®¿é—®ä¸‹é¢çš„æ•™ç¨‹æŸ¥çœ‹å¯¹åº”çš„ä½¿ç”¨ç¤ºä¾‹\n\n*   [Flux 1.1 Pro Ultra Image API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹å·¥ä½œæµ](https://docs.comfy.org/zh-CN/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)\n\n## å·¥ä½œåŽŸç†\n\nFlux 1.1 \\[pro\\] Ultra æ¨¡å¼åˆ©ç”¨ä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æž¶æž„å’Œé«˜æ•ˆçš„ GPU åŠ é€ŸæŠ€æœ¯ï¼Œåœ¨ä¸ç‰ºç‰²é€Ÿåº¦çš„æƒ…å†µä¸‹å®žçŽ°é«˜åˆ†è¾¨çŽ‡å›¾åƒç”Ÿæˆã€‚å½“è¯·æ±‚å‘é€åˆ° API åŽï¼Œç³»ç»Ÿä¼šè§£æžæç¤ºè¯ï¼Œåº”ç”¨é€‚å½“çš„å‚æ•°ï¼Œç„¶åŽå¹¶è¡Œè®¡ç®—å›¾åƒï¼Œæœ€ç»ˆç”Ÿæˆå¹¶è¿”å›žé«˜åˆ†è¾¨çŽ‡ç»“æžœã€‚ ä¸Žå¸¸è§„æ¨¡åž‹ç›¸æ¯”ï¼ŒUltra æ¨¡å¼ç‰¹åˆ«å…³æ³¨ç»†èŠ‚ä¿ç•™å’Œå¤§å°ºå¯¸ä¸‹çš„ä¸€è‡´æ€§ï¼Œä»¥ç¡®ä¿å³ä½¿åœ¨ 4MP çš„é«˜åˆ†è¾¨çŽ‡ä¸‹ä¹Ÿèƒ½ä¿æŒä»¤äººå°è±¡æ·±åˆ»çš„è´¨é‡ã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass FluxProUltraImageNode(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    MINIMUM_RATIO = 1 / 4\n    MAXIMUM_RATIO = 4 / 1\n    MINIMUM_RATIO_STR = \"1:4\"\n    MAXIMUM_RATIO_STR = \"4:1\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"prompt_upsampling\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result).\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"16:9\",\n                        \"tooltip\": \"Aspect ratio of image; must be between 1:4 and 4:1.\",\n                    },\n                ),\n                \"raw\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"When True, generate less processed, more natural-looking images.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image_prompt\": (IO.IMAGE,),\n                \"image_prompt_strength\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.1,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Blend between the prompt and the image prompt.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, aspect_ratio: str):\n        try:\n            validate_aspect_ratio(\n                aspect_ratio,\n                minimum_ratio=cls.MINIMUM_RATIO,\n                maximum_ratio=cls.MAXIMUM_RATIO,\n                minimum_ratio_str=cls.MINIMUM_RATIO_STR,\n                maximum_ratio_str=cls.MAXIMUM_RATIO_STR,\n            )\n        except Exception as e:\n            return str(e)\n        return True\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/bfl\"\n\n    def api_call(\n        self,\n        prompt: str,\n        aspect_ratio: str,\n        prompt_upsampling=False,\n        raw=False,\n        seed=0,\n        image_prompt=None,\n        image_prompt_strength=0.1,\n        auth_token=None,\n        **kwargs,\n    ):\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/bfl/flux-pro-1.1-ultra/generate\",\n                method=HttpMethod.POST,\n                request_model=BFLFluxProUltraGenerateRequest,\n                response_model=BFLFluxProGenerateResponse,\n            ),\n            request=BFLFluxProUltraGenerateRequest(\n                prompt=prompt,\n                prompt_upsampling=prompt_upsampling,\n                seed=seed,\n                aspect_ratio=validate_aspect_ratio(\n                    aspect_ratio,\n                    minimum_ratio=self.MINIMUM_RATIO,\n                    maximum_ratio=self.MAXIMUM_RATIO,\n                    minimum_ratio_str=self.MINIMUM_RATIO_STR,\n                    maximum_ratio_str=self.MAXIMUM_RATIO_STR,\n                ),\n                raw=raw,\n                image_prompt=(\n                    image_prompt\n                    if image_prompt is None\n                    else convert_image_to_base64(image_prompt)\n                ),\n                image_prompt_strength=(\n                    None if image_prompt is None else round(image_prompt_strength, 2)\n                ),\n            ),\n            auth_token=auth_token,\n        )\n        output_image = handle_bfl_synchronous_operation(operation)\n        return (output_image,)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v1",
  "markdown": "# Ideogram V1 - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\nclass IdeogramV1(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V1 model.\n\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"turbo\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to use turbo mode (faster generation, potentially lower quality)\",\n                    }\n                ),\n            },\n            \"optional\": {\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V2_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Description of what to exclude from the image\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v1\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        turbo=False,\n        aspect_ratio=\"1:1\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        negative_prompt=\"\",\n        num_images=1,\n        auth_token=None,\n    ):\n        # Determine the model based on turbo setting\n        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)\n        model = \"V_1_TURBO\" if turbo else \"V_1\"\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/ideogram/generate\",\n                method=HttpMethod.POST,\n                request_model=IdeogramGenerateRequest,\n                response_model=IdeogramGenerateResponse,\n            ),\n            request=IdeogramGenerateRequest(\n                image_request=ImageRequest(\n                    prompt=prompt,\n                    model=model,\n                    num_images=num_images,\n                    seed=seed,\n                    aspect_ratio=aspect_ratio if aspect_ratio != \"ASPECT_1_1\" else None,\n                    magic_prompt_option=(\n                        magic_prompt_option if magic_prompt_option != \"AUTO\" else None\n                    ),\n                    negative_prompt=negative_prompt if negative_prompt else None,\n                )\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v2",
  "markdown": "# Ideogram V2 - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\n\nclass IdeogramV2(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V2 model.\n\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"turbo\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                        \"tooltip\": \"Whether to use turbo mode (faster generation, potentially lower quality)\",\n                    }\n                ),\n            },\n            \"optional\": {\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V2_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation. Ignored if resolution is not set to AUTO.\",\n                    },\n                ),\n                \"resolution\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V1_V1_RES_MAP.keys()),\n                        \"default\": \"Auto\",\n                        \"tooltip\": \"The resolution for image generation. If not set to AUTO, this overrides the aspect_ratio setting.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"style_type\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"GENERAL\", \"REALISTIC\", \"DESIGN\", \"RENDER_3D\", \"ANIME\"],\n                        \"default\": \"NONE\",\n                        \"tooltip\": \"Style type for generation (V2 only)\",\n                    },\n                ),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Description of what to exclude from the image\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n                #\"color_palette\": (\n                #    IO.STRING,\n                #    {\n                #        \"multiline\": False,\n                #        \"default\": \"\",\n                #        \"tooltip\": \"Color palette preset name or hex colors with weights\",\n                #    },\n                #),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v2\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        turbo=False,\n        aspect_ratio=\"1:1\",\n        resolution=\"Auto\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        style_type=\"NONE\",\n        negative_prompt=\"\",\n        num_images=1,\n        color_palette=\"\",\n        auth_token=None,\n    ):\n        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)\n        resolution = V1_V1_RES_MAP.get(resolution, None)\n        # Determine the model based on turbo setting\n        model = \"V_2_TURBO\" if turbo else \"V_2\"\n\n        # Handle resolution vs aspect_ratio logic\n        # If resolution is not AUTO, it overrides aspect_ratio\n        final_resolution = None\n        final_aspect_ratio = None\n\n        if resolution != \"AUTO\":\n            final_resolution = resolution\n        else:\n            final_aspect_ratio = aspect_ratio if aspect_ratio != \"ASPECT_1_1\" else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/ideogram/generate\",\n                method=HttpMethod.POST,\n                request_model=IdeogramGenerateRequest,\n                response_model=IdeogramGenerateResponse,\n            ),\n            request=IdeogramGenerateRequest(\n                image_request=ImageRequest(\n                    prompt=prompt,\n                    model=model,\n                    num_images=num_images,\n                    seed=seed,\n                    aspect_ratio=final_aspect_ratio,\n                    resolution=final_resolution,\n                    magic_prompt_option=(\n                        magic_prompt_option if magic_prompt_option != \"AUTO\" else None\n                    ),\n                    style_type=style_type if style_type != \"NONE\" else None,\n                    negative_prompt=negative_prompt if negative_prompt else None,\n                    color_palette=color_palette if color_palette else None,\n                )\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-image-to-image",
  "markdown": "# Luma Image to Image - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\næ­¤èŠ‚ç‚¹è¿žæŽ¥åˆ°Luma AIçš„æ–‡æœ¬åˆ°å›¾åƒAPIï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è¯¦ç»†çš„æ–‡æœ¬æç¤ºè¯ç”Ÿæˆå›¾åƒã€‚Luma AIä»¥å…¶å‡ºè‰²çš„çœŸå®žæ„Ÿå’Œç»†èŠ‚è¡¨çŽ°è€Œé—»åï¼Œç‰¹åˆ«æ“…é•¿ç”Ÿæˆç…§ç‰‡çº§åˆ«çš„é€¼çœŸå†…å®¹å’Œè‰ºæœ¯é£Žæ ¼å›¾åƒã€‚\n\nLuma Image to Image èŠ‚ç‚¹åˆ†æžè¾“å…¥å›¾åƒå¹¶ç»“åˆæ–‡æœ¬æç¤ºè¯æ¥å¼•å¯¼ä¿®æ”¹è¿‡ç¨‹ã€‚å®ƒä½¿ç”¨ Luma AI çš„ç”Ÿæˆæ¨¡åž‹ï¼Œæ ¹æ®æç¤ºè¯å¯¹å›¾åƒè¿›è¡Œåˆ›æ–°æ€§çš„å˜åŒ–ã€‚ èŠ‚ç‚¹æµç¨‹ï¼š\n\n```\n\nclass LumaImageModifyNode(ComfyNodeABC):\n    \"\"\"\n    Modifies images synchronously based on prompt and aspect ratio.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE,),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"image_weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.02,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of the image; the closer to 0.0, the less the image will be modified.\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaImageModel],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {},\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        image: torch.Tensor,\n        image_weight: float,\n        seed,\n        auth_token=None,\n        **kwargs,\n    ):\n        # first, upload image\n        download_urls = upload_images_to_comfyapi(\n            image, max_images=1, auth_token=auth_token\n        )\n        image_url = download_urls[0]\n        # next, make Luma call with download url provided\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations/image\",\n                method=HttpMethod.POST,\n                request_model=LumaImageGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaImageGenerationRequest(\n                prompt=prompt,\n                model=model,\n                modify_image_ref=LumaModifyImageRef(\n                    url=image_url, weight=round(image_weight, 2)\n                ),\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        img_response = requests.get(response_poll.assets.image)\n        img = process_image_response(img_response)\n        return (img,)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v3",
  "markdown": "# Ideogram V3 - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Ideogram V3 èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg) æ­¤èŠ‚ç‚¹è¿žæŽ¥åˆ°Ideogram V3 APIï¼Œæ¥å®Œæˆå¯¹åº”çš„å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚ ç›®å‰æ­¤èŠ‚ç‚¹æ”¯æŒä¸¤ç§å›¾åƒç”Ÿæˆæ¨¡å¼:\n\n*   **æ–‡ç”Ÿå›¾æ¨¡å¼** - ä»Žçº¯æ–‡æœ¬æç¤ºè¯ç”Ÿæˆå…¨æ–°å›¾åƒ\n*   **å±€éƒ¨é‡ç»˜æ¨¡å¼ï¼ˆInpaintingï¼‰** - é€šè¿‡æä¾›åŽŸå§‹å›¾åƒå’Œé®ç½©æ¥é‡æ–°ç”Ÿæˆç‰¹å®šåŒºåŸŸ\n\n### æ–‡ç”Ÿå›¾æ¨¡å¼\n\nè¿™æ˜¯é»˜è®¤æ¨¡å¼ï¼Œå½“æ²¡æœ‰æä¾›å›¾åƒå’Œé®ç½©è¾“å…¥æ—¶æ¿€æ´»ã€‚åªéœ€æä¾›æç¤ºè¯å’Œæ‰€éœ€çš„å‚æ•°ï¼š\n\n1.  åœ¨æç¤ºè¯å­—æ®µä¸­æè¿°ä½ æƒ³è¦çš„å›¾åƒ\n2.  é€‰æ‹©é€‚å½“çš„å®½é«˜æ¯”æˆ–åˆ†è¾¨çŽ‡\n3.  è°ƒæ•´å…¶ä»–å‚æ•°å¦‚é­”æ³•æç¤ºã€ç§å­å’Œæ¸²æŸ“è´¨é‡\n4.  è¿è¡ŒèŠ‚ç‚¹ç”Ÿæˆå›¾åƒ\n\n### å±€éƒ¨é‡ç»˜æ¨¡å¼\n\n**é‡è¦æç¤º**ï¼šæ­¤æ¨¡å¼è¦æ±‚åŒæ—¶æä¾›å›¾åƒå’Œé®ç½©è¾“å…¥ã€‚å¦‚æžœåªæä¾›å…¶ä¸­ä¸€ä¸ªï¼ŒèŠ‚ç‚¹å°†æŠ›å‡ºé”™è¯¯ã€‚\n\n1.  å°†åŽŸå§‹å›¾åƒè¿žæŽ¥åˆ°`image`è¾“å…¥ç«¯å£\n2.  åˆ›å»ºä¸€ä¸ªä¸ŽåŽŸå›¾ç›¸åŒå°ºå¯¸çš„é®ç½©ï¼Œç™½è‰²åŒºåŸŸè¡¨ç¤ºè¦é‡æ–°ç”Ÿæˆçš„éƒ¨åˆ†\n3.  å°†é®ç½©è¿žæŽ¥åˆ°`mask`è¾“å…¥ç«¯å£\n4.  åœ¨æç¤ºè¯ä¸­æè¿°è¦åœ¨é®ç½©åŒºåŸŸç”Ÿæˆçš„å†…å®¹\n5.  è¿è¡ŒèŠ‚ç‚¹æ‰§è¡Œå±€éƒ¨ç¼–è¾‘\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | æè¿°è¦ç”Ÿæˆå†…å®¹çš„æ–‡æœ¬æç¤ºè¯ |\n| aspect\\_ratio | é€‰æ‹©é¡¹ | â€1:1â€ | å›¾åƒå®½é«˜æ¯”ï¼ˆä»…æ–‡ç”Ÿå›¾æ¨¡å¼æœ‰æ•ˆï¼‰ |\n| resolution | é€‰æ‹©é¡¹ | â€Autoâ€ | å›¾åƒåˆ†è¾¨çŽ‡ï¼Œè®¾ç½®åŽä¼šè¦†ç›–å®½é«˜æ¯”è®¾ç½® |\n| magic\\_prompt\\_option | é€‰æ‹©é¡¹ | â€AUTOâ€ | é­”æ³•æç¤ºå¢žå¼ºé€‰é¡¹ï¼šAUTOã€ONæˆ–OFF |\n| seed | æ•´æ•°  | 0   | éšæœºç§å­å€¼ï¼Œè®¾ä¸º0åˆ™éšæœºç”Ÿæˆ |\n| num\\_images | æ•´æ•°  | 1   | ç”Ÿæˆå›¾åƒæ•°é‡(1-8) |\n| rendering\\_speed | é€‰æ‹©é¡¹ | â€BALANCEDâ€ | æ¸²æŸ“é€Ÿåº¦ï¼šBALANCEDã€TURBOæˆ–QUALITY |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| image | å›¾åƒ  | ç”¨äºŽå±€éƒ¨é‡ç»˜æ¨¡å¼çš„è¾“å…¥å›¾åƒï¼ˆ**å¿…é¡»ä¸Žmaskä¸€èµ·æä¾›**ï¼‰ |\n| mask | é®ç½©  | ç”¨äºŽå±€éƒ¨é‡ç»˜çš„é®ç½©ï¼Œç™½è‰²åŒºåŸŸå°†è¢«æ›¿æ¢ï¼ˆ**å¿…é¡»ä¸Žimageä¸€èµ·æä¾›**ï¼‰ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | ç”Ÿæˆçš„å›¾åƒç»“æžœ |\n\n## å·¥ä½œåŽŸç†\n\nIdeogram V3èŠ‚ç‚¹ä½¿ç”¨æœ€å…ˆè¿›çš„AIæ¨¡åž‹å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œèƒ½å¤Ÿç†è§£å¤æ‚çš„è®¾è®¡æ„å›¾å’Œæ–‡å­—æŽ’ç‰ˆéœ€æ±‚ã€‚å®ƒæ”¯æŒä¸¤ç§ä¸»è¦æ¨¡å¼ï¼š\n\n1.  **ç”Ÿæˆæ¨¡å¼**ï¼šä»Žæ–‡æœ¬æç¤ºåˆ›å»ºå…¨æ–°å›¾åƒ\n2.  **ç¼–è¾‘æ¨¡å¼**ï¼šä½¿ç”¨åŽŸå§‹å›¾åƒ+é®ç½©ç»„åˆï¼Œåªæ›¿æ¢é®ç½©æŒ‡å®šçš„åŒºåŸŸ\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass IdeogramV3(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously using the Ideogram V3 model.\n\n    Supports both regular image generation from text prompts and image editing with mask.\n    Images links are available for a limited period of time; if you would like to keep the image, you must download it.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation or editing\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": list(V3_RATIO_MAP.keys()),\n                        \"default\": \"1:1\",\n                        \"tooltip\": \"The aspect ratio for image generation. Ignored if resolution is not set to Auto.\",\n                    },\n                ),\n                \"resolution\": (\n                    IO.COMBO,\n                    {\n                        \"options\": V3_RESOLUTIONS,\n                        \"default\": \"Auto\",\n                        \"tooltip\": \"The resolution for image generation. If not set to Auto, this overrides the aspect_ratio setting.\",\n                    },\n                ),\n                \"magic_prompt_option\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"AUTO\", \"ON\", \"OFF\"],\n                        \"default\": \"AUTO\",\n                        \"tooltip\": \"Determine if MagicPrompt should be used in generation\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"step\": 1,\n                        \"control_after_generate\": True,\n                        \"display\": \"number\",\n                    },\n                ),\n                \"num_images\": (\n                    IO.INT,\n                    {\"default\": 1, \"min\": 1, \"max\": 8, \"step\": 1, \"display\": \"number\"},\n                ),\n                \"rendering_speed\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"BALANCED\", \"TURBO\", \"QUALITY\"],\n                        \"default\": \"BALANCED\",\n                        \"tooltip\": \"Controls the trade-off between generation speed and quality\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/ideogram/v3\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        image=None,\n        mask=None,\n        resolution=\"Auto\",\n        aspect_ratio=\"1:1\",\n        magic_prompt_option=\"AUTO\",\n        seed=0,\n        num_images=1,\n        rendering_speed=\"BALANCED\",\n        auth_token=None,\n    ):\n        # Check if both image and mask are provided for editing mode\n        if image is not None and mask is not None:\n            # Edit mode\n            path = \"/proxy/ideogram/ideogram-v3/edit\"\n\n            # Process image and mask\n            input_tensor = image.squeeze().cpu()\n\n            # Validate mask dimensions match image\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n\n            # Process image\n            img_np = (input_tensor.numpy() * 255).astype(np.uint8)\n            img = Image.fromarray(img_np)\n            img_byte_arr = io.BytesIO()\n            img.save(img_byte_arr, format=\"PNG\")\n            img_byte_arr.seek(0)\n            img_binary = img_byte_arr\n            img_binary.name = \"image.png\"\n\n            # Process mask - white areas will be replaced\n            mask_np = (mask.squeeze().cpu().numpy() * 255).astype(np.uint8)\n            mask_img = Image.fromarray(mask_np)\n            mask_byte_arr = io.BytesIO()\n            mask_img.save(mask_byte_arr, format=\"PNG\")\n            mask_byte_arr.seek(0)\n            mask_binary = mask_byte_arr\n            mask_binary.name = \"mask.png\"\n\n            # Create edit request\n            edit_request = IdeogramV3EditRequest(\n                prompt=prompt,\n                rendering_speed=rendering_speed,\n            )\n\n            # Add optional parameters\n            if magic_prompt_option != \"AUTO\":\n                edit_request.magic_prompt = magic_prompt_option\n            if seed != 0:\n                edit_request.seed = seed\n            if num_images > 1:\n                edit_request.num_images = num_images\n\n            # Execute the operation for edit mode\n            operation = SynchronousOperation(\n                endpoint=ApiEndpoint(\n                    path=path,\n                    method=HttpMethod.POST,\n                    request_model=IdeogramV3EditRequest,\n                    response_model=IdeogramGenerateResponse,\n                ),\n                request=edit_request,\n                files={\n                    \"image\": img_binary,\n                    \"mask\": mask_binary,\n                },\n                content_type=\"multipart/form-data\",\n                auth_token=auth_token,\n            )\n\n        elif image is not None or mask is not None:\n            # If only one of image or mask is provided, raise an error\n            raise Exception(\"Ideogram V3 image editing requires both an image AND a mask\")\n        else:\n            # Generation mode\n            path = \"/proxy/ideogram/ideogram-v3/generate\"\n\n            # Create generation request\n            gen_request = IdeogramV3Request(\n                prompt=prompt,\n                rendering_speed=rendering_speed,\n            )\n\n            # Handle resolution vs aspect ratio\n            if resolution != \"Auto\":\n                gen_request.resolution = resolution\n            elif aspect_ratio != \"1:1\":\n                v3_aspect = V3_RATIO_MAP.get(aspect_ratio)\n                if v3_aspect:\n                    gen_request.aspect_ratio = v3_aspect\n\n            # Add optional parameters\n            if magic_prompt_option != \"AUTO\":\n                gen_request.magic_prompt = magic_prompt_option\n            if seed != 0:\n                gen_request.seed = seed\n            if num_images > 1:\n                gen_request.num_images = num_images\n\n            # Execute the operation for generation mode\n            operation = SynchronousOperation(\n                endpoint=ApiEndpoint(\n                    path=path,\n                    method=HttpMethod.POST,\n                    request_model=IdeogramV3Request,\n                    response_model=IdeogramGenerateResponse,\n                ),\n                request=gen_request,\n                auth_token=auth_token,\n            )\n\n        # Execute the operation and process response\n        response = operation.execute()\n\n        if not response.data or len(response.data) == 0:\n            raise Exception(\"No images were generated in the response\")\n\n        image_urls = [image_data.url for image_data in response.data if image_data.url]\n\n        if not image_urls:\n            raise Exception(\"No image URLs were generated in the response\")\n\n        return (download_and_process_images(image_urls),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-reference",
  "markdown": "# Luma Reference - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Luma Reference èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-reference.jpg) Luma Reference èŠ‚ç‚¹å…è®¸ä½ è®¾ç½®å‚è€ƒå›¾åƒå’Œæƒé‡ï¼Œç”¨äºŽæŒ‡å¯¼Lumaå›¾åƒç”ŸæˆèŠ‚ç‚¹çš„åˆ›ä½œè¿‡ç¨‹ï¼Œä½¿å¾—ç”Ÿæˆçš„å›¾åƒæ›´æŽ¥è¿‘å‚è€ƒå›¾åƒçš„ç‰¹å®šç‰¹å¾ã€‚\n\n## èŠ‚ç‚¹åŠŸèƒ½\n\næ­¤èŠ‚ç‚¹ä½œä¸ºLumaç”ŸæˆèŠ‚ç‚¹çš„è¾…åŠ©å·¥å…·ï¼Œå…è®¸ç”¨æˆ·æä¾›å‚è€ƒå›¾åƒæ¥å½±å“ç”Ÿæˆç»“æžœã€‚å®ƒè®©ç”¨æˆ·èƒ½å¤Ÿè®¾ç½®å‚è€ƒå›¾åƒçš„æƒé‡ï¼Œä»¥æŽ§åˆ¶å‚è€ƒå›¾åƒå¯¹æœ€ç»ˆç»“æžœçš„å½±å“ç¨‹åº¦ã€‚ å¤šä¸ª Luma Reference èŠ‚ç‚¹å¯ä»¥ä¸²è”ï¼Œæ ¹æ®å¯¹åº”çš„ API è¦æ±‚ï¼Œæœ€å¤šè¿è¡ŒåŒæ—¶ä¸²è” 4 ä¸ªè¿›è¡Œå·¥ä½œ\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | ä½œä¸ºå‚è€ƒçš„è¾“å…¥å›¾åƒ |\n| weight | æµ®ç‚¹æ•° | 1.0 | æŽ§åˆ¶å‚è€ƒå›¾åƒçš„å½±å“å¼ºåº¦ (0-1) |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| luma\\_ref | LUMA\\_REF | åŒ…å«å›¾åƒå’Œæƒé‡çš„å‚è€ƒå¯¹è±¡ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Luma Text to Image å·¥ä½œæµç¤ºä¾‹\n\nLuma Text to Image å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-image)\n\n## å·¥ä½œåŽŸç†\n\nLuma Reference èŠ‚ç‚¹æŽ¥æ”¶å›¾åƒè¾“å…¥å¹¶å…è®¸è®¾ç½®æƒé‡å€¼ã€‚è¯¥èŠ‚ç‚¹ä¸ç›´æŽ¥ç”Ÿæˆæˆ–ä¿®æ”¹å›¾åƒï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªåŒ…å«å›¾åƒæ•°æ®å’Œæƒé‡ä¿¡æ¯çš„å‚è€ƒå¯¹è±¡ï¼ŒåŽç»­ä¼ é€’ç»™Lumaç”ŸæˆèŠ‚ç‚¹ã€‚ åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼ŒLuma AI ä¼šåˆ†æžå‚è€ƒå›¾åƒçš„ç‰¹å¾ï¼Œå¹¶æ ¹æ®è®¾å®šçš„æƒé‡å°†è¿™äº›ç‰¹å¾èžå…¥åˆ°ç”Ÿæˆç»“æžœä¸­ã€‚è¾ƒé«˜çš„æƒé‡å€¼æ„å‘³ç€ç”Ÿæˆçš„å›¾åƒå°†æ›´æŽ¥è¿‘å‚è€ƒå›¾åƒçš„ç‰¹å¾ï¼Œè€Œè¾ƒä½Žçš„æƒé‡å€¼åˆ™è¡¨ç¤ºå‚è€ƒå›¾åƒåªä¼šè½»å¾®å½±å“æœ€ç»ˆç»“æžœã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass LumaReferenceNode(ComfyNodeABC):\n    \"\"\"\n    Holds an image and weight for use with Luma Generate Image node.\n    \"\"\"\n\n    RETURN_TYPES = (LumaIO.LUMA_REF,)\n    RETURN_NAMES = (\"luma_ref\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_luma_reference\"\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Image to use as reference.\",\n                    },\n                ),\n                \"weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of image reference.\",\n                    },\n                ),\n            },\n            \"optional\": {\"luma_ref\": (LumaIO.LUMA_REF,)},\n        }\n\n    def create_luma_reference(\n        self, image: torch.Tensor, weight: float, luma_ref: LumaReferenceChain = None\n    ):\n        if luma_ref is not None:\n            luma_ref = luma_ref.clone()\n        else:\n            luma_ref = LumaReferenceChain()\n        luma_ref.add(LumaReference(image=image, weight=round(weight, 2)))\n        return (luma_ref,)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-text-to-image",
  "markdown": "# Luma Text to Image - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Luma Text to Image èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg) Luma Text to Image èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨Luma AIçš„å…ˆè¿›å›¾åƒç”ŸæˆåŠŸèƒ½ï¼Œé€šè¿‡æ–‡æœ¬æè¿°åˆ›å»ºé«˜åº¦é€¼çœŸå’Œè‰ºæœ¯åŒ–çš„å›¾åƒã€‚\n\n## èŠ‚ç‚¹åŠŸèƒ½\n\næ­¤èŠ‚ç‚¹è¿žæŽ¥åˆ°Luma AIçš„æ–‡æœ¬åˆ°å›¾åƒAPIï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è¯¦ç»†çš„æ–‡æœ¬æç¤ºè¯ç”Ÿæˆå›¾åƒã€‚Luma AIä»¥å…¶å‡ºè‰²çš„çœŸå®žæ„Ÿå’Œç»†èŠ‚è¡¨çŽ°è€Œé—»åï¼Œç‰¹åˆ«æ“…é•¿ç”Ÿæˆç…§ç‰‡çº§åˆ«çš„é€¼çœŸå†…å®¹å’Œè‰ºæœ¯é£Žæ ¼å›¾åƒã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | æè¿°è¦ç”Ÿæˆå†…å®¹çš„æ–‡æœ¬æç¤ºè¯ |\n| model | é€‰æ‹©é¡¹ | \\-  | é€‰æ‹©ä½¿ç”¨çš„ç”Ÿæˆæ¨¡åž‹ |\n| aspect\\_ratio | é€‰æ‹©é¡¹ | 16:9 | è®¾ç½®è¾“å‡ºå›¾åƒçš„å®½é«˜æ¯” |\n| seed | æ•´æ•°  | 0   | ç§å­å€¼ï¼Œç”¨äºŽç¡®å®šèŠ‚ç‚¹æ˜¯å¦åº”é‡æ–°è¿è¡Œï¼Œä½†å®žé™…ç»“æžœä¸Žç§å­æ— å…³ |\n| style\\_image\\_weight | æµ®ç‚¹æ•° | 1.0 | æ ·å¼å›¾åƒçš„æƒé‡ï¼ŒèŒƒå›´0.0-1.0ï¼Œä»…åœ¨æä¾›style\\_imageæ—¶ç”Ÿæ•ˆï¼Œè¶Šå¤§é£Žæ ¼å‚è€ƒè¶Šå¼º |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| image\\_luma\\_ref | LUMA\\_REF | Lumaå‚è€ƒèŠ‚ç‚¹è¿žæŽ¥ï¼Œé€šè¿‡è¾“å…¥å›¾åƒå½±å“ç”Ÿæˆç»“æžœï¼Œæœ€å¤šå¯è€ƒè™‘4å¼ å›¾åƒ |\n| style\\_image | å›¾åƒ  | æ ·å¼å‚è€ƒå›¾åƒï¼Œä»…ä½¿ç”¨1å¼ å›¾åƒ |\n| character\\_image | å›¾åƒ  | è§’è‰²å‚è€ƒå›¾åƒï¼Œå¯ä»¥æ˜¯å¤šå¼ å›¾åƒçš„æ‰¹æ¬¡ï¼Œæœ€å¤šå¯è€ƒè™‘4å¼ å›¾åƒ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | ç”Ÿæˆçš„å›¾åƒç»“æžœ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\nLuma Text to Image å·¥ä½œæµè¯¦ç»†ä½¿ç”¨æŒ‡å—\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-image)\n\n## å·¥ä½œåŽŸç†\n\nLuma Text to Image èŠ‚ç‚¹åˆ†æžç”¨æˆ·æä¾›çš„æ–‡æœ¬æç¤ºè¯ï¼Œé€šè¿‡Luma AIçš„ç”Ÿæˆæ¨¡åž‹åˆ›å»ºç›¸åº”çš„å›¾åƒã€‚è¯¥è¿‡ç¨‹åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ç†è§£æ–‡æœ¬æè¿°å¹¶å°†å…¶è½¬æ¢ä¸ºè§†è§‰è¡¨çŽ°ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è°ƒæ•´å„ç§å‚æ•°æ¥ç²¾ç»†æŽ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼ŒåŒ…æ‹¬åˆ†è¾¨çŽ‡ã€å¼•å¯¼å°ºåº¦å’Œè´Ÿé¢æç¤ºè¯ã€‚ æ­¤å¤–ï¼ŒèŠ‚ç‚¹æ”¯æŒä½¿ç”¨å‚è€ƒå›¾åƒå’Œæ¦‚å¿µå¼•å¯¼æ¥è¿›ä¸€æ­¥å½±å“ç”Ÿæˆç»“æžœï¼Œä½¿åˆ›ä½œè€…èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å®žçŽ°ä»–ä»¬çš„åˆ›æ„æ„¿æ™¯ã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass LumaImageGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously based on prompt and aspect ratio.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaImageModel],),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in LumaAspectRatio],\n                    {\n                        \"default\": LumaAspectRatio.ratio_16_9,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n                \"style_image_weight\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 1.0,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Weight of style image. Ignored if no style_image provided.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image_luma_ref\": (\n                    LumaIO.LUMA_REF,\n                    {\n                        \"tooltip\": \"Luma Reference node connection to influence generation with input images; up to 4 images can be considered.\"\n                    },\n                ),\n                \"style_image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"Style reference image; only 1 image will be used.\"},\n                ),\n                \"character_image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Character reference images; can be a batch of multiple, up to 4 images can be considered.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        aspect_ratio: str,\n        seed,\n        style_image_weight: float,\n        image_luma_ref: LumaReferenceChain = None,\n        style_image: torch.Tensor = None,\n        character_image: torch.Tensor = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # handle image_luma_ref\n        api_image_ref = None\n        if image_luma_ref is not None:\n            api_image_ref = self._convert_luma_refs(\n                image_luma_ref, max_refs=4, auth_token=auth_token\n            )\n        # handle style_luma_ref\n        api_style_ref = None\n        if style_image is not None:\n            api_style_ref = self._convert_style_image(\n                style_image, weight=style_image_weight, auth_token=auth_token\n            )\n        # handle character_ref images\n        character_ref = None\n        if character_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                character_image, max_images=4, auth_token=auth_token\n            )\n            character_ref = LumaCharacterRef(\n                identity0=LumaImageIdentity(images=download_urls)\n            )\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations/image\",\n                method=HttpMethod.POST,\n                request_model=LumaImageGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaImageGenerationRequest(\n                prompt=prompt,\n                model=model,\n                aspect_ratio=aspect_ratio,\n                image_ref=api_image_ref,\n                style_ref=api_style_ref,\n                character_ref=character_ref,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        img_response = requests.get(response_poll.assets.image)\n        img = process_image_response(img_response)\n        return (img,)\n\n    def _convert_luma_refs(\n        self, luma_ref: LumaReferenceChain, max_refs: int, auth_token=None\n    ):\n        luma_urls = []\n        ref_count = 0\n        for ref in luma_ref.refs:\n            download_urls = upload_images_to_comfyapi(\n                ref.image, max_images=1, auth_token=auth_token\n            )\n            luma_urls.append(download_urls[0])\n            ref_count += 1\n            if ref_count >= max_refs:\n                break\n        return luma_ref.create_api_model(download_urls=luma_urls, max_refs=max_refs)\n\n    def _convert_style_image(\n        self, style_image: torch.Tensor, weight: float, auth_token=None\n    ):\n        chain = LumaReferenceChain(\n            first_ref=LumaReference(image=style_image, weight=weight)\n        )\n        return self._convert_luma_refs(chain, max_refs=1, auth_token=auth_token)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/openai/openai-dalle3",
  "markdown": "# OpenAI DALLÂ·E 3 - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Stability Stable Image Ultra èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg) æ­¤èŠ‚ç‚¹è¿žæŽ¥åˆ°OpenAIçš„DALLÂ·E 3 APIï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è¯¦ç»†çš„æ–‡æœ¬æç¤ºè¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚DALLÂ·E 3æ˜¯ OpenAI çš„å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œç›¸æ¯”å‰ä»£æä¾›äº†æ˜¾è‘—æå‡çš„å›¾åƒè´¨é‡ã€æ›´ç²¾ç¡®çš„æç¤ºè¯ç†è§£å’Œæ›´ä¼˜ç§€çš„ç»†èŠ‚è¡¨çŽ°èƒ½åŠ›ã€‚\n\n```\n\nclass OpenAIDalle3(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's DALLÂ·E 3 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are shortâ€‘lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for DALLÂ·E\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"quality\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"standard\", \"hd\"],\n                        \"default\": \"standard\",\n                        \"tooltip\": \"Image quality\",\n                    },\n                ),\n                \"style\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"natural\", \"vivid\"],\n                        \"default\": \"natural\",\n                        \"tooltip\": \"Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"1024x1024\", \"1024x1792\", \"1792x1024\"],\n                        \"default\": \"1024x1024\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        style=\"natural\",\n        quality=\"standard\",\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"dall-e-3\"\n\n        # build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/openai/images/generations\",\n                method=HttpMethod.POST,\n                request_model=OpenAIImageGenerationRequest,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=OpenAIImageGenerationRequest(\n                model=model,\n                prompt=prompt,\n                quality=quality,\n                size=size,\n                style=style,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/vace",
  "markdown": "# ComfyUI Wan2.1 VACE Video Examples\n\n## About VACE\n\nVACE 14B is an open-source unified video editing model launched by the Alibaba Tongyi Wanxiang team. Through integrating multi-task capabilities, supporting high-resolution processing and flexible multi-modal input mechanisms, this model significantly improves the efficiency and quality of video creation. The model is open-sourced under the [Apache-2.0](https://github.com/ali-vilab/VACE?tab=Apache-2.0-1-ov-file) license and can be used for personal or commercial purposes. Here is a comprehensive analysis of its core features and technical highlights:\n\n*   Multi-modal input: Supports multiple input forms including text, images, video, masks, and control signals\n*   Unified architecture: Single model supports multiple tasks with freely combinable functions\n*   Motion transfer: Generates coherent actions based on reference videos\n*   Local replacement: Replaces specific areas in videos through masks\n*   Video extension: Completes actions or extends backgrounds\n*   Background replacement: Preserves subjects while changing environmental backgrounds\n\nCurrently VACE has released two versions - 1.3B and 14B. Compared to the 1.3B version, the 14B version supports 720P resolution output with better image details and stability.\n\n| Model | 480P | 720P |\n| --- | --- | --- |\n| [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B) | âœ…   | âŒ   |\n| [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B) | âœ…   | âœ…   |\n\nRelated model weights and code repositories:\n\n*   [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B)\n*   [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B)\n*   [Github](https://github.com/ali-vilab/VACE)\n*   [VACE Project Homepage](https://ali-vilab.github.io/VACE-Page/)\n\n## Model Download and Loading in Workflows\n\nSince the workflows covered in this document all use the same workflow template, we can first complete the model download and loading information introduction, then enable/disable different inputs through Bypassing different nodes to achieve different workflows. The model download information is already embedded in the workflow information in specific examples, so you can also complete the model download when downloading specific example workflows.\n\n### Model Download\n\n**diffusion\\_models** [wan2.1\\_vace\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_14B_fp16.safetensors) [wan2.1\\_vace\\_1.3B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_1.3B_fp16.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\nChoose one version from **Text encoders** to download\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\nFile save location\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€ wan2.1_vace_14B_fp16.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors # or umt5_xxl_fp16.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚       â””â”€â”€  wan_2.1_vae.safetensors\n```\n\n### Model Loading\n\nSince the models used in the workflows covered in this document are consistent, the workflows are also the same, and only the nodes are bypassed to enable/disable different inputs, please refer to the following image to ensure that the corresponding models are correctly loaded in different workflows. ![Wan2.1 VACE Model Loading](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-model-loading.jpg)\n\n1.  Make sure the `Load Diffusion Model` node has loaded `wan2.1_vace_14B_fp16.safetensors`\n2.  Make sure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors` or `umt5_xxl_fp16.safetensors`\n3.  Make sure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n\n### How to toggle Node Bypass Status\n\nWhen a node is set to Bypass status, data passing through the node will not be affected by the node and will be output directly. We often set nodes to Bypass status when we donâ€™t need them. Here are three ways to toggle a nodeâ€™s Bypass status: ![Toggle Bypass](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/nodes/cancel-bypass.jpg)\n\n1.  After selecting the node, click the arrow in the indicator section of the selection toolbox to quickly toggle the nodeâ€™s Bypass status\n2.  After selecting the node, right-click the node and select `Mode` -> `Always` to switch to Always mode\n3.  After selecting the node, right-click the node and select the `Bypass` option to toggle the Bypass status\n\n## VACE Text-to-Video Workflow\n\n### 1\\. Workflow Download\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow\n\n### 2\\. Complete the Workflow Step by Step\n\n![image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg) Please follow the numbered steps in the image to ensure smooth workflow execution\n\n1.  Enter positive prompts in the `CLIP Text Encode (Positive Prompt)` node\n2.  Enter negative prompts in the `CLIP Text Encode (Negative Prompt)` node\n3.  Set the image dimensions (640x640 resolution recommended for first run) and frame count (video duration) in `WanVaceToVideo`\n4.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n5.  Once generated, the video will automatically save to `ComfyUI/output/video` directory (subfolder location depends on `save video` node settings)\n\n## VACE Image-to-Video Workflow\n\nYou can continue using the workflow above, just unbypass the `Load image` node in **Load reference image** and input your image. You can also use the image below - in this file weâ€™ve already set up the corresponding parameters.\n\n### 1\\. Workflow Download\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow\n\nPlease download the image below as input ![vace-i2v-input](https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/video/wan/vace/i2v/input.jpg)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg) Please follow the numbered steps in the image to ensure smooth workflow execution\n\n1.  Input the corresponding image in the `Load image` node\n2.  You can modify and edit prompts like in the text-to-video workflow\n3.  Set the image dimensions (640x640 resolution recommended for first run) and frame count (video duration) in `WanVaceToVideo`\n4.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n5.  Once generated, the video will automatically save to `ComfyUI/output/video` directory (subfolder location depends on `save video` node settings)\n\n### 3\\. Additional Workflow Notes\n\nVACE also supports inputting multiple reference images in a single image to generate corresponding videos. You can see related examples on the VACE project [page](https://ali-vilab.github.io/VACE-Page/)\n\n## VACE Video-to-Video Workflow\n\n### 1\\. Workflow Download\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow\n\nWe will use the following materials as input:\n\n1.  Input image for reference ![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/vace/v2v/input.jpg) \n2.  The video below has been preprocessed and will be used to control video generation\n\n3.  The video below is the original video. You can download these materials and use preprocessing nodes like [comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to preprocess the images\n\n### 2\\. Complete the Workflow Step by Step\n\n![Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg) Please follow the numbered steps in the image to ensure smooth workflow execution\n\n1.  Input the reference image in the `Load Image` node under `Load reference image`\n2.  Input the control video in the `Load Video` node under `Load control video`. Since the provided video is preprocessed, no additional processing is needed\n3.  If you need to preprocess the original video yourself, you can modify the `Image preprocessing` group or use `comfyui_controlnet_aux` nodes to complete the preprocessing\n4.  Modify prompts\n5.  Set the image dimensions (640x640 resolution recommended for first run) and frame count (video duration) in `WanVaceToVideo`\n6.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n7.  Once generated, the video will automatically save to `ComfyUI/output/video` directory (subfolder location depends on `save video` node settings)\n\n## VACE Video Outpainting Workflow\n\n\\[To be updated\\]\n\n## VACE First-Last Frame Video Generation\n\n\\[To be updated\\] To ensure that the first and last frames are effective, the video `length` setting must satisfy that `length-1` is divisible by 4. The corresponding `Batch_size` setting must satisfy `Batch_size = length - 2`\n\nPlease refer to the documentation below to learn about related nodes\n\n[\n\n## WanVaceToVideo Node Documentation\n\nWanVaceToVideo Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/conditioning/video-models/wan-vace-to-video)[\n\n## TrimVideoLatent Node Documentation\n\nComfyUI TrimVideoLatent Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/latent/video/trim-video-latent)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-creative-upscale",
  "markdown": "# Recraft Creative Upscale - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”ŸRecraft Creative UpscaleèŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg) Recraft Creative Upscale èŠ‚ç‚¹ä½¿ç”¨ Recraft çš„ API å¢žåŠ å›¾åƒåˆ†è¾¨çŽ‡ï¼Œè¿˜åˆ›é€ æ€§åœ°å¢žå¼ºå’Œä¸°å¯Œå›¾åƒç»†èŠ‚ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | éœ€è¦åˆ›æ„æ”¾å¤§çš„è¾“å…¥å›¾åƒ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | åˆ›æ„æ”¾å¤§åŽçš„é«˜åˆ†è¾¨çŽ‡å›¾åƒ |\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass RecraftCreativeUpscaleNode(RecraftCrispUpscaleNode):\n    \"\"\"\n    Upscale image synchronously.\n    Enhances a given raster image using â€˜creative upscaleâ€™ tool, boosting resolution with a focus on refining small details and faces.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_PATH = \"/proxy/recraft/images/creativeUpscale\"\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-controls",
  "markdown": "# Recraft Controls - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Recraft Controls èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg) Recraft Controls èŠ‚ç‚¹å…è®¸ä½ å®šä¹‰ä¸€ç³»åˆ—æŽ§åˆ¶å‚æ•°ï¼ˆå¦‚é¢œè‰²å’ŒèƒŒæ™¯é¢œè‰²æŒ‡å¯¼ï¼‰ï¼Œç”¨äºŽç²¾ç¡®æŒ‡å¯¼Recraftçš„å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚è¿™ä¸ªèŠ‚ç‚¹å°†å¤šç§æŽ§åˆ¶è¾“å…¥æ•´åˆä¸ºä¸€ä¸ªç»Ÿä¸€çš„æŽ§åˆ¶å¯¹è±¡ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| colors | Recraft Color | ç”¨äºŽç”Ÿæˆå›¾åƒçš„é¢œè‰²æŽ§åˆ¶ |\n| background\\_color | Recraft Color | èƒŒæ™¯é¢œè‰²æŽ§åˆ¶ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| recraft\\_controls | Recraft Controls | æŽ§åˆ¶é…ç½®å¯¹è±¡ï¼Œè¿žæŽ¥åˆ°Recraftç”ŸæˆèŠ‚ç‚¹ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Recraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\nRecraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## å·¥ä½œåŽŸç†\n\nèŠ‚ç‚¹å¤„ç†æµç¨‹:\n\n1.  æ”¶é›†è¾“å…¥çš„æŽ§åˆ¶å‚æ•°ï¼ˆcolorså’Œbackground\\_colorï¼‰\n2.  å°†è¿™äº›å‚æ•°æ•´åˆåˆ°ä¸€ä¸ªç»“æž„åŒ–çš„æŽ§åˆ¶å¯¹è±¡ä¸­\n3.  è¾“å‡ºæ­¤æŽ§åˆ¶å¯¹è±¡ï¼Œå¯è¿žæŽ¥åˆ°å„ç§Recraftç”ŸæˆèŠ‚ç‚¹\n\nå½“è¿žæŽ¥åˆ°Recraftç”ŸæˆèŠ‚ç‚¹åŽï¼Œè¿™äº›æŽ§åˆ¶å‚æ•°ä¼šå½±å“AIçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œä½¿AIèƒ½å¤Ÿè€ƒè™‘å¤šç§å› ç´ ï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡æœ¬æç¤ºçš„è¯­ä¹‰å†…å®¹ã€‚å¦‚æžœé…ç½®äº†é¢œè‰²è¾“å…¥ï¼ŒAIå°†å°è¯•åœ¨ç”Ÿæˆçš„å›¾åƒä¸­åˆç†åœ°ä½¿ç”¨è¿™äº›é¢œè‰²ã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass RecraftControlsNode:\n    \"\"\"\n    Create Recraft Controls for customizing Recraft generation.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.CONTROLS,)\n    RETURN_NAMES = (\"recraft_controls\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_controls\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n            },\n            \"optional\": {\n                \"colors\": (RecraftIO.COLOR,),\n                \"background_color\": (RecraftIO.COLOR,),\n            }\n        }\n\n    def create_controls(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None):\n        return (RecraftControls(colors=colors, background_color=background_color), )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale",
  "markdown": "# Recraft Crisp Upscale - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”ŸRecraft Crisp UpscaleèŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg) Recraft Crisp Upscale èŠ‚ç‚¹åˆ©ç”¨ Recraft çš„ API å¢žå¼ºå›¾åƒåˆ†è¾¨çŽ‡å’Œæ¸…æ™°åº¦ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | éœ€è¦æ”¾å¤§çš„è¾“å…¥å›¾åƒ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | æ”¾å¤§å’Œå¢žå¼ºåŽçš„å›¾åƒ |\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass RecraftCrispUpscaleNode:\n    \"\"\"\n    Upscale image synchronously.\n    Enhances a given raster image using â€˜crisp upscaleâ€™ tool, increasing image resolution, making the image sharper and cleaner.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_PATH = \"/proxy/recraft/images/crispUpscale\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=self.RECRAFT_PATH,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-color-rgb",
  "markdown": "# Recraft Color RGB - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n ![ComfyUI åŽŸç”Ÿ Recraft Color RGB èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg) Recraft Color RGB èŠ‚ç‚¹å…è®¸ä½ å®šä¹‰ç²¾ç¡®çš„RGBé¢œè‰²å€¼ï¼Œç”¨äºŽæŽ§åˆ¶Recraftå›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„é¢œè‰²ä½¿ç”¨ã€‚\n\n## èŠ‚ç‚¹åŠŸèƒ½\n\næ­¤èŠ‚ç‚¹åˆ›å»ºä¸€ä¸ªé¢œè‰²é…ç½®å¯¹è±¡ï¼Œå¯ä»¥è¿žæŽ¥åˆ°Recraft ControlsèŠ‚ç‚¹ï¼Œç”¨äºŽæŒ‡å®šç”Ÿæˆå›¾åƒä¸­åº”ä½¿ç”¨çš„é¢œè‰²ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| r   | æ•´æ•°  | 0   | çº¢è‰²é€šé“å€¼(0-255) |\n| g   | æ•´æ•°  | 0   | ç»¿è‰²é€šé“å€¼(0-255) |\n| b   | æ•´æ•°  | 0   | è“è‰²é€šé“å€¼(0-255) |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| recraft\\_color | Recraft Color | é¢œè‰²é…ç½®å¯¹è±¡ï¼Œè¿žæŽ¥åˆ°Recraft ControlsèŠ‚ç‚¹ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Recraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\nRecraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass RecraftColorRGBNode:\n    \"\"\"\n    Create Recraft Color by choosing specific RGB values.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.COLOR,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    RETURN_NAMES = (\"recraft_color\",)\n    FUNCTION = \"create_color\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"r\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Red value of color.\"\n                }),\n                \"g\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Green value of color.\"\n                }),\n                \"b\": (IO.INT, {\n                    \"default\": 0,\n                    \"min\": 0,\n                    \"max\": 255,\n                    \"tooltip\": \"Blue value of color.\"\n                }),\n            },\n            \"optional\": {\n                \"recraft_color\": (RecraftIO.COLOR,),\n            }\n        }\n\n    def create_color(self, r: int, g: int, b: int, recraft_color: RecraftColorChain=None):\n        recraft_color = recraft_color.clone() if recraft_color else RecraftColorChain()\n        recraft_color.add(RecraftColor(r, g, b))\n        return (recraft_color, )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/openai/openai-dalle2",
  "markdown": "# OpenAI DALLÂ·E 2 - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\n\nclass OpenAIDalle2(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's DALLÂ·E 2 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are shortâ€‘lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for DALLÂ·E\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"256x256\", \"512x512\", \"1024x1024\"],\n                        \"default\": \"1024x1024\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"How many images to generate\",\n                    },\n                ),\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        image=None,\n        mask=None,\n        n=1,\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"dall-e-2\"\n        path = \"/proxy/openai/images/generations\"\n        content_type = \"application/json\"\n        request_class = OpenAIImageGenerationRequest\n        img_binary = None\n\n        if image is not None and mask is not None:\n            path = \"/proxy/openai/images/edits\"\n            content_type = \"multipart/form-data\"\n            request_class = OpenAIImageEditRequest\n\n            input_tensor = image.squeeze().cpu()\n            height, width, channels = input_tensor.shape\n            rgba_tensor = torch.ones(height, width, 4, device=\"cpu\")\n            rgba_tensor[:, :, :channels] = input_tensor\n\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n            rgba_tensor[:, :, 3] = 1 - mask.squeeze().cpu()\n\n            rgba_tensor = downscale_image_tensor(rgba_tensor.unsqueeze(0)).squeeze()\n\n            image_np = (rgba_tensor.numpy() * 255).astype(np.uint8)\n            img = Image.fromarray(image_np)\n            img_byte_arr = io.BytesIO()\n            img.save(img_byte_arr, format=\"PNG\")\n            img_byte_arr.seek(0)\n            img_binary = img_byte_arr  # .getvalue()\n            img_binary.name = \"image.png\"\n        elif image is not None or mask is not None:\n            raise Exception(\"Dall-E 2 image editing requires an image AND a mask\")\n\n        # Build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=path,\n                method=HttpMethod.POST,\n                request_model=request_class,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=request_class(\n                model=model,\n                prompt=prompt,\n                n=n,\n                size=size,\n                seed=seed,\n            ),\n            files=(\n                {\n                    \"image\": img_binary,\n                }\n                if img_binary\n                else None\n            ),\n            content_type=content_type,\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/controlnet/controlnet",
  "markdown": "# ComfyUI ControlNet Usage Example - ComfyUI\n\nAchieving precise control over image creation in AI image generation cannot be done with just one click. It typically requires numerous generation attempts to produce a satisfactory image. However, the emergence of **ControlNet** has effectively addressed this challenge. ControlNet is a conditional control generation model based on diffusion models (such as Stable Diffusion), first proposed by [Lvmin Zhang](https://lllyasviel.github.io/) and Maneesh Agrawala et al. in 2023 in the paper [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543). ControlNet models significantly enhance the controllability of image generation and the ability to reproduce details by introducing multimodal input conditions, such as edge detection maps, depth maps, and pose keypoints. These conditioning constraints make image generation more controllable, allowing multiple ControlNet models to be used simultaneously during the drawing process for better results. Before ControlNet, we could only rely on the model to generate images repeatedly until we were satisfied with the results, which involved a lot of randomness. ![Images generated with random seeds in ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/generated_with_random_seed.jpg) With the advent of ControlNet, we can control image generation by introducing additional conditions. For example, we can use a simple sketch to guide the image generation process, producing images that closely align with our sketch. ![Sketch-controlled image generation in ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/scribble_example.jpg) In this example, we will guide you through installing and using ControlNet models in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), and complete a sketch-controlled image generation example. ![ComfyUI ControlNet Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)\n\n## ControlNet Image Preprocessing Information\n\nDifferent types of ControlNet models typically require different types of reference images: ![Reference Images](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/main/examples/CNAuxBanner.jpg?raw=true)\n\n> Image source: [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\nSince the current **Comfy Core** nodes do not include all types of **preprocessors**, in the actual examples in this documentation, we will provide pre-processed images. However, in practical use, you may need to use custom nodes to preprocess images to meet the requirements of different ControlNet models. Below are some relevant custom nodes:\n\n*   [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n*   [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n## ComfyUI ControlNet Workflow Example Explanation\n\n### 1\\. ControlNet Workflow Assets\n\nPlease download the workflow image below and drag it into ComfyUI to load the workflow: ![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)\n\nPlease download the image below, which we will use as input: ![ComfyUI Sketch Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_input.png)\n\n### 2\\. Manual Model Installation\n\n*   [dreamCreationVirtual3DECommerce\\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n*   [control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ dreamCreationVirtual3DECommerce_v10.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ vae-ft-mse-840000-ema-pruned.safetensors\nâ”‚   â””â”€â”€ controlnet/\nâ”‚       â””â”€â”€ control_v11p_sd15_scribble_fp16.safetensors\n```\n\n### 3\\. Step-by-Step Workflow Execution\n\n![ComfyUI Workflow - ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_scribble.png)\n\n1.  Ensure that `Load Checkpoint` can load **dreamCreationVirtual3DECommerce\\_v10.safetensors**\n2.  Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**\n3.  Click `Upload` in the `Load Image` node to upload the input image provided earlier\n4.  Ensure that `Load ControlNet` can load **control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors**\n5.  Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n### Load ControlNet Node Explanation\n\n![load controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_controlnet_model.jpg) Models located in `ComfyUI\\models\\controlnet` will be detected by ComfyUI and can be loaded through this node.\n\n### Apply ControlNet Node Explanation\n\n![apply controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg) This node accepts the ControlNet model loaded by `load controlnet` and generates corresponding control conditions based on the input image. **Input Types**\n\n| Parameter Name | Function |\n| --- | --- |\n| `positive` | Positive conditioning |\n| `negative` | Negative conditioning |\n| `control_net` | The ControlNet model to be applied |\n| `image` | Preprocessed image used as reference for ControlNet application |\n| `vae` | VAE model input |\n| `strength` | Strength of ControlNet application; higher values increase ControlNetâ€™s influence on the generated image |\n| `start_percent` | Determines when to start applying ControlNet as a percentage; e.g., 0.2 means ControlNet guidance begins when 20% of diffusion is complete |\n| `end_percent` | Determines when to stop applying ControlNet as a percentage; e.g., 0.8 means ControlNet guidance stops when 80% of diffusion is complete |\n\n**Output Types**\n\n| Parameter Name | Function |\n| --- | --- |\n| `positive` | Positive conditioning data processed by ControlNet |\n| `negative` | Negative conditioning data processed by ControlNet |\n\nYou can use chain connections to apply multiple ControlNet models, as shown in the image below. You can also refer to the [Mixing ControlNet Models](https://docs.comfy.org/tutorials/controlnet/mixing-controlnets) guide to learn more about combining multiple ControlNet models. ![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg) \n\n## Start Your Exploration\n\n1.  Try creating similar sketches, or even draw your own, and use ControlNet models to generate images to experience the benefits of ControlNet.\n2.  Adjust the `Control Strength` parameter in the Apply ControlNet node to control the influence of the ControlNet model on the generated image.\n3.  Visit the [ControlNet-v1-1\\_fp16\\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) repository to download other types of ControlNet models and try using them to generate images."
},
{
  "url": "https://docs.comfy.org/specs/workflow_json",
  "markdown": "# Workflow JSON - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyWorkflow1_0\",\n  \"definitions\": {\n    \"ComfyWorkflow1_0\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"version\": {\n          \"type\": \"number\",\n          \"const\": 1\n        },\n        \"config\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"links_ontop\": {\n                      \"type\": \"boolean\"\n                    },\n                    \"align_to_grid\": {\n                      \"type\": \"boolean\"\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"state\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"lastGroupid\": {\n              \"type\": \"number\"\n            },\n            \"lastNodeId\": {\n              \"type\": \"number\"\n            },\n            \"lastLinkId\": {\n              \"type\": \"number\"\n            },\n            \"lastRerouteId\": {\n              \"type\": \"number\"\n            }\n          },\n          \"additionalProperties\": true\n        },\n        \"groups\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"title\": {\n                \"type\": \"string\"\n              },\n              \"bounding\": {\n                \"type\": \"array\",\n                \"minItems\": 4,\n                \"maxItems\": 4,\n                \"items\": [\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"font_size\": {\n                \"type\": \"number\"\n              },\n              \"locked\": {\n                \"type\": \"boolean\"\n              }\n            },\n            \"required\": [\n              \"title\",\n              \"bounding\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"nodes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"size\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"flags\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"collapsed\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"pinned\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"allow_interaction\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"horizontal\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"skip_repeated_outputs\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"order\": {\n                \"type\": \"number\"\n              },\n              \"mode\": {\n                \"type\": \"number\"\n              },\n              \"inputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"link\": {\n                      \"type\": [\n                        \"number\",\n                        \"null\"\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"outputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"links\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        {\n                          \"type\": \"null\"\n                        }\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"properties\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"Node name for S&R\": {\n                    \"type\": \"string\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"widgets_values\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\"\n                  },\n                  {\n                    \"type\": \"object\",\n                    \"additionalProperties\": {}\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"bgcolor\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"type\",\n              \"pos\",\n              \"size\",\n              \"flags\",\n              \"order\",\n              \"mode\",\n              \"properties\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"links\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"type\": \"number\"\n              },\n              \"origin_id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"origin_slot\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"target_id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"target_slot\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"string\"\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    }\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"parentId\": {\n                \"type\": \"number\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"origin_id\",\n              \"origin_slot\",\n              \"target_id\",\n              \"target_slot\",\n              \"type\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"reroutes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"type\": \"number\"\n              },\n              \"parentId\": {\n                \"type\": \"number\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"linkIds\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"number\"\n                    }\n                  },\n                  {\n                    \"type\": \"null\"\n                  }\n                ]\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"pos\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"extra\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"ds\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"scale\": {\n                          \"type\": \"number\"\n                        },\n                        \"offset\": {\n                          \"anyOf\": [\n                            {\n                              \"type\": \"object\",\n                              \"properties\": {\n                                \"0\": {\n                                  \"type\": \"number\"\n                                },\n                                \"1\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              \"required\": [\n                                \"0\",\n                                \"1\"\n                              ],\n                              \"additionalProperties\": true\n                            },\n                            {\n                              \"type\": \"array\",\n                              \"minItems\": 2,\n                              \"maxItems\": 2,\n                              \"items\": [\n                                {\n                                  \"type\": \"number\"\n                                },\n                                {\n                                  \"type\": \"number\"\n                                }\n                              ]\n                            }\n                          ]\n                        }\n                      },\n                      \"required\": [\n                        \"scale\",\n                        \"offset\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"info\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"name\": {\n                          \"type\": \"string\"\n                        },\n                        \"author\": {\n                          \"type\": \"string\"\n                        },\n                        \"description\": {\n                          \"type\": \"string\"\n                        },\n                        \"version\": {\n                          \"type\": \"string\"\n                        },\n                        \"created\": {\n                          \"type\": \"string\"\n                        },\n                        \"modified\": {\n                          \"type\": \"string\"\n                        },\n                        \"software\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"required\": [\n                        \"name\",\n                        \"author\",\n                        \"description\",\n                        \"version\",\n                        \"created\",\n                        \"modified\",\n                        \"software\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"linkExtensions\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"parentId\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    },\n                    \"reroutes\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          },\n                          \"pos\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"0\": {\n                                    \"type\": \"number\"\n                                  },\n                                  \"1\": {\n                                    \"type\": \"number\"\n                                  }\n                                },\n                                \"required\": [\n                                  \"0\",\n                                  \"1\"\n                                ],\n                                \"additionalProperties\": true\n                              },\n                              {\n                                \"type\": \"array\",\n                                \"minItems\": 2,\n                                \"maxItems\": 2,\n                                \"items\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"number\"\n                                  }\n                                ]\n                              }\n                            ]\n                          },\n                          \"linkIds\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              {\n                                \"type\": \"null\"\n                              }\n                            ]\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"pos\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"models\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"url\": {\n                \"type\": \"string\",\n                \"format\": \"uri\"\n              },\n              \"hash\": {\n                \"type\": \"string\"\n              },\n              \"hash_type\": {\n                \"type\": \"string\"\n              },\n              \"directory\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"name\",\n              \"url\",\n              \"directory\"\n            ],\n            \"additionalProperties\": false\n          }\n        }\n      },\n      \"required\": [\n        \"version\",\n        \"state\",\n        \"nodes\"\n      ],\n      \"additionalProperties\": true\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fcustom-nodes%2Fjs%2Fjavascript_bottom_panel_tabs",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fregistry%2Fspecifications",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-kontext",
  "markdown": "# ComfyUI Flux.1 Kontext Pro Image API Node Official Example\n\nFLUX.1 Kontext is a professional image-to-image editing model developed by Black Forest Labs, focusing on intelligent understanding of image context and precise editing. It can perform various editing tasks without complex descriptions, including object modification, style transfer, background replacement, character consistency editing, and text editing. The core advantage of Kontext lies in its excellent context understanding ability and character consistency maintenance, ensuring that key elements such as character features and composition layout remain stable even after multiple iterations of editing. Currently, ComfyUI has supported two models of Flux.1 Kontext:\n\n*   **Kontext Pro** is ideal for editing, composing, and remixing.\n*   **Kontext Max** pushes the limits on typography, prompt precision, and speed.\n\nIn this guide, we will briefly introduce how to use the Flux.1 Kontext API nodes to perform image editing through corresponding workflows.\n\n## Flux.1 Kontext Multiple Image Input Workflow\n\nWe have recently updated to support multiple image input workflows. Using the new `Image Stitch` node, you can stitch multiple images into a single image and edit it using Flux.1 Kontext.\n\n### 1\\. Workflow File Download\n\nThe `metadata` of the images below contains the workflow information. Please download and drag them into ComfyUI to load the corresponding workflow. ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/multiple_image_input.png) Download the following images for input or use your own images: ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/girl.jpg) ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/dog.jpg) ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/sofa.jpg)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ComfyUI Flux.1 Kontext Pro Image API Node Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg) You can follow the numbered steps in the image to complete the workflow:\n\n1.  Upload the provided images in the `Load image` node\n2.  Modify the necessary parameters in `Flux.1 Kontext Pro Image`:\n    *   `prompt` Enter the prompt for the image you want to edit\n    *   `aspect_ratio` Set the aspect ratio of the original image, which must be between 1:4 and 4:1\n    *   `prompt_upsampling` Set whether to use prompt upsampling. If enabled, it will automatically modify the prompt to get richer results, but the results are not reproducible\n3.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image editing\n4.  After waiting for the API to return results, you can view the edited image in the `Save Image` node, and the corresponding image will also be saved to the `ComfyUI/output/` directory\n\n### 1\\. Workflow File Download\n\nThe `metadata` of the image below contains the workflow information. Please download and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image.png) Download the image below for input or use your own image: ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image_input.png)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ComfyUI Flux.1 Kontext Pro Image API Node Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg) You can follow the numbered steps in the image to complete the workflow:\n\n1.  Load the image you want to edit in the `Load Image` node\n2.  (Optional) Modify the necessary parameters in `Flux.1 Kontext Pro Image`\n3.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image editing\n4.  After waiting for the API to return results, you can view the edited image in the `Save Image` node, and the corresponding image will also be saved to the `ComfyUI/output/` directory\n\n## Flux.1 Kontext Max Image API Node Workflow\n\n### 1\\. Workflow File Download\n\nThe `metadata` of the image below contains the workflow information. Please download and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI Flux.1 Kontext Max Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image.png) Download the image below for input or use your own image for demonstration: ![ComfyUI Flux.1 Kontext Max Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image_input.png)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ComfyUI Flux.1 Kontext Max Image API Node Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg) You can follow the numbered steps in the image to complete the workflow:\n\n1.  Load the image you want to edit in the `Load Image` node\n2.  (Optional) Modify the necessary parameters in `Flux.1 Kontext Max Image`\n3.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image editing\n4.  After waiting for the API to return results, you can view the edited image in the `Save Image` node, and the corresponding image will also be saved to the `ComfyUI/output/` directory\n\n## Flux Kontext Prompt Techniques\n\n### 1\\. Basic Modifications\n\n*   Simple and direct: `\"Change the car color to red\"`\n*   Maintain style: `\"Change to daytime while maintaining the same style of the painting\"`\n\n### 2\\. Style Transfer\n\n**Principles:**\n\n*   Clearly name style: `\"Transform to Bauhaus art style\"`\n*   Describe characteristics: `\"Transform to oil painting with visible brushstrokes, thick paint texture\"`\n*   Preserve composition: `\"Change to Bauhaus style while maintaining the original composition\"`\n\n### 3\\. Character Consistency\n\n**Framework:**\n\n*   Specific description: `\"The woman with short black hair\"` instead of â€œsheâ€\n*   Preserve features: `\"while maintaining the same facial features, hairstyle, and expression\"`\n*   Step-by-step modifications: Change background first, then actions\n\n### 4\\. Text Editing\n\n*   Use quotes: `\"Replace 'joy' with 'BFL'\"`\n*   Maintain format: `\"Replace text while maintaining the same font style\"`\n\n## Common Problem Solutions\n\n### Character Changes Too Much\n\nâŒ Wrong: `\"Transform the person into a Viking\"` âœ… Correct: `\"Change the clothes to be a viking warrior while preserving facial features\"`\n\n### Composition Position Changes\n\nâŒ Wrong: `\"Put him on a beach\"` âœ… Correct: `\"Change the background to a beach while keeping the person in the exact same position, scale, and pose\"`\n\n### Style Application Inaccuracy\n\nâŒ Wrong: `\"Make it a sketch\"` âœ… Correct: `\"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\"`\n\n## Core Principles\n\n1.  **Be Specific and Clear** - Use precise descriptions, avoid vague terms\n2.  **Step-by-step Editing** - Break complex modifications into multiple simple steps\n3.  **Explicit Preservation** - State what should remain unchanged\n4.  **Verb Selection** - Use â€œchangeâ€, â€œreplaceâ€ rather than â€œtransformâ€\n\n## Best Practice Templates\n\n**Object Modification:** `\"Change [object] to [new state], keep [content to preserve] unchanged\"` **Style Transfer:** `\"Transform to [specific style], while maintaining [composition/character/other] unchanged\"` **Background Replacement:** `\"Change the background to [new background], keep the subject in the exact same position and pose\"` **Text Editing:** `\"Replace '[original text]' with '[new text]', maintain the same font style\"`\n\n> **Remember:** The more specific, the better. Kontext excels at understanding detailed instructions and maintaining consistency."
},
{
  "url": "https://docs.comfy.org/tutorials/image/omnigen/omnigen2",
  "markdown": "# ComfyUI OmniGen2 Native Workflow Examples\n\n## About OmniGen2\n\nOmniGen2 is a powerful and efficient unified multimodal generation model with approximately **7B** total parameters (3B text model + 4B image generation model). Unlike OmniGen v1, OmniGen2 adopts an innovative dual-path Transformer architecture with completely independent text autoregressive model and image diffusion model, achieving parameter decoupling and specialized optimization.\n\n### Model Highlights\n\n*   **Visual Understanding**: Inherits the powerful image content interpretation and analysis capabilities of the Qwen-VL-2.5 foundation model\n*   **Text-to-Image Generation**: Creates high-fidelity and aesthetically pleasing images from text prompts\n*   **Instruction-guided Image Editing**: Performs complex, instruction-based image modifications, achieving state-of-the-art performance among open-source models\n*   **Contextual Generation**: Versatile capabilities to process and flexibly combine diverse inputs (including people, reference objects, and scenes), producing novel and coherent visual outputs\n\n### Technical Features\n\n*   **Dual-path Architecture**: Based on Qwen 2.5 VL (3B) text encoder + independent diffusion Transformer (4B)\n*   **Omni-RoPE Position Encoding**: Supports multi-image spatial positioning and identity distinction\n*   **Parameter Decoupling Design**: Avoids negative impact of text generation on image quality\n*   Support for complex text understanding and image understanding\n*   Controllable image generation and editing\n*   Excellent detail preservation capabilities\n*   Unified architecture supporting multiple image generation tasks\n*   Text generation capability: Can generate clear text content within images\n\n## OmniGen2 Model Download\n\nSince this article involves different workflows, the corresponding model files and installation locations are as follows. The download information for model files is also included in the corresponding workflows: **Diffusion Models**\n\n*   [omnigen2\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/omnigen2_fp16.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/resolve/main/split_files/vae/ae.safetensors)\n\n**Text Encoders**\n\n*   [qwen\\_2.5\\_vl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_fp16.safetensors)\n\nFile save location:\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ omnigen2_fp16.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ text_encoders/\nâ”‚       â””â”€â”€ qwen_2.5_vl_fp16.safetensors\n```\n\n## ComfyUI OmniGen2 Text-to-Image Workflow\n\n### 1\\. Download Workflow File\n\n![Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_t2i.png)\n\n### 2\\. Complete Workflow Step by Step\n\n![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg) Please follow the numbered steps in the image for step-by-step confirmation to ensure smooth operation of the corresponding workflow:\n\n1.  **Load Main Model**: Ensure the `Load Diffusion Model` node loads `omnigen2_fp16.safetensors`\n2.  **Load Text Encoder**: Ensure the `Load CLIP` node loads `qwen_2.5_vl_fp16.safetensors`\n3.  **Load VAE**: Ensure the `Load VAE` node loads `ae.safetensors`\n4.  **Set Image Dimensions**: Set the generated image dimensions in the `EmptySD3LatentImage` node (recommended 1024x1024)\n5.  **Input Prompts**:\n    *   Input positive prompts in the first `CLipTextEncode` node (content you want to appear in the image)\n    *   Input negative prompts in the second `CLipTextEncode` node (content you donâ€™t want to appear in the image)\n6.  **Start Generation**: Click the `Queue Prompt` button, or use the shortcut `Ctrl(cmd) + Enter` to execute text-to-image generation\n7.  **View Results**: After generation is complete, the corresponding images will be automatically saved to the `ComfyUI/output/` directory, and you can also preview them in the `SaveImage` node\n\n## ComfyUI OmniGen2 Image Editing Workflow\n\nOmniGen2 has rich image editing capabilities and supports adding text to images\n\n### 1\\. Download Workflow File\n\n![Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_image_edit.png) Download the image below, which we will use as the input image. ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/input_fairy.png) \n\n### 2\\. Complete Workflow Step by Step\n\n![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg)\n\n1.  **Load Main Model**: Ensure the `Load Diffusion Model` node loads `omnigen2_fp16.safetensors`\n2.  **Load Text Encoder**: Ensure the `Load CLIP` node loads `qwen_2.5_vl_fp16.safetensors`\n3.  **Load VAE**: Ensure the `Load VAE` node loads `ae.safetensors`\n4.  **Upload Image**: Upload the provided image in the `Load Image` node\n5.  **Input Prompts**:\n    *   Input positive prompts in the first `CLipTextEncode` node (content you want to appear in the image)\n    *   Input negative prompts in the second `CLipTextEncode` node (content you donâ€™t want to appear in the image)\n6.  **Start Generation**: Click the `Queue Prompt` button, or use the shortcut `Ctrl(cmd) + Enter` to execute text-to-image generation\n7.  **View Results**: After generation is complete, the corresponding images will be automatically saved to the `ComfyUI/output/` directory, and you can also preview them in the `SaveImage` node\n\n### 3\\. Additional Workflow Instructions\n\n*   If you want to enable the second image input, you can use the shortcut **Ctrl + B** to enable the corresponding node inputs for nodes that are in pink/purple state in the workflow\n*   If you want to customize dimensions, you can delete the `Get image size` node linked to the `EmptySD3LatentImage` node and input custom dimensions"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image",
  "markdown": "# Luma Text to Image API Node ComfyUI Official Example\n\nThe [Luma Text to Image](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image) node allows you to generate high-quality images from text prompts using Luma AIâ€™s advanced technology, capable of creating photorealistic content and artistic style images. In this guide, weâ€™ll show you how to set up workflows using this node for text-to-image generation.\n\n## Luma Text to Image Node Documentation\n\nYou can refer to the following documentation for detailed parameter settings:\n\n[\n\n## Luma Text to Image Node Documentation\n\nLuma Text to Image API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)[\n\n## Luma Reference Node Documentation\n\nLuma Reference API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)\n\nWhen the `Luma Text to Image` node is used without any image inputs, it functions as a text-to-image workflow. In this guide, weâ€™ve created examples using `style_image` and `image_luma_ref` to showcase Luma AIâ€™s excellent image processing capabilities.\n\n### 1\\. Download Workflow Files\n\nThe workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow. ![Luma Text to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/luma_t2i.png) Please download these images for input: ![Input Image - Reference](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_ref.png) ![Input Image - Style](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_style.png)\n\n### 2\\. Follow Steps to Run the Workflow\n\n![Luma Text to Image Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg) Follow the numbered steps in the image to complete the basic workflow:\n\n1.  Upload the reference image in the `Load image` node\n2.  Upload the style reference image in the `Load image (renamed to styleref)` node\n3.  (Optional) Modify the prompts in the `Luma Text to Image` node\n4.  (Optional) Adjust the `style_image_weight` to control the style reference imageâ€™s influence\n5.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image\n6.  After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory\n\n![Style Image Weight Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg)\n\n### 3\\. Additional Notes\n\n*   The [node](https://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image) allows up to 4 reference images and character references simultaneously.\n*   To enable multiple image inputs, right-click on the purple â€œBypassedâ€ nodes and set their `mode` to `always`"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fdevelopment%2Fcomfyui-server%2Fcomms_messages",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/video/ltxv",
  "markdown": "# LTX-Video - ComfyUI\n\n[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) is a very efficient video model by lightricks. The important thing with this model is to give it long descriptive prompts.\n\n## Multi Frame Control\n\nAllows you to control the video with a series of images. You can download the input images: [starting frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) and [ending frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png). ![LTX-Video Multi Frame Control](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/workflow.webp)\n\n## Image to Video\n\nAllows you to control the video with a first [frame image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png). ![LTX-Video Image to Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/workflow.webp)\n\n## Text to Video\n\n![LTX-Video Text to Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/t2v.webp)\n\n## Requirements\n\nDownload the following models and place them in the locations specified below:\n\n*   [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true)\n\n```\nâ”œâ”€â”€ checkpoints/\nâ”‚   â””â”€â”€ ltx-video-2b-v0.9.5.safetensors\nâ””â”€â”€ text_encoders/\n    â””â”€â”€ t5xxl_fp16.safetensors\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/openai/chat",
  "markdown": "# OpenAI Chat API Node ComfyUI Official Example\n\nOpenAI is a company focused on generative AI, providing powerful conversational capabilities. Currently, ComfyUI has integrated the OpenAI API, allowing you to directly use the related nodes in ComfyUI to complete conversational functions. In this guide, we will walk you through completing the corresponding conversational functionality.\n\n## OpenAI Chat Workflow\n\n### 1\\. Workflow File Download\n\nPlease download the Json file below and drag it into ComfyUI to load the corresponding workflow.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/openai/api_openai_chat.json)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![OpenAI Chat Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg)\n\nYou can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Load Image` node, load the image you need AI to interpret\n2.  (Optional) If needed, you can modify the settings in `OpenAI Chat Advanced Options` to have AI execute specific tasks\n3.  In the `OpenAI Chat` node, you can modify `Prompt` to set the conversation prompt, or modify `model` to select different models\n4.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the conversation.\n5.  After waiting for the API to return results, you can view the corresponding AI returned content in the `Preview Any` node.\n\n### 3\\. Additional Notes\n\n*   Currently, the file input node `OpenAI Chat Input Files` requires files to be uploaded to the `ComfyUI/input/` directory first. This node is being improved, and we will modify the template after updates\n*   The workflow provides an example using `Batch Images` for input. If you have multiple images that need AI interpretation, you can refer to the step diagram and use right-click to set the corresponding node mode to `Always` to enable it"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/runway/image-generation",
  "markdown": "# Runway API Node Image Generation ComfyUI Official Example\n\nRunway is a company focused on generative AI, providing powerful image generation capabilities. Its models support features such as style transfer, image extension, and detail control. Currently, ComfyUI has integrated the Runway API, allowing you to directly use the related nodes in ComfyUI for image generation. In this guide, we will walk you through the following workflows:\n\n*   Text-to-image\n*   Reference-to-image\n\n## Runway Image Text-to-Image Workflow\n\n### 1\\. Workflow File Download\n\nThe image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI Runway Image Text to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/text_to_image.png)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway Image Text to Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg) You can refer to the numbers in the image to complete the basic text-to-image workflow execution:\n\n1.  In the `Runway Text to Image` node, input your prompt in the `prompt` field\n2.  (Optional) Adjust the `ratio` setting to set different output aspect ratios\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute image generation.\n4.  After waiting for the API to return results, you can view the generated image in the `Save Image` node (right-click to save). The corresponding image will also be saved to the `ComfyUI/output/` directory.\n\n## Runway Image Reference-to-Image Workflow\n\n### 1\\. Workflow and Input Image Download\n\nThe image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow. ![ComfyUI Runway Image Reference to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/runway_reference_to_image.png) Download the image below for input ![ComfyUI Runway Image Reference to Image Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/input.png)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![ComfyUI Runway Image Reference to Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg) You can refer to the numbers in the image to complete the basic reference-to-image workflow execution:\n\n1.  In the `Load Image` node, load the provided input image\n2.  In the `Runway Text to Image` node, input your prompt in the `prompt` field and adjust dimensions\n3.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute image generation.\n4.  After waiting for the API to return results, you can view the generated image in the `Save Image` node (right-click to save). The corresponding image will also be saved to the `ComfyUI/output/` directory."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image",
  "markdown": "# Stability AI Stable Diffusion 3.5 - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Stability AI Stable Diffusion 3.5 èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg) Stability AI Stable Diffusion 3.5 Image èŠ‚ç‚¹ä½¿ç”¨ Stability AI çš„ Stable Diffusion 3.5 API ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚å®ƒæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºè¯åˆ›å»ºè¯¦ç»†çš„è§†è§‰å†…å®¹ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | æ‚¨å¸Œæœ›åœ¨è¾“å‡ºå›¾åƒä¸­çœ‹åˆ°çš„å†…å®¹ã€‚å¼ºæœ‰åŠ›ã€æè¿°æ€§çš„æç¤ºè¯ï¼Œæ¸…æ™°å®šä¹‰å…ƒç´ ã€é¢œè‰²å’Œä¸»é¢˜å°†å¸¦æ¥æ›´å¥½çš„ç»“æžœ |\n| model | é€‰æ‹©é¡¹ | \\-  | é€‰æ‹©ä½¿ç”¨çš„Stability SD 3.5æ¨¡åž‹ |\n| aspect\\_ratio | é€‰æ‹©é¡¹ | â€1:1â€ | ç”Ÿæˆå›¾åƒçš„å®½é«˜æ¯” |\n| style\\_preset | é€‰æ‹©é¡¹ | â€Noneâ€ | å¯é€‰çš„æœŸæœ›å›¾åƒé£Žæ ¼é¢„è®¾ |\n| cfg\\_scale | æµ®ç‚¹æ•° | 4.0 | æ‰©æ•£è¿‡ç¨‹å¯¹æç¤ºæ–‡æœ¬çš„éµå¾ªç¨‹åº¦ï¼ˆæ›´é«˜çš„å€¼ä½¿å›¾åƒæ›´æŽ¥è¿‘æ‚¨çš„æç¤ºè¯ï¼‰ã€‚èŒƒå›´ï¼š1.0 - 10.0ï¼Œæ­¥é•¿ï¼š0.1 |\n| seed | æ•´æ•°  | 0   | ç”¨äºŽåˆ›å»ºå™ªå£°çš„éšæœºç§å­ï¼ŒèŒƒå›´0-4294967294 |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | è¾“å…¥å›¾åƒã€‚å½“æä¾›å›¾åƒæ—¶ï¼ŒèŠ‚ç‚¹å°†åˆ‡æ¢åˆ°å›¾åƒåˆ°å›¾åƒæ¨¡å¼ |\n| negative\\_prompt | å­—ç¬¦ä¸² | \"\"  | æ‚¨ä¸å¸Œæœ›åœ¨è¾“å‡ºå›¾åƒä¸­çœ‹åˆ°çš„å…³é”®è¯ã€‚è¿™æ˜¯ä¸€ä¸ªé«˜çº§åŠŸèƒ½ |\n| image\\_denoise | æµ®ç‚¹æ•° | 0.5 | è¾“å…¥å›¾åƒçš„åŽ»å™ªç¨‹åº¦ã€‚0.0äº§ç”Ÿä¸Žè¾“å…¥å®Œå…¨ç›¸åŒçš„å›¾åƒï¼Œ1.0åˆ™ç›¸å½“äºŽæ²¡æœ‰æä¾›ä»»ä½•å›¾åƒã€‚èŒƒå›´ï¼š0.0 - 1.0ï¼Œæ­¥é•¿ï¼š0.01ã€‚ä»…åœ¨æä¾›è¾“å…¥å›¾åƒæ—¶æœ‰æ•ˆ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | ç”Ÿæˆçš„å›¾åƒ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\nStability AI Stable Diffusion 3.5 Image å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)\n\n## æ³¨æ„äº‹é¡¹\n\n*   å½“æä¾›è¾“å…¥å›¾åƒæ—¶ï¼ŒèŠ‚ç‚¹å°†ä»Žæ–‡æœ¬åˆ°å›¾åƒæ¨¡å¼åˆ‡æ¢åˆ°å›¾åƒåˆ°å›¾åƒæ¨¡å¼\n*   åœ¨å›¾åƒåˆ°å›¾åƒæ¨¡å¼ä¸‹ï¼Œå®½é«˜æ¯”å‚æ•°å°†è¢«å¿½ç•¥\n*   æ¨¡å¼é€‰æ‹©ä¼šæ ¹æ®æ˜¯å¦æä¾›å›¾åƒè‡ªåŠ¨åˆ‡æ¢ï¼š\n    *   æœªæä¾›å›¾åƒï¼šæ–‡æœ¬åˆ°å›¾åƒæ¨¡å¼\n    *   æä¾›å›¾åƒï¼šå›¾åƒåˆ°å›¾åƒæ¨¡å¼\n*   å¦‚æžœstyle\\_presetè®¾ç½®ä¸ºâ€Noneâ€ï¼Œåˆ™ä¸ä¼šåº”ç”¨ä»»ä½•é¢„è®¾é£Žæ ¼\n\n## æºç \n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-07)\\]\n\n```\nclass StabilityStableImageSD_3_5Node:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Stability AI\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results.\"\n                    },\n                ),\n                \"model\": ([x.value for x in Stability_SD3_5_Model],),\n                \"aspect_ratio\": ([x.value for x in StabilityAspectRatio],\n                    {\n                        \"default\": StabilityAspectRatio.ratio_1_1,\n                        \"tooltip\": \"Aspect ratio of generated image.\",\n                    },\n                ),\n                \"style_preset\": (get_stability_style_presets(),\n                    {\n                        \"tooltip\": \"Optional desired style of generated image.\",\n                    },\n                ),\n                \"cfg_scale\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 4.0,\n                        \"min\": 1.0,\n                        \"max\": 10.0,\n                        \"step\": 0.1,\n                        \"tooltip\": \"How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 4294967294,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (IO.IMAGE,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"Keywords of what you do not wish to see in the output image. This is an advanced feature.\"\n                    },\n                ),\n                \"image_denoise\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(self, model: str, prompt: str, aspect_ratio: str, style_preset: str, seed: int, cfg_scale: float,\n                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,\n                 auth_token=None):\n        validate_string(prompt, strip_whitespace=False)\n        # prepare image binary if image present\n        image_binary = None\n        mode = Stability_SD3_5_GenerationMode.text_to_image\n        if image is not None:\n            image_binary = tensor_to_bytesio(image, total_pixels=1504*1504).read()\n            mode = Stability_SD3_5_GenerationMode.image_to_image\n            aspect_ratio = None\n        else:\n            image_denoise = None\n\n        if not negative_prompt:\n            negative_prompt = None\n        if style_preset == \"None\":\n            style_preset = None\n\n        files = {\n            \"image\": image_binary\n        }\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/stability/v2beta/stable-image/generate/sd3\",\n                method=HttpMethod.POST,\n                request_model=StabilityStable3_5Request,\n                response_model=StabilityStableUltraResponse,\n            ),\n            request=StabilityStable3_5Request(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                aspect_ratio=aspect_ratio,\n                seed=seed,\n                strength=image_denoise,\n                style_preset=style_preset,\n                cfg_scale=cfg_scale,\n                model=model,\n                mode=mode,\n            ),\n            files=files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.finish_reason != \"SUCCESS\":\n            raise Exception(f\"Stable Diffusion 3.5 Image generation failed: {response_api.finish_reason}.\")\n\n        image_data = base64.b64decode(response_api.image)\n        returned_image = bytesio_to_image_tensor(BytesIO(image_data))\n\n        return (returned_image,)\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftutorials%2Fbasic%2Fupscale",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/google/google-veo2-video",
  "markdown": "# Google Veo2 Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\n\nclass VeoVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos from text prompts using Google's Veo API.\n\n    This node can create videos from text descriptions and optional image inputs,\n    with control over parameters like aspect ratio, duration, and more.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text description of the video\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"16:9\", \"9:16\"],\n                        \"default\": \"16:9\",\n                        \"tooltip\": \"Aspect ratio of the output video\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Negative text prompt to guide what to avoid in the video\",\n                    },\n                ),\n                \"duration_seconds\": (\n                    IO.INT,\n                    {\n                        \"default\": 5,\n                        \"min\": 5,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"Duration of the output video in seconds\",\n                    },\n                ),\n                \"enhance_prompt\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": True,\n                        \"tooltip\": \"Whether to enhance the prompt with AI assistance\",\n                    }\n                ),\n                \"person_generation\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"ALLOW\", \"BLOCK\"],\n                        \"default\": \"ALLOW\",\n                        \"tooltip\": \"Whether to allow generating people in the video\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFF,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation (0 for random)\",\n                    },\n                ),\n                \"image\": (IO.IMAGE, {\n                    \"default\": None,\n                    \"tooltip\": \"Optional reference image to guide video generation\",\n                }),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (IO.VIDEO,)\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Veo\"\n    DESCRIPTION = \"Generates videos from text prompts using Google's Veo API\"\n    API_NODE = True\n\n    def generate_video(\n        self,\n        prompt,\n        aspect_ratio=\"16:9\",\n        negative_prompt=\"\",\n        duration_seconds=5,\n        enhance_prompt=True,\n        person_generation=\"ALLOW\",\n        seed=0,\n        image=None,\n        auth_token=None,\n    ):\n        # Prepare the instances for the request\n        instances = []\n\n        instance = {\n            \"prompt\": prompt\n        }\n\n        # Add image if provided\n        if image is not None:\n            image_base64 = convert_image_to_base64(image)\n            if image_base64:\n                instance[\"image\"] = {\n                    \"bytesBase64Encoded\": image_base64,\n                    \"mimeType\": \"image/png\"\n                }\n\n        instances.append(instance)\n\n        # Create parameters dictionary\n        parameters = {\n            \"aspectRatio\": aspect_ratio,\n            \"personGeneration\": person_generation,\n            \"durationSeconds\": duration_seconds,\n            \"enhancePrompt\": enhance_prompt,\n        }\n\n        # Add optional parameters if provided\n        if negative_prompt:\n            parameters[\"negativePrompt\"] = negative_prompt\n        if seed > 0:\n            parameters[\"seed\"] = seed\n\n        # Initial request to start video generation\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/veo/generate\",\n                method=HttpMethod.POST,\n                request_model=Veo2GenVidRequest,\n                response_model=Veo2GenVidResponse\n            ),\n            request=Veo2GenVidRequest(\n                instances=instances,\n                parameters=parameters\n            ),\n            auth_token=auth_token\n        )\n\n        initial_response = initial_operation.execute()\n        operation_name = initial_response.name\n\n        logging.info(f\"Veo generation started with operation name: {operation_name}\")\n\n        # Define status extractor function\n        def status_extractor(response):\n            # Only return \"completed\" if the operation is done, regardless of success or failure\n            # We'll check for errors after polling completes\n            return \"completed\" if response.done else \"pending\"\n\n        # Define progress extractor function\n        def progress_extractor(response):\n            # Could be enhanced if the API provides progress information\n            return None\n\n        # Define the polling operation\n        poll_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=\"/proxy/veo/poll\",\n                method=HttpMethod.POST,\n                request_model=Veo2GenVidPollRequest,\n                response_model=Veo2GenVidPollResponse\n            ),\n            completed_statuses=[\"completed\"],\n            failed_statuses=[],  # No failed statuses, we'll handle errors after polling\n            status_extractor=status_extractor,\n            progress_extractor=progress_extractor,\n            request=Veo2GenVidPollRequest(\n                operationName=operation_name\n            ),\n            auth_token=auth_token,\n            poll_interval=5.0\n        )\n\n        # Execute the polling operation\n        poll_response = poll_operation.execute()\n\n        # Now check for errors in the final response\n        # Check for error in poll response\n        if hasattr(poll_response, 'error') and poll_response.error:\n            error_message = f\"Veo API error: {poll_response.error.message} (code: {poll_response.error.code})\"\n            logging.error(error_message)\n            raise Exception(error_message)\n\n        # Check for RAI filtered content\n        if (hasattr(poll_response.response, 'raiMediaFilteredCount') and\n            poll_response.response.raiMediaFilteredCount > 0):\n\n            # Extract reason message if available\n            if (hasattr(poll_response.response, 'raiMediaFilteredReasons') and\n                poll_response.response.raiMediaFilteredReasons):\n                reason = poll_response.response.raiMediaFilteredReasons[0]\n                error_message = f\"Content filtered by Google's Responsible AI practices: {reason} ({poll_response.response.raiMediaFilteredCount} videos filtered.)\"\n            else:\n                error_message = f\"Content filtered by Google's Responsible AI practices ({poll_response.response.raiMediaFilteredCount} videos filtered.)\"\n\n            logging.error(error_message)\n            raise Exception(error_message)\n\n        # Extract video data\n        video_data = None\n        if poll_response.response and hasattr(poll_response.response, 'videos') and poll_response.response.videos and len(poll_response.response.videos) > 0:\n            video = poll_response.response.videos[0]\n\n            # Check if video is provided as base64 or URL\n            if hasattr(video, 'bytesBase64Encoded') and video.bytesBase64Encoded:\n                # Decode base64 string to bytes\n                video_data = base64.b64decode(video.bytesBase64Encoded)\n            elif hasattr(video, 'gcsUri') and video.gcsUri:\n                # Download from URL\n                video_url = video.gcsUri\n                video_response = requests.get(video_url)\n                video_data = video_response.content\n            else:\n                raise Exception(\"Video returned but no data or URL was provided\")\n        else:\n            raise Exception(\"Video generation completed but no video was returned\")\n\n        if not video_data:\n            raise Exception(\"No video data was returned\")\n\n        logging.info(\"Video generation completed successfully\")\n\n        # Convert video data to BytesIO object\n        video_io = io.BytesIO(video_data)\n\n        # Return VideoFromFile object\n        return (VideoFromFile(video_io),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v",
  "markdown": "# Kling Text to Video (Camera Control) - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Kling Text to Video (Camera Control) èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg) Kling Text to Video (Camera Control) èŠ‚ç‚¹å…è®¸ç”¨æˆ·å°†æ–‡æœ¬è½¬æ¢ä¸ºå…·æœ‰ä¸“ä¸šæ‘„åƒæœºåŠ¨ä½œçš„è§†é¢‘ã€‚è¯¥èŠ‚ç‚¹æ˜¯æ ‡å‡† Kling Text to Video èŠ‚ç‚¹çš„æ‰©å±•ç‰ˆæœ¬ï¼Œå¢žåŠ äº†æ‘„åƒæœºæŽ§åˆ¶åŠŸèƒ½ã€‚\n\n```\n\nclass KlingCameraControlT2VNode(KlingTextToVideoNode):\n    \"\"\"\n    Kling Text to Video Camera Control Node. This node is a text to video node, but it supports controlling the camera.\n    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingText2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingText2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"camera_control\": (\n                    \"CAMERA_CONTROL\",\n                    {\n                        \"tooltip\": \"Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Transform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text.\"\n\n    def api_call(\n        self,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        camera_control: Optional[CameraControl] = None,\n        auth_token: Optional[str] = None,\n    ):\n        return super().api_call(\n            model_name=\"kling-v1-5\",\n            cfg_scale=cfg_scale,\n            mode=\"pro\",\n            aspect_ratio=aspect_ratio,\n            duration=\"5\",\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            camera_control=camera_control,\n            auth_token=auth_token,\n        )\n\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video",
  "markdown": "# Kling Image to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\n\nclass KlingImage2VideoNode(KlingNodeBase):\n    \"\"\"Kling Image to Video Node\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"model_name\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"model_name\",\n                    enum_type=KlingVideoGenModelName,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"mode\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"mode\",\n                    enum_type=KlingVideoGenMode,\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=KlingVideoGenAspectRatio,\n                ),\n                \"duration\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"duration\",\n                    enum_type=KlingVideoGenDuration,\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (\"VIDEO\", \"STRING\", \"STRING\")\n    RETURN_NAMES = (\"VIDEO\", \"video_id\", \"duration\")\n    DESCRIPTION = \"Kling Image to Video Node\"\n\n    def get_response(self, task_id: str, auth_token: str) -> KlingImage2VideoResponse:\n        return poll_until_finished(\n            auth_token,\n            ApiEndpoint(\n                path=f\"{PATH_IMAGE_TO_VIDEO}/{task_id}\",\n                method=HttpMethod.GET,\n                request_model=KlingImage2VideoRequest,\n                response_model=KlingImage2VideoResponse,\n            ),\n        )\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        model_name: str,\n        cfg_scale: float,\n        mode: str,\n        aspect_ratio: str,\n        duration: str,\n        camera_control: Optional[KlingCameraControl] = None,\n        end_frame: Optional[torch.Tensor] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_I2V)\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_IMAGE_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=KlingImage2VideoRequest,\n                response_model=KlingImage2VideoResponse,\n            ),\n            request=KlingImage2VideoRequest(\n                model_name=KlingVideoGenModelName(model_name),\n                image=tensor_to_base64_string(start_frame),\n                image_tail=(\n                    tensor_to_base64_string(end_frame)\n                    if end_frame is not None\n                    else None\n                ),\n                prompt=prompt,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                cfg_scale=cfg_scale,\n                mode=KlingVideoGenMode(mode),\n                aspect_ratio=KlingVideoGenAspectRatio(aspect_ratio),\n                duration=KlingVideoGenDuration(duration),\n                camera_control=camera_control,\n            ),\n            auth_token=auth_token,\n        )\n\n        task_creation_response = initial_operation.execute()\n        validate_task_creation_response(task_creation_response)\n        task_id = task_creation_response.data.task_id\n\n        final_response = self.get_response(task_id, auth_token)\n        validate_video_result_response(final_response)\n\n        video = get_video_from_response(final_response)\n        return video_result_to_node_output(video)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video",
  "markdown": "# Kling Text to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Kling Text to Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg) Kling Text to Video èŠ‚ç‚¹é€šè¿‡è¿žæŽ¥Klingçš„APIæœåŠ¡ï¼Œå®žçŽ°æ–‡æœ¬åˆ°è§†é¢‘çš„ç”ŸæˆåŠŸèƒ½ã€‚ç”¨æˆ·åªéœ€æä¾›æè¿°æ€§æ–‡æœ¬ï¼Œå³å¯åˆ›å»ºå¯¹åº”çš„è§†é¢‘å†…å®¹ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | æè¿°è¦ç”Ÿæˆè§†é¢‘å†…å®¹çš„æ–‡æœ¬æç¤ºè¯ |\n| negative\\_prompt | å­—ç¬¦ä¸² | \"\"  | æŒ‡å®šä¸å¸Œæœ›åœ¨è§†é¢‘ä¸­å‡ºçŽ°çš„å…ƒç´  |\n| cfg\\_scale | æµ®ç‚¹æ•° | 7.0 | é…ç½®ç¼©æ”¾å€¼ï¼ŒæŽ§åˆ¶å¯¹æç¤ºè¯çš„éµå¾ªç¨‹åº¦ |\n| model\\_name | é€‰æ‹©é¡¹ | â€kling-v2-masterâ€ | ä½¿ç”¨çš„è§†é¢‘ç”Ÿæˆæ¨¡åž‹ |\n| aspect\\_ratio | é€‰æ‹©é¡¹ | AspectRatioæžšä¸¾å€¼ | è¾“å‡ºè§†é¢‘çš„å®½é«˜æ¯” |\n| duration | é€‰æ‹©é¡¹ | Durationæžšä¸¾å€¼ | ç”Ÿæˆè§†é¢‘çš„æŒç»­æ—¶é—´ |\n| mode | é€‰æ‹©é¡¹ | Modeæžšä¸¾å€¼ | è§†é¢‘ç”Ÿæˆæ¨¡å¼ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n| Kling ID | å­—ç¬¦ä¸² | ä»»åŠ¡è¯†åˆ«ID |\n| Duration (sec) | å­—ç¬¦ä¸² | è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ |\n\n## å·¥ä½œåŽŸç†\n\nèŠ‚ç‚¹å°†æ–‡æœ¬æç¤ºè¯å‘é€åˆ°Klingçš„APIæœåŠ¡å™¨ï¼Œç³»ç»Ÿå¤„ç†åŽè¿”å›žç”Ÿæˆçš„è§†é¢‘ç»“æžœã€‚å¤„ç†è¿‡ç¨‹åŒ…æ‹¬åˆå§‹è¯·æ±‚å’Œä»»åŠ¡çŠ¶æ€è½®è¯¢ï¼Œå½“ä»»åŠ¡å®ŒæˆåŽï¼ŒèŠ‚ç‚¹ä¼šä¸‹è½½è§†é¢‘å¹¶è¾“å‡ºç»“æžœã€‚ ç”¨æˆ·å¯ä»¥é€šè¿‡è°ƒæ•´å„ç§å‚æ•°æ¥æŽ§åˆ¶ç”Ÿæˆæ•ˆæžœï¼ŒåŒ…æ‹¬è´Ÿé¢æç¤ºè¯ã€é…ç½®ç¼©æ”¾å€¼ä»¥åŠè§†é¢‘å±žæ€§ç­‰ã€‚ç³»ç»Ÿä¼šéªŒè¯æç¤ºè¯é•¿åº¦ï¼Œç¡®ä¿è¯·æ±‚ç¬¦åˆAPIè¦æ±‚ã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass KlingTextToVideoNode(KlingNodeBase):\n    \"\"\"Kling Text to Video Node\"\"\"\n\n    @staticmethod\n    def poll_for_task_status(task_id: str, auth_token: str) -> KlingText2VideoResponse:\n        \"\"\"Polls the Kling API endpoint until the task reaches a terminal state.\"\"\"\n        polling_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"{PATH_TEXT_TO_VIDEO}/{task_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=KlingText2VideoResponse,\n            ),\n            completed_statuses=[\n                TaskStatus.succeed.value,\n            ],\n            failed_statuses=[TaskStatus.failed.value],\n            status_extractor=lambda response: (\n                response.data.task_status.value\n                if response.data and response.data.task_status\n                else None\n            ),\n            auth_token=auth_token,\n        )\n        return polling_operation.execute()\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING, KlingText2VideoRequest, \"negative_prompt\", multiline=True\n                ),\n                \"model_name\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"model_name\",\n                    enum_type=ModelName,\n                    default=\"kling-v2-master\",\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingText2VideoRequest, \"cfg_scale\"\n                ),\n                \"mode\": model_field_to_node_input(\n                    IO.COMBO, KlingText2VideoRequest, \"mode\", enum_type=Mode\n                ),\n                \"duration\": model_field_to_node_input(\n                    IO.COMBO, KlingText2VideoRequest, \"duration\", enum_type=Duration\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingText2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (\"VIDEO\", \"STRING\", \"STRING\")\n    RETURN_NAMES = (\"VIDEO\", \"Kling ID\", \"Duration (sec)\")\n    DESCRIPTION = \"Kling Text to Video Node\"\n\n    def api_call(\n        self,\n        prompt: str,\n        negative_prompt: str,\n        model_name: str,\n        cfg_scale: float,\n        mode: str,\n        duration: int,\n        aspect_ratio: str,\n        camera_control: Optional[CameraControl] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile, str, str]:\n        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_T2V)\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_TEXT_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=KlingText2VideoRequest,\n                response_model=KlingText2VideoResponse,\n            ),\n            request=KlingText2VideoRequest(\n                prompt=prompt if prompt else None,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                duration=Duration(duration),\n                mode=Mode(mode),\n                model_name=ModelName(model_name),\n                cfg_scale=cfg_scale,\n                aspect_ratio=AspectRatio(aspect_ratio),\n                camera_control=camera_control,\n            ),\n            auth_token=auth_token,\n        )\n\n        initial_response = initial_operation.execute()\n        if not is_valid_initial_response(initial_response):\n            error_msg = f\"Kling initial request failed. Code: {initial_response.code}, Message: {initial_response.message}, Data: {initial_response.data}\"\n            logging.error(error_msg)\n            raise KlingApiError(error_msg)\n\n        task_id = initial_response.data.task_id\n        final_response = self.poll_for_task_status(task_id, auth_token)\n        if not is_valid_video_response(final_response):\n            error_msg = (\n                f\"Kling task {task_id} succeeded but no video data found in response.\"\n            )\n            logging.error(error_msg)\n            raise KlingApiError(error_msg)\n\n        video = final_response.data.task_result.videos[0]\n        logging.debug(\"Kling task %s succeeded. Video URL: %s\", task_id, video.url)\n        return (\n            download_url_to_video_output(video.url),\n            str(video.id),\n            str(video.duration),\n        )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-concepts",
  "markdown": "# Luma Concepts - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Luma Concepts èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg) Luma Concepts èŠ‚ç‚¹å…è®¸ä½ å°†é¢„å®šä¹‰çš„é•œå¤´æ¦‚å¿µåº”ç”¨åˆ°Lumaç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæä¾›æ›´ç²¾ç¡®çš„é•œå¤´å’Œè§†è§’æŽ§åˆ¶ï¼Œæ— éœ€å¤æ‚çš„æç¤ºè¯æè¿°ã€‚\n\n## èŠ‚ç‚¹åŠŸèƒ½\n\næ­¤èŠ‚ç‚¹ä½œä¸ºLumaç”ŸæˆèŠ‚ç‚¹çš„è¾…åŠ©å·¥å…·ï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿé€‰æ‹©å’Œåº”ç”¨é¢„å®šä¹‰çš„é•œå¤´æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µåŒ…æ‹¬ä¸åŒçš„æ‹æ‘„è§’åº¦(å¦‚ä¿¯è§†ã€ä»°è§†)ã€é•œå¤´è·ç¦»(å¦‚ç‰¹å†™ã€è¿œæ™¯)ã€è¿åŠ¨æ–¹å¼(å¦‚æŽ¨è¿›ã€è·Ÿéš)ç­‰æ‘„å½±å‚æ•°ã€‚å®ƒç®€åŒ–äº†åˆ›ä½œå·¥ä½œæµç¨‹ï¼Œæä¾›äº†ä¸€ç§ç›´è§‚çš„æ–¹å¼æ¥æŽ§åˆ¶ç”Ÿæˆç»“æžœçš„é•œå¤´æ•ˆæžœã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| concept1 | é€‰æ‹©é¡¹ | ç¬¬ä¸€ä¸ªé•œå¤´æ¦‚å¿µé€‰æ‹©ï¼ŒåŒ…å«å¤šç§é¢„è®¾é•œå¤´é€‰é¡¹å’Œâ€noneâ€ |\n| concept2 | é€‰æ‹©é¡¹ | ç¬¬äºŒä¸ªé•œå¤´æ¦‚å¿µé€‰æ‹©ï¼ŒåŒ…å«å¤šç§é¢„è®¾é•œå¤´é€‰é¡¹å’Œâ€noneâ€ |\n| concept3 | é€‰æ‹©é¡¹ | ç¬¬ä¸‰ä¸ªé•œå¤´æ¦‚å¿µé€‰æ‹©ï¼ŒåŒ…å«å¤šç§é¢„è®¾é•œå¤´é€‰é¡¹å’Œâ€noneâ€ |\n| concept4 | é€‰æ‹©é¡¹ | ç¬¬å››ä¸ªé•œå¤´æ¦‚å¿µé€‰æ‹©ï¼ŒåŒ…å«å¤šç§é¢„è®¾é•œå¤´é€‰é¡¹å’Œâ€noneâ€ |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPTS | å¯é€‰çš„é¢å¤–Camera Conceptsï¼Œä¼šä¸Žæ­¤å¤„é€‰æ‹©çš„é•œå¤´æ¦‚å¿µåˆå¹¶ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPT | åŒ…å«æ‰€æœ‰é€‰å®šé•œå¤´æ¦‚å¿µçš„ç»„åˆå¯¹è±¡ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Luma Text to video å·¥ä½œæµç¤ºä¾‹\n\nLuma Text to Video å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-video)[\n\n## Luma Text to Image å·¥ä½œæµç¤ºä¾‹\n\nLuma Image to Video å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-image-to-video)\n\n## å·¥ä½œåŽŸç†\n\nLuma Concepts èŠ‚ç‚¹æä¾›äº†ä¸°å¯Œçš„é¢„å®šä¹‰é•œå¤´æ¦‚å¿µä¾›é€‰æ‹©ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºŽ:\n\n*   ä¸åŒçš„æ‹æ‘„è·ç¦»(å¦‚ç‰¹å†™ã€ä¸­æ™¯ã€è¿œæ™¯)\n*   è§†è§’é«˜åº¦(å¦‚åœ°å¹³ã€ä¿¯è§†ã€ä»°è§†)\n*   è¿åŠ¨æ–¹å¼(å¦‚æŽ¨è¿›ã€è·Ÿéšã€çŽ¯ç»•)\n*   ç‰¹æ®Šæ•ˆæžœ(å¦‚æ‰‹æŒã€ç¨³å®šã€æ¼‚æµ®)\n\nç”¨æˆ·å¯ä»¥ä»Žè¿™äº›é€‰é¡¹ä¸­é€‰æ‹©æœ€å¤š4ä¸ªæ¦‚å¿µç»„åˆä½¿ç”¨ã€‚èŠ‚ç‚¹ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€é€‰é•œå¤´æ¦‚å¿µçš„å¯¹è±¡ï¼Œè¯¥å¯¹è±¡éšåŽä¼ é€’ç»™Lumaç”ŸæˆèŠ‚ç‚¹ã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼ŒLuma AIä¼šæ ¹æ®è¿™äº›é•œå¤´æ¦‚å¿µæ¥å½±å“ç”Ÿæˆç»“æžœçš„è§†è§’å’Œæž„å›¾ï¼Œç¡®ä¿è¾“å‡ºå›¾åƒä½“çŽ°æ‰€é€‰çš„æ‘„å½±æ•ˆæžœã€‚ é€šè¿‡ç»„åˆå¤šä¸ªé•œå¤´æ¦‚å¿µï¼Œç”¨æˆ·å¯ä»¥åˆ›å»ºå¤æ‚çš„é•œå¤´æŒ‡å¯¼ï¼Œè€Œæ— éœ€æ’°å†™è¯¦ç»†çš„æç¤ºè¯æè¿°ã€‚è¿™å¯¹äºŽéœ€è¦ç‰¹å®šæ‘„å½±è§†è§’æˆ–æž„å›¾çš„åœºæ™¯ç‰¹åˆ«æœ‰ç”¨ã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass LumaConceptsNode(ComfyNodeABC):\n    \"\"\"\n    Holds one or more Camera Concepts for use with Luma Text to Video and Luma Image to Video nodes.\n    \"\"\"\n\n    RETURN_TYPES = (LumaIO.LUMA_CONCEPTS,)\n    RETURN_NAMES = (\"luma_concepts\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_concepts\"\n    CATEGORY = \"api node/image/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"concept1\": (get_luma_concepts(include_none=True),),\n                \"concept2\": (get_luma_concepts(include_none=True),),\n                \"concept3\": (get_luma_concepts(include_none=True),),\n                \"concept4\": (get_luma_concepts(include_none=True),),\n            },\n            \"optional\": {\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to add to the ones chosen here.\"\n                    },\n                ),\n            },\n        }\n\n    def create_concepts(\n        self,\n        concept1: str,\n        concept2: str,\n        concept3: str,\n        concept4: str,\n        luma_concepts: LumaConceptChain = None,\n    ):\n        chain = LumaConceptChain(str_list=[concept1, concept2, concept3, concept4])\n        if luma_concepts is not None:\n            chain = luma_concepts.clone_and_merge(chain)\n        return (chain,)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-image-to-video",
  "markdown": "# Luma Image to Video - ComfyUI åŽŸç”ŸAPIèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Luma Image to Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg) Luma Image to Video èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨Luma AIçš„å…ˆè¿›æŠ€æœ¯ï¼Œå°†é™æ€å›¾åƒè½¬æ¢ä¸ºæµç•…ã€åŠ¨æ€çš„è§†é¢‘å†…å®¹ï¼Œä¸ºå›¾åƒèµ‹äºˆç”Ÿå‘½åŠ›å’ŒåŠ¨æ€ç‰¹æ€§ã€‚\n\n## èŠ‚ç‚¹åŠŸèƒ½\n\næ­¤èŠ‚ç‚¹è¿žæŽ¥åˆ°Luma AIçš„å›¾åƒåˆ°è§†é¢‘APIï¼Œè®©ç”¨æˆ·èƒ½å¤ŸåŸºäºŽè¾“å…¥å›¾åƒåˆ›å»ºåŠ¨æ€è§†é¢‘ã€‚å®ƒå¯ä»¥ç†è§£å›¾åƒä¸­çš„å†…å®¹å¹¶ç”Ÿæˆè‡ªç„¶ã€è¿žè´¯çš„åŠ¨ä½œï¼ŒåŒæ—¶ä¿æŒåŽŸå§‹å›¾åƒçš„è§†è§‰é£Žæ ¼å’Œç‰¹æ€§ã€‚ç»“åˆæ–‡æœ¬æç¤ºè¯ï¼Œç”¨æˆ·å¯ä»¥ç²¾ç¡®æŽ§åˆ¶ç”Ÿæˆè§†é¢‘çš„åŠ¨æ€æ•ˆæžœã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | æè¿°è§†é¢‘åŠ¨ä½œå’Œå†…å®¹çš„æ–‡æœ¬æç¤ºè¯ |\n| model | é€‰æ‹©é¡¹ | \\-  | ä½¿ç”¨çš„è§†é¢‘ç”Ÿæˆæ¨¡åž‹ |\n| resolution | é€‰æ‹©é¡¹ | â€540pâ€ | è¾“å‡ºè§†é¢‘åˆ†è¾¨çŽ‡ |\n| duration | é€‰æ‹©é¡¹ | \\-  | è§†é¢‘æ—¶é•¿é€‰é¡¹ |\n| loop | å¸ƒå°”å€¼ | False | æ˜¯å¦å¾ªçŽ¯æ’­æ”¾è§†é¢‘ |\n| seed | æ•´æ•°  | 0   | ç§å­å€¼ï¼Œç”¨äºŽç¡®å®šèŠ‚ç‚¹æ˜¯å¦åº”é‡æ–°è¿è¡Œï¼Œä½†å®žé™…ç»“æžœä¸Žç§å­æ— å…³ |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| first\\_image | å›¾åƒ  | è§†é¢‘çš„ç¬¬ä¸€å¸§å›¾åƒï¼ˆä¸Žlast\\_imageè‡³å°‘éœ€è¦æä¾›ä¸€ä¸ªï¼‰ |\n| last\\_image | å›¾åƒ  | è§†é¢‘çš„æœ€åŽä¸€å¸§å›¾åƒï¼ˆä¸Žfirst\\_imageè‡³å°‘éœ€è¦æä¾›ä¸€ä¸ªï¼‰ |\n| luma\\_concepts | LUMA\\_CONCEPTS | ç”¨äºŽæŽ§åˆ¶ç›¸æœºè¿åŠ¨å’Œé•œå¤´æ•ˆæžœçš„æ¦‚å¿µå¼•å¯¼ |\n\n### å‚æ•°è¦æ±‚\n\n*   **first\\_image** å’Œ **last\\_image** è‡³å°‘éœ€è¦æä¾›å…¶ä¸­ä¸€ä¸ª\n*   æ¯ä¸ªå›¾åƒè¾“å…¥ï¼ˆfirst\\_imageå’Œlast\\_imageï¼‰æœ€å¤šåªæŽ¥å—1å¼ å›¾ç‰‡\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\nLuma Image to Video å·¥ä½œæµæ•™ç¨‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-image-to-video)\n\n## å·¥ä½œåŽŸç†\n\nLuma Image to Video èŠ‚ç‚¹åˆ†æžè¾“å…¥å›¾åƒçš„å†…å®¹å’Œç»“æž„ï¼Œç„¶åŽç»“åˆæ–‡æœ¬æç¤ºè¯æ¥ç¡®å®šå¦‚ä½•ä¸ºå›¾åƒæ·»åŠ åŠ¨æ€æ•ˆæžœã€‚å®ƒä½¿ç”¨Luma AIçš„ç”Ÿæˆæ¨¡åž‹ç†è§£å›¾åƒä¸­çš„å¯¹è±¡ã€äººç‰©æˆ–åœºæ™¯ï¼Œå¹¶åˆ›å»ºåˆç†ã€è¿žè´¯çš„åŠ¨ä½œåºåˆ—ã€‚ ç”¨æˆ·å¯ä»¥é€šè¿‡æç¤ºè¯æè¿°æœŸæœ›çš„åŠ¨ä½œç±»åž‹ã€æ–¹å‘å’Œå¼ºåº¦ï¼ŒèŠ‚ç‚¹å°†æ®æ­¤ç”Ÿæˆç›¸åº”çš„è§†é¢‘æ•ˆæžœã€‚é€šè¿‡è®¾ç½®ä¸åŒçš„å‚æ•°ï¼Œå¦‚åˆ†è¾¨çŽ‡å’Œæ—¶é•¿ï¼Œç”¨æˆ·å¯ä»¥è¿›ä¸€æ­¥å®šåˆ¶è¾“å‡ºè§†é¢‘çš„ç‰¹æ€§ã€‚ æ­¤å¤–ï¼Œé€šè¿‡æä¾›èµ·å§‹å¸§å’Œæœ€åŽå¸§å‚è€ƒå›¾åƒï¼Œç”¨æˆ·å¯ä»¥æŒ‡å®šè§†é¢‘çš„èµ·å§‹å’Œç»“æŸçŠ¶æ€ï¼Œä½¿åŠ¨ä½œæœç‰¹å®šæ–¹å‘å‘å±•ã€‚æ¦‚å¿µå¼•å¯¼åŠŸèƒ½åˆ™å…è®¸ç”¨æˆ·è¿›ä¸€æ­¥æŽ§åˆ¶è§†é¢‘çš„æ•´ä½“é£Žæ ¼ã€ç›¸æœºè¿åŠ¨å’Œç¾Žå­¦æ•ˆæžœã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass LumaImageToVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt, input images, and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaVideoModel],),\n                # \"aspect_ratio\": ([ratio.value for ratio in LumaAspectRatio], {\n                #     \"default\": LumaAspectRatio.ratio_16_9,\n                # }),\n                \"resolution\": (\n                    [resolution.value for resolution in LumaVideoOutputResolution],\n                    {\n                        \"default\": LumaVideoOutputResolution.res_540p,\n                    },\n                ),\n                \"duration\": ([dur.value for dur in LumaVideoModelOutputDuration],),\n                \"loop\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"first_image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"First frame of generated video.\"},\n                ),\n                \"last_image\": (IO.IMAGE, {\"tooltip\": \"Last frame of generated video.\"}),\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        resolution: str,\n        duration: str,\n        loop: bool,\n        seed,\n        first_image: torch.Tensor = None,\n        last_image: torch.Tensor = None,\n        luma_concepts: LumaConceptChain = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        if first_image is None and last_image is None:\n            raise Exception(\n                \"At least one of first_image and last_image requires an input.\"\n            )\n        keyframes = self._convert_to_keyframes(first_image, last_image, auth_token)\n        duration = duration if model != LumaVideoModel.ray_1_6 else None\n        resolution = resolution if model != LumaVideoModel.ray_1_6 else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations\",\n                method=HttpMethod.POST,\n                request_model=LumaGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaGenerationRequest(\n                prompt=prompt,\n                model=model,\n                aspect_ratio=LumaAspectRatio.ratio_16_9,  # ignored, but still needed by the API for some reason\n                resolution=resolution,\n                duration=duration,\n                loop=loop,\n                keyframes=keyframes,\n                concepts=luma_concepts.create_api_model() if luma_concepts else None,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.assets.video)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n    def _convert_to_keyframes(\n        self,\n        first_image: torch.Tensor = None,\n        last_image: torch.Tensor = None,\n        auth_token=None,\n    ):\n        if first_image is None and last_image is None:\n            return None\n        frame0 = None\n        frame1 = None\n        if first_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                first_image, max_images=1, auth_token=auth_token\n            )\n            frame0 = LumaImageReference(type=\"image\", url=download_urls[0])\n        if last_image is not None:\n            download_urls = upload_images_to_comfyapi(\n                last_image, max_images=1, auth_token=auth_token\n            )\n            frame1 = LumaImageReference(type=\"image\", url=download_urls[0])\n        return LumaKeyframes(frame0=frame0, frame1=frame1)\n\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra",
  "markdown": "# Stability AI Stable Image Ultra API Node ComfyUI Official Example\n\nThe [Stability Stable Image Ultra](https://docs.comfy.org/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg) node allows you to use Stability AIâ€™s Stable Image Ultra model to create high-quality, detailed image content through text prompts or reference images. In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.\n\n## Stability AI Stable Image Ultra Text-to-Image Workflow\n\n### 1\\. Workflow File Download\n\nThe workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow. ![Stability AI Stable Image Ultra Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_image_ultra_t2i.png)\n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![Stability AI Stable Image Ultra Text-to-Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg) You can follow the numbered steps in the image to complete the basic text-to-image workflow:\n\n1.  (Optional) Modify the `prompt` parameter in the `Stability AI Stable Image Ultra` node to input your desired image description. More detailed prompts often lead to better image quality. You can use the `(word:weight)` format to control specific word weights, for example: `The sky was crisp (blue:0.3) and (green:0.8)` indicates the sky is blue and green, but green is more prominent.\n2.  (Optional) Select the `style_preset` parameter to control the visual style of the image. Different preset styles will produce images with different stylistic characteristics, such as â€œcinematicâ€, â€œanimeâ€, etc. Selecting â€œNoneâ€ will not apply any specific style.\n3.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.\n4.  After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.\n\n### 3\\. Additional Notes\n\n*   **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions will lead to better results. It can include elements like scene, subject, colors, lighting, and style.\n*   **Style Preset**: Provides multiple preset styles such as cinematic, anime, digital art, etc., which can quickly define the overall style of the image.\n*   **Negative Prompt**: Used to specify elements you donâ€™t want to appear in the generated image, helping avoid common issues like extra limbs or distorted faces.\n*   **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during the creative process.\n*   Currently, the `Load Image` node is in â€œBypassâ€ mode. To enable it, refer to the step guide and right-click on the corresponding node to set â€œModeâ€ to â€œAlwaysâ€ to enable input, which will switch to image-to-image mode.\n\n### 1\\. Workflow File Download\n\nThe workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow. ![Stability Stable Image Ultra Image-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/stable_image_ultra_i2i.png) Download the image below which we will use as input ![Stability Stable Image Ultra Image-to-Image Workflow Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/input.png) \n\n### 2\\. Complete the Workflow Execution Step by Step\n\n![Stability Stable Image Ultra Image-to-Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg) You can follow the numbered steps in the image to complete the image-to-image workflow:\n\n1.  Load a reference image through the `Load Image` node, which will serve as the basis for generation.\n2.  (Optional) Modify the `prompt` parameter in the `Stability Stable Image Ultra` node to describe elements you want to change or enhance in the reference image.\n3.  (Optional) Adjust the `image_denoise` parameter (range 0.0-1.0) to control the degree of modification to the original image:\n    *   Values closer to 0.0 will make the generated image more similar to the input reference image\n    *   Values closer to 1.0 will make the generated image more like pure text-to-image generation\n4.  (Optional) You can also set `style_preset` and other parameters to further control the generation effect.\n5.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.\n6.  After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.\n\n### 3\\. Additional Notes\n\n**Image Denoise**: This parameter determines how much of the original imageâ€™s features are preserved during generation, and is the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths ![Stability Stable Image Ultra Image-to-Image Denoise Explanation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg)\n\n*   **Reference Image Selection**: Choosing images with clear subjects and good composition usually leads to better results.\n*   **Prompt Tips**: In image-to-image mode, prompts should focus more on what you want to change or enhance, rather than describing all elements already present in the image.\n\nYou can refer to the documentation below for detailed parameter settings and more information about the corresponding nodes\n\n[\n\n## Stability Stable Image Ultra Node Documentation\n\nStability Stable Image Ultra API Node Documentation\n\n\n\n](https://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)"
},
{
  "url": "https://docs.comfy.org/tutorials/video/cosmos/cosmos-predict2-video2world",
  "markdown": "# Cosmos Predict2 Video2World ComfyUI Official Example\n\nCosmos-Predict2 is NVIDIAâ€™s next-generation physical world foundation model, specifically designed for high-quality visual generation and prediction tasks in physical AI scenarios. The model features exceptional physical accuracy, environmental interactivity, and detail reproduction capabilities, enabling realistic simulation of complex physical phenomena and dynamic scenes. Cosmos-Predict2 supports various generation methods including Text-to-Image (Text2Image) and Video-to-World (Video2World), and is widely used in industrial simulation, autonomous driving, urban planning, scientific research, and other fields. It serves as a crucial foundational tool for promoting deep integration of intelligent vision and the physical world. GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2) huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959) This guide will walk you through completing **Video2World** generation in ComfyUI. For the text-to-image section, please refer to the following part:\n\n[\n\n## Cosmos Predict2 Text to Image\n\nUsing Cosmos-Predict2 for text-to-image generation\n\n\n\n](https://docs.comfy.org/tutorials/image/cosmos/cosmos-predict2-t2i)\n\nWhen testing the 2B version, it takes around 16GB VRAM.\n\n### 1\\. Workflow File\n\nPlease download the video below and drag it into ComfyUI to load the workflow. The workflow already has embedded model download links.\n\n[\n\nDownload Json Format Workflow File\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/cosmos/predict2/cosmos_predict2_2B_video2world_480p_16fps.json)\n\nPlease download the following image as input: ![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/cosmos/predict2/input.png)\n\n### 2\\. Manual Model Installation\n\nIf the model download wasnâ€™t successful, you can try to download them manually by yourself in this section. **Diffusion model**\n\n*   [cosmos\\_predict2\\_2B\\_video2world\\_480p\\_16fps.safetensors](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged/resolve/main/cosmos_predict2_2B_video2world_480p_16fps.safetensors)\n\nFor other weights, please visit [Cosmos\\_Predict2\\_repackaged](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged) to download **Text encoder** [oldt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/resolve/main/text_encoders/oldt5_xxl_fp8_e4m3fn_scaled.safetensors) **VAE** [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors) File Storage Location\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€ cosmos_predict2_2B_video2world_480p_16fps.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ oldt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚       â””â”€â”€  wan_2.1_vae.safetensors\n```\n\n### 3\\. Complete Workflow Step by Step\n\n![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/cosmos/cosmos_predict2_2B_video2world_480p_16fps_step_guide.jpg) Please follow the steps in the image to run the workflow:\n\n1.  Ensure the `Load Diffusion Model` node has loaded `cosmos_predict2_2B_video2world_480p_16fps.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `oldt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Upload the provided input image in the `Load Image` node\n5.  (Optional) If you need first and last frame control, use the shortcut `Ctrl(cmd) + B` to enable last frame input\n6.  (Optional) You can modify the prompts in the `ClipTextEncode` node\n7.  (Optional) Modify the size and frame count in the `CosmosPredict2ImageToVideoLatent` node\n8.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n9.  Once generation is complete, the video will automatically save to the `ComfyUI/output/` directory, you can also preview it in the `save video` node"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pika/pika-image-to-video",
  "markdown": "# Pika 2.2 Image to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Pika 2.2 Image to Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg) Pika 2.2 Image to Video èŠ‚ç‚¹é€šè¿‡è¿žæŽ¥Pikaæœ€æ–°çš„2.2ç‰ˆæœ¬APIï¼Œå°†é™æ€å›¾åƒè½¬å˜ä¸ºåŠ¨æ€è§†é¢‘ã€‚è¯¥èŠ‚ç‚¹èƒ½å¤Ÿä¿ç•™åŽŸå§‹å›¾åƒçš„è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶æ ¹æ®æ–‡æœ¬æç¤ºè¯æ·»åŠ è‡ªç„¶çš„åŠ¨æ€æ•ˆæžœã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | è¦è½¬æ¢ä¸ºè§†é¢‘çš„è¾“å…¥å›¾åƒ |\n| prompt\\_text | å­—ç¬¦ä¸² | \"\"  | æè¿°è§†é¢‘åŠ¨ä½œå’Œå†…å®¹çš„æ–‡æœ¬æç¤ºè¯ |\n| negative\\_prompt | å­—ç¬¦ä¸² | \"\"  | æŒ‡å®šä¸å¸Œæœ›åœ¨è§†é¢‘ä¸­å‡ºçŽ°çš„å…ƒç´  |\n| seed | æ•´æ•°  | 0   | ç”Ÿæˆè¿‡ç¨‹çš„éšæœºç§å­ |\n| resolution | é€‰æ‹©é¡¹ | â€1080pâ€ | ç”Ÿæˆè§†é¢‘çš„åˆ†è¾¨çŽ‡ |\n| duration | é€‰æ‹©é¡¹ | â€5sâ€ | ç”Ÿæˆè§†é¢‘çš„æŒç»­æ—¶é—´ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n\n## å·¥ä½œæµç¨‹\n\nèŠ‚ç‚¹å°†è¾“å…¥å›¾åƒå’Œç›¸å…³å‚æ•°ï¼ˆæç¤ºè¯ã€åˆ†è¾¨çŽ‡ã€æŒç»­æ—¶é—´ç­‰ï¼‰é€šè¿‡å¤šéƒ¨åˆ†è¡¨å•æ•°æ®å‘é€åˆ°Pikaçš„APIæœåŠ¡å™¨ã€‚APIå¤„ç†åŽè¿”å›žç”Ÿæˆçš„è§†é¢‘ç»“æžœã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è°ƒæ•´æç¤ºè¯ã€è´Ÿé¢æç¤ºè¯ã€éšæœºç§å­ç­‰å‚æ•°æ¥æŽ§åˆ¶ç”Ÿæˆæ•ˆæžœã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-05)\\]\n\n```\n\nclass PikaImageToVideoV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Image to Video Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\"tooltip\": \"The image to convert to video\"},\n                ),\n                **cls.get_base_inputs_types(PikaBodyGenerate22I2vGenerate22I2vPost),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    DESCRIPTION = \"Sends an image and prompt to the Pika API v2.2 to generate a video.\"\n    RETURN_TYPES = (\"VIDEO\",)\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika 2.2 Image to Video.\"\"\"\n        # Convert image to BytesIO\n        image_bytes_io = tensor_to_bytesio(image)\n        image_bytes_io.seek(0)  # Reset stream position\n\n        # Prepare file data for multipart upload\n        pika_files = {\"image\": (\"image.png\", image_bytes_io, \"image/png\")}\n\n        # Prepare non-file data using the Pydantic model\n        pika_request_data = PikaBodyGenerate22I2vGenerate22I2vPost(\n            promptText=prompt_text,\n            negativePrompt=negative_prompt,\n            seed=seed,\n            resolution=resolution,\n            duration=duration,\n        )\n\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_IMAGE_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22I2vGenerate22I2vPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=pika_request_data,\n            files=pika_files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftutorials%2Fapi-nodes%2Fluma%2Fluma-text-to-video",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls",
  "markdown": "# Kling Camera Controls - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\n\nclass KlingCameraControls(KlingNodeBase):\n    \"\"\"Kling Camera Controls Node\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"camera_control_type\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\n                            camera_control_type.value\n                            for camera_control_type in CameraType\n                        ],\n                        \"default\": \"simple\",\n                        \"tooltip\": \"Predefined camera movements type. simple: Customizable camera movement. down_back: Camera descends and moves backward. forward_up: Camera moves forward and tilts up. right_turn_forward: Rotate right and move forward. left_turn_forward: Rotate left and move forward.\",\n                    },\n                ),\n                \"horizontal_movement\": get_camera_control_input_config(\n                    \"Controls camera's movement along horizontal axis (x-axis). Negative indicates left, positive indicates right\"\n                ),\n                \"vertical_movement\": get_camera_control_input_config(\n                    \"Controls camera's movement along vertical axis (y-axis). Negative indicates downward, positive indicates upward.\"\n                ),\n                \"pan\": get_camera_control_input_config(\n                    \"Controls camera's rotation in vertical plane (x-axis). Negative indicates downward rotation, positive indicates upward rotation.\",\n                    default=0.5,\n                ),\n                \"tilt\": get_camera_control_input_config(\n                    \"Controls camera's rotation in horizontal plane (y-axis). Negative indicates left rotation, positive indicates right rotation.\",\n                ),\n                \"roll\": get_camera_control_input_config(\n                    \"Controls camera's rolling amount (z-axis). Negative indicates counterclockwise, positive indicates clockwise.\",\n                ),\n                \"zoom\": get_camera_control_input_config(\n                    \"Controls change in camera's focal length. Negative indicates narrower field of view, positive indicates wider field of view.\",\n                ),\n            }\n        }\n\n    DESCRIPTION = \"Kling Camera Controls Node. Not all model and mode combinations support camera control. Please refer to the Kling API documentation for more information.\"\n    RETURN_TYPES = (\"CAMERA_CONTROL\",)\n    RETURN_NAMES = (\"camera_control\",)\n    FUNCTION = \"main\"\n\n    @classmethod\n    def VALIDATE_INPUTS(\n        cls,\n        horizontal_movement: float,\n        vertical_movement: float,\n        pan: float,\n        tilt: float,\n        roll: float,\n        zoom: float,\n    ) -> bool | str:\n        if not is_valid_camera_control_configs(\n            [\n                horizontal_movement,\n                vertical_movement,\n                pan,\n                tilt,\n                roll,\n                zoom,\n            ]\n        ):\n            return \"Invalid camera control configs: at least one of the values must be non-zero\"\n        return True\n\n    def main(\n        self,\n        camera_control_type: str,\n        horizontal_movement: float,\n        vertical_movement: float,\n        pan: float,\n        tilt: float,\n        roll: float,\n        zoom: float,\n    ) -> tuple[CameraControl]:\n        return (\n            CameraControl(\n                type=CameraType(camera_control_type),\n                config=CameraConfig(\n                    horizontal=horizontal_movement,\n                    vertical=vertical_movement,\n                    pan=pan,\n                    roll=roll,\n                    tilt=tilt,\n                    zoom=zoom,\n                ),\n            ),\n        )\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pika/pika-scenes",
  "markdown": "# Pika 2.2 Scenes - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Pika 2.2 Scenes èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg) Pika 2.2 Scenes èŠ‚ç‚¹å…è®¸ä½ ä¸Šä¼ å¤šå¼ å›¾åƒä½œä¸ºç´ æï¼Œå¹¶ç”Ÿæˆä¸€ä¸ªåŒ…å«è¿™äº›ç´ æå¯¹è±¡çš„é«˜è´¨é‡è§†é¢‘ã€‚èŠ‚ç‚¹åˆ©ç”¨Pikaçš„2.2ç‰ˆæœ¬APIï¼ŒåŸºäºŽè¿™äº›å›¾åƒåˆ›å»ºè¿žè´¯çš„åœºæ™¯è¿‡æ¸¡è§†é¢‘ã€‚\n\nPika 2.2 Scenes èŠ‚ç‚¹åˆ†æžæ‰€æœ‰è¾“å…¥å›¾åƒï¼Œç„¶åŽåˆ›å»ºä¸€ä¸ªåŒ…å«è¿™äº›å›¾åƒå…ƒç´ çš„è§†é¢‘ã€‚èŠ‚ç‚¹å°†å›¾åƒå’Œå‚æ•°å‘é€åˆ°Pikaçš„APIæœåŠ¡å™¨ï¼Œå¤„ç†å®ŒæˆåŽè¿”å›žç”Ÿæˆçš„è§†é¢‘ç»“æžœã€‚ ç”¨æˆ·å¯ä»¥é€šè¿‡æç¤ºè¯å¼•å¯¼è§†é¢‘çš„é£Žæ ¼å’Œå†…å®¹ï¼Œé€šè¿‡è´Ÿé¢æç¤ºè¯æŽ’é™¤ä¸éœ€è¦çš„å…ƒç´ ã€‚èŠ‚ç‚¹æ”¯æŒä¸Šä¼ æœ€å¤š5å¼ å›¾åƒä½œä¸ºç´ æï¼Œå¹¶ä¼šæ ¹æ®æŒ‡å®šçš„ç»„åˆæ¨¡å¼ã€åˆ†è¾¨çŽ‡ã€æŒç»­æ—¶é—´å’Œå®½é«˜æ¯”ç”Ÿæˆæœ€ç»ˆè§†é¢‘ã€‚\n\n```\n\nclass PikaScenesV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Scenes Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        image_ingredient_input = (\n            IO.IMAGE,\n            {\"tooltip\": \"Image that will be used as ingredient to create a video.\"},\n        )\n        return {\n            \"required\": {\n                **cls.get_base_inputs_types(\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                ),\n                \"ingredients_mode\": model_field_to_node_input(\n                    IO.COMBO,\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                    \"ingredientsMode\",\n                    enum_type=IngredientsMode,\n                    default=\"creative\",\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.FLOAT,\n                    PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                    \"aspectRatio\",\n                    step=0.001,\n                    min=0.4,\n                    max=2.5,\n                    default=1.7777777777777777,\n                ),\n            },\n            \"optional\": {\n                \"image_ingredient_1\": image_ingredient_input,\n                \"image_ingredient_2\": image_ingredient_input,\n                \"image_ingredient_3\": image_ingredient_input,\n                \"image_ingredient_4\": image_ingredient_input,\n                \"image_ingredient_5\": image_ingredient_input,\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    DESCRIPTION = \"Combine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them.\"\n    RETURN_TYPES = (\"VIDEO\",)\n\n    def api_call(\n        self,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        ingredients_mode: str,\n        aspect_ratio: float,\n        image_ingredient_1: Optional[torch.Tensor] = None,\n        image_ingredient_2: Optional[torch.Tensor] = None,\n        image_ingredient_3: Optional[torch.Tensor] = None,\n        image_ingredient_4: Optional[torch.Tensor] = None,\n        image_ingredient_5: Optional[torch.Tensor] = None,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika Scenes 2.2.\"\"\"\n        all_image_bytes_io = []\n        for image in [\n            image_ingredient_1,\n            image_ingredient_2,\n            image_ingredient_3,\n            image_ingredient_4,\n            image_ingredient_5,\n        ]:\n            if image is not None:\n                image_bytes_io = tensor_to_bytesio(image)\n                image_bytes_io.seek(0)\n                all_image_bytes_io.append(image_bytes_io)\n\n        # Prepare files data for multipart upload\n        pika_files = [\n            (\"images\", (f\"image_{i}.png\", image_bytes_io, \"image/png\"))\n            for i, image_bytes_io in enumerate(all_image_bytes_io)\n        ]\n\n        # Prepare non-file data using the Pydantic model\n        pika_request_data = PikaBodyGenerate22C2vGenerate22PikascenesPost(\n            ingredientsMode=ingredients_mode,\n            promptText=prompt_text,\n            negativePrompt=negative_prompt,\n            seed=seed,\n            resolution=resolution,\n            duration=duration,\n            aspectRatio=aspect_ratio,\n        )\n\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_PIKASCENES,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22C2vGenerate22PikascenesPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=pika_request_data,\n            files=pika_files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video",
  "markdown": "# Kling Start-End Frame to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Kling Start-End Frame to Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg) Kling Start-End Frame to Video èŠ‚ç‚¹å…è®¸ä½ æä¾›å¼€å§‹å’Œç»“æŸå›¾åƒï¼Œç”Ÿæˆåœ¨ä¸¤è€…ä¹‹é—´å¹³æ»‘è¿‡æ¸¡çš„è§†é¢‘åºåˆ—ã€‚è¯¥èŠ‚ç‚¹ä¼šè‡ªåŠ¨åˆ›å»ºæ‰€æœ‰ä¸­é—´å¸§ï¼Œäº§ç”Ÿæµç•…çš„å˜æ¢æ•ˆæžœã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| start\\_frame | å›¾åƒ  | è§†é¢‘çš„èµ·å§‹å¸§å›¾åƒ |\n| end\\_frame | å›¾åƒ  | è§†é¢‘çš„ç»“æŸå¸§å›¾åƒ |\n| prompt | å­—ç¬¦ä¸² | æè¿°è§†é¢‘å†…å®¹å’Œè¿‡æ¸¡æ•ˆæžœçš„æ–‡æœ¬æç¤ºè¯ |\n| negative\\_prompt | å­—ç¬¦ä¸² | æŒ‡å®šä¸å¸Œæœ›åœ¨è§†é¢‘ä¸­å‡ºçŽ°çš„å…ƒç´  |\n| cfg\\_scale | æµ®ç‚¹æ•° | é…ç½®ç¼©æ”¾å€¼ï¼ŒæŽ§åˆ¶å¯¹æç¤ºè¯çš„éµå¾ªç¨‹åº¦ |\n| aspect\\_ratio | é€‰æ‹©é¡¹ | è¾“å‡ºè§†é¢‘çš„å®½é«˜æ¯” |\n| mode | é€‰æ‹©é¡¹ | è§†é¢‘ç”Ÿæˆé…ç½®ï¼Œæ ¼å¼ä¸ºâ€æ¨¡å¼/æ—¶é•¿/æ¨¡åž‹åç§°â€ |\n\n### Mode é€‰é¡¹\n\nèŠ‚ç‚¹æ”¯æŒä»¥ä¸‹æ¨¡å¼é€‰é¡¹ï¼š\n\n*   standard mode / 5s duration / kling-v1\n*   standard mode / 5s duration / kling-v1-5\n*   pro mode / 5s duration / kling-v1\n*   pro mode / 5s duration / kling-v1-5\n*   pro mode / 5s duration / kling-v1-6\n*   pro mode / 10s duration / kling-v1-5\n*   pro mode / 10s duration / kling-v1-6\n\né»˜è®¤å€¼ä¸º â€œpro mode / 5s duration / kling-v1â€\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n\n## å·¥ä½œåŽŸç†\n\nKling Start-End Frame to Video èŠ‚ç‚¹åˆ†æžèµ·å§‹å¸§å’Œç»“æŸå¸§å›¾åƒï¼Œç„¶åŽåˆ›å»ºè¿žæŽ¥è¿™ä¸¤ä¸ªçŠ¶æ€çš„å¹³æ»‘è¿‡æ¸¡åºåˆ—ã€‚èŠ‚ç‚¹å°†å›¾åƒå’Œå‚æ•°å‘é€åˆ°Klingçš„APIæœåŠ¡å™¨ï¼ŒåŽè€…ç”Ÿæˆæ‰€æœ‰å¿…è¦çš„ä¸­é—´å¸§ï¼Œåˆ›å»ºæµç•…çš„å˜æ¢æ•ˆæžœã€‚ å¯ä»¥é€šè¿‡æç¤ºè¯å¼•å¯¼è¿‡æ¸¡æ•ˆæžœçš„é£Žæ ¼å’Œå†…å®¹ï¼Œè€Œè´Ÿé¢æç¤ºè¯åˆ™å¸®åŠ©é¿å…ä¸éœ€è¦çš„å…ƒç´ ã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\n\nclass KlingStartEndFrameNode(KlingImage2VideoNode):\n    \"\"\"\n    Kling First Last Frame Node. This node allows creation of a video from a first and last frame. It calls the normal image to video endpoint, but only allows the subset of input options that support the `image_tail` request field.\n    \"\"\"\n\n    @staticmethod\n    def get_mode_string_mapping() -> dict[str, tuple[str, str, str]]:\n        \"\"\"\n        Returns a mapping of mode strings to their corresponding (mode, duration, model_name) tuples.\n        Only includes config combos that support the `image_tail` request field.\n        \"\"\"\n        return {\n            \"standard mode / 5s duration / kling-v1\": (\"std\", \"5\", \"kling-v1\"),\n            \"standard mode / 5s duration / kling-v1-5\": (\"std\", \"5\", \"kling-v1-5\"),\n            \"pro mode / 5s duration / kling-v1\": (\"pro\", \"5\", \"kling-v1\"),\n            \"pro mode / 5s duration / kling-v1-5\": (\"pro\", \"5\", \"kling-v1-5\"),\n            \"pro mode / 5s duration / kling-v1-6\": (\"pro\", \"5\", \"kling-v1-6\"),\n            \"pro mode / 10s duration / kling-v1-5\": (\"pro\", \"10\", \"kling-v1-5\"),\n            \"pro mode / 10s duration / kling-v1-6\": (\"pro\", \"10\", \"kling-v1-6\"),\n        }\n\n    @classmethod\n    def INPUT_TYPES(s):\n        modes = list(KlingStartEndFrameNode.get_mode_string_mapping().keys())\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"end_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image_tail\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"mode\": (\n                    modes,\n                    {\n                        \"default\": modes[2],\n                        \"tooltip\": \"The configuration to use for the video generation following the format: mode / duration / model_name.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Generate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last.\"\n\n    def parse_inputs_from_mode(self, mode: str) -> tuple[str, str, str]:\n        \"\"\"Parses the mode input into a tuple of (model_name, duration, mode).\"\"\"\n        return KlingStartEndFrameNode.get_mode_string_mapping()[mode]\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        end_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        mode: str,\n        auth_token: Optional[str] = None,\n    ):\n        mode, duration, model_name = self.parse_inputs_from_mode(mode)\n        return super().api_call(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model_name=model_name,\n            start_frame=start_frame,\n            cfg_scale=cfg_scale,\n            mode=mode,\n            aspect_ratio=aspect_ratio,\n            duration=duration,\n            end_frame=end_frame,\n            auth_token=auth_token,\n        )\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pixverse/pixverse-transition-video",
  "markdown": "# PixVerse Transition Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ PixVerse Transition Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg) PixVerse Transition Video èŠ‚ç‚¹è¿žæŽ¥åˆ° PixVerse çš„è¿‡æ¸¡è§†é¢‘ç”Ÿæˆ APIï¼Œå…è®¸ä½ æä¾›å¼€å§‹å’Œç»“æŸå›¾åƒï¼Œç”Ÿæˆåœ¨ä¸¤è€…ä¹‹é—´å¹³æ»‘è¿‡æ¸¡çš„è§†é¢‘åºåˆ—ã€‚èŠ‚ç‚¹ä¼šè‡ªåŠ¨åˆ›å»ºæ‰€æœ‰ä¸­é—´å¸§ï¼Œäº§ç”Ÿæµç•…çš„å˜æ¢æ•ˆæžœï¼Œç‰¹åˆ«é€‚åˆåˆ›å»ºå˜å½¢ã€åœºæ™¯è½¬æ¢å’Œå¯¹è±¡æ¼”å˜ç­‰è§†è§‰æ•ˆæžœã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| first\\_frame | å›¾åƒ  | \\-  | è§†é¢‘çš„èµ·å§‹å¸§å›¾åƒ |\n| last\\_frame | å›¾åƒ  | \\-  | è§†é¢‘çš„ç»“æŸå¸§å›¾åƒ |\n| prompt | å­—ç¬¦ä¸² | \"\"  | æè¿°è§†é¢‘å†…å®¹å’Œè¿‡æ¸¡æ•ˆæžœçš„æ–‡æœ¬æç¤ºè¯ |\n| quality | é€‰æ‹©é¡¹ | â€PixverseQuality.res\\_540pâ€ | ç”Ÿæˆè§†é¢‘çš„è´¨é‡çº§åˆ« |\n| duration\\_seconds | é€‰æ‹©é¡¹ | \\-  | ç”Ÿæˆè§†é¢‘çš„æŒç»­æ—¶é—´ |\n| motion\\_mode | é€‰æ‹©é¡¹ | \\-  | è§†é¢‘çš„åŠ¨ä½œæ¨¡å¼ |\n| seed | æ•´æ•°  | 0   | ç”Ÿæˆè¿‡ç¨‹çš„éšæœºç§å­ï¼ŒèŒƒå›´0-2147483647 |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| negative\\_prompt | å­—ç¬¦ä¸² | \"\"  | æŒ‡å®šä¸å¸Œæœ›åœ¨è§†é¢‘ä¸­å‡ºçŽ°çš„å…ƒç´  |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | å¯é€‰çš„PixVerseæ¨¡æ¿é…ç½®ï¼Œå½±å“ç”Ÿæˆé£Žæ ¼ |\n\n### å‚æ•°é™åˆ¶è¯´æ˜Ž\n\n*   å½“qualityè®¾ç½®ä¸º1080pæ—¶ï¼ŒåŠ¨ä½œæ¨¡å¼(motion\\_mode)ä¼šå¼ºåˆ¶è®¾ä¸ºnormalï¼ŒæŒç»­æ—¶é—´(duration\\_seconds)ä¼šå¼ºåˆ¶è®¾ä¸º5ç§’\n*   å½“æŒç»­æ—¶é—´(duration\\_seconds)ä¸ç­‰äºŽ5ç§’æ—¶ï¼ŒåŠ¨ä½œæ¨¡å¼(motion\\_mode)ä¼šå¼ºåˆ¶è®¾ä¸ºnormal\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n\n## æºç å‚è€ƒ\n\n```\n\nclass PixverseTransitionVideoNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"first_frame\": (\n                    IO.IMAGE,\n                ),\n                \"last_frame\": (\n                    IO.IMAGE,\n                ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"quality\": (\n                    [resolution.value for resolution in PixverseQuality],\n                    {\n                        \"default\": PixverseQuality.res_540p,\n                    },\n                ),\n                \"duration_seconds\": ([dur.value for dur in PixverseDuration],),\n                \"motion_mode\": ([mode.value for mode in PixverseMotionMode],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"pixverse_template\": (\n                    PixverseIO.TEMPLATE,\n                    {\n                        \"tooltip\": \"An optional template to influence style of generation, created by the Pixverse Template node.\"\n                    }\n                )\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        first_frame: torch.Tensor,\n        last_frame: torch.Tensor,\n        prompt: str,\n        quality: str,\n        duration_seconds: int,\n        motion_mode: str,\n        seed,\n        negative_prompt: str=None,\n        pixverse_template: int=None,\n        auth_token=None,\n        **kwargs,\n    ):\n        first_frame_id = upload_image_to_pixverse(first_frame, auth_token=auth_token)\n        last_frame_id = upload_image_to_pixverse(last_frame, auth_token=auth_token)\n\n        # 1080p is limited to 5 seconds duration\n        # only normal motion_mode supported for 1080p or for non-5 second duration\n        if quality == PixverseQuality.res_1080p:\n            motion_mode = PixverseMotionMode.normal\n            duration_seconds = PixverseDuration.dur_5\n        elif duration_seconds != PixverseDuration.dur_5:\n            motion_mode = PixverseMotionMode.normal\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/pixverse/video/transition/generate\",\n                method=HttpMethod.POST,\n                request_model=PixverseTransitionVideoRequest,\n                response_model=PixverseVideoResponse,\n            ),\n            request=PixverseTransitionVideoRequest(\n                first_frame_img=first_frame_id,\n                last_frame_img=last_frame_id,\n                prompt=prompt,\n                quality=quality,\n                duration=duration_seconds,\n                motion_mode=motion_mode,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                template_id=pixverse_template,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.Resp is None:\n            raise Exception(f\"Pixverse request failed: '{response_api.ErrMsg}'\")\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/pixverse/video/result/{response_api.Resp.video_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=PixverseGenerationStatusResponse,\n            ),\n            completed_statuses=[PixverseStatus.successful],\n            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],\n            status_extractor=lambda x: x.Resp.status,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.Resp.url)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftutorials%2Fapi-nodes%2Fmoonvalley%2Fmoonvalley-video-generation",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/tutorials/flux/flux-1-kontext-dev",
  "markdown": "# ComfyUI Flux Kontext Dev Native Workflow Example\n\n## About FLUX.1 Kontext Dev\n\nFLUX.1 Kontext is a breakthrough multimodal image editing model from Black Forest Labs that supports simultaneous text and image input, intelligently understanding image context and performing precise editing. Its development version is an open-source diffusion transformer model with 12 billion parameters, featuring excellent context understanding and character consistency maintenance, ensuring that key elements such as character features and composition layout remain stable even after multiple iterative edits. It shares the same core capabilities as the FLUX.1 Kontext suite:\n\n*   Character Consistency: Preserves unique elements in images across multiple scenes and environments, such as reference characters or objects in the image.\n*   Editing: Makes targeted modifications to specific elements in the image without affecting other parts.\n*   Style Reference: Generates novel scenes while preserving the unique style of the reference image according to text prompts.\n*   Interactive Speed: Minimal latency in image generation and editing.\n\nWhile the previously released API version offers the highest fidelity and speed, FLUX.1 Kontext \\[Dev\\] runs entirely on local machines, providing unparalleled flexibility for developers, researchers, and advanced users who wish to experiment.\n\n### Version Information\n\n*   **\\[FLUX.1 Kontext \\[pro\\]** - Commercial version, focused on rapid iterative editing\n*   **FLUX.1 Kontext \\[max\\]** - Experimental version with stronger prompt adherence\n*   **FLUX.1 Kontext \\[dev\\]** - Open source version (used in this tutorial), 12B parameters, mainly for research\n\nCurrently in ComfyUI, you can use all these versions, where [Pro and Max versions](https://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-kontext) can be called through API nodes, while the Dev open source version please refer to the instructions in this guide.\n\n## Workflow Description\n\nIn this tutorial, we cover two types of workflows, which are essentially the same:\n\n*   A workflow using the **FLUX.1 Kontext Image Edit** group node, making the interface and workflow reuse simpler\n*   Another workflow without using group nodes, showing the complete original workflow.\n\nThe main advantage of using group nodes is workflow conciseness - you can reuse group nodes to implement complex workflows and quickly reuse node groups. Additionally, in the new version of the frontend, weâ€™ve added a quick group node addition feature for Flux.1 Kontext Dev: ![Quick Add Group Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/selcetion_toolbox_edit.jpg)\n\n## Model Download\n\nTo run the workflows in this guide successfully, you first need to download the following model files. You can also directly get the model download links from the corresponding workflows, which already contain the model file download information. **Diffusion Model**\n\n*   [flux1-dev-kontext\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/flux1-kontext-dev_ComfyUI/resolve/main/split_files/diffusion_models/flux1-dev-kontext_fp8_scaled.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/blob/main/split_files/vae/ae.safetensors)\n\n**Text Encoder**\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/clip_l.safetensors)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors) or [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors)\n\nModel save location\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ flux1-dev-kontext_fp8_scaled.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensor\nâ”‚   â””â”€â”€ ðŸ“‚ text_encoders/\nâ”‚       â”œâ”€â”€ clip_l.safetensors\nâ”‚       â””â”€â”€ t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn_scaled.safetensors\n```\n\nThis workflow is a normal workflow, but it uses the `Load Image(from output)` node to load the image to be edited, making it more convenient for you to access the edited image for multiple rounds of editing.\n\n### 1\\. Workflow and Input Image Download\n\nDownload the following files and drag them into ComfyUI to load the corresponding workflow ![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/flux_1_kontext_dev_basic.png) **Input Image** ![ComfyUI Flux Kontext Native Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/rabbit.jpg)\n\n### 2\\. Complete the workflow step by step\n\n ![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg) You can refer to the numbers in the image to complete the workflow run:\n\n1.  In the `Load Diffusion Model` node, load the `flux1-dev-kontext_fp8_scaled.safetensors` model\n2.  In the `DualCLIP Load` node, ensure that `clip_l.safetensors` and `t5xxl_fp16.safetensors` or `t5xxl_fp8_e4m3fn_scaled.safetensors` are loaded\n3.  In the `Load VAE` node, ensure that `ae.safetensors` model is loaded\n4.  In the `Load Image(from output)` node, load the provided input image\n5.  In the `CLIP Text Encode` node, modify the prompts, only English is supported\n6.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Flux.1 Kontext Dev Grouped Workflow\n\nThis workflow uses the **FLUX.1 Kontext Image Edit** group node, making the interface and workflow reuse simpler. This example also uses two images as input, using the `Image Stitch` node to combine two images into one, and then using Flux.1 Kontext for editing.\n\n### 1\\. Workflow and Input Image Download\n\nDownload the following files and drag them into ComfyUI to load the corresponding workflow ![ComfyUI Flux Kontext Native Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/flux_1_kontext_dev_grouped.png) **Input Images** ![ComfyUI Flux Kontext Native Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/doll_1.webp) ![ComfyUI Flux Kontext Native Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/doll_2.webp)\n\n### 2\\. Complete the workflow step by step\n\n ![Workflow Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_1_kontext_dev_grouped_step_guide.jpg) You can refer to the numbers in the image to complete the workflow run:\n\n1.  In the `Load VAE` node, load the `ae.safetensors` model\n2.  In the `Load Image` node, load the first provided input image\n3.  In the `Load Image` node, load the second provided input image\n4.  Since other models and related nodes are packaged in the group node, you need to follow the reference in the step diagram to ensure that the corresponding models are correctly loaded and write prompts\n5.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## New Flux.1 Kontext Dev Selection Toolbox Feature\n\nTo make it easier for users to edit with Flux.1 Kontext Dev, we have added a selection toolbox feature. This feature allows users to quickly and conveniently add the `FLUX.1 Kontext Image Edit` group node. You can watch the video demo below. When you select the `Load Image` node, you can find the new edit button in the selection toolbox.\n\n## Flux Kontext Prompt Techniques\n\n### 1\\. Basic Modifications\n\n*   Simple and direct: `\"Change the car color to red\"`\n*   Maintain style: `\"Change to daytime while maintaining the same style of the painting\"`\n\n### 2\\. Style Transfer\n\n**Principles:**\n\n*   Clearly name style: `\"Transform to Bauhaus art style\"`\n*   Describe characteristics: `\"Transform to oil painting with visible brushstrokes, thick paint texture\"`\n*   Preserve composition: `\"Change to Bauhaus style while maintaining the original composition\"`\n\n### 3\\. Character Consistency\n\n**Framework:**\n\n*   Specific description: `\"The woman with short black hair\"` instead of â€œsheâ€\n*   Preserve features: `\"while maintaining the same facial features, hairstyle, and expression\"`\n*   Step-by-step modifications: Change background first, then actions\n\n### 4\\. Text Editing\n\n*   Use quotes: `\"Replace 'joy' with 'BFL'\"`\n*   Maintain format: `\"Replace text while maintaining the same font style\"`\n\n## Common Problem Solutions\n\n### Character Changes Too Much\n\nâŒ Wrong: `\"Transform the person into a Viking\"` âœ… Correct: `\"Change the clothes to be a viking warrior while preserving facial features\"`\n\n### Composition Position Changes\n\nâŒ Wrong: `\"Put him on a beach\"` âœ… Correct: `\"Change the background to a beach while keeping the person in the exact same position, scale, and pose\"`\n\n### Style Application Inaccuracy\n\nâŒ Wrong: `\"Make it a sketch\"` âœ… Correct: `\"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\"`\n\n## Core Principles\n\n1.  **Be Specific and Clear** - Use precise descriptions, avoid vague terms\n2.  **Step-by-step Editing** - Break complex modifications into multiple simple steps\n3.  **Explicit Preservation** - State what should remain unchanged\n4.  **Verb Selection** - Use â€œchangeâ€, â€œreplaceâ€ rather than â€œtransformâ€\n\n## Best Practice Templates\n\n**Object Modification:** `\"Change [object] to [new state], keep [content to preserve] unchanged\"` **Style Transfer:** `\"Transform to [specific style], while maintaining [composition/character/other] unchanged\"` **Background Replacement:** `\"Change the background to [new background], keep the subject in the exact same position and pose\"` **Text Editing:** `\"Replace '[original text]' with '[new text]', maintain the same font style\"`\n\n> **Remember:** The more specific, the better. Kontext excels at understanding detailed instructions and maintaining consistency."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video",
  "markdown": "# PixVerse Image to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ PixVerse Image to Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg) PixVerse Image to Video èŠ‚ç‚¹é€šè¿‡ PixVerse çš„APIæœåŠ¡ï¼Œå¯ä»¥å°†é™æ€å›¾åƒè½¬æ¢ä¸ºåŠ¨æ€è§†é¢‘ã€‚å®ƒèƒ½ä¿ç•™åŽŸå§‹å›¾åƒçš„è§†è§‰ç‰¹å¾ï¼ŒåŒæ—¶æ ¹æ®æ–‡æœ¬æç¤ºè¯æ·»åŠ è‡ªç„¶çš„åŠ¨æ€æ•ˆæžœã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | è¦è½¬æ¢ä¸ºè§†é¢‘çš„è¾“å…¥å›¾åƒ |\n| prompt | å­—ç¬¦ä¸² | \"\"  | æè¿°è§†é¢‘åŠ¨ä½œå’Œå†…å®¹çš„æ–‡æœ¬æç¤ºè¯ |\n| negative\\_prompt | å­—ç¬¦ä¸² | \"\"  | æŒ‡å®šä¸å¸Œæœ›åœ¨è§†é¢‘ä¸­å‡ºçŽ°çš„å…ƒç´  |\n| seed | æ•´æ•°  | \\-1 | ç”Ÿæˆè¿‡ç¨‹çš„éšæœºç§å­ï¼Œ-1ä¸ºéšæœº |\n| quality | é€‰æ‹©é¡¹ | â€highâ€ | ç”Ÿæˆè§†é¢‘çš„è´¨é‡çº§åˆ« |\n| aspect\\_ratio | é€‰æ‹©é¡¹ | â€r16\\_9â€ | è¾“å‡ºè§†é¢‘çš„å®½é«˜æ¯” |\n| duration | é€‰æ‹©é¡¹ | â€seconds\\_4â€ | ç”Ÿæˆè§†é¢‘çš„æŒç»­æ—¶é—´ |\n| motion\\_mode | é€‰æ‹©é¡¹ | â€standardâ€ | è§†é¢‘çš„åŠ¨ä½œæ¨¡å¼ |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| pixverse\\_template | PIXVERSE\\_TEMPLATE | None | å¯é€‰çš„PixVerseæ¨¡æ¿é…ç½® |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-05)\\]\n\n```\nclass PixverseImageToVideoNode(ComfyNodeABC):\n    \"\"\"\n    Pixverse Image to Video\n\n    Generates videos from an image and prompts.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"prompt\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n                \"negative_prompt\": (\"STRING\", {\"multiline\": True, \"default\": \"\"}),\n                \"seed\": (\"INT\", {\"default\": -1, \"min\": -1, \"max\": 0xffffffffffffffff}),\n                \"quality\": (list(PixverseQuality.__members__.keys()), {\"default\": \"high\"}),\n                \"aspect_ratio\": (list(PixverseAspectRatio.__members__.keys()), {\"default\": \"r16_9\"}),\n                \"duration\": (list(PixverseDuration.__members__.keys()), {\"default\": \"seconds_4\"}),\n                \"motion_mode\": (list(PixverseMotionMode.__members__.keys()), {\"default\": \"standard\"}),\n            },\n            \"optional\": {\n                \"pixverse_template\": (\"PIXVERSE_TEMPLATE\",),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from an image and prompts using Pixverse's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Pixverse\"\n    API_NODE = True\n    OUTPUT_NODE = True\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/flux/flux-1-text-to-image",
  "markdown": "# ComfyUI Flux.1 Text-to-Image Workflow Example\n\n ![Flux](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_example.png) Flux is one of the largest open-source text-to-image generation models, with 12B parameters and an original file size of approximately 23GB. It was developed by [Black Forest Labs](https://blackforestlabs.ai/), a team founded by former Stable Diffusion team members. Flux is known for its excellent image quality and flexibility, capable of generating high-quality, diverse images. Currently, the Flux.1 model has several main versions:\n\n*   **Flux.1 Pro:** The best performing model, closed-source, only available through API calls.\n*   **[Flux.1 \\[dev\\]ï¼š](https://huggingface.co/black-forest-labs/FLUX.1-dev)** Open-source but limited to non-commercial use, distilled from the Pro version, with performance close to the Pro version.\n*   **[Flux.1 \\[schnell\\]ï¼š](https://huggingface.co/black-forest-labs/FLUX.1-schnell)** Uses the Apache2.0 license, requires only 4 steps to generate images, suitable for low-spec hardware.\n\n**Flux.1 Model Features**\n\n*   **Hybrid Architecture:** Combines the advantages of Transformer networks and diffusion models, effectively integrating text and image information, improving the alignment accuracy between generated images and prompts, with excellent fidelity to complex prompts.\n*   **Parameter Scale:** Flux has 12B parameters, capturing more complex pattern relationships and generating more realistic, diverse images.\n*   **Supports Multiple Styles:** Supports diverse styles, with excellent performance for various types of images.\n\nIn this example, weâ€™ll introduce text-to-image examples using both Flux.1 Dev and Flux.1 Schnell versions, including the full version model and the simplified FP8 Checkpoint version.\n\n*   **Flux Full Version:** Best performance, but requires larger VRAM resources and installation of multiple model files.\n*   **Flux FP8 Checkpoint:** Requires only one fp8 version of the model, but quality is slightly reduced compared to the full version.\n\n## Flux.1 Full Version Text-to-Image Example\n\n### Flux.1 Dev\n\n#### 1\\. Workflow File\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Flux Dev Original Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_t5fp16.png) \n\n#### 2\\. Manual Model Installation\n\nPlease download the following model files:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) Recommended when your VRAM is greater than 32GB.\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)\n\nStorage location:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ t5xxl_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ flux1-dev.safetensors\n```\n\n#### 3\\. Steps to Run the Workflow\n\nPlease refer to the image below to ensure all model files are loaded correctly ![ComfyUI Flux Dev Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg)\n\n1.  Ensure the `DualCLIPLoader` node has the following models loaded:\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n2.  Ensure the `Load Diffusion Model` node has `flux1-dev.safetensors` loaded\n3.  Make sure the `Load VAE` node has `ae.safetensors` loaded\n4.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n### Flux.1 Schnell\n\n#### 1\\. Workflow File\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Flux Schnell Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_t5fp8.png)\n\n#### 2\\. Manual Models Installation\n\nComplete model file list:\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)\n\nFile storage location:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ t5xxl_fp8_e4m3fn.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ flux1-schnell.safetensors\n```\n\n#### 3\\. Steps to Run the Workflow\n\n![Flux Schnell Version Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg)\n\n1.  Ensure the `DualCLIPLoader` node has the following models loaded:\n    *   clip\\_name1: t5xxl\\_fp8\\_e4m3fn.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n2.  Ensure the `Load Diffusion Model` node has `flux1-schnell.safetensors` loaded\n3.  Ensure the `Load VAE` node has `ae.safetensors` loaded\n4.  Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow\n\n## Flux.1 FP8 Checkpoint Version Text-to-Image Example\n\nThe fp8 version is a quantized version of the original Flux.1 fp16 version. To some extent, the quality of this version will be lower than that of the fp16 version, but it also requires less VRAM, and you only need to install one model file to try running it.\n\n### Flux.1 Dev\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Flux Dev fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_fp8.png) Please download [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory. Ensure that the corresponding `Load Checkpoint` node loads `flux1-dev-fp8.safetensors`, and you can try to run the workflow.\n\n### Flux.1 Schnell\n\nPlease download the image below and drag it into ComfyUI to load the workflow. ![Flux Schnell fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_fp8.png) Please download [flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory. Ensure that the corresponding `Load Checkpoint` node loads `flux1-schnell-fp8.safetensors`, and you can try to run the workflow."
},
{
  "url": "https://docs.comfy.org/tutorials/image/hidream/hidream-i1",
  "markdown": "# ComfyUI Native HiDream-I1 Text-to-Image Workflow Example\n\n![HiDream-I1 Demo](https://raw.githubusercontent.com/HiDream-ai/HiDream-I1/main/assets/demo.jpg) HiDream-I1 is a text-to-image model officially open-sourced by HiDream-ai on April 7, 2025. The model has 17B parameters and is released under the [MIT license](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE), supporting personal projects, scientific research, and commercial use. It currently performs excellently in multiple benchmark tests.\n\n## Model Features\n\n**Hybrid Architecture Design** A combination of Diffusion Transformer (DiT) and Mixture of Experts (MoE) architecture:\n\n*   Based on Diffusion Transformer (DiT), with dual-stream MMDiT modules processing multimodal information and single-stream DiT modules optimizing global consistency.\n*   Dynamic routing mechanism flexibly allocates computing resources, enhancing complex scene processing capabilities and delivering excellent performance in color restoration, edge processing, and other details.\n\n**Multimodal Text Encoder Integration** Integrates four text encoders:\n\n*   OpenCLIP ViT-bigG, OpenAI CLIP ViT-L (visual semantic alignment)\n*   T5-XXL (long text parsing)\n*   Llama-3.1-8B-Instruct (instruction understanding) This combination achieves SOTA performance in complex semantic parsing of colors, quantities, spatial relationships, etc., with Chinese prompt support significantly outperforming similar open-source models.\n\n**Original Model Versions** HiDream-ai provides three versions of the HiDream-I1 model to meet different needs. Below are the links to the original model repositories:\n\n| Model Name | Description | Inference Steps | Repository Link |\n| --- | --- | --- | --- |\n| HiDream-I1-Full | Full version | 50  | [ðŸ¤— HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full) |\n| HiDream-I1-Dev | Distilled dev | 28  | [ðŸ¤— HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev) |\n| HiDream-I1-Fast | Distilled fast | 16  | [ðŸ¤— HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) |\n\n## About This Workflow Example\n\nIn this example, we will use the repackaged version from ComfyOrg. You can find all the model files weâ€™ll use in this example in the [HiDream-I1\\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) repository.\n\nThe model requirements for different ComfyUI native HiDream-I1 workflows are basically the same, with only the [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) files being different. If you donâ€™t know which version to choose, please refer to the following suggestions:\n\n*   **HiDream-I1-Full** can generate the highest quality images\n*   **HiDream-I1-Dev** balances high-quality image generation with speed\n*   **HiDream-I1-Fast** can generate images in just 16 steps, suitable for scenarios requiring real-time iteration\n\nFor the **dev** and **fast** versions, negative prompts are not needed, so please set the `cfg` parameter to `1.0` during sampling. We have noted the corresponding parameter settings in the relevant workflows.\n\n### Model Installation\n\nThe following model files are common files that we will use. Please click on the corresponding links to download and save them according to the model file save location. We will guide you to download the corresponding **diffusion models** in the corresponding workflows. **text\\_encoders**ï¼š\n\n*   [clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)\n*   [clip\\_g\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)\n*   [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.\n*   [llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Fluxâ€™s VAE model, if you have used Fluxâ€™s workflow before, you may have already downloaded this file.\n\n**diffusion models** We will guide you to download the corresponding model files in the corresponding workflows. Model file save location\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€â”€ clip_l_hidream.safetensors\nâ”‚   â”‚   â”œâ”€â”€â”€ clip_g_hidream.safetensors\nâ”‚   â”‚   â”œâ”€â”€â”€ t5xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â”‚   â””â”€â”€â”€ llama_3.1_8b_instruct_fp8_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ diffusion_models/\nâ”‚       â””â”€â”€ ...               # We will guide you to install in the corresponding version workflow       \n```\n\n### HiDream-I1 Full Version Workflow\n\n#### 1\\. Model File Download\n\nPlease select the appropriate version based on your hardware. Click the link and download the corresponding model file to save it to the `ComfyUI/models/diffusion_models/` folder.\n\n*   FP8 version: [hidream\\_i1\\_full\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) requires more than 16GB of VRAM\n*   Full version: [hidream\\_i1\\_full\\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) requires more than 27GB of VRAM\n\n#### 2\\. Workflow File Download\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![HiDream-I1 Full Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_full.png) \n\n#### 3\\. Complete the Workflow Step by Step\n\n![HiDream-I1 Full Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg) Complete the workflow execution step by step\n\n1.  Make sure the `Load Diffusion Model` node is using the `hidream_i1_full_fp8.safetensors` file\n2.  Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node is using the `ae.safetensors` file\n4.  For the **full** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`\n5.  For the `Ksampler` node, you need to make the following settings\n    *   Set `steps` to `50`\n    *   Set `cfg` to `5.0`\n    *   (Optional) Set `sampler` to `lcm`\n    *   (Optional) Set `scheduler` to `normal`\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n### HiDream-I1 Dev Version Workflow\n\n#### 1\\. Model File Download\n\nPlease select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.\n\n*   FP8 version: [hidream\\_i1\\_dev\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) requires more than 16GB of VRAM\n*   Full version: [hidream\\_i1\\_dev\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) requires more than 27GB of VRAM\n\n#### 2\\. Workflow File Download\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![HiDream-I1 Dev Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_dev.png)\n\n#### 3\\. Complete the Workflow Step by Step\n\n ![HiDream-I1 Dev Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg) Complete the workflow execution step by step\n\n1.  Make sure the `Load Diffusion Model` node is using the `hidream_i1_dev_fp8.safetensors` file\n2.  Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node is using the `ae.safetensors` file\n4.  For the **dev** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `6.0`\n5.  For the `Ksampler` node, you need to make the following settings\n    *   Set `steps` to `28`\n    *   (Important) Set `cfg` to `1.0`\n    *   (Optional) Set `sampler` to `lcm`\n    *   (Optional) Set `scheduler` to `normal`\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n### HiDream-I1 Fast Version Workflow\n\n#### 1\\. Model File Download\n\nPlease select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.\n\n*   FP8 version: [hidream\\_i1\\_fast\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 16GB of VRAM\n*   Full version: [hidream\\_i1\\_fast\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_bf16.safetensors?download=true) requires more than 27GB of VRAM\n\n#### 2\\. Workflow File Download\n\nPlease download the image below and drag it into ComfyUI to load the corresponding workflow ![HiDream-I1 Fast Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_fast.png)\n\n#### 3\\. Complete the Workflow Step by Step\n\n![HiDream-I1 Fast Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg) Complete the workflow execution step by step\n\n1.  Make sure the `Load Diffusion Model` node is using the `hidream_i1_fast_fp8.safetensors` file\n2.  Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  Make sure the `Load VAE` node is using the `ae.safetensors` file\n4.  For the **fast** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`\n5.  For the `Ksampler` node, you need to make the following settings\n    *   Set `steps` to `16`\n    *   (Important) Set `cfg` to `1.0`\n    *   (Optional) Set `sampler` to `lcm`\n    *   (Optional) Set `scheduler` to `normal`\n6.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation\n\n### GGUF Version Models\n\n*   [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)\n*   [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)\n\nYou need to use the â€œUnet Loader (GGUF)â€ node in City96â€™s [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) to replace the â€œLoad Diffusion Modelâ€ node.\n\n### NF4 Version Models\n\n*   [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)\n*   Use the [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) node to use the NF4 version model."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Finterface%2Fappearance",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video",
  "markdown": "# PixVerse Text to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\n\nclass PixverseTextToVideoNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in PixverseAspectRatio],\n                ),\n                \"quality\": (\n                    [resolution.value for resolution in PixverseQuality],\n                    {\n                        \"default\": PixverseQuality.res_540p,\n                    },\n                ),\n                \"duration_seconds\": ([dur.value for dur in PixverseDuration],),\n                \"motion_mode\": ([mode.value for mode in PixverseMotionMode],),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2147483647,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed for video generation.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"pixverse_template\": (\n                    PixverseIO.TEMPLATE,\n                    {\n                        \"tooltip\": \"An optional template to influence style of generation, created by the Pixverse Template node.\"\n                    }\n                )\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        aspect_ratio: str,\n        quality: str,\n        duration_seconds: int,\n        motion_mode: str,\n        seed,\n        negative_prompt: str=None,\n        pixverse_template: int=None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # 1080p is limited to 5 seconds duration\n        # only normal motion_mode supported for 1080p or for non-5 second duration\n        if quality == PixverseQuality.res_1080p:\n            motion_mode = PixverseMotionMode.normal\n            duration_seconds = PixverseDuration.dur_5\n        elif duration_seconds != PixverseDuration.dur_5:\n            motion_mode = PixverseMotionMode.normal\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/pixverse/video/text/generate\",\n                method=HttpMethod.POST,\n                request_model=PixverseTextVideoRequest,\n                response_model=PixverseVideoResponse,\n            ),\n            request=PixverseTextVideoRequest(\n                prompt=prompt,\n                aspect_ratio=aspect_ratio,\n                quality=quality,\n                duration=duration_seconds,\n                motion_mode=motion_mode,\n                negative_prompt=negative_prompt if negative_prompt else None,\n                template_id=pixverse_template,\n                seed=seed,\n            ),\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.Resp is None:\n            raise Exception(f\"Pixverse request failed: '{response_api.ErrMsg}'\")\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/pixverse/video/result/{response_api.Resp.video_id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=PixverseGenerationStatusResponse,\n            ),\n            completed_statuses=[PixverseStatus.successful],\n            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],\n            status_extractor=lambda x: x.Resp.status,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.Resp.url)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v",
  "markdown": "# Kling Image to Video (Camera Control) - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Kling Image to Video (Camera Control) èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg) Kling Image to Video (Camera Control) èŠ‚ç‚¹å¯å°†é™æ€å›¾åƒè½¬æ¢ä¸ºå…·æœ‰ä¸“ä¸šé•œå¤´åŠ¨ä½œçš„è§†é¢‘ï¼Œæ”¯æŒå˜ç„¦ã€æ—‹è½¬ã€å¹³ç§»ã€å€¾æ–œå’Œç¬¬ä¸€äººç§°è§†è§’ç­‰æ‘„åƒæœºæŽ§åˆ¶åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒå¯¹åŽŸå§‹å›¾åƒå†…å®¹çš„å…³æ³¨ã€‚\n\n```\n\nclass KlingCameraControlI2VNode(KlingImage2VideoNode):\n    \"\"\"\n    Kling Image to Video Camera Control Node. This node is a image to video node, but it supports controlling the camera.\n    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"start_frame\": model_field_to_node_input(\n                    IO.IMAGE, KlingImage2VideoRequest, \"image\"\n                ),\n                \"prompt\": model_field_to_node_input(\n                    IO.STRING, KlingImage2VideoRequest, \"prompt\", multiline=True\n                ),\n                \"negative_prompt\": model_field_to_node_input(\n                    IO.STRING,\n                    KlingImage2VideoRequest,\n                    \"negative_prompt\",\n                    multiline=True,\n                ),\n                \"cfg_scale\": model_field_to_node_input(\n                    IO.FLOAT, KlingImage2VideoRequest, \"cfg_scale\"\n                ),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.COMBO,\n                    KlingImage2VideoRequest,\n                    \"aspect_ratio\",\n                    enum_type=AspectRatio,\n                ),\n                \"camera_control\": (\n                    \"CAMERA_CONTROL\",\n                    {\n                        \"tooltip\": \"Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    DESCRIPTION = \"Transform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image.\"\n\n    def api_call(\n        self,\n        start_frame: torch.Tensor,\n        prompt: str,\n        negative_prompt: str,\n        cfg_scale: float,\n        aspect_ratio: str,\n        camera_control: CameraControl,\n        auth_token: Optional[str] = None,\n    ):\n        return super().api_call(\n            model_name=\"kling-v1-5\",\n            start_frame=start_frame,\n            cfg_scale=cfg_scale,\n            mode=\"pro\",\n            aspect_ratio=aspect_ratio,\n            duration=\"5\",\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            camera_control=camera_control,\n            auth_token=auth_token,\n        )\n\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/comfy-cli/getting-started",
  "markdown": "# å¿«é€Ÿå…¥é—¨ - ComfyUI\n\n### æ¦‚è¿°\n\n`comfy-cli` æ˜¯ä¸€ä¸ª [å‘½ä»¤è¡Œå·¥å…·](https://github.com/Comfy-Org/comfy-cli)ï¼Œå¯ä»¥å¸®åŠ©æ›´è½»æ¾åœ°å®‰è£…å’Œç®¡ç† Comfyã€‚\n\n### å®‰è£… CLI\n\nèŽ·å– shell è‡ªåŠ¨è¡¥å…¨æç¤ºï¼š\n\n```\ncomfy --install-completion\n```\n\n### å®‰è£… ComfyUI\n\nä½¿ç”¨ä»»æ„é«˜äºŽ 3.9 çš„ Python ç‰ˆæœ¬åˆ›å»ºä¸€ä¸ªè™šæ‹ŸçŽ¯å¢ƒã€‚\n\nå®‰è£… ComfyUI\n\n### è¿è¡Œ ComfyUI\n\n### ç®¡ç†è‡ªå®šä¹‰èŠ‚ç‚¹\n\n```\ncomfy node install <NODE_NAME>\n```\n\næˆ‘ä»¬ä½¿ç”¨ `cm-cli` æ¥å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹ã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜… [æ–‡æ¡£](https://github.com/ltdrdata/ComfyUI-Manager/blob/main/docs/en/cm-cli.md)ã€‚\n\n### ç®¡ç†æ¨¡åž‹\n\nä½¿ç”¨ `comfy-cli` ä¸‹è½½æ¨¡åž‹éžå¸¸ç®€å•ã€‚åªéœ€è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n\n```\ncomfy model download <url> models/checkpoints\n```\n\n### è´¡çŒ®\n\næˆ‘ä»¬é¼“åŠ±å¯¹ comfy-cli çš„è´¡çŒ®ï¼å¦‚æžœæ‚¨æœ‰å»ºè®®ã€æƒ³æ³•æˆ–é”™è¯¯æŠ¥å‘Šï¼Œè¯·åœ¨æˆ‘ä»¬çš„ [GitHub ä»“åº“](https://github.com/Comfy-Org/comfy-cli/issues) ä¸Šæäº¤é—®é¢˜ã€‚å¦‚æžœæ‚¨æƒ³è´¡çŒ®ä»£ç ï¼Œè¯· fork ä»“åº“å¹¶æäº¤ pull requestã€‚ æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [å¼€å‘æŒ‡å—](https://github.com/Comfy-Org/comfy-cli/blob/main/DEV_README.md)ã€‚\n\n### æ•°æ®åˆ†æž\n\næˆ‘ä»¬ä¼šè·Ÿè¸ª CLI çš„ä½¿ç”¨æƒ…å†µä»¥æ”¹è¿›ç”¨æˆ·ä½“éªŒã€‚æ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤ç¦ç”¨æ­¤åŠŸèƒ½ï¼š\n\nè¦é‡æ–°å¯ç”¨è·Ÿè¸ªï¼Œè¯·è¿è¡Œï¼š"
},
{
  "url": "https://docs.comfy.org/zh-CN/comfy-cli/reference",
  "markdown": "# å‚è€ƒ - ComfyUI\n\n## CLI\n\n## èŠ‚ç‚¹\n\n**ç”¨æ³•**:\n\n```\n$ comfy node [OPTIONS] COMMAND [ARGS]...\n```\n\n**é€‰é¡¹**:\n\n*   `--install-completion`: ä¸ºå½“å‰ shell å®‰è£…è‡ªåŠ¨è¡¥å…¨åŠŸèƒ½ã€‚\n*   `--show-completion`: æ˜¾ç¤ºå½“å‰ shell çš„è‡ªåŠ¨è¡¥å…¨åŠŸèƒ½ï¼Œå¯ç”¨äºŽå¤åˆ¶æˆ–è‡ªå®šä¹‰å®‰è£…ã€‚\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n**å‘½ä»¤**:\n\n*   `deps-in-workflow`\n*   `disable`\n*   `enable`\n*   `fix`\n*   `install`\n*   `install-deps`\n*   `reinstall`\n*   `restore-dependencies`\n*   `restore-snapshot`\n*   `save-snapshot`: ä¿å­˜å½“å‰ ComfyUI çŽ¯å¢ƒçš„å¿«ç…§â€¦\n*   `show`\n*   `simple-show`\n*   `uninstall`\n*   `update`\n\n### `deps-in-workflow`\n\n**ç”¨æ³•**:\n\n```\n$ deps-in-workflow [OPTIONS]\n```\n\n**é€‰é¡¹**:\n\n*   `--workflow TEXT`: å·¥ä½œæµæ–‡ä»¶ (.json/.png) \\[å¿…éœ€\\]\n*   `--output TEXT`: è¾“å‡ºæ–‡ä»¶ (.json/.png) \\[å¿…éœ€\\]\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `disable`\n\n**ç”¨æ³•**:\n\n```\n$ disable [OPTIONS] ARGS...\n```\n\n**å‚æ•°**:\n\n*   `ARGS...`: ç¦ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹ \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `enable`\n\n**ç”¨æ³•**:\n\n```\n$ enable [OPTIONS] ARGS...\n```\n\n**å‚æ•°**:\n\n*   `ARGS...`: å¯ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹ \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `fix`\n\n**ç”¨æ³•**:\n\n**å‚æ•°**:\n\n*   `ARGS...`: ä¿®å¤æŒ‡å®šè‡ªå®šä¹‰èŠ‚ç‚¹çš„ä¾èµ–é¡¹ \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `install`\n\n**ç”¨æ³•**:\n\n```\n$ install [OPTIONS] ARGS...\n```\n\n**å‚æ•°**:\n\n*   `ARGS...`: å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹ \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `install-deps`\n\n**ç”¨æ³•**:\n\n**é€‰é¡¹**:\n\n*   `--deps TEXT`: ä¾èµ–é¡¹è§„èŒƒæ–‡ä»¶ (.json)\n*   `--workflow TEXT`: å·¥ä½œæµæ–‡ä»¶ (.json/.png)\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `reinstall`\n\n**ç”¨æ³•**:\n\n```\n$ reinstall [OPTIONS] ARGS...\n```\n\n**å‚æ•°**:\n\n*   `ARGS...`: é‡æ–°å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹ \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `restore-dependencies`\n\n**ç”¨æ³•**:\n\n```\n$ restore-dependencies [OPTIONS]\n```\n\n**é€‰é¡¹**:\n\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `restore-snapshot`\n\n**ç”¨æ³•**:\n\n```\n$ restore-snapshot [OPTIONS] PATH\n```\n\n**å‚æ•°**:\n\n*   `PATH`: \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `save-snapshot`\n\nä¿å­˜å½“å‰ ComfyUI çŽ¯å¢ƒçš„å¿«ç…§ã€‚ **ç”¨æ³•**:\n\n```\n$ save-snapshot [OPTIONS]\n```\n\n**é€‰é¡¹**:\n\n*   `--output TEXT`: æŒ‡å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„ (.json/.yaml)ã€‚\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `show`\n\n**ç”¨æ³•**:\n\n**å‚æ•°**:\n\n*   `ARGS...`: \\[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list\\] \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `simple-show`\n\n**ç”¨æ³•**:\n\n```\n$ simple-show [OPTIONS] ARGS...\n```\n\n**å‚æ•°**:\n\n*   `ARGS...`: \\[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list\\] \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `uninstall`\n\n**ç”¨æ³•**:\n\n```\n$ uninstall [OPTIONS] ARGS...\n```\n\n**å‚æ•°**:\n\n*   `ARGS...`: å¸è½½è‡ªå®šä¹‰èŠ‚ç‚¹ \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `update`\n\n**ç”¨æ³•**:\n\n```\n$ update [OPTIONS] ARGS...\n```\n\n**å‚æ•°**:\n\n*   `ARGS...`: æ›´æ–°è‡ªå®šä¹‰èŠ‚ç‚¹ \\[å¿…éœ€\\]\n\n**é€‰é¡¹**:\n\n*   `--channel TEXT`: æŒ‡å®šæ“ä½œæ¨¡å¼\n*   `--mode TEXT`: \\[remote|local|cache\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n## æ¨¡åž‹\n\n**ç”¨æ³•**:\n\n```\n$ comfy model [OPTIONS] COMMAND [ARGS]...\n```\n\n**é€‰é¡¹**:\n\n*   `--install-completion`: ä¸ºå½“å‰ shell å®‰è£…è‡ªåŠ¨è¡¥å…¨åŠŸèƒ½ã€‚\n*   `--show-completion`: æ˜¾ç¤ºå½“å‰ shell çš„è‡ªåŠ¨è¡¥å…¨åŠŸèƒ½ï¼Œå¯ç”¨äºŽå¤åˆ¶æˆ–è‡ªå®šä¹‰å®‰è£…ã€‚\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n**å‘½ä»¤**:\n\n*   `download`: ä¸‹è½½æ¨¡åž‹åˆ°æŒ‡å®šçš„ç›¸å¯¹è·¯å¾„â€¦\n*   `list`: æ˜¾ç¤ºå½“å‰æ‰€æœ‰å·²ä¸‹è½½æ¨¡åž‹çš„åˆ—è¡¨â€¦\n*   `remove`: åˆ é™¤ä¸€ä¸ªæˆ–å¤šä¸ªå·²ä¸‹è½½çš„æ¨¡åž‹ï¼Œâ€¦\n\n### `download`\n\nå¦‚æžœæ¨¡åž‹å°šæœªä¸‹è½½ï¼Œåˆ™å°†å…¶ä¸‹è½½åˆ°æŒ‡å®šçš„ç›¸å¯¹è·¯å¾„ã€‚ **ç”¨æ³•**:\n\n**é€‰é¡¹**:\n\n*   `--url TEXT`: æ¨¡åž‹ä¸‹è½½çš„ URL \\[å¿…éœ€\\]\n*   `--relative-path TEXT`: ä»Žå½“å‰å·¥ä½œåŒºåˆ°å®‰è£…æ¨¡åž‹çš„ç›¸å¯¹è·¯å¾„ã€‚ \\[é»˜è®¤å€¼: models/checkpoints\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `list`\n\nä»¥è¡¨æ ¼æ ¼å¼æ˜¾ç¤ºå½“å‰å·²ä¸‹è½½çš„æ‰€æœ‰æ¨¡åž‹åˆ—è¡¨ã€‚ **ç”¨æ³•**:\n\n**é€‰é¡¹**:\n\n*   `--relative-path TEXT`: ä»Žå½“å‰å·¥ä½œåŒºåˆ°å­˜å‚¨æ¨¡åž‹çš„ç›¸å¯¹è·¯å¾„ã€‚ \\[é»˜è®¤å€¼: models/checkpoints\\]\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\n### `remove`\n\né€šè¿‡ç›´æŽ¥æŒ‡å®šæˆ–äº¤äº’å¼é€‰æ‹©ï¼Œåˆ é™¤ä¸€ä¸ªæˆ–å¤šä¸ªå·²ä¸‹è½½çš„æ¨¡åž‹ã€‚ **ç”¨æ³•**:\n\n**é€‰é¡¹**:\n\n*   `--relative-path TEXT`: ä»Žå½“å‰å·¥ä½œåŒºåˆ°å­˜å‚¨æ¨¡åž‹çš„ç›¸å¯¹è·¯å¾„ã€‚ \\[é»˜è®¤å€¼: models/checkpoints\\]\n*   `--model-names TEXT`: è¦åˆ é™¤çš„æ¨¡åž‹æ–‡ä»¶ååˆ—è¡¨ï¼Œç”¨ç©ºæ ¼åˆ†éš”ã€‚\n*   `--help`: æ˜¾ç¤ºæ­¤æ¶ˆæ¯å¹¶é€€å‡ºã€‚\n\nåœ¨æ­¤é¡µé¢\n\n*   [CLI](#cli)\n*   [èŠ‚ç‚¹](#%E8%8A%82%E7%82%B9)\n*   [deps-in-workflow](#deps-in-workflow)\n*   [disable](#disable)\n*   [enable](#enable)\n*   [fix](#fix)\n*   [install](#install)\n*   [install-deps](#install-deps)\n*   [reinstall](#reinstall)\n*   [restore-dependencies](#restore-dependencies)\n*   [restore-snapshot](#restore-snapshot)\n*   [save-snapshot](#save-snapshot)\n*   [show](#show)\n*   [simple-show](#simple-show)\n*   [uninstall](#uninstall)\n*   [update](#update)\n*   [æ¨¡åž‹](#%E6%A8%A1%E5%9E%8B)\n*   [download](#download)\n*   [list](#list)\n*   [remove](#remove)"
},
{
  "url": "https://docs.comfy.org/zh-CN/comfy-cli/troubleshooting",
  "markdown": "# å¼€å§‹ - ComfyUI\n\n### \n\n[â€‹](#%E5%89%8D%E6%8F%90%E6%9D%A1%E4%BB%B6)\n\nå‰ææ¡ä»¶\n\nä½ éœ€è¦åœ¨ä½ çš„ç³»ç»Ÿä¸Šå·²ç»å®‰è£…å¥½äº† Gitã€‚å¯ä»¥[åœ¨è¿™é‡Œ](https://git-scm.com/downloads)ä¸‹è½½ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/datatypes",
  "markdown": "# æ•°æ®ç±»åž‹ - ComfyUI\n\nè¿™äº›æ˜¯æœ€é‡è¦çš„å†…ç½®æ•°æ®ç±»åž‹ã€‚æ‚¨ä¹Ÿå¯ä»¥[å®šä¹‰è‡ªå·±çš„æ•°æ®ç±»åž‹](https://docs.comfy.org/zh-CN/custom-nodes/backend/more_on_inputs#custom-datatypes)ã€‚ æ•°æ®ç±»åž‹åœ¨å®¢æˆ·ç«¯ç”¨äºŽé˜²æ­¢å·¥ä½œæµå°†é”™è¯¯å½¢å¼çš„æ•°æ®ä¼ é€’ç»™èŠ‚ç‚¹ - æœ‰ç‚¹åƒå¼ºç±»åž‹ã€‚ JavaScript å®¢æˆ·ç«¯ä»£ç é€šå¸¸ä¸å…è®¸å°†èŠ‚ç‚¹è¾“å‡ºè¿žæŽ¥åˆ°ä¸åŒæ•°æ®ç±»åž‹çš„è¾“å…¥ï¼Œ ä¸è¿‡ä¸‹é¢ä¼šæåˆ°ä¸€äº›ä¾‹å¤–æƒ…å†µã€‚\n\n## Comfy æ•°æ®ç±»åž‹\n\n### COMBO\n\n*   `INPUT_TYPES` ä¸­ä¸éœ€è¦é¢å¤–å‚æ•°\n*   Python æ•°æ®ç±»åž‹ï¼šå®šä¹‰ä¸º `list[str]`ï¼Œè¾“å‡ºå€¼ä¸º `str`\n\nè¡¨ç¤ºä¸‹æ‹‰èœå•ç»„ä»¶ã€‚ ä¸Žå…¶ä»–æ•°æ®ç±»åž‹ä¸åŒï¼Œ`COMBO` åœ¨ `INPUT_TYPES` ä¸­ä¸æ˜¯é€šè¿‡ `str` æŒ‡å®šçš„ï¼Œè€Œæ˜¯é€šè¿‡ `list[str]` æŒ‡å®šï¼Œ å¯¹åº”ä¸‹æ‹‰åˆ—è¡¨ä¸­çš„é€‰é¡¹ï¼Œé»˜è®¤é€‰ä¸­ç¬¬ä¸€ä¸ªé€‰é¡¹ã€‚ `COMBO` è¾“å…¥é€šå¸¸åœ¨è¿è¡Œæ—¶åŠ¨æ€ç”Ÿæˆã€‚ä¾‹å¦‚ï¼Œåœ¨å†…ç½®çš„ `CheckpointLoaderSimple` èŠ‚ç‚¹ä¸­ï¼Œæ‚¨ä¼šçœ‹åˆ°\n\n```\n\"ckpt_name\": (folder_paths.get_filename_list(\"checkpoints\"), )\n```\n\nor they might just be a fixed list of options,\n\n```\n\"play_sound\": ([\"no\",\"yes\"], {}),\n```\n\n### åŽŸå§‹ç±»åž‹å’Œé‡è·¯ç”±\n\nåŽŸå§‹ç±»åž‹å’Œé‡è·¯ç”±èŠ‚ç‚¹ä»…å­˜åœ¨äºŽå®¢æˆ·ç«¯ã€‚å®ƒä»¬æ²¡æœ‰å›ºæœ‰çš„æ•°æ®ç±»åž‹ï¼Œä½†åœ¨è¿žæŽ¥æ—¶ä¼šé‡‡ç”¨æ‰€è¿žæŽ¥è¾“å…¥æˆ–è¾“å‡ºçš„æ•°æ®ç±»åž‹ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒä»¬ä¸èƒ½è¿žæŽ¥åˆ° `*` è¾“å…¥â€¦ï¼‰\n\n## Python æ•°æ®ç±»åž‹\n\n### INT\n\n*   `INPUT_TYPES` ä¸­çš„é¢å¤–å‚æ•°ï¼š\n    *   `default` æ˜¯å¿…éœ€çš„\n    *   `min` å’Œ `max` æ˜¯å¯é€‰çš„\n*   Python æ•°æ®ç±»åž‹ `int`\n\n### FLOAT\n\n*   `INPUT_TYPES` ä¸­çš„é¢å¤–å‚æ•°ï¼š\n    *   `default` æ˜¯å¿…éœ€çš„\n    *   `min`ã€`max`ã€`step` æ˜¯å¯é€‰çš„\n*   Python æ•°æ®ç±»åž‹ `float`\n\n### STRING\n\n*   `INPUT_TYPES` ä¸­çš„é¢å¤–å‚æ•°ï¼š\n    *   `default` æ˜¯å¿…éœ€çš„\n*   Python æ•°æ®ç±»åž‹ `str`\n\n### BOOLEAN\n\n*   `INPUT_TYPES` ä¸­çš„é¢å¤–å‚æ•°ï¼š\n    *   `default` æ˜¯å¿…éœ€çš„\n*   Python æ•°æ®ç±»åž‹ `bool`\n\n## å¼ é‡æ•°æ®ç±»åž‹\n\n### IMAGE\n\n*   `INPUT_TYPES` ä¸­ä¸éœ€è¦é¢å¤–å‚æ•°\n*   Python æ•°æ®ç±»åž‹ `torch.Tensor`ï¼Œå½¢çŠ¶ä¸º \\[B,H,W,C\\]\n\nä¸€æ‰¹ `B` å¼ å›¾åƒï¼Œé«˜åº¦ `H`ï¼Œå®½åº¦ `W`ï¼Œå…·æœ‰ `C` ä¸ªé€šé“ï¼ˆé€šå¸¸ `C=3` è¡¨ç¤º `RGB`ï¼‰ã€‚\n\n### LATENT\n\n*   `INPUT_TYPES` ä¸­ä¸éœ€è¦é¢å¤–å‚æ•°\n*   Python æ•°æ®ç±»åž‹ `dict`ï¼ŒåŒ…å«ä¸€ä¸ªå½¢çŠ¶ä¸º \\[B,C,H,W\\] çš„ `torch.Tensor`\n\nä¼ å…¥çš„ `dict` åŒ…å«é”® `samples`ï¼Œè¿™æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º \\[B,C,H,W\\] çš„ `torch.Tensor`ï¼Œè¡¨ç¤º ä¸€æ‰¹ `B` ä¸ªæ½œç©ºé—´è¡¨ç¤ºï¼Œå…·æœ‰ `C` ä¸ªé€šé“ï¼ˆçŽ°æœ‰ stable diffusion æ¨¡åž‹é€šå¸¸ `C=4`ï¼‰ï¼Œé«˜åº¦ `H`ï¼Œå®½åº¦ `W`ã€‚ é«˜åº¦å’Œå®½åº¦æ˜¯å¯¹åº”å›¾åƒå°ºå¯¸çš„ 1/8ï¼ˆè¿™æ˜¯æ‚¨åœ¨ Empty Latent Image èŠ‚ç‚¹ä¸­è®¾ç½®çš„å€¼ï¼‰ã€‚ å­—å…¸ä¸­çš„å…¶ä»–æ¡ç›®åŒ…å«æ½œç©ºé—´è’™ç‰ˆç­‰å†…å®¹ã€‚\n\n### MASK\n\n*   `INPUT_TYPES` ä¸­ä¸éœ€è¦é¢å¤–å‚æ•°\n*   Python æ•°æ®ç±»åž‹ `torch.Tensor`ï¼Œå½¢çŠ¶ä¸º \\[H,W\\] æˆ– \\[B,C,H,W\\]\n\n### AUDIO\n\n*   `INPUT_TYPES` ä¸­ä¸éœ€è¦é¢å¤–å‚æ•°\n*   Python æ•°æ®ç±»åž‹ `dict`ï¼ŒåŒ…å«ä¸€ä¸ªå½¢çŠ¶ä¸º \\[B, C, T\\] çš„ `torch.Tensor` å’Œé‡‡æ ·çŽ‡ã€‚\n\nä¼ å…¥çš„ `dict` åŒ…å«é”® `waveform`ï¼Œè¿™æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º \\[B, C, T\\] çš„ `torch.Tensor`ï¼Œè¡¨ç¤º ä¸€æ‰¹ `B` ä¸ªéŸ³é¢‘æ ·æœ¬ï¼Œå…·æœ‰ `C` ä¸ªé€šé“ï¼ˆ`C=2` è¡¨ç¤ºç«‹ä½“å£°ï¼Œ`C=1` è¡¨ç¤ºå•å£°é“ï¼‰ï¼Œä»¥åŠ `T` ä¸ªæ—¶é—´æ­¥ï¼ˆå³éŸ³é¢‘æ ·æœ¬çš„æ•°é‡ï¼‰ã€‚ `dict` è¿˜åŒ…å«å¦ä¸€ä¸ªé”® `sample_rate`ï¼Œè¡¨ç¤ºéŸ³é¢‘çš„é‡‡æ ·çŽ‡ã€‚\n\n## è‡ªå®šä¹‰é‡‡æ ·æ•°æ®ç±»åž‹\n\n### Noise\n\n`NOISE` æ•°æ®ç±»åž‹è¡¨ç¤ºå™ªå£°çš„_æ¥æº_ï¼ˆè€Œä¸æ˜¯å™ªå£°æœ¬èº«ï¼‰ã€‚å®ƒå¯ä»¥ç”±ä»»ä½•æä¾›ç”Ÿæˆå™ªå£°æ–¹æ³•çš„ Python å¯¹è±¡è¡¨ç¤ºï¼Œ æ–¹æ³•ç­¾åä¸º `generate_noise(self, input_latent:Tensor) -> Tensor`ï¼Œä»¥åŠä¸€ä¸ªå±žæ€§ `seed:Optional[int]`ã€‚\n\nå½“éœ€è¦æ·»åŠ å™ªå£°æ—¶ï¼Œæ½œç©ºé—´è¡¨ç¤ºä¼šè¢«ä¼ å…¥è¿™ä¸ªæ–¹æ³•ï¼Œå®ƒåº”è¯¥è¿”å›žä¸€ä¸ªåŒ…å«å™ªå£°çš„ç›¸åŒå½¢çŠ¶çš„ `Tensor`ã€‚ å‚è§[å™ªå£°æ··åˆç¤ºä¾‹](https://docs.comfy.org/zh-CN/custom-nodes/backend/snippets#creating-noise-variations)\n\n### Sampler\n\n`SAMPLER` æ•°æ®ç±»åž‹è¡¨ç¤ºä¸€ä¸ªé‡‡æ ·å™¨ï¼Œå®ƒç”±ä¸€ä¸ªæä¾› `sample` æ–¹æ³•çš„ Python å¯¹è±¡è¡¨ç¤ºã€‚ Stable diffusion é‡‡æ ·è¶…å‡ºäº†æœ¬æŒ‡å—çš„èŒƒå›´ï¼›å¦‚æžœæ‚¨æƒ³æ·±å…¥ç ”ç©¶è¿™éƒ¨åˆ†ä»£ç ï¼Œè¯·æŸ¥çœ‹ `comfy/samplers.py`ã€‚\n\n### Sigmas\n\n`SIGMAS` æ•°æ®ç±»åž‹è¡¨ç¤ºç”±è°ƒåº¦å™¨äº§ç”Ÿçš„é‡‡æ ·è¿‡ç¨‹ä¸­æ¯ä¸ªæ­¥éª¤å‰åŽçš„ sigma å€¼ã€‚ å®ƒè¡¨ç¤ºä¸ºä¸€ä¸ªä¸€ç»´å¼ é‡ï¼Œé•¿åº¦ä¸º `steps+1`ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”æ­¥éª¤ä¹‹å‰é¢„æœŸçš„å™ªå£°é‡ï¼Œ æœ€åŽä¸€ä¸ªå€¼è¡¨ç¤ºæœ€ç»ˆæ­¥éª¤ä¹‹åŽçš„å™ªå£°é‡ã€‚ å¯¹äºŽ SDXL æ¨¡åž‹ï¼Œä¸€ä¸ªå…·æœ‰ 20 æ­¥å’ŒåŽ»å™ªå€¼ä¸º 1 çš„ `normal` è°ƒåº¦å™¨ä¼šäº§ç”Ÿï¼š\n\n```\ntensor([14.6146, 10.7468,  8.0815,  6.2049,  4.8557,  \n         3.8654,  3.1238,  2.5572,  2.1157,  1.7648,  \n         1.4806,  1.2458,  1.0481,  0.8784,  0.7297,  \n         0.5964,  0.4736,  0.3555,  0.2322,  0.0292,  0.0000])\n```\n\n### Guider\n\n`GUIDER` æ˜¯åŽ»å™ªè¿‡ç¨‹çš„æ³›åŒ–ï¼Œç”±æç¤ºè¯æˆ–ä»»ä½•å…¶ä»–å½¢å¼çš„æ¡ä»¶â€å¼•å¯¼â€ã€‚åœ¨ Comfy ä¸­ï¼Œguider ç”± ä¸€ä¸ªæä¾› `__call__(*args, **kwargs)` æ–¹æ³•çš„å¯è°ƒç”¨ Python å¯¹è±¡è¡¨ç¤ºï¼Œè¯¥æ–¹æ³•ç”±é‡‡æ ·å™¨è°ƒç”¨ã€‚ `__call__` æ–¹æ³•æŽ¥æ”¶ï¼ˆåœ¨ `args[0]` ä¸­ï¼‰ä¸€æ‰¹å™ªå£°æ½œç©ºé—´è¡¨ç¤ºï¼ˆå¼ é‡ `[B,C,H,W]`ï¼‰ï¼Œå¹¶è¿”å›žå™ªå£°çš„é¢„æµ‹ï¼ˆç›¸åŒå½¢çŠ¶çš„å¼ é‡ï¼‰ã€‚\n\n## æ¨¡åž‹æ•°æ®ç±»åž‹\n\nç¨³å®šæ‰©æ•£æ¨¡åž‹è¿˜æœ‰ä¸€äº›æ›´æŠ€æœ¯æ€§çš„æ•°æ®ç±»åž‹ã€‚æœ€é‡è¦çš„æ˜¯ `MODEL`ã€`CLIP`ã€`VAE` å’Œ `CONDITIONING`ã€‚ ç›®å‰è¿™äº›å†…å®¹è¶…å‡ºäº†æœ¬æŒ‡å—çš„èŒƒå›´ï¼\n\n## é™„åŠ å‚æ•°\n\nä»¥ä¸‹æ˜¯è¾“å…¥å®šä¹‰çš„â€é¢å¤–é€‰é¡¹â€éƒ¨åˆ†å¯ä»¥ä½¿ç”¨çš„å®˜æ–¹æ”¯æŒé”®çš„åˆ—è¡¨ã€‚\n\n| é”®å  | æè¿°  |\n| --- | --- |\n| `default` | æŽ§ä»¶çš„é»˜è®¤å€¼ |\n| `min` | æ•°å­—ç±»åž‹(`FLOAT` æˆ– `INT`)çš„æœ€å°å€¼ |\n| `max` | æ•°å­—ç±»åž‹(`FLOAT` æˆ– `INT`)çš„æœ€å¤§å€¼ |\n| `step` | æŽ§ä»¶çš„å¢žå‡æ­¥é•¿ |\n| `label_on` | å¸ƒå°”å€¼ä¸º `True` æ—¶åœ¨ UI ä¸­æ˜¾ç¤ºçš„æ ‡ç­¾ (`BOOL`) |\n| `label_off` | å¸ƒå°”å€¼ä¸º `False` æ—¶åœ¨ UI ä¸­æ˜¾ç¤ºçš„æ ‡ç­¾ (`BOOL`) |\n| `defaultInput` | é»˜è®¤ä½¿ç”¨è¾“å…¥æ’æ§½è€Œä¸æ˜¯æ”¯æŒçš„æŽ§ä»¶ |\n| `forceInput` | ä¸Ž `defaultInput` ç›¸åŒï¼Œä¸”ä¸å…è®¸è½¬æ¢ä¸ºæŽ§ä»¶ |\n| `multiline` | ä½¿ç”¨å¤šè¡Œæ–‡æœ¬æ¡† (`STRING`) |\n| `placeholder` | å½“ä¸ºç©ºæ—¶åœ¨ UI ä¸­æ˜¾ç¤ºçš„å ä½æ–‡æœ¬ (`STRING`) |\n| `dynamicPrompts` | ä½¿å‰ç«¯è¯„ä¼°åŠ¨æ€æç¤ºè¯ |\n| `lazy` | å£°æ˜Žæ­¤è¾“å…¥ä½¿ç”¨[å»¶è¿Ÿæ±‚å€¼](https://docs.comfy.org/zh-CN/custom-nodes/backend/lazy_evaluation) |\n| `rawLink` | å½“å­˜åœ¨é“¾æŽ¥æ—¶ï¼Œæ‚¨å°†æ”¶åˆ°é“¾æŽ¥è€Œä¸æ˜¯æ±‚å€¼åŽçš„å€¼ï¼ˆå³ `[\"nodeId\", <outputIndex>]`ï¼‰ã€‚ä¸»è¦åœ¨èŠ‚ç‚¹ä½¿ç”¨[èŠ‚ç‚¹æ‰©å±•](https://docs.comfy.org/zh-CN/custom-nodes/backend/expansion)æ—¶æœ‰ç”¨ã€‚ |"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/expansion",
  "markdown": "# èŠ‚ç‚¹æ‰©å±• - ComfyUI\n\n## èŠ‚ç‚¹æ‰©å±•\n\né€šå¸¸ï¼Œå½“èŠ‚ç‚¹æ‰§è¡Œæ—¶ï¼Œæ‰§è¡Œå‡½æ•°ä¼šç«‹å³è¿”å›žè¯¥èŠ‚ç‚¹çš„è¾“å‡ºç»“æžœã€‚â€œèŠ‚ç‚¹æ‰©å±•â€æ˜¯ä¸€ç§ç›¸å¯¹é«˜çº§çš„æŠ€æœ¯ï¼Œå…è®¸èŠ‚ç‚¹è¿”å›žä¸€ä¸ªæ–°çš„å­å›¾èŠ‚ç‚¹ï¼Œè¯¥å­å›¾å°†æ›¿ä»£åŽŸèŠ‚ç‚¹åœ¨å›¾ä¸­ã€‚è¿™ç§æŠ€æœ¯ä½¿è‡ªå®šä¹‰èŠ‚ç‚¹èƒ½å¤Ÿå®žçŽ°å¾ªçŽ¯åŠŸèƒ½ã€‚\n\n### ç®€å•ç¤ºä¾‹\n\né¦–å…ˆï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªèŠ‚ç‚¹æ‰©å±•çš„ç®€å•ç¤ºä¾‹ï¼š\n\n```\ndef load_and_merge_checkpoints(self, checkpoint_path1, checkpoint_path2, ratio):\n    from comfy_execution.graph_utils import GraphBuilder # é€šå¸¸åœ¨æ–‡ä»¶é¡¶éƒ¨\n    graph = GraphBuilder()\n    checkpoint_node1 = graph.node(\"CheckpointLoaderSimple\", checkpoint_path=checkpoint_path1)\n    checkpoint_node2 = graph.node(\"CheckpointLoaderSimple\", checkpoint_path=checkpoint_path2)\n    merge_model_node = graph.node(\"ModelMergeSimple\", model1=checkpoint_node1.out(0), model2=checkpoint_node2.out(0), ratio=ratio)\n    merge_clip_node = graph.node(\"ClipMergeSimple\", clip1=checkpoint_node1.out(1), clip2=checkpoint_node2.out(1), ratio=ratio)\n    return {\n        # è¿”å›ž (MODEL, CLIP, VAE) è¾“å‡º\n        \"result\": (merge_model_node.out(0), merge_clip_node.out(0), checkpoint_node1.out(2)),\n        \"expand\": graph.finalize(),\n    }\n```\n\nè™½ç„¶è¿™ä¸ªèŠ‚ç‚¹ä»¥å‰å¯ä»¥é€šè¿‡æ‰‹åŠ¨è°ƒç”¨ ComfyUI å†…éƒ¨æ¥å®žçŽ°ï¼Œä½†ä½¿ç”¨æ‰©å±•æ„å‘³ç€æ¯ä¸ªå­èŠ‚ç‚¹å°†å•ç‹¬ç¼“å­˜ï¼ˆæ‰€ä»¥å¦‚æžœä½ æ›´æ”¹ `model2`ï¼Œä½ ä¸éœ€è¦é‡æ–°åŠ è½½ `model1`ï¼‰ã€‚\n\n### è¦æ±‚\n\nä¸ºäº†æ‰§è¡ŒèŠ‚ç‚¹æ‰©å±•ï¼Œä¸€ä¸ªèŠ‚ç‚¹å¿…é¡»è¿”å›žä¸€ä¸ªåŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š\n\n1.  `result`: ä¸€ä¸ªåŒ…å«èŠ‚ç‚¹è¾“å‡ºçš„å…ƒç»„ã€‚è¿™å¯èƒ½æ˜¯ä¸€ä¸ªæ··åˆçš„æœ€ç»ˆå€¼ï¼ˆåƒä½ ä»Žæ­£å¸¸èŠ‚ç‚¹è¿”å›žçš„é‚£æ ·ï¼‰å’ŒèŠ‚ç‚¹è¾“å‡ºã€‚\n2.  `expand`: è¦æ‰§è¡Œæ‰©å±•çš„æœ€ç»ˆå›¾ã€‚å¦‚æžœæ‚¨ä¸ä½¿ç”¨ `GraphBuilder`ï¼Œè¯·å‚è§ä¸‹æ–‡ã€‚\n\n#### ä¸ä½¿ç”¨ GraphBuilder çš„é¢å¤–è¦æ±‚\n\n`expand` é”®çš„æ ¼å¼ä¸Ž ComfyUI API æ ¼å¼ç›¸åŒã€‚ä»¥ä¸‹è¦æ±‚ç”± `GraphBuilder` å¤„ç†ï¼Œä½†å¦‚æžœæ‚¨é€‰æ‹©ä¸ä½¿ç”¨å®ƒï¼Œåˆ™å¿…é¡»æ‰‹åŠ¨å¤„ç†ï¼š\n\n1.  èŠ‚ç‚¹ ID å¿…é¡»åœ¨æ•´ä¸ªå›¾ä¸­å”¯ä¸€ã€‚ï¼ˆè¿™åŒ…æ‹¬åœ¨å¤šæ¬¡ä½¿ç”¨åˆ—è¡¨æ—¶ç”±äºŽä½¿ç”¨åˆ—è¡¨è€Œå¯¼è‡´çš„åŒä¸€èŠ‚ç‚¹çš„å¤šæ¬¡æ‰§è¡Œã€‚ï¼‰\n2.  èŠ‚ç‚¹ ID å¿…é¡»ç¡®å®šä¸”åœ¨å¤šæ¬¡æ‰§è¡Œå›¾ä¸­ä¸€è‡´ï¼ˆåŒ…æ‹¬ç”±äºŽç¼“å­˜è€Œå¯¼è‡´çš„éƒ¨åˆ†æ‰§è¡Œï¼‰ã€‚\n\nå³ä½¿æ‚¨ä¸æƒ³ä½¿ç”¨ `GraphBuilder` æ¥å®žé™…æž„å»ºå›¾ï¼ˆä¾‹å¦‚ï¼Œå› ä¸ºæ‚¨ä»Žæ–‡ä»¶åŠ è½½äº†å›¾çš„åŽŸå§‹ JSONï¼‰ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ `GraphBuilder.alloc_prefix()` å‡½æ•°ç”Ÿæˆä¸€ä¸ªå‰ç¼€ï¼Œå¹¶ä½¿ç”¨ `comfy.graph_utils.add_graph_prefix` ä¿®å¤çŽ°æœ‰å›¾ä»¥æ»¡è¶³è¿™äº›è¦æ±‚ã€‚\n\n### é«˜æ•ˆçš„å­å›¾ç¼“å­˜\n\nè™½ç„¶æ‚¨å¯ä»¥å‘å­å›¾ä¸­çš„èŠ‚ç‚¹ä¼ é€’éžæ–‡å­—è¾“å…¥ï¼ˆå¦‚ torch å¼ é‡ï¼‰ï¼Œä½†è¿™å¯èƒ½ä¼šæŠ‘åˆ¶å­å›¾å†…éƒ¨çš„ç¼“å­˜ã€‚å½“å¯èƒ½æ—¶ï¼Œæ‚¨åº”è¯¥ä¼ é€’å­å›¾å¯¹è±¡çš„é“¾æŽ¥ï¼Œè€Œä¸æ˜¯èŠ‚ç‚¹æœ¬èº«ã€‚ï¼ˆæ‚¨å¯ä»¥åœ¨è¾“å…¥çš„[é™„åŠ å‚æ•°](https://docs.comfy.org/zh-CN/custom-nodes/backend/datatypes#additional-parameters)ä¸­å£°æ˜Žä¸€ä¸ª `rawLink` æ¥è½»æ¾å®žçŽ°è¿™ä¸€ç‚¹ã€‚ï¼‰"
},
{
  "url": "https://docs.comfy.org/zh-CN/changelog/index",
  "markdown": "# æ›´æ–°æ—¥å¿— - ComfyUI\n\n**é«˜çº§é‡‡æ ·ä¸Žè®­ç»ƒåŸºç¡€è®¾æ–½æ”¹è¿›**æœ¬ç‰ˆæœ¬ä¸ºAIç ”ç©¶äººå‘˜å’Œå·¥ä½œæµç¨‹åˆ›å»ºè€…å¼•å…¥äº†é‡‡æ ·ç®—æ³•ã€è®­ç»ƒåŠŸèƒ½å’ŒèŠ‚ç‚¹åŠŸèƒ½çš„é‡å¤§å¢žå¼ºï¼š\n\n## æ–°çš„é‡‡æ ·å’Œç”ŸæˆåŠŸèƒ½\n\n*   **SA-Solveré‡‡æ ·å™¨**ï¼šæ–°çš„é‡æž„SA-Solveré‡‡æ ·ç®—æ³•ï¼Œä¸ºå¤æ‚ç”Ÿæˆå·¥ä½œæµæä¾›å¢žå¼ºçš„æ•°å€¼ç¨³å®šæ€§å’Œè´¨é‡\n*   **å®žéªŒæ€§CFGNormèŠ‚ç‚¹**ï¼šé«˜çº§æ— åˆ†ç±»å™¨å¼•å¯¼æ ‡å‡†åŒ–ï¼Œç”¨äºŽæ”¹è¿›ç”Ÿæˆè´¨é‡å’Œé£Žæ ¼ä¸€è‡´æ€§çš„æŽ§åˆ¶\n*   **åµŒå¥—åŒCFGæ”¯æŒ**ï¼šä¸ºDualCFGGuiderèŠ‚ç‚¹æ·»åŠ åµŒå¥—é£Žæ ¼é…ç½®ï¼Œæä¾›æ›´å¤æ‚çš„å¼•å¯¼æŽ§åˆ¶æ¨¡å¼\n*   **SamplingPercentToSigmaèŠ‚ç‚¹**ï¼šç”¨äºŽä»Žé‡‡æ ·ç™¾åˆ†æ¯”ç²¾ç¡®è®¡ç®—sigmaçš„æ–°å®žç”¨èŠ‚ç‚¹ï¼Œæé«˜å·¥ä½œæµç¨‹çµæ´»æ€§\n\n## å¢žå¼ºçš„è®­ç»ƒåŠŸèƒ½\n\n*   **å¤šå›¾åƒ-æè¿°æ•°æ®é›†æ”¯æŒ**ï¼šLoRAè®­ç»ƒèŠ‚ç‚¹çŽ°åœ¨å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªå›¾åƒ-æè¿°æ•°æ®é›†ï¼Œç®€åŒ–è®­ç»ƒå·¥ä½œæµç¨‹\n*   **æ›´å¥½çš„è®­ç»ƒå¾ªçŽ¯å®žçŽ°**ï¼šä¼˜åŒ–çš„è®­ç»ƒç®—æ³•ï¼Œåœ¨æ¨¡åž‹å¾®è°ƒè¿‡ç¨‹ä¸­æ”¹å–„æ”¶æ•›æ€§å’Œç¨³å®šæ€§\n*   **å¢žå¼ºçš„é”™è¯¯æ£€æµ‹**ï¼šä¸ºLoRAæ“ä½œæ·»åŠ æ¨¡åž‹æ£€æµ‹é”™è¯¯æç¤ºï¼Œåœ¨å‡ºçŽ°é—®é¢˜æ—¶æä¾›æ›´æ¸…æ™°çš„åé¦ˆ\n\n## å¹³å°å’Œæ€§èƒ½æ”¹è¿›\n\n*   **å¼‚æ­¥èŠ‚ç‚¹æ”¯æŒ**ï¼šå®Œå…¨æ”¯æŒå¼‚æ­¥èŠ‚ç‚¹å‡½æ•°ï¼Œä¼˜åŒ–æ—©æœŸæ‰§è¡Œï¼Œæ”¹å–„I/Oå¯†é›†åž‹æ“ä½œçš„å·¥ä½œæµç¨‹æ€§èƒ½\n*   **Chromaçµæ´»æ€§**ï¼šåœ¨Chromaä¸­å–æ¶ˆç¡¬ç¼–ç çš„patch\\_sizeå‚æ•°ï¼Œå…è®¸æ›´å¥½åœ°é€‚åº”ä¸åŒçš„æ¨¡åž‹é…ç½®\n*   **LTXV VAEè§£ç å™¨**ï¼šåˆ‡æ¢åˆ°æ”¹è¿›çš„é»˜è®¤å¡«å……æ¨¡å¼ï¼Œæé«˜LTXVæ¨¡åž‹çš„å›¾åƒè´¨é‡\n*   **Safetensorså†…å­˜ç®¡ç†**ï¼šä¸ºmmapé—®é¢˜æ·»åŠ è§£å†³æ–¹æ¡ˆï¼Œæé«˜åŠ è½½å¤§åž‹æ¨¡åž‹æ–‡ä»¶æ—¶çš„å¯é æ€§\n\n## APIå’Œé›†æˆå¢žå¼º\n\n*   **è‡ªå®šä¹‰æç¤ºID**ï¼šAPIçŽ°åœ¨å…è®¸æŒ‡å®šæç¤ºIDï¼Œä»¥ä¾¿æ›´å¥½åœ°è·Ÿè¸ªå’Œç®¡ç†å·¥ä½œæµç¨‹\n*   **Kling APIä¼˜åŒ–**ï¼šå¢žåŠ è½®è¯¢è¶…æ—¶æ—¶é—´ï¼Œé˜²æ­¢è§†é¢‘ç”Ÿæˆå·¥ä½œæµç¨‹ä¸­çš„ç”¨æˆ·è¶…æ—¶\n*   **åŽ†å²ä»¤ç‰Œæ¸…ç†**ï¼šä»ŽåŽ†å²é¡¹ç›®ä¸­åˆ é™¤æ•æ„Ÿä»¤ç‰Œä»¥æé«˜å®‰å…¨æ€§\n*   **Python 3.9å…¼å®¹æ€§**ï¼šä¿®å¤å…¼å®¹æ€§é—®é¢˜ï¼Œç¡®ä¿æ›´å¹¿æ³›çš„å¹³å°æ”¯æŒ\n\n## é”™è¯¯ä¿®å¤å’Œç¨³å®šæ€§\n\n*   **MaskCompositeä¿®å¤**ï¼šè§£å†³ç›®æ ‡è’™ç‰ˆå…·æœ‰2ä¸ªç»´åº¦æ—¶çš„é”™è¯¯ï¼Œæé«˜è’™ç‰ˆå·¥ä½œæµç¨‹å¯é æ€§\n*   **Frescaè¾“å…¥/è¾“å‡º**ï¼šä¿®æ­£Frescaæ¨¡åž‹å·¥ä½œæµç¨‹çš„è¾“å…¥å’Œè¾“å‡ºå¤„ç†\n*   **å¼•ç”¨é”™è¯¯ä¿®å¤**ï¼šè§£å†³GeminièŠ‚ç‚¹å®žçŽ°ä¸­çš„é”™è¯¯å¼•ç”¨é—®é¢˜\n*   **è¡Œç»“æŸæ ‡å‡†åŒ–**ï¼šè‡ªåŠ¨æ£€æµ‹å’Œåˆ é™¤Windowsè¡Œç»“æŸç¬¦ï¼Œç¡®ä¿è·¨å¹³å°ä¸€è‡´æ€§\n\n## å¼€å‘è€…ä½“éªŒ\n\n*   **è­¦å‘Šç³»ç»Ÿ**ï¼šæ·»åŠ torchå¯¼å…¥é”™è¯¯è­¦å‘Šï¼Œä»¥æ•èŽ·å¸¸è§é…ç½®é—®é¢˜\n*   **æ¨¡æ¿æ›´æ–°**ï¼šå¤šä¸ªæ¨¡æ¿ç‰ˆæœ¬æ›´æ–°ï¼ˆ0.1.36ã€0.1.37ã€0.1.39ï¼‰ï¼Œæ”¹è¿›è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘\n*   **æ–‡æ¡£**ï¼šå¢žå¼ºä¾¿æºå¼é…ç½®ä¸­fast\\_fp16\\_accumulationçš„æ–‡æ¡£\n\nè¿™äº›æ”¹è¿›ä½¿ComfyUIåœ¨ç”Ÿäº§å·¥ä½œæµç¨‹ä¸­æ›´åŠ ç¨³å¥ï¼ŒåŒæ—¶å¼•å…¥äº†å¯¹é«˜çº§AIç ”ç©¶å’Œåˆ›æ„åº”ç”¨å¿…ä¸å¯å°‘çš„å¼ºå¤§æ–°é‡‡æ ·æŠ€æœ¯å’Œè®­ç»ƒåŠŸèƒ½ã€‚\n\n**é«˜çº§é‡‡æ ·å’Œæ¨¡åž‹æŽ§åˆ¶å¢žå¼º**æ­¤ç‰ˆæœ¬åœ¨é‡‡æ ·ç®—æ³•å’Œæ¨¡åž‹æŽ§åˆ¶ç³»ç»Ÿæ–¹é¢æä¾›äº†é‡å¤§æ”¹è¿›ï¼Œç‰¹åˆ«æœ‰åˆ©äºŽé«˜çº§AIç ”ç©¶äººå‘˜å’Œå·¥ä½œæµåˆ›å»ºè€…ï¼š\n\n## æ–°é‡‡æ ·åŠŸèƒ½\n\n*   **TCFGèŠ‚ç‚¹**ï¼šå¢žå¼ºçš„åˆ†ç±»å™¨æ— å…³å¼•å¯¼æŽ§åˆ¶ï¼Œä¸ºæ‚¨çš„å·¥ä½œæµæä¾›æ›´ç»†è‡´çš„ç”ŸæˆæŽ§åˆ¶\n*   **ER-SDEé‡‡æ ·å™¨**ï¼šä»ŽVEè¿ç§»åˆ°VPç®—æ³•ï¼Œé…å¤‡æ–°çš„é‡‡æ ·å™¨èŠ‚ç‚¹ï¼Œä¸ºå¤æ‚ç”Ÿæˆä»»åŠ¡æä¾›æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§\n*   **è·³å±‚å¼•å¯¼ï¼ˆSLGï¼‰**ï¼šç”¨äºŽæŽ¨ç†æœŸé—´ç²¾ç¡®å±‚çº§æŽ§åˆ¶çš„æ›¿ä»£å®žçŽ°ï¼Œå®Œç¾Žé€‚ç”¨äºŽé«˜çº§æ¨¡åž‹å¯¼å‘å·¥ä½œæµ\n\n## å¢žå¼ºçš„å¼€å‘å·¥å…·\n\n*   **è‡ªå®šä¹‰èŠ‚ç‚¹ç®¡ç†**ï¼šæ–°çš„`--whitelist-custom-nodes`å‚æ•°ä¸Ž`--disable-all-custom-nodes`é…å¯¹ï¼Œæä¾›ç²¾ç¡®çš„å¼€å‘æŽ§åˆ¶\n*   **æ€§èƒ½ä¼˜åŒ–**ï¼šåŒCFGèŠ‚ç‚¹çŽ°åœ¨åœ¨CFGä¸º1.0æ—¶è‡ªåŠ¨ä¼˜åŒ–ï¼Œå‡å°‘è®¡ç®—å¼€é”€\n*   **GitHub Actionsé›†æˆ**ï¼šè‡ªåŠ¨åŒ–å‘å¸ƒwebhooké€šçŸ¥è®©å¼€å‘è€…åŠæ—¶äº†è§£æ–°æ›´æ–°\n\n## å›¾åƒå¤„ç†æ”¹è¿›\n\n*   **æ–°å˜æ¢èŠ‚ç‚¹**ï¼šæ·»åŠ äº†ImageRotateå’ŒImageFlipèŠ‚ç‚¹ï¼Œå¢žå¼ºå›¾åƒæ“ä½œå·¥ä½œæµ\n*   **ImageColorToMaskä¿®å¤**ï¼šä¿®æ­£äº†æŽ©ç å€¼è¿”å›žï¼Œæä¾›æ›´å‡†ç¡®çš„åŸºäºŽé¢œè‰²çš„æŽ©ç æ“ä½œ\n*   **3Dæ¨¡åž‹æ”¯æŒ**ï¼šä¸Šä¼ 3Dæ¨¡åž‹åˆ°è‡ªå®šä¹‰å­æ–‡ä»¶å¤¹ï¼Œä¸ºå¤æ‚é¡¹ç›®æä¾›æ›´å¥½çš„ç»„ç»‡\n\n## å¼•å¯¼å’Œæ¡ä»¶å¢žå¼º\n\n*   **PerpNegå¼•å¯¼å™¨**ï¼šæ›´æ–°äº†æ”¹è¿›çš„å‰åŽCFGå¤„ç†ä»¥åŠæ€§èƒ½ä¼˜åŒ–\n*   **æ½œåœ¨æ¡ä»¶ä¿®å¤**ï¼šè§£å†³äº†å¤šæ­¥éª¤å·¥ä½œæµä¸­ç´¢å¼• > 0 çš„æ¡ä»¶é—®é¢˜\n*   **åŽ»å™ªæ­¥éª¤**ï¼šä¸ºå¤šä¸ªé‡‡æ ·å™¨æ·»åŠ åŽ»å™ªæ­¥éª¤æ”¯æŒï¼ŒèŽ·å¾—æ›´æ¸…æ´çš„è¾“å‡º\n\n## å¹³å°ç¨³å®šæ€§\n\n*   **PyTorchå…¼å®¹æ€§**ï¼šä¿®å¤äº†PyTorch nightlyæž„å»ºçš„è¿žç»­å†…å­˜é—®é¢˜\n*   **FP8å›žé€€**ï¼šå½“FP8æ“ä½œé‡åˆ°å¼‚å¸¸æ—¶è‡ªåŠ¨å›žé€€åˆ°å¸¸è§„æ“ä½œ\n*   **éŸ³é¢‘å¤„ç†**ï¼šç§»é™¤äº†å·²å¼ƒç”¨çš„torchaudio.saveå‡½æ•°ä¾èµ–å¹¶ä¿®å¤è­¦å‘Š\n\n## æ¨¡åž‹é›†æˆ\n\n*   **MoonvalleyèŠ‚ç‚¹**ï¼šä¸ºMoonvalleyæ¨¡åž‹å·¥ä½œæµæ·»åŠ åŽŸç”Ÿæ”¯æŒ\n*   **è°ƒåº¦å™¨é‡æ–°æŽ’åº**ï¼šç®€å•è°ƒåº¦å™¨çŽ°åœ¨é»˜è®¤ä¼˜å…ˆï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ\n*   **æ¨¡æ¿æ›´æ–°**ï¼šå¤šä¸ªæ¨¡æ¿ç‰ˆæœ¬æ›´æ–°ï¼ˆ0.1.31-0.1.35ï¼‰ï¼Œæ”¹è¿›è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘\n\n## å®‰å…¨æ€§å’Œå®‰å…¨ä¿æŠ¤\n\n*   **å®‰å…¨åŠ è½½**ï¼šåœ¨ä¸å®‰å…¨åŠ è½½æ–‡ä»¶æ—¶æ·»åŠ è­¦å‘Šï¼Œæ–‡æ¡£è¯´æ˜Žæ£€æŸ¥ç‚¹æ–‡ä»¶é»˜è®¤å®‰å…¨åŠ è½½\n*   **æ–‡ä»¶éªŒè¯**ï¼šå¢žå¼ºæ£€æŸ¥ç‚¹åŠ è½½å®‰å…¨æŽªæ–½ï¼Œç¡®ä¿å·¥ä½œæµå®‰å…¨æ‰§è¡Œ\n\nè¿™äº›æ”¹è¿›ä½¿ComfyUIåœ¨ç”Ÿäº§å·¥ä½œæµä¸­æ›´åŠ ç¨³å¥ï¼ŒåŒæ—¶ä¸ºä½¿ç”¨é«˜çº§é‡‡æ ·æŠ€æœ¯å’Œæ¨¡åž‹æŽ§åˆ¶ç³»ç»Ÿçš„AIè‰ºæœ¯å®¶æ‰©å±•äº†åˆ›ä½œå¯èƒ½æ€§ã€‚\n\n**å¢žå¼ºæ¨¡åž‹æ”¯æŒä¸Žå·¥ä½œæµå¯é æ€§**æœ¬æ¬¡å‘å¸ƒåœ¨æ¨¡åž‹å…¼å®¹æ€§å’Œå·¥ä½œæµç¨³å®šæ€§æ–¹é¢å¸¦æ¥äº†é‡å¤§æ”¹è¿›ï¼š\n\n*   **æ‰©å±•æ¨¡åž‹æ–‡æ¡£**ï¼šä¸º Flux Kontext å’Œ Omnigen 2 æ¨¡åž‹æ·»åŠ äº†å…¨é¢çš„æ”¯æŒæ–‡æ¡£ï¼Œè®©åˆ›ä½œè€…æ›´å®¹æ˜“å°†è¿™äº›å¼ºå¤§çš„æ¨¡åž‹é›†æˆåˆ°ä»–ä»¬çš„å·¥ä½œæµä¸­\n*   **VAE ç¼–ç æ”¹è¿›**ï¼šç§»é™¤äº† VAE ç¼–ç è¿‡ç¨‹ä¸­ä¸å¿…è¦çš„éšæœºå™ªå£°æ³¨å…¥ï¼Œä½¿å·¥ä½œæµè¿è¡Œçš„è¾“å‡ºæ›´åŠ ä¸€è‡´å’Œå¯é¢„æµ‹\n*   **å†…å­˜ç®¡ç†ä¿®å¤**ï¼šè§£å†³äº†ä¸“é—¨å½±å“ Kontext æ¨¡åž‹ä½¿ç”¨çš„å…³é”®å†…å­˜ä¼°ç®—é”™è¯¯ï¼Œé˜²æ­¢å†…å­˜ä¸è¶³é”™è¯¯å¹¶æé«˜å·¥ä½œæµç¨³å®šæ€§\n\nè¿™äº›å˜æ›´æå‡äº†é«˜çº§æ¨¡åž‹å·¥ä½œæµçš„å¯é æ€§ï¼ŒåŒæ—¶ä¿æŒäº† ComfyUI ä¸ºä»Žäº‹å‰æ²¿ç”Ÿæˆæ¨¡åž‹å·¥ä½œçš„ AI è‰ºæœ¯å®¶å’Œç ”ç©¶äººå‘˜æä¾›çš„çµæ´»æ€§ã€‚\n\n**ä¸»è¦æ¨¡åž‹æ”¯æŒæ–°å¢ž**\n\n*   **Cosmos Predict2 æ”¯æŒ**ï¼šå…¨é¢å®žçŽ°æ–‡æœ¬åˆ°å›¾åƒï¼ˆ2B å’Œ 14B æ¨¡åž‹ï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆå·¥ä½œæµï¼Œæ‰©å±•è§†é¢‘åˆ›ä½œåŠŸèƒ½\n*   **å¢žå¼ºçš„ Flux å…¼å®¹æ€§**ï¼šChroma Text Encoder çŽ°åœ¨èƒ½ä¸Žå¸¸è§„ Flux æ¨¡åž‹æ— ç¼åä½œï¼Œæå‡æ–‡æœ¬æ¡ä»¶è´¨é‡\n*   **LoRA è®­ç»ƒé›†æˆ**ï¼šä½¿ç”¨æƒé‡é€‚é…å™¨æ–¹æ¡ˆçš„å…¨æ–°åŽŸç”Ÿ LoRA è®­ç»ƒèŠ‚ç‚¹ï¼Œæ”¯æŒåœ¨ ComfyUI å·¥ä½œæµä¸­ç›´æŽ¥è¿›è¡Œæ¨¡åž‹å¾®è°ƒ\n\n**æ€§èƒ½å’Œç¡¬ä»¶ä¼˜åŒ–**\n\n*   **AMD GPU å¢žå¼º**ï¼šåœ¨ GFX1201 å’Œå…¶ä»–å…¼å®¹çš„ AMD GPU ä¸Šå¯ç”¨ FP8 æ“ä½œå’Œ PyTorch æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ é€ŸæŽ¨ç†\n*   **Apple Silicon ä¿®å¤**ï¼šè§£å†³äº† Apple è®¾å¤‡ä¸Šé•¿æœŸå­˜åœ¨çš„ FP16 æ³¨æ„åŠ›é—®é¢˜ï¼Œæå‡ Mac ç”¨æˆ·çš„ç¨³å®šæ€§\n*   **Flux æ¨¡åž‹ç¨³å®šæ€§**ï¼šè§£å†³äº†ç‰¹å®š Flux æ¨¡åž‹åœ¨ FP16 ç²¾åº¦ä¸‹ç”Ÿæˆé»‘è‰²å›¾åƒçš„é—®é¢˜\n\n**é«˜çº§é‡‡æ ·æ”¹è¿›**\n\n*   **Rectified Flow (RF) é‡‡æ ·å™¨**ï¼šæ–°å¢žæ”¯æŒ RF çš„ SEEDS å’Œå¤šæ­¥ DPM++ SDE é‡‡æ ·å™¨ï¼Œä¸ºå‰æ²¿æ¨¡åž‹æä¾›æ›´å¤šé‡‡æ ·é€‰é¡¹\n*   **ModelSamplingContinuousEDM**ï¼šæ–°å¢ž cosmos\\_rflow é€‰é¡¹ï¼Œå¢žå¼ºå¯¹ Cosmos æ¨¡åž‹çš„é‡‡æ ·æŽ§åˆ¶\n*   **å†…å­˜ä¼˜åŒ–**ï¼šæ”¹è¿›äº†æ”¯æŒæ— é™åˆ†è¾¨çŽ‡çš„ Cosmos æ¨¡åž‹çš„å†…å­˜ä¼°ç®—\n\n**å¼€å‘è€…å’Œé›†æˆåŠŸèƒ½**\n\n*   **SQLite æ•°æ®åº“æ”¯æŒ**ï¼šå¢žå¼ºè‡ªå®šä¹‰èŠ‚ç‚¹å’Œå·¥ä½œæµå­˜å‚¨çš„æ•°æ®ç®¡ç†åŠŸèƒ½\n*   **PyProject.toml é›†æˆ**ï¼šä»Ž pyproject æ–‡ä»¶è‡ªåŠ¨æ³¨å†Œ web æ–‡ä»¶å¤¹å’Œé…ç½®è®¾ç½®\n*   **å‰ç«¯çµæ´»æ€§**ï¼šæ”¯æŒè¯­ä¹‰åŒ–ç‰ˆæœ¬åŽç¼€å’Œé¢„å‘å¸ƒå‰ç«¯ç‰ˆæœ¬ï¼Œé€‚ç”¨äºŽè‡ªå®šä¹‰éƒ¨ç½²\n*   **åˆ†è¯å™¨å¢žå¼º**ï¼šé€šè¿‡ tokenizer\\_data é…ç½® min\\_length è®¾ç½®ï¼Œä¼˜åŒ–æ–‡æœ¬å¤„ç†\n\n**ä½¿ç”¨ä½“éªŒæ”¹è¿›**\n\n*   **Kontext å®½é«˜æ¯”ä¿®å¤**ï¼šè§£å†³äº†ä»…é™å°ç»„ä»¶çš„é™åˆ¶ï¼ŒçŽ°åœ¨åœ¨æ‰€æœ‰è¿žæŽ¥æ¨¡å¼ä¸‹éƒ½èƒ½æ­£å¸¸å·¥ä½œ\n*   **SaveLora ä¸€è‡´æ€§**ï¼šç»Ÿä¸€æ‰€æœ‰ä¿å­˜èŠ‚ç‚¹çš„æ–‡ä»¶åæ ¼å¼ï¼Œä¼˜åŒ–æ–‡ä»¶ç»„ç»‡\n*   **Python ç‰ˆæœ¬è­¦å‘Š**ï¼šä¸ºè¿‡æ—¶çš„ Python å®‰è£…æ·»åŠ è­¦æŠ¥ï¼Œé˜²æ­¢å…¼å®¹æ€§é—®é¢˜\n*   **WebcamCapture ä¿®å¤**ï¼šä¿®æ­£äº† IS\\_CHANGED ç­¾åï¼Œç¡®ä¿å®žæ—¶è¾“å…¥å·¥ä½œæµçš„å¯é æ€§\n\næ­¤ç‰ˆæœ¬æ˜¾è‘—æ‰©å±•äº† ComfyUI çš„æ¨¡åž‹ç”Ÿæ€ç³»ç»Ÿæ”¯æŒï¼ŒåŒæ—¶æä¾›äº†å…³é”®çš„ç¨³å®šæ€§æ”¹è¿›å’Œè·¨å¹³å°ç¡¬ä»¶å…¼å®¹æ€§å¢žå¼ºã€‚\n\næœ¬æ¬¡å‘å¸ƒä¸º ComfyUI åˆ›ä½œè€…å¸¦æ¥äº†å¼ºå¤§çš„æ–°å·¥ä½œæµå®žç”¨å·¥å…·å’Œæ€§èƒ½ä¼˜åŒ–ï¼š\n\n## æ–°çš„å·¥ä½œæµå·¥å…·\n\n*   **ImageStitch èŠ‚ç‚¹**ï¼šåœ¨å·¥ä½œæµä¸­æ— ç¼æ‹¼æŽ¥å¤šä¸ªå›¾åƒ - éžå¸¸é€‚åˆåˆ›å»ºå¯¹æ¯”ç½‘æ ¼æˆ–å¤åˆè¾“å‡º\n*   **GetImageSize èŠ‚ç‚¹**ï¼šæå–å›¾åƒå°ºå¯¸å¹¶æ”¯æŒæ‰¹å¤„ç†ï¼Œå¯¹äºŽåŠ¨æ€è°ƒæ•´å¤§å°çš„å·¥ä½œæµè‡³å…³é‡è¦\n*   **Regex Replace èŠ‚ç‚¹**ï¼šé«˜çº§æ–‡æœ¬å¤„ç†åŠŸèƒ½ï¼Œé€‚ç”¨äºŽæç¤ºè¯å·¥ç¨‹å’Œå­—ç¬¦ä¸²å¤„ç†å·¥ä½œæµ\n\n## å¢žå¼ºçš„æ¨¡åž‹å…¼å®¹æ€§\n\n*   **æ”¹è¿›çš„å¼ é‡å¤„ç†**ï¼šç®€åŒ–çš„åˆ—è¡¨å¤„ç†ä½¿å¤æ‚çš„å¤šæ¨¡åž‹å·¥ä½œæµæ›´åŠ å¯é \n*   **BFL API ä¼˜åŒ–**ï¼šå®Œå–„äº†å¯¹ Kontext \\[pro\\] å’Œ \\[max\\] æ¨¡åž‹çš„æ”¯æŒï¼Œæä¾›æ›´æ¸…æ™°çš„èŠ‚ç‚¹ç•Œé¢\n*   **æ€§èƒ½æå‡**ï¼šåœ¨è‰²åº¦å¤„ç†ä¸­ä½¿ç”¨èžåˆä¹˜åŠ è¿ç®—ï¼ŒåŠ å¿«ç”Ÿæˆé€Ÿåº¦\n\n## å¼€å‘è€…ä½“éªŒæ”¹è¿›\n\n*   **è‡ªå®šä¹‰èŠ‚ç‚¹æ”¯æŒ**ï¼šæ·»åŠ  pyproject.toml æ”¯æŒï¼Œæ”¹å–„è‡ªå®šä¹‰èŠ‚ç‚¹ä¾èµ–ç®¡ç†\n*   **å¸®åŠ©èœå•é›†æˆ**ï¼šåœ¨èŠ‚ç‚¹åº“ä¾§è¾¹æ ä¸­æ–°å¢žå¸®åŠ©ç³»ç»Ÿï¼ŒåŠ å¿«èŠ‚ç‚¹å‘çŽ°é€Ÿåº¦\n*   **API æ–‡æ¡£**ï¼šå¢žå¼º API èŠ‚ç‚¹æ–‡æ¡£ï¼Œæ”¯æŒå·¥ä½œæµè‡ªåŠ¨åŒ–\n\n## å‰ç«¯å’Œ UI å¢žå¼º\n\n*   **å‰ç«¯æ›´æ–°è‡³ v1.21.7**ï¼šå¤šé¡¹ç¨³å®šæ€§ä¿®å¤å’Œæ€§èƒ½æ”¹è¿›\n*   **è‡ªå®šä¹‰ API åŸºç¡€æ”¯æŒ**ï¼šæ”¹è¿›äº†è‡ªå®šä¹‰éƒ¨ç½²é…ç½®çš„å­è·¯å¾„å¤„ç†\n*   **å®‰å…¨åŠ å›º**ï¼šä¿®å¤ XSS æ¼æ´žï¼Œç¡®ä¿å·¥ä½œæµåˆ†äº«æ›´å®‰å…¨\n\n## é”™è¯¯ä¿®å¤å’Œç¨³å®šæ€§\n\n*   **Pillow å…¼å®¹æ€§**ï¼šæ›´æ–°äº†å·²å¼ƒç”¨çš„ API è°ƒç”¨ï¼Œä¿æŒä¸Žæœ€æ–°å›¾åƒå¤„ç†åº“çš„å…¼å®¹æ€§\n*   **ROCm æ”¯æŒ**ï¼šæ”¹è¿›äº† AMD GPU ç”¨æˆ·çš„ç‰ˆæœ¬æ£€æµ‹\n*   **æ¨¡æ¿æ›´æ–°**ï¼šå¢žå¼ºäº†è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘çš„é¡¹ç›®æ¨¡æ¿\n\nè¿™äº›æ›´æ–°å¼ºåŒ–äº† ComfyUI å¤„ç†å¤æ‚ AI å·¥ä½œæµçš„åŸºç¡€ï¼ŒåŒæ—¶é€šè¿‡æ”¹è¿›çš„æ–‡æ¡£å’Œè¾…åŠ©å·¥å…·è®©å¹³å°å¯¹æ–°ç”¨æˆ·æ›´åŠ å‹å¥½ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/images_and_masks",
  "markdown": "# å›¾åƒã€æ½œå˜é‡å’Œè’™ç‰ˆ - ComfyUI\n\nåœ¨å¤„ç†è¿™äº›æ•°æ®ç±»åž‹æ—¶ï¼Œä½ éœ€è¦äº†è§£ `torch.Tensor` ç±»ã€‚  \nå®Œæ•´æ–‡æ¡£è¯·è§[è¿™é‡Œ](https://pytorch.org/docs/stable/tensors.html)ï¼Œæˆ–è€…å‚è€ƒ Comfy æ‰€éœ€å…³é”®æ¦‚å¿µçš„[ä»‹ç»](https://docs.comfy.org/zh-CN/custom-nodes/backend/tensors)ã€‚\n\nä¸‹æ–¹å¤§éƒ¨åˆ†æ¦‚å¿µéƒ½åœ¨[ç¤ºä¾‹ä»£ç ç‰‡æ®µ](https://docs.comfy.org/zh-CN/custom-nodes/backend/snippets)ä¸­æœ‰è¯´æ˜Žã€‚\n\n## å›¾åƒï¼ˆImagesï¼‰\n\nIMAGE æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º `[B,H,W,C]` çš„ `torch.Tensor`ï¼Œå…¶ä¸­ `C=3`ã€‚å¦‚æžœä½ éœ€è¦ä¿å­˜æˆ–åŠ è½½å›¾åƒï¼Œéœ€è¦åœ¨ `PIL.Image` æ ¼å¼å’Œå¼ é‡ä¹‹é—´è¿›è¡Œè½¬æ¢â€”â€”è¯·å‚è§ä¸‹æ–¹ä»£ç ç‰‡æ®µï¼æ³¨æ„ï¼Œæœ‰äº› `pytorch` æ“ä½œæä¾›ï¼ˆæˆ–æœŸæœ›ï¼‰`[B,C,H,W]`ï¼Œå³â€œé€šé“ä¼˜å…ˆâ€ï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†è®¡ç®—æ•ˆçŽ‡ã€‚è¯·åŠ¡å¿…å°å¿ƒåŒºåˆ†ã€‚\n\n### ä½¿ç”¨ PIL.Image\n\nå¦‚æžœä½ æƒ³åŠ è½½å’Œä¿å­˜å›¾åƒï¼Œä½ éœ€è¦ç”¨åˆ° PILï¼š\n\n```\nfrom PIL import Image, ImageOps\n```\n\n## è’™ç‰ˆï¼ˆMasksï¼‰\n\nMASK æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º `[B,H,W]` çš„ `torch.Tensor`ã€‚  \nåœ¨è®¸å¤šåœºæ™¯ä¸‹ï¼Œè’™ç‰ˆçš„å€¼ä¸ºäºŒå€¼ï¼ˆ0 æˆ– 1ï¼‰ï¼Œç”¨äºŽæŒ‡ç¤ºå“ªäº›åƒç´ éœ€è¦è¿›è¡Œç‰¹å®šæ“ä½œã€‚  \næœ‰æ—¶è’™ç‰ˆçš„å€¼ä¼šåœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œç”¨äºŽè¡¨ç¤ºé®ç½©çš„ç¨‹åº¦ï¼ˆä¾‹å¦‚è°ƒæ•´é€æ˜Žåº¦ã€æ»¤é•œæˆ–å›¾å±‚åˆæˆï¼‰ã€‚\n\n### Load Image èŠ‚ç‚¹ç”Ÿæˆçš„è’™ç‰ˆ\n\n`LoadImage` èŠ‚ç‚¹ä¼šä½¿ç”¨å›¾åƒçš„ alpha é€šé“ï¼ˆå³â€œRGBAâ€ä¸­çš„â€œAâ€ï¼‰æ¥åˆ›å»ºè’™ç‰ˆã€‚  \nalpha é€šé“çš„å€¼ä¼šè¢«å½’ä¸€åŒ–åˆ° \\[0,1\\]ï¼ˆtorch.float32ï¼‰ï¼Œç„¶åŽå†å–åã€‚  \n`LoadImage` èŠ‚ç‚¹åœ¨åŠ è½½å›¾åƒæ—¶æ€»ä¼šç”Ÿæˆä¸€ä¸ª MASK è¾“å‡ºã€‚è®¸å¤šå›¾ç‰‡ï¼ˆå¦‚ JPEGï¼‰æ²¡æœ‰ alpha é€šé“ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œ`LoadImage` ä¼šåˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º `[1, 64, 64]` çš„é»˜è®¤è’™ç‰ˆã€‚\n\n### ç†è§£è’™ç‰ˆçš„å½¢çŠ¶\n\nåœ¨ `numpy`ã€`PIL` ç­‰åº“ä¸­ï¼Œå•é€šé“å›¾åƒï¼ˆå¦‚è’™ç‰ˆï¼‰é€šå¸¸è¡¨ç¤ºä¸ºäºŒç»´æ•°ç»„ï¼Œå½¢çŠ¶ä¸º `[H,W]`ã€‚  \nè¿™æ„å‘³ç€ `C`ï¼ˆé€šé“ï¼‰ç»´åº¦æ˜¯éšå¼çš„ï¼Œå› æ­¤ä¸Ž IMAGE ç±»åž‹ä¸åŒï¼Œè’™ç‰ˆçš„æ‰¹é‡é€šå¸¸åªæœ‰ä¸‰ç»´ï¼š`[B, H, W]`ã€‚  \næœ‰æ—¶ä½ ä¼šé‡åˆ° `B` ç»´è¢«éšå¼ squeeze çš„è’™ç‰ˆï¼Œå˜æˆäº† `[H,W]` çš„å¼ é‡ã€‚ åœ¨ä½¿ç”¨ MASK æ—¶ï¼Œä½ ç»å¸¸éœ€è¦é€šè¿‡ unsqueeze åŒ¹é…å½¢çŠ¶ï¼Œå˜æˆ `[B,H,W,C]`ï¼Œå…¶ä¸­ `C=1`ã€‚  \nè¦ç»™ `C` ç»´ unsqueezeï¼Œè¯·ç”¨ `unsqueeze(-1)`ï¼Œè¦ç»™ `B` ç»´ unsqueezeï¼Œè¯·ç”¨ `unsqueeze(0)`ã€‚  \nå¦‚æžœä½ çš„èŠ‚ç‚¹æŽ¥æ”¶ MASK ä½œä¸ºè¾“å…¥ï¼Œå»ºè®®æ€»æ˜¯æ£€æŸ¥ `len(mask.shape)`ã€‚\n\n## æ½œå˜é‡ï¼ˆLatentsï¼‰\n\nLATENT æ˜¯ä¸€ä¸ª `dict`ï¼›æ½œå˜é‡æ ·æœ¬é€šè¿‡é”® `samples` å¼•ç”¨ï¼Œå½¢çŠ¶ä¸º `[B,C,H,W]`ï¼Œå…¶ä¸­ `C=4`ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/lazy_evaluation",
  "markdown": "# å»¶è¿Ÿæ±‚å€¼ - ComfyUI\n\n## å»¶è¿Ÿæ±‚å€¼\n\né»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨èŠ‚ç‚¹è¿è¡Œä¹‹å‰ï¼Œæ‰€æœ‰ `required` å’Œ `optional` è¾“å…¥éƒ½ä¼šè¢«æ±‚å€¼ã€‚ç„¶è€Œï¼Œæœ‰æ—¶æŸäº›è¾“å…¥å¯èƒ½ä¸ä¼šè¢«ä½¿ç”¨ï¼Œå¯¹å…¶è¿›è¡Œæ±‚å€¼ä¼šå¯¼è‡´ä¸å¿…è¦çš„å¤„ç†ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½ä»Žå»¶è¿Ÿæ±‚å€¼ä¸­å—ç›Šçš„èŠ‚ç‚¹ç¤ºä¾‹ï¼š\n\n1.  ä¸€ä¸ª `ModelMergeSimple` èŠ‚ç‚¹ï¼Œå…¶ä¸­æ¯”ä¾‹è¦ä¹ˆæ˜¯ `0.0`ï¼ˆè¿™ç§æƒ…å†µä¸‹ä¸éœ€è¦åŠ è½½ç¬¬ä¸€ä¸ªæ¨¡åž‹ï¼‰ è¦ä¹ˆæ˜¯ `1.0`ï¼ˆè¿™ç§æƒ…å†µä¸‹ä¸éœ€è¦åŠ è½½ç¬¬äºŒä¸ªæ¨¡åž‹ï¼‰ã€‚\n2.  ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„æ’å€¼ï¼Œå…¶ä¸­æ¯”ä¾‹ï¼ˆæˆ–è’™ç‰ˆï¼‰è¦ä¹ˆå®Œå…¨æ˜¯ `0.0` è¦ä¹ˆå®Œå…¨æ˜¯ `1.0`ã€‚\n3.  ä¸€ä¸ª Switch èŠ‚ç‚¹ï¼Œå…¶ä¸­ä¸€ä¸ªè¾“å…¥å†³å®šå…¶ä»–è¾“å…¥ä¸­çš„å“ªä¸€ä¸ªä¼šè¢«ä¼ é€’ã€‚\n\n### åˆ›å»ºå»¶è¿Ÿè¾“å…¥\n\nå°†è¾“å…¥è®¾ç½®ä¸ºâ€å»¶è¿Ÿâ€è¾“å…¥éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š\n\n1.  åœ¨ `INPUT_TYPES` è¿”å›žçš„å­—å…¸ä¸­å°†è¾“å…¥æ ‡è®°ä¸ºå»¶è¿Ÿ\n2.  å®šä¹‰ä¸€ä¸ªåä¸º `check_lazy_status` çš„æ–¹æ³•ï¼ˆæ³¨æ„ï¼š_ä¸æ˜¯_ç±»æ–¹æ³•ï¼‰ï¼Œè¯¥æ–¹æ³•å°†åœ¨æ±‚å€¼ä¹‹å‰è¢«è°ƒç”¨æ¥ç¡®å®šæ˜¯å¦éœ€è¦æ›´å¤šè¾“å…¥ã€‚\n\nä¸ºäº†æ¼”ç¤ºè¿™äº›ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªâ€MixImagesâ€èŠ‚ç‚¹ï¼Œå®ƒæ ¹æ®è’™ç‰ˆåœ¨ä¸¤ä¸ªå›¾åƒä¹‹é—´è¿›è¡Œæ’å€¼ã€‚å¦‚æžœæ•´ä¸ªè’™ç‰ˆéƒ½æ˜¯ `0.0`ï¼Œæˆ‘ä»¬ä¸éœ€è¦å¯¹ç¬¬äºŒä¸ªå›¾åƒä¹‹å‰çš„ä»»ä½•éƒ¨åˆ†è¿›è¡Œæ±‚å€¼ã€‚å¦‚æžœæ•´ä¸ªè’™ç‰ˆéƒ½æ˜¯ `1.0`ï¼Œæˆ‘ä»¬å¯ä»¥è·³è¿‡å¯¹ç¬¬ä¸€ä¸ªå›¾åƒçš„æ±‚å€¼ã€‚\n\n#### å®šä¹‰ `INPUT_TYPES`\n\nå°†è¾“å…¥å£°æ˜Žä¸ºå»¶è¿Ÿè¾“å…¥å¾ˆç®€å•ï¼Œåªéœ€åœ¨è¾“å…¥çš„é€‰é¡¹å­—å…¸ä¸­æ·»åŠ ä¸€ä¸ª `lazy: True` é”®å€¼å¯¹å³å¯ã€‚\n\n```\n@classmethod\ndef INPUT_TYPES(cls):\n    return {\n        \"required\": {\n            \"image1\": (\"IMAGE\",{\"lazy\": True}),\n            \"image2\": (\"IMAGE\",{\"lazy\": True}),\n            \"mask\": (\"MASK\",),\n        },\n    }\n```\n\nåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`image1` å’Œ `image2` éƒ½è¢«æ ‡è®°ä¸ºå»¶è¿Ÿè¾“å…¥ï¼Œä½† `mask` æ€»æ˜¯ä¼šè¢«æ±‚å€¼ã€‚\n\n#### å®šä¹‰ `check_lazy_status`\n\nä¸€ä¸ª `check_lazy_status` æ–¹æ³•åœ¨å­˜åœ¨ä¸€ä¸ªæˆ–å¤šä¸ªå°šæœªå¯ç”¨çš„å»¶è¿Ÿè¾“å…¥æ—¶è¢«è°ƒç”¨ã€‚è¿™ä¸ªæ–¹æ³•æŽ¥æ”¶ä¸Žæ ‡å‡†æ‰§è¡Œå‡½æ•°ç›¸åŒçš„å‚æ•°ã€‚æ‰€æœ‰å¯ç”¨çš„è¾“å…¥éƒ½ä¼šä»¥å…¶æœ€ç»ˆå€¼ä¼ é€’ï¼Œè€Œä¸å¯ç”¨çš„å»¶è¿Ÿè¾“å…¥åˆ™ä¼šæœ‰ä¸€ä¸ª `None` å€¼ã€‚ `check_lazy_status` æ–¹æ³•çš„è´£ä»»æ˜¯è¿”å›žä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä»»ä½•éœ€è¦ç»§ç»­æ‰§è¡Œçš„å»¶è¿Ÿè¾“å…¥çš„åç§°ã€‚å¦‚æžœæ‰€æœ‰å»¶è¿Ÿè¾“å…¥éƒ½å¯ç”¨ï¼Œè¯¥æ–¹æ³•åº”è¿”å›žä¸€ä¸ªç©ºåˆ—è¡¨ã€‚ æ³¨æ„ï¼Œ`check_lazy_status` æ–¹æ³•å¯èƒ½ä¼šè¢«å¤šæ¬¡è°ƒç”¨ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½åœ¨è¯„ä¼°ä¸€ä¸ªå»¶è¿Ÿè¾“å…¥åŽå‘çŽ°éœ€è¦è¯„ä¼°å¦ä¸€ä¸ªå»¶è¿Ÿè¾“å…¥ã€‚\n\n```\ndef check_lazy_status(self, mask, image1, image2):\n    mask_min = mask.min()\n    mask_max = mask.max()\n    needed = []\n    if image1 is None and (mask_min != 1.0 or mask_max != 1.0):\n        needed.append(\"image1\")\n    if image2 is None and (mask_min != 0.0 or mask_max != 0.0):\n        needed.append(\"image2\")\n    return needed\n```\n\n### å®Œæ•´ç¤ºä¾‹\n\n```\nclass LazyMixImages:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image1\": (\"IMAGE\",{\"lazy\": True}),\n                \"image2\": (\"IMAGE\",{\"lazy\": True}),\n                \"mask\": (\"MASK\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"mix\"\n\n    CATEGORY = \"Examples\"\n\n    def check_lazy_status(self, mask, image1, image2):\n        mask_min = mask.min()\n        mask_max = mask.max()\n        needed = []\n        if image1 is None and (mask_min != 1.0 or mask_max != 1.0):\n            needed.append(\"image1\")\n        if image2 is None and (mask_min != 0.0 or mask_max != 0.0):\n            needed.append(\"image2\")\n        return needed\n\n    # Not trying to handle different batch sizes here just to keep the demo simple\n    def mix(self, mask, image1, image2):\n        mask_min = mask.min()\n        mask_max = mask.max()\n        if mask_min == 0.0 and mask_max == 0.0:\n            return (image1,)\n        elif mask_min == 1.0 and mask_max == 1.0:\n            return (image2,)\n\n        result = image1 * (1. - mask) + image2 * mask,\n        return (result[0],)\n```\n\n## æ‰§è¡Œé˜»å¡ž\n\nè™½ç„¶å»¶è¿Ÿæ±‚å€¼æ˜¯æŽ¨èçš„æ–¹å¼æ¥â€ç¦ç”¨â€å›¾çš„ä¸€éƒ¨åˆ†ï¼Œä½†æœ‰æ—¶ä½ æƒ³è¦ç¦ç”¨ä¸€ä¸ªæ²¡æœ‰å®žçŽ°å»¶è¿Ÿæ±‚å€¼çš„ `OUTPUT` èŠ‚ç‚¹ã€‚å¦‚æžœæ˜¯ä½ å¼€å‘çš„èŠ‚ç‚¹ï¼Œä½ åº”è¯¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼æ·»åŠ å»¶è¿Ÿæ±‚å€¼ï¼š\n\n1.  æ·»åŠ ä¸€ä¸ª `enabled` çš„å¿…éœ€ï¼ˆå¦‚æžœè¿™æ˜¯ä¸€ä¸ªæ–°èŠ‚ç‚¹ï¼‰æˆ–å¯é€‰ï¼ˆå¦‚æžœä½ å…³å¿ƒå‘åŽå…¼å®¹æ€§ï¼‰è¾“å…¥ï¼Œé»˜è®¤å€¼ä¸º `True`\n2.  å°†æ‰€æœ‰å…¶ä»–è¾“å…¥è®¾ç½®ä¸ºå»¶è¿Ÿè¾“å…¥\n3.  ä»…åœ¨ `enabled` ä¸º `True` æ—¶è¯„ä¼°å…¶ä»–è¾“å…¥\n\nå¦‚æžœä½ æ— æ³•æŽ§åˆ¶è¯¥èŠ‚ç‚¹ï¼Œä½ å¯ä»¥ä½¿ç”¨ `comfy_execution.graph.ExecutionBlocker`ã€‚è¿™ä¸ªç‰¹æ®Šå¯¹è±¡å¯ä»¥ä½œä¸ºä»»ä½•è¾“å‡ºç«¯å£çš„è¿”å›žå€¼ã€‚ä»»ä½•æŽ¥æ”¶åˆ° `ExecutionBlocker` ä½œä¸ºè¾“å…¥çš„èŠ‚ç‚¹éƒ½ä¼šè·³è¿‡æ‰§è¡Œï¼Œå¹¶å°†è¯¥ `ExecutionBlocker` ä½œä¸ºå…¶æ‰€æœ‰è¾“å‡ºè¿”å›žã€‚\n\n### ä½¿ç”¨æ–¹æ³•\n\næœ‰ä¸¤ç§æ–¹å¼å¯ä»¥æž„é€ å’Œä½¿ç”¨ `ExecutionBlocker`ï¼š\n\n1.  å‘æž„é€ å‡½æ•°ä¼ å…¥ `None` æ¥é™é»˜é˜»æ­¢æ‰§è¡Œã€‚è¿™åœ¨é˜»æ­¢æ‰§è¡Œæ˜¯æˆåŠŸè¿è¡Œçš„ä¸€éƒ¨åˆ†æ—¶å¾ˆæœ‰ç”¨â€”â€”æ¯”å¦‚ç¦ç”¨æŸä¸ªè¾“å‡ºã€‚\n\n```\ndef silent_passthrough(self, passthrough, blocked):\n    if blocked:\n        return (ExecutionBlocker(None),)\n    else:\n        return (passthrough,)\n```\n\n2.  å‘æž„é€ å‡½æ•°ä¼ å…¥ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå½“èŠ‚ç‚¹å› æŽ¥æ”¶åˆ°è¯¥å¯¹è±¡è€Œè¢«é˜»æ­¢æ‰§è¡Œæ—¶æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯ã€‚è¿™åœ¨ä½ æƒ³æ˜¾ç¤ºæœ‰æ„ä¹‰çš„é”™è¯¯æ¶ˆæ¯æ—¶å¾ˆæœ‰ç”¨ï¼Œæ¯”å¦‚å½“æœ‰äººä½¿ç”¨æ— æ„ä¹‰çš„è¾“å‡ºæ—¶â€”â€”ä¾‹å¦‚ï¼ŒåŠ è½½ä¸åŒ…å« VAE çš„æ¨¡åž‹æ—¶çš„ `VAE` è¾“å‡ºã€‚\n\n```\ndef load_checkpoint(self, ckpt_name):\n    ckpt_path = folder_paths.get_full_path(\"checkpoints\", ckpt_name)\n    model, clip, vae = load_checkpoint(ckpt_path)\n    if vae is None:\n        # è¿™ä¸ªé”™è¯¯ä¿¡æ¯æ¯”åœ¨åŽç»­èŠ‚ç‚¹ä¸­å‡ºçŽ° \"'NoneType' has no attribute\" é”™è¯¯æ›´æœ‰ç”¨\n        vae = ExecutionBlocker(f\"No VAE contained in the loaded model {ckpt_name}\")\n    return (model, clip, vae)\n```"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/wan-video",
  "markdown": "# ComfyUI Wan2.1 Video Examples - ComfyUI\n\nWan2.1 Video series is a video generation model open-sourced by Alibaba in February 2025 under the [Apache 2.0 license](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). It offers two versions:\n\n*   14B (14 billion parameters)\n*   1.3B (1.3 billion parameters) Covering multiple tasks including text-to-video (T2V) and image-to-video (I2V). The model not only outperforms existing open-source models in performance but more importantly, its lightweight version requires only 8GB of VRAM to run, significantly lowering the barrier to entry.\n\n*   [Wan2.1 Code Repository](https://github.com/Wan-Video/Wan2.1)\n*   [Wan2.1 Model Repository](https://huggingface.co/Wan-AI)\n\n## Wan2.1 ComfyUI Native Workflow Examples\n\n## Model Installation\n\nAll models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). Below are the common models youâ€™ll need for the examples in this guide, which you can download in advance: Choose one version from **Text encoders** to download:\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\nFile storage locations:\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ diffusion_models/\nâ”‚   â”œâ”€â”€ ...                  # Let's download the models in the corresponding workflow\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€  wan_2.1_vae.safetensors\nâ”‚   â””â”€â”€ clip_vision/\nâ”‚       â””â”€â”€  clip_vision_h.safetensors   \n```\n\n## Wan2.1 Text-to-Video Workflow\n\nBefore starting the workflow, please download [wan2.1\\_t2v\\_1.3B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.\n\n> If you need other t2v precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.\n\n### 1\\. Workflow File Download\n\nDownload the file below and drag it into ComfyUI to load the corresponding workflow: ![Wan2.1 Text-to-Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_t2v_1.3b.webp)\n\n### 2\\. Complete the Workflow Step by Step\n\n![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg)\n\n1.  Make sure the `Load Diffusion Model` node has loaded the `wan2.1_t2v_1.3B_fp16.safetensors` model\n2.  Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model\n3.  Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model\n4.  (Optional) You can modify the video dimensions in the `EmptyHunyuanLatentVideo` node if needed\n5.  (Optional) If you need to modify the prompts (positive and negative), make changes in the `CLIP Text Encoder` node at number `5`\n6.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation\n\n## Wan2.1 Image-to-Video Workflow\n\n**Since Wan Video separates the 480P and 720P models**, weâ€™ll need to provide examples for both resolutions in this guide. In addition to using different models, they also have slight parameter differences.\n\n### 480P Version\n\n#### 1\\. Workflow and Input Image\n\nDownload the image below and drag it into ComfyUI to load the corresponding workflow: ![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_480P.webp) Weâ€™ll use the following image as input: ![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/flux_dev_example.png)\n\n#### 2\\. Model Download\n\nPlease download [wan2.1\\_i2v\\_480p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.\n\n#### 3\\. Complete the Workflow Step by Step\n\n![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg)\n\n1.  Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_480p_14B_fp16.safetensors` model\n2.  Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model\n3.  Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model\n4.  Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model\n5.  Upload the provided input image in the `Load Image` node\n6.  (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node\n7.  (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed\n8.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation\n\n### 720P Version\n\n#### 1\\. Workflow and Input Image\n\nDownload the image below and drag it into ComfyUI to load the corresponding workflow: ![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_720P.webp) Weâ€™ll use the following image as input: ![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/magician.png)\n\n#### 2\\. Model Download\n\nPlease download [wan2.1\\_i2v\\_720p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.\n\n#### 3\\. Complete the Workflow Step by Step\n\n![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg)\n\n1.  Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_720p_14B_fp16.safetensors` model\n2.  Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model\n3.  Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model\n4.  Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model\n5.  Upload the provided input image in the `Load Image` node\n6.  (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node\n7.  (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed\n8.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/wan-ati",
  "markdown": "# Wan ATI ComfyUI Native Workflow Tutorial\n\n**ATI (Any Trajectory Instruction)** is a controllable video generation framework proposed by the ByteDance team. ATI is implemented based on Wan2.1 and supports unified control of objects, local regions, and camera motion in videos through arbitrary trajectory instructions. Project URL: [https://github.com/bytedance/ATI](https://github.com/bytedance/ATI)\n\n## Key Features\n\n*   **Unified Motion Control**: Supports trajectory control for multiple motion types including objects, local regions, and camera movements.\n*   **Interactive Trajectory Editor**: Visual tool that allows users to freely draw and edit motion trajectories on images.\n*   **Wan2.1 Compatible**: Based on the official Wan2.1 implementation, compatible with environments and model structures.\n*   **Rich Visualization Tools**: Supports visualization of input trajectories, output videos, and trajectory overlays.\n\n## WAN ATI Trajectory Control Workflow Example\n\n### 1\\. Workflow Download\n\nDownload the video below and drag it into ComfyUI to load the corresponding workflow\n\nWe will use the following image as input: ![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/ati/input.jpg) \n\n### 2\\. Model Download\n\nIf you havenâ€™t successfully downloaded the model files from the workflow, you can try downloading them manually using the links below **Diffusion Model**\n\n*   [Wan2\\_1-I2V-ATI-14B\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**Text encoders** Chose one of following model\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**clip\\_vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)\n\nFile save location\n\n```\nComfyUI/\nâ”œâ”€â”€â”€ðŸ“‚ models/\nâ”‚   â”œâ”€â”€â”€ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors\nâ”‚   â”œâ”€â”€â”€ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors # or other version\nâ”‚   â”œâ”€â”€â”€ðŸ“‚ clip_vision/\nâ”‚   â”‚   â””â”€â”€â”€ clip_vision_h.safetensors\nâ”‚   â””â”€â”€â”€ðŸ“‚ vae/\nâ”‚       â””â”€â”€  wan_2.1_vae.safetensors\n```\n\n### 3\\. Complete the workflow execution step by step\n\n![Workflow step diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan_ati_guide.jpg) Please follow the numbered steps in the image to ensure smooth execution of the corresponding workflow\n\n1.  Ensure the `Load Diffusion Model` node has loaded the `Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors` model\n2.  Ensure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model\n3.  Ensure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model\n4.  Ensure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model\n5.  Upload the provided input image in the `Load Image` node\n6.  Trajectory editing: Currently there is no corresponding trajectory editor in ComfyUI yet, you can use the following link to complete trajectory editing\n    *   [Online Trajectory Editing Tool](https://comfyui-wiki.github.io/Trajectory-Annotation-Tool/)\n7.  If you need to modify the prompts (positive and negative), please make changes in the `CLIP Text Encoder` node numbered `5`\n8.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/fun-control",
  "markdown": "# ComfyUI Wan2.1 Fun Control Video Examples\n\n**Wan2.1-Fun-Control** is an open-source video generation and control project developed by Alibaba team. It introduces innovative Control Codes mechanisms combined with deep learning and multimodal conditional inputs to generate high-quality videos that conform to preset control conditions. The project focuses on precisely guiding generated video content through multimodal control conditions. Currently, the Fun Control model supports various control conditions, including **Canny (line art), Depth, OpenPose (human posture), MLSD (geometric edges), and trajectory control.** The model also supports multi-resolution video prediction with options for 512, 768, and 1024 resolutions at 16 frames per second, generating videos up to 81 frames (approximately 5 seconds) in length. Model versions:\n\n*   **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**\n*   **14B** High-performance: Model size reaches 32GB+, offering better results but **requiring higher VRAM**\n\nHere are the relevant code repositories:\n\n*   [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)\n*   [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)\n*   Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)\n\nComfyUI now **natively supports** the Wan2.1 Fun Control model. Before starting this tutorial, please update your ComfyUI to ensure youâ€™re using a version after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82). In this guide, weâ€™ll provide two workflows:\n\n1.  A workflow using only native Comfy Core nodes\n2.  A workflow using custom nodes\n\n## Model Installation\n\nYou only need to install these models once. The workflow images also contain model download information, so you can choose your preferred download method. The following models can be found at [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) Click the corresponding links to download. If youâ€™ve used Wan-related workflows before, you only need to download the **Diffusion models**. **Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:\n\n*   [wan2.1\\_fun\\_control\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)\n*   [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-Control.safetensors` after downloading\n\n**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\nFile storage location:\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ wan2.1_fun_control_1.3B_bf16.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ wan_2.1_vae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ clip_vision/\nâ”‚       â””â”€â”€  clip_vision_h.safetensors                 \n```\n\n## ComfyUI Native Workflow\n\nIn this workflow, we use videos converted to **WebP format** since the `Load Image` node doesnâ€™t currently support mp4 format. We also use **Canny Edge** to preprocess the original video. Because many users encounter installation failures and environment issues when installing custom nodes, this version of the workflow uses only native nodes to ensure a smoother experience. Thanks to our powerful ComfyUI authors who provide feature-rich nodes. If you want to directly check the related version, see [Workflow Using Custom Nodes](#workflow-using-custom-nodes).\n\n### 1\\. Workflow File Download\n\n#### 1.1 Workflow File\n\nDownload the image below and drag it into ComfyUI to load the workflow: ![Wan2.1 Fun Control Native Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_native.webp)\n\n#### 1.2 Input Images and Videos Download\n\nPlease download the following image and video for input: ![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_remix.png) ![Input Reference Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_video.webp)\n\n### 2\\. Complete the Workflow Step by Step\n\n![Wan2.1 Fun Control Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_native_flow_diagram.png)\n\n1.  Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Load Image` node (renamed to `Start_image`)\n6.  Upload the control video to the second `Load Image` node. Note: This node currently doesnâ€™t support mp4, only WebP videos\n7.  (Optional) Modify the prompt (both English and Chinese are supported)\n8.  (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions\n9.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n\n### 3\\. Usage Notes\n\n*   Since we need to input the same number of frames as the control video into the `WanFunControlToVideo` node, if the specified frame count exceeds the actual control video frames, the excess frames may display scenes not conforming to control conditions. Weâ€™ll address this issue in the [Workflow Using Custom Nodes](#workflow-using-custom-nodes)\n*   Avoid setting overly large dimensions, as this can make the sampling process very time-consuming. Try generating smaller images first, then upscale\n*   Use your imagination to build upon this workflow by adding text-to-image or other types of workflows to achieve direct text-to-video generation or style transfer\n*   Use tools like [ComfyUI-comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) for richer control options\n\n## Workflow Using Custom Nodes\n\nWeâ€™ll need to install the following two custom nodes:\n\n*   [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n*   [ComfyUI-comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\nYou can use [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to install missing nodes or follow the installation instructions for each custom node package.\n\n### 1\\. Workflow File Download\n\n#### 1.1 Workflow File\n\nDownload the image below and drag it into ComfyUI to load the workflow: ![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.webp)\n\n#### 1.2 Input Images and Videos Download\n\nPlease download the following image and video for input: ![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-robot's_eye.png) \n\n### 2\\. Complete the Workflow Step by Step\n\n![Wan2.1 Fun Control Workflow Using Custom Nodes Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png)\n\n> The model part is essentially the same. If youâ€™ve already experienced the native-only workflow, you can directly upload the corresponding images and run it.\n\n1.  Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Load Image` node\n6.  Upload an mp4 format video to the `Load Video(Upload)` custom node. Note that the workflow has adjusted the default `frame_load_cap`\n7.  For the current image, the `DWPose Estimator` only uses the `detect_face` option\n8.  (Optional) Modify the prompt (both English and Chinese are supported)\n9.  (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions\n10.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n\n### 3\\. Workflow Notes\n\nThanks to the ComfyUI community authors for their custom node packages:\n\n*   This example uses `Load Video(Upload)` to support mp4 videos\n*   The `video_info` obtained from `Load Video(Upload)` allows us to maintain the same `fps` for the output video\n*   You can replace `DWPose Estimator` with other preprocessors from the `ComfyUI-comfyui_controlnet_aux` node package\n*   Prompts support multiple languages\n\n## Usage Tips\n\n![Apply Multi Control Videos](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/apply_multi_control_videos.jpg)\n\n*   A useful tip is that you can combine multiple image preprocessing techniques and then use the `Image Blend` node to achieve the goal of applying multiple control methods simultaneously.\n*   You can use the `Video Combine` node from `ComfyUI-VideoHelperSuite` to save videos in mp4 format\n*   We use `SaveAnimatedWEBP` because we currently donâ€™t support embedding workflow into **mp4** and some other custom nodes may not support embedding workflow too. To preserve the workflow in the video, we choose `SaveAnimatedWEBP` node.\n*   In the `WanFunControlToVideo` node, `control_video` is not mandatory, so sometimes you can skip using a control video, first generate a very small video size like 320x320, and then use them as control video input to achieve consistent results.\n*   [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n*   [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/fun-inp",
  "markdown": "# ComfyUI Wan2.1 Fun InP Video Examples\n\n## About Wan2.1-Fun-InP\n\n**Wan-Fun InP** is an open-source video generation model released by Alibaba, part of the Wan2.1-Fun series, focusing on generating videos from images with first and last frame control. **Key features**:\n\n*   **First and last frame control**: Supports inputting both first and last frame images to generate transitional video between them, enhancing video coherence and creative freedom. Compared to earlier community versions, Alibabaâ€™s official model produces more stable and significantly higher quality results.\n*   **Multi-resolution support**: Supports generating videos at 512Ã—512, 768Ã—768, 1024Ã—1024 and other resolutions to accommodate different scenario requirements.\n\n**Model versions**:\n\n*   **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**\n*   **14B** High-performance: Model size reaches 32GB+, offering better results but requiring **higher VRAM**\n\nBelow are the relevant model weights and code repositories:\n\n*   [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)\n*   [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)\n*   Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)\n\n## Wan2.1 Fun InP Workflow\n\nDownload the image below and drag it into ComfyUI to load the workflow: ![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_inp/wan2.1_fun_inp.webp)\n\n### 1\\. Workflow File Download\n\n### 2\\. Manual Model Installation\n\nIf automatic model downloading is ineffective, please download the models manually and save them to the corresponding folders. The following models can be found at [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) **Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:\n\n*   [wan2.1\\_fun\\_inp\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)\n*   [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-InP.safetensors` after downloading\n\n**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\nFile storage location:\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ wan2.1_fun_inp_1.3B_bf16.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ wan_2.1_vae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ clip_vision/\nâ”‚       â””â”€â”€  clip_vision_h.safetensors                 \n```\n\n### 3\\. Complete the Workflow Step by Step\n\n![ComfyUI Wan2.1 Fun InP Video Generation Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_inp_flow_diagram.png)\n\n1.  Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_inp_1.3B_bf16.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Load Image` node (renamed to `Start_image`)\n6.  Upload the ending frame to the second `Load Image` node\n7.  (Optional) Modify the prompt (both English and Chinese are supported)\n8.  (Optional) Adjust the video size in `WanFunInpaintToVideo`, avoiding overly large dimensions\n9.  Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation\n\n### 4\\. Workflow Notes\n\n*   When using Wan Fun InP, you may need to frequently modify prompts to ensure the accuracy of the corresponding scene transitions.\n\n*   [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n*   [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n*   [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)"
},
{
  "url": "https://docs.comfy.org/tutorials/video/wan/wan-flf",
  "markdown": "# ComfyUI Wan2.1 FLF2V Native Example\n\nWan FLF2V (First-Last Frame Video Generation) is an open-source video generation model developed by the Alibaba Tongyi Wanxiang team. Its open-source license is [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). Users only need to provide two images as the starting and ending frames, and the model automatically generates intermediate transition frames, outputting a logically coherent and naturally flowing 720p high-definition video. **Core Technical Highlights**\n\n1.  **Precise First-Last Frame Control**: The matching rate of first and last frames reaches 98%, defining video boundaries through starting and ending scenes, intelligently filling intermediate dynamic changes to achieve scene transitions and object morphing effects.\n2.  **Stable and Smooth Video Generation**: Using CLIP semantic features and cross-attention mechanisms, the video jitter rate is reduced by 37% compared to similar models, ensuring natural and smooth transitions.\n3.  **Multi-functional Creative Capabilities**: Supports dynamic embedding of Chinese and English subtitles, generation of anime/realistic/fantasy and other styles, adapting to different creative needs.\n4.  **720p HD Output**: Directly generates 1280Ã—720 resolution videos without post-processing, suitable for social media and commercial applications.\n5.  **Open-source Ecosystem Support**: Model weights, code, and training framework are fully open-sourced, supporting deployment on mainstream AI platforms.\n\n**Technical Principles and Architecture**\n\n1.  **DiT Architecture**: Based on diffusion models and Diffusion Transformer architecture, combined with Full Attention mechanism to optimize spatiotemporal dependency modeling, ensuring video coherence.\n2.  **3D Causal Variational Encoder**: Wan-VAE technology compresses HD frames to 1/128 size while retaining subtle dynamic details, significantly reducing memory requirements.\n3.  **Three-stage Training Strategy**: Starting from 480P resolution pre-training, gradually upgrading to 720P, balancing generation quality and computational efficiency through phased optimization.\n\n**Related Links**\n\n*   **GitHub Repository**: [GitHub](https://github.com/Wan-Video/Wan2.1)\n*   **Hugging Face Model Page**: [Hugging Face](https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P)\n*   **ModelScope Community**: [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)\n\n## Wan2.1 FLF2V 720P ComfyUI Native Workflow Example\n\n### 1\\. Download Workflow Files and Related Input Files\n\nPlease download the WebP file below, and drag it into ComfyUI to load the corresponding workflow. The workflow has embedded the corresponding model download file information. ![Wan2.1 FLF2V 720P f16 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/wan2.1_flf2v_720_f16.webp) Please download the two images below, which we will use as the starting and ending frames of the video ![start_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/start_image.png) ![end_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/end_image.png)\n\n### 2\\. Manual Model Installation\n\nIf corresponding All models involved in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). **diffusion\\_models** Choose one version based on your hardware conditions\n\n*   FP16:[wan2.1\\_flf2v\\_720p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp16.safetensors?download=true)\n*   FP8:[wan2.1\\_flf2v\\_720p\\_14B\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors)\n\nChoose one version from **Text encoders** for download,\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\nFile Storage Location\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€ wan2.1_flf2v_720p_14B_fp16.safetensors           # or FP8 version\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors           # or your chosen version\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€  wan_2.1_vae.safetensors\nâ”‚   â””â”€â”€ clip_vision/\nâ”‚       â””â”€â”€  clip_vision_h.safetensors   \n```\n\n### 3\\. Complete Workflow Execution Step by Step\n\n![Wan2.1 FLF2V 720P Native Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg)\n\n1.  Ensure the `Load Diffusion Model` node has loaded `wan2.1_flf2v_720p_14B_fp16.safetensors` or `wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors`\n2.  Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`\n4.  Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`\n5.  Upload the starting frame to the `Start_image` node\n6.  Upload the ending frame to the `End_image` node\n7.  (Optional) Modify the positive and negative prompts, both Chinese and English are supported\n8.  (**Important**) In `WanFirstLastFrameToVideo` we use 720_1280 as default size.because itâ€™s a 720P model, so using a small size will not yield good output. Please use size around 720_1280 for good generation.\n9.  Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/overview",
  "markdown": "# ComfyUI å†…ç½®èŠ‚ç‚¹ - ComfyUI\n\nå†…ç½®èŠ‚ç‚¹æ˜¯ ComfyUI çš„é»˜è®¤èŠ‚ç‚¹ï¼Œå®ƒä»¬æ˜¯ ComfyUI çš„æ ¸å¿ƒåŠŸèƒ½,ä½ æ— éœ€é¢å¤–å®‰è£…ç¬¬ä¸‰æ–¹è‡ªå®šä¹‰èŠ‚ç‚¹åŒ…ï¼Œå°±å¯ä»¥ä½¿ç”¨çš„èŠ‚ç‚¹ã€‚\n\n## èŠ‚ç‚¹æ–‡æ¡£æ›´æ–°è¯´æ˜Ž\n\næˆ‘ä»¬ç›®å‰å·²ç»æ”¯æŒäº†å†…ç½®çš„èŠ‚ç‚¹å¸®åŠ©æ–‡æ¡£ï¼Œæ‰€ä»¥æ­¤éƒ¨åˆ†çš„æ–‡æ¡£å†…å®¹æ˜¯å®šæœŸä»Ž [è¿™ä¸ªä»“åº“](https://github.com/Comfy-Org/embedded-docs) ä¸­åŒæ­¥è¿‡æ¥çš„ï¼Œç›®å‰æˆ‘ä»¬ä¼šæ¯å‘¨è¿›è¡Œä¸€æ¬¡äººå·¥åŒæ­¥å’Œå†…å®¹æ›´æ–°ã€‚\n\n## è´¡çŒ®å†…å®¹\n\nå¦‚æžœä½ å‘çŽ°æˆ‘ä»¬çš„å†…å®¹é”™è¯¯ï¼Œæˆ–è€…æƒ³è¦è¡¥å……æˆ‘ä»¬ç¼ºå¤±çš„å†…å®¹ï¼Œè¯·åœ¨ [è¿™ä¸ªä»“åº“](https://github.com/Comfy-Org/embedded-docs) ä¸­æäº¤ issue æˆ– pr æ¥å¸®åŠ©æˆ‘ä»¬æ”¹è¿›ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_about_panel_badges",
  "markdown": "# å…³äºŽé¢æ¿å¾½ç«  - ComfyUI\n\nå…³äºŽé¢æ¿å¾½ç«  API å…è®¸æ‰©å±•ä¸º ComfyUI çš„å…³äºŽé¡µé¢æ·»åŠ è‡ªå®šä¹‰å¾½ç« ã€‚è¿™äº›å¾½ç« å¯ä»¥æ˜¾ç¤ºæœ‰å…³ä½ çš„æ‰©å±•çš„ä¿¡æ¯ï¼Œå¹¶åŒ…å«æŒ‡å‘æ–‡æ¡£ã€æºä»£ç æˆ–å…¶ä»–èµ„æºçš„é“¾æŽ¥ã€‚\n\n## åŸºæœ¬ç”¨æ³•\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  aboutPageBadges: [\n    {\n      label: \"Documentation\",\n      url: \"https://example.com/docs\",\n      icon: \"pi pi-file\"\n    },\n    {\n      label: \"GitHub\",\n      url: \"https://github.com/username/repo\",\n      icon: \"pi pi-github\"\n    }\n  ]\n});\n```\n\n## å¾½ç« é…ç½®\n\næ¯ä¸ªå¾½ç« éƒ½éœ€è¦ä»¥ä¸‹æ‰€æœ‰å±žæ€§ï¼š\n\n```\n{\n  label: string,           // å¾½ç« ä¸Šæ˜¾ç¤ºçš„æ–‡æœ¬\n  url: string,             // ç‚¹å‡»å¾½ç« æ—¶æ‰“å¼€çš„ URL\n  icon: string             // å›¾æ ‡ç±»åï¼ˆä¾‹å¦‚ PrimeVue å›¾æ ‡ï¼‰\n}\n```\n\n## å›¾æ ‡é€‰é¡¹\n\nå¾½ç« å›¾æ ‡ä½¿ç”¨ PrimeVue çš„å›¾æ ‡é›†ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨å›¾æ ‡ï¼š\n\n*   æ–‡æ¡£ï¼š`pi pi-file` æˆ– `pi pi-book`\n*   GitHubï¼š`pi pi-github`\n*   å¤–éƒ¨é“¾æŽ¥ï¼š`pi pi-external-link`\n*   ä¿¡æ¯ï¼š`pi pi-info-circle`\n*   ä¸‹è½½ï¼š`pi pi-download`\n*   ç½‘ç«™ï¼š`pi pi-globe`\n*   Discordï¼š`pi pi-discord`\n\nå®Œæ•´çš„å¯ç”¨å›¾æ ‡åˆ—è¡¨è¯·å‚è€ƒ [PrimeVue å›¾æ ‡æ–‡æ¡£](https://primevue.org/icons/)ã€‚\n\n## ç¤ºä¾‹\n\n```\napp.registerExtension({\n  name: \"BadgeExample\",\n  aboutPageBadges: [\n    {\n      label: \"Website\",\n      url: \"https://example.com\",\n      icon: \"pi pi-home\"\n    },\n    {\n      label: \"Donate\",\n      url: \"https://example.com/donate\",\n      icon: \"pi pi-heart\"\n    },\n    {\n      label: \"Documentation\",\n      url: \"https://example.com/docs\",\n      icon: \"pi pi-book\"\n    }\n  ]\n});\n```\n\nå¾½ç« ä¼šæ˜¾ç¤ºåœ¨è®¾ç½®å¯¹è¯æ¡†çš„å…³äºŽé¢æ¿ä¸­ï¼Œå¯ä»¥é€šè¿‡ ComfyUI ç•Œé¢å³ä¸Šè§’çš„é½¿è½®å›¾æ ‡è¿›å…¥ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_bottom_panel_tabs",
  "markdown": "# åº•éƒ¨é¢æ¿æ ‡ç­¾é¡µ - ComfyUI\n\nåº•éƒ¨é¢æ¿æ ‡ç­¾é¡µ API å…è®¸æ‰©å±•ä¸º ComfyUI ç•Œé¢çš„åº•éƒ¨é¢æ¿æ·»åŠ è‡ªå®šä¹‰æ ‡ç­¾é¡µã€‚è¿™å¯¹äºŽæ·»åŠ æ—¥å¿—ã€è°ƒè¯•å·¥å…·æˆ–è‡ªå®šä¹‰é¢æ¿ç­‰åŠŸèƒ½éžå¸¸æœ‰ç”¨ã€‚\n\n## åŸºæœ¬ç”¨æ³•\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  bottomPanelTabs: [\n    {\n      id: \"customTab\",\n      title: \"Custom Tab\",\n      type: \"custom\",\n      render: (el) => {\n        el.innerHTML = '<div>è¿™æ˜¯æˆ‘çš„è‡ªå®šä¹‰æ ‡ç­¾é¡µå†…å®¹</div>';\n      }\n    }\n  ]\n});\n```\n\n## æ ‡ç­¾é¡µé…ç½®\n\næ¯ä¸ªæ ‡ç­¾é¡µéƒ½éœ€è¦ `id`ã€`title` å’Œ `type`ï¼Œä»¥åŠä¸€ä¸ªæ¸²æŸ“å‡½æ•°ï¼š\n\n```\n{\n  id: string,              // æ ‡ç­¾é¡µçš„å”¯ä¸€æ ‡è¯†ç¬¦\n  title: string,           // æ ‡ç­¾é¡µä¸Šæ˜¾ç¤ºçš„æ ‡é¢˜\n  type: string,            // æ ‡ç­¾é¡µç±»åž‹ï¼ˆé€šå¸¸ä¸º \"custom\"ï¼‰\n  icon?: string,           // å›¾æ ‡ç±»åï¼ˆå¯é€‰ï¼‰\n  render: (element) => void // ç”¨äºŽå¡«å……æ ‡ç­¾é¡µå†…å®¹çš„å‡½æ•°\n}\n```\n\n`render` å‡½æ•°ä¼šæŽ¥æ”¶ä¸€ä¸ª DOM å…ƒç´ ï¼Œä½ åº”åœ¨å…¶ä¸­æ’å…¥æ ‡ç­¾é¡µçš„å†…å®¹ã€‚\n\n## äº¤äº’å…ƒç´ \n\nä½ å¯ä»¥æ·»åŠ å¦‚æŒ‰é’®ç­‰äº¤äº’å…ƒç´ ï¼š\n\n```\napp.registerExtension({\n  name: \"InteractiveTabExample\",\n  bottomPanelTabs: [\n    {\n      id: \"controlsTab\",\n      title: \"Controls\",\n      type: \"custom\",\n      render: (el) => {\n        el.innerHTML = `\n          <div style=\"padding: 10px;\">\n            <button id=\"runBtn\">è¿è¡Œå·¥ä½œæµ</button>\n          </div>\n        `;\n        \n        // æ·»åŠ äº‹ä»¶ç›‘å¬å™¨\n        el.querySelector('#runBtn').addEventListener('click', () => {\n          app.queuePrompt();\n        });\n      }\n    }\n  ]\n});\n```\n\n## ä½¿ç”¨ React ç»„ä»¶\n\nä½ å¯ä»¥åœ¨åº•éƒ¨é¢æ¿æ ‡ç­¾é¡µä¸­æŒ‚è½½ React ç»„ä»¶ï¼š\n\n```\n// åœ¨ä½ çš„æ‰©å±•ä¸­å¼•å…¥ React ä¾èµ–\nimport React from \"react\";\nimport ReactDOM from \"react-dom/client\";\n\n// ç®€å•çš„ React ç»„ä»¶\nfunction TabContent() {\n  const [count, setCount] = React.useState(0);\n  \n  return (\n    <div style={{ padding: \"10px\" }}>\n      <h3>React ç»„ä»¶</h3>\n      <p>è®¡æ•°ï¼š{count}</p>\n      <button onClick={() => setCount(count + 1)}>é€’å¢ž</button>\n    </div>\n  );\n}\n\n// æ³¨å†Œå¸¦æœ‰ React å†…å®¹çš„æ‰©å±•\napp.registerExtension({\n  name: \"ReactTabExample\",\n  bottomPanelTabs: [\n    {\n      id: \"reactTab\",\n      title: \"React Tab\",\n      type: \"custom\",\n      render: (el) => {\n        const container = document.createElement(\"div\");\n        container.id = \"react-tab-container\";\n        el.appendChild(container);\n        \n        // æŒ‚è½½ React ç»„ä»¶\n        ReactDOM.createRoot(container).render(\n          <React.StrictMode>\n            <TabContent />\n          </React.StrictMode>\n        );\n      }\n    }\n  ]\n});\n```\n\n## ç‹¬ç«‹æ³¨å†Œ\n\nä½ ä¹Ÿå¯ä»¥åœ¨ `registerExtension` ä¹‹å¤–æ³¨å†Œæ ‡ç­¾é¡µï¼š\n\n```\napp.extensionManager.registerBottomPanelTab({\n  id: \"standAloneTab\",\n  title: \"Stand-Alone Tab\",\n  type: \"custom\",\n  render: (el) => {\n    el.innerHTML = '<div>æ­¤æ ‡ç­¾é¡µæ˜¯ç‹¬ç«‹æ³¨å†Œçš„</div>';\n  }\n});\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_commands_keybindings",
  "markdown": "# å‘½ä»¤ä¸Žå¿«æ·é”®ç»‘å®š - ComfyUI\n\nå‘½ä»¤ä¸Žå¿«æ·é”®ç»‘å®š API å…è®¸æ‰©å±•æ³¨å†Œè‡ªå®šä¹‰å‘½ä»¤ï¼Œå¹¶å°†å…¶ä¸Žé”®ç›˜å¿«æ·é”®å…³è”ã€‚è¿™æ ·ç”¨æˆ·å¯ä»¥æ— éœ€é¼ æ ‡ï¼Œå¿«é€Ÿè§¦å‘æ“ä½œã€‚\n\n## åŸºæœ¬ç”¨æ³•\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  // æ³¨å†Œå‘½ä»¤\n  commands: [\n    {\n      id: \"myCommand\",\n      label: \"æˆ‘çš„å‘½ä»¤\",\n      function: () => {\n        console.log(\"å‘½ä»¤å·²æ‰§è¡Œï¼\");\n      }\n    }\n  ],\n  // å°†å¿«æ·é”®ä¸Žå‘½ä»¤å…³è”\n  keybindings: [\n    {\n      combo: { key: \"k\", ctrl: true },\n      commandId: \"myCommand\"\n    }\n  ]\n});\n```\n\n## å‘½ä»¤é…ç½®\n\næ¯ä¸ªå‘½ä»¤éƒ½éœ€è¦ `id`ã€`label` å’Œ `function`ï¼š\n\n```\n{\n  id: string,              // å‘½ä»¤çš„å”¯ä¸€æ ‡è¯†ç¬¦\n  label: string,           // å‘½ä»¤æ˜¾ç¤ºåç§°\n  function: () => void     // å‘½ä»¤è¢«è§¦å‘æ—¶æ‰§è¡Œçš„å‡½æ•°\n}\n```\n\n## å¿«æ·é”®é…ç½®\n\næ¯ä¸ªå¿«æ·é”®éƒ½éœ€è¦ `combo` å’Œ `commandId`ï¼š\n\n```\n{\n  combo: {                 // æŒ‰é”®ç»„åˆ\n    key: string,           // ä¸»é”®ï¼ˆå•ä¸ªå­—ç¬¦æˆ–ç‰¹æ®ŠæŒ‰é”®ï¼‰\n    ctrl?: boolean,        // æ˜¯å¦éœ€è¦ Ctrl é”®ï¼ˆå¯é€‰ï¼‰\n    shift?: boolean,       // æ˜¯å¦éœ€è¦ Shift é”®ï¼ˆå¯é€‰ï¼‰\n    alt?: boolean,         // æ˜¯å¦éœ€è¦ Alt é”®ï¼ˆå¯é€‰ï¼‰\n    meta?: boolean         // æ˜¯å¦éœ€è¦ Meta/Command é”®ï¼ˆå¯é€‰ï¼‰\n  },\n  commandId: string        // è¦è§¦å‘çš„å‘½ä»¤ ID\n}\n```\n\n### ç‰¹æ®ŠæŒ‰é”®\n\nå¯¹äºŽéžå­—ç¬¦æŒ‰é”®ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å€¼ä¹‹ä¸€ï¼š\n\n*   æ–¹å‘é”®ï¼š`\"ArrowUp\"`ã€`\"ArrowDown\"`ã€`\"ArrowLeft\"`ã€`\"ArrowRight\"`\n*   åŠŸèƒ½é”®ï¼š`\"F1\"` åˆ° `\"F12\"`\n*   å…¶ä»–ç‰¹æ®ŠæŒ‰é”®ï¼š`\"Escape\"`ã€`\"Tab\"`ã€`\"Enter\"`ã€`\"Backspace\"`ã€`\"Delete\"`ã€`\"Home\"`ã€`\"End\"`ã€`\"PageUp\"`ã€`\"PageDown\"`\n\n## å‘½ä»¤ç¤ºä¾‹\n\n```\napp.registerExtension({\n  name: \"CommandExamples\",\n  commands: [\n    {\n      id: \"runWorkflow\",\n      label: \"è¿è¡Œå·¥ä½œæµ\",\n      function: () => {\n        app.queuePrompt();\n      }\n    },\n    {\n      id: \"clearWorkflow\",\n      label: \"æ¸…ç©ºå·¥ä½œæµ\",\n      function: () => {\n        if (confirm(\"ç¡®å®šè¦æ¸…ç©ºå·¥ä½œæµå—ï¼Ÿ\")) {\n          app.graph.clear();\n        }\n      }\n    },\n    {\n      id: \"saveWorkflow\",\n      label: \"ä¿å­˜å·¥ä½œæµ\",\n      function: () => {\n        app.graphToPrompt().then(workflow => {\n          const blob = new Blob([JSON.stringify(workflow)], {type: \"application/json\"});\n          const url = URL.createObjectURL(blob);\n          const a = document.createElement(\"a\");\n          a.href = url;\n          a.download = \"workflow.json\";\n          a.click();\n          URL.revokeObjectURL(url);\n        });\n      }\n    }\n  ]\n});\n```\n\n## å¿«æ·é”®ç¤ºä¾‹\n\n```\napp.registerExtension({\n  name: \"KeybindingExamples\",\n  commands: [\n    /* ä¸Šè¿°å‘½ä»¤å®šä¹‰ */\n  ],\n  keybindings: [\n    // Ctrl+R è¿è¡Œå·¥ä½œæµ\n    {\n      combo: { key: \"r\", ctrl: true },\n      commandId: \"runWorkflow\"\n    },\n    // Ctrl+Shift+C æ¸…ç©ºå·¥ä½œæµ\n    {\n      combo: { key: \"c\", ctrl: true, shift: true },\n      commandId: \"clearWorkflow\"\n    },\n    // Ctrl+S ä¿å­˜å·¥ä½œæµ\n    {\n      combo: { key: \"s\", ctrl: true },\n      commandId: \"saveWorkflow\"\n    },\n    // F5 è¿è¡Œå·¥ä½œæµï¼ˆå¤‡ç”¨ï¼‰\n    {\n      combo: { key: \"F5\" },\n      commandId: \"runWorkflow\"\n    }\n  ]\n});\n```\n\n## æ³¨æ„äº‹é¡¹ä¸Žé™åˆ¶\n\n*   ComfyUI æ ¸å¿ƒä¸­å®šä¹‰çš„å¿«æ·é”®æ— æ³•è¢«æ‰©å±•è¦†ç›–ã€‚è¯·åœ¨ä»¥ä¸‹æºç æ–‡ä»¶ä¸­æŸ¥çœ‹æ ¸å¿ƒå¿«æ·é”®ï¼š\n    *   [æ ¸å¿ƒå‘½ä»¤](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/composables/useCoreCommands.ts)\n    *   [æ ¸å¿ƒèœå•å‘½ä»¤](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreMenuCommands.ts)\n    *   [æ ¸å¿ƒå¿«æ·é”®](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreKeybindings.ts)\n    *   [ä¿ç•™æŒ‰é”®ç»„åˆ](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/reservedKeyCombos.ts)\n*   æŸäº›æŒ‰é”®ç»„åˆè¢«æµè§ˆå™¨ä¿ç•™ï¼ˆå¦‚ Ctrl+F ç”¨äºŽæœç´¢ï¼‰ï¼Œæ— æ³•è¢«è¦†ç›–\n*   å¦‚æžœå¤šä¸ªæ‰©å±•æ³¨å†Œäº†ç›¸åŒçš„å¿«æ·é”®ï¼Œè¡Œä¸ºæœªå®šä¹‰"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_dialog",
  "markdown": "# å¯¹è¯æ¡† API - ComfyUI\n\nå¯¹è¯æ¡† API æä¾›äº†åœ¨æ¡Œé¢ç«¯å’Œ Web çŽ¯å¢ƒä¸‹éƒ½èƒ½ä¸€è‡´å·¥ä½œçš„æ ‡å‡†åŒ–å¯¹è¯æ¡†ã€‚æ‰©å±•ä½œè€…ä¼šå‘çŽ° prompt å’Œ confirm æ–¹æ³•æœ€ä¸ºå®žç”¨ã€‚\n\n## åŸºæœ¬ç”¨æ³•\n\n### è¾“å…¥å¯¹è¯æ¡†ï¼ˆPrompt Dialogï¼‰\n\n```\n// æ˜¾ç¤ºä¸€ä¸ªè¾“å…¥å¯¹è¯æ¡†\napp.extensionManager.dialog.prompt({\n  title: \"ç”¨æˆ·è¾“å…¥\",\n  message: \"è¯·è¾“å…¥ä½ çš„å§“åï¼š\",\n  defaultValue: \"User\"\n}).then(result => {\n  if (result !== null) {\n    console.log(`è¾“å…¥å†…å®¹: ${result}`);\n  }\n});\n```\n\n### ç¡®è®¤å¯¹è¯æ¡†ï¼ˆConfirm Dialogï¼‰\n\n```\n// æ˜¾ç¤ºä¸€ä¸ªç¡®è®¤å¯¹è¯æ¡†\napp.extensionManager.dialog.confirm({\n  title: \"ç¡®è®¤æ“ä½œ\",\n  message: \"ä½ ç¡®å®šè¦ç»§ç»­å—ï¼Ÿ\",\n  type: \"default\"\n}).then(result => {\n  console.log(result ? \"ç”¨æˆ·å·²ç¡®è®¤\" : \"ç”¨æˆ·å·²å–æ¶ˆ\");\n});\n```\n\n### Prompt\n\n```\napp.extensionManager.dialog.prompt({\n  title: string,             // å¯¹è¯æ¡†æ ‡é¢˜\n  message: string,           // æ˜¾ç¤ºçš„æ¶ˆæ¯/é—®é¢˜\n  defaultValue?: string      // è¾“å…¥æ¡†çš„åˆå§‹å€¼ï¼ˆå¯é€‰ï¼‰\n}).then((result: string | null) => {\n  // result æ˜¯è¾“å…¥çš„æ–‡æœ¬ï¼Œè‹¥å–æ¶ˆåˆ™ä¸º null\n});\n```\n\n### Confirm\n\n```\napp.extensionManager.dialog.confirm({\n  title: string,             // å¯¹è¯æ¡†æ ‡é¢˜\n  message: string,           // æ˜¾ç¤ºçš„æ¶ˆæ¯\n  type?: \"default\" | \"overwrite\" | \"delete\" | \"dirtyClose\" | \"reinstall\", // å¯¹è¯æ¡†ç±»åž‹ï¼ˆå¯é€‰ï¼‰\n  itemList?: string[],       // è¦æ˜¾ç¤ºçš„é¡¹ç›®åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰\n  hint?: string              // æ˜¾ç¤ºçš„æç¤ºæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰\n}).then((result: boolean | null) => {\n  // result ä¸º true è¡¨ç¤ºç¡®è®¤ï¼Œfalse è¡¨ç¤ºæ‹’ç»ï¼Œnull è¡¨ç¤ºå–æ¶ˆ\n});\n```\n\nå¦‚éœ€äº†è§£ ComfyUI ä¸­å…¶ä»–ä¸“ç”¨å¯¹è¯æ¡†ï¼Œæ‰©å±•ä½œè€…å¯å‚è€ƒæºç ä¸­çš„ `dialogService.ts` æ–‡ä»¶ã€‚"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Finterface%2Fsettings%2Fmask-editor",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_examples",
  "markdown": "# å¸¦æ³¨é‡Šçš„ç¤ºä¾‹ - ComfyUI\n\nä¸æ–­å¢žé•¿çš„ç¤ºä¾‹ä»£ç ç‰‡æ®µé›†åˆâ€¦â€¦\n\n## å³é”®èœå•\n\n### èƒŒæ™¯èœå•\n\nä¸»èƒŒæ™¯èœå•ï¼ˆåœ¨ç”»å¸ƒä¸Šå³é”®ï¼‰æ˜¯é€šè¿‡è°ƒç”¨ `LGraph.getCanvasMenuOptions` ç”Ÿæˆçš„ã€‚æ·»åŠ è‡ªå®šä¹‰èœå•é€‰é¡¹çš„ä¸€ç§æ–¹å¼æ˜¯åŠ«æŒè¿™ä¸ªè°ƒç”¨ï¼š\n\n```\n/* åœ¨ setup() ä¸­ */\n    const original_getCanvasMenuOptions = LGraphCanvas.prototype.getCanvasMenuOptions;\n    LGraphCanvas.prototype.getCanvasMenuOptions = function () {\n        // èŽ·å–åŸºç¡€é€‰é¡¹\n        const options = original_getCanvasMenuOptions.apply(this, arguments);\n        options.push(null); // æ’å…¥åˆ†éš”çº¿\n        options.push({\n            content: \"èœå•çš„æ–‡æœ¬\",\n            callback: async () => {\n                // æ‰§è¡Œä»»æ„æ“ä½œ\n            }\n        })\n        return options;\n    }\n```\n\n### èŠ‚ç‚¹èœå•\n\nå½“ä½ åœ¨èŠ‚ç‚¹ä¸Šå³é”®æ—¶ï¼Œèœå•åŒæ ·æ˜¯é€šè¿‡ `node.getExtraMenuOptions` ç”Ÿæˆçš„ã€‚ä½†è¿™æ¬¡ä¸æ˜¯è¿”å›žä¸€ä¸ª options å¯¹è±¡ï¼Œè€Œæ˜¯å°†å…¶ä½œä¸ºå‚æ•°ä¼ å…¥â€¦â€¦\n\n```\n/* åœ¨ beforeRegisterNodeDef() ä¸­ */\nif (nodeType?.comfyClass==\"MyNodeClass\") { \n    const original_getExtraMenuOptions = nodeType.prototype.getExtraMenuOptions;\n    nodeType.prototype.getExtraMenuOptions = function(_, options) {\n        original_getExtraMenuOptions?.apply(this, arguments);\n        options.push({\n            content: \"åšç‚¹æœ‰è¶£çš„äº‹\",\n            callback: async () => {\n                // æœ‰è¶£çš„æ“ä½œ\n            }\n        })\n    }   \n}\n```\n\n### å­èœå•\n\nå¦‚æžœä½ æƒ³è¦å­èœå•ï¼Œå¯ä»¥æä¾›ä¸€ä¸ªå›žè°ƒï¼Œä½¿ç”¨ `LiteGraph.ContextMenu` åˆ›å»ºå®ƒï¼š\n\n```\nfunction make_submenu(value, options, e, menu, node) {\n    const submenu = new LiteGraph.ContextMenu(\n        [\"é€‰é¡¹ 1\", \"é€‰é¡¹ 2\", \"é€‰é¡¹ 3\"],\n        { \n            event: e, \n            callback: function (v) { \n                // ç”¨ v (==\"é€‰é¡¹ x\") åšç‚¹ä»€ä¹ˆ\n            }, \n            parentMenu: menu, \n            node:node\n        }\n    )\n}\n\n/* ... */\n    options.push(\n        {\n            content: \"å¸¦é€‰é¡¹çš„èœå•\",\n            has_submenu: true,\n            callback: make_submenu,\n        }\n    )\n```\n\n## æ•èŽ· UI äº‹ä»¶\n\nè¿™å’Œä½ é¢„æœŸçš„ä¸€æ ·â€”â€”åœ¨ DOM ä¸­æ‰¾åˆ° UI å…ƒç´ å¹¶æ·»åŠ  eventListenerã€‚`setup()` æ˜¯åšè¿™ä»¶äº‹çš„å¥½åœ°æ–¹ï¼Œå› ä¸ºæ­¤æ—¶é¡µé¢å·²å®Œå…¨åŠ è½½ã€‚ä¾‹å¦‚ï¼Œæ£€æµ‹â€é˜Ÿåˆ—â€æŒ‰é’®çš„ç‚¹å‡»ï¼š\n\n```\nfunction queue_button_pressed() { console.log(\"é˜Ÿåˆ—æŒ‰é’®è¢«æŒ‰ä¸‹ï¼\") }\ndocument.getElementById(\"queue-button\").addEventListener(\"click\", queue_button_pressed);\n```\n\n## æ£€æµ‹å·¥ä½œæµå¼€å§‹\n\nè¿™æ˜¯ä¼—å¤š `api` äº‹ä»¶ä¹‹ä¸€ï¼š\n\n```\nimport { api } from \"../../scripts/api.js\";\n/* åœ¨ setup() ä¸­ */\n    function on_execution_start() { \n        /* æ‰§è¡Œä»»æ„æ“ä½œ */\n    }\n    api.addEventListener(\"execution_start\", on_execution_start);\n```\n\n## æ£€æµ‹å·¥ä½œæµè¢«ä¸­æ–­\n\nè¿™æ˜¯ä¸€ä¸ªåŠ«æŒ api çš„ç®€å•ä¾‹å­ï¼š\n\n```\nimport { api } from \"../../scripts/api.js\";\n/* åœ¨ setup() ä¸­ */\n    const original_api_interrupt = api.interrupt;\n    api.interrupt = function () {\n        /* åœ¨è°ƒç”¨åŽŸæ–¹æ³•å‰åšç‚¹ä»€ä¹ˆ */\n        original_api_interrupt.apply(this, arguments);\n        /* æˆ–è€…åœ¨ä¹‹åŽ */\n    }\n```\n\n## æ•èŽ·èŠ‚ç‚¹ç‚¹å‡»\n\n`node` æœ‰ä¸€ä¸ª mouseDown æ–¹æ³•å¯ä»¥è¢«åŠ«æŒã€‚ è¿™æ¬¡æˆ‘ä»¬æ³¨æ„ä¼ é€’ä»»ä½•è¿”å›žå€¼ã€‚\n\n```\nasync nodeCreated(node) {\n    if (node?.comfyClass === \"My Node Name\") {\n        const original_onMouseDown = node.onMouseDown;\n        node.onMouseDown = function( e, pos, canvas ) {\n            alert(\"å“Žå‘¦ï¼\");\n            return original_onMouseDown?.apply(this, arguments);\n        }        \n    }\n}\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Ftutorials%2Fimage%2Fcosmos%2Fcosmos-predict2-t2i",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipTextEncode",
  "markdown": "# CLIPæ–‡æœ¬ç¼–ç  - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\n`CLIPæ–‡æœ¬ç¼–ç  (CLIPTextEncode)` è¿™ä¸ªèŠ‚ç‚¹å°±åƒä¸€ä½ç¿»è¯‘å®˜ï¼Œå®ƒèƒ½å°†ä½ ç”¨æ–‡å­—æè¿°çš„åˆ›ä½œæƒ³æ³•è½¬æ¢æˆAIèƒ½å¤Ÿç†è§£çš„ç‰¹æ®Šâ€è¯­è¨€â€ï¼Œå¸®åŠ©AIå‡†ç¡®ç†è§£ä½ æƒ³è¦åˆ›ä½œçš„å›¾åƒå†…å®¹ã€‚ æƒ³è±¡ä½ åœ¨å’Œä¸€ä½å¤–å›½ç”»å®¶æ²Ÿé€šï¼Œä½ éœ€è¦ä¸€ä½ç¿»è¯‘æ¥å¸®åŠ©ä½ å‡†ç¡®ä¼ è¾¾ä½ æƒ³è¦çš„ç”»ä½œæ•ˆæžœã€‚è¿™ä¸ªèŠ‚ç‚¹å°±åƒé‚£ä½ç¿»è¯‘ï¼Œå®ƒä½¿ç”¨CLIPæ¨¡åž‹ï¼ˆä¸€ä¸ªç»è¿‡å¤§é‡å›¾æ–‡è®­ç»ƒçš„AIæ¨¡åž‹ï¼‰æ¥ç†è§£ä½ çš„æ–‡å­—æè¿°ï¼Œå¹¶å°†å…¶è½¬æ¢æˆAIç»˜ç”»æ¨¡åž‹èƒ½ç†è§£çš„â€è¯­è¨€æŒ‡ä»¤â€ã€‚\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | è¾“å…¥æ–¹å¼ | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | åŠŸèƒ½è¯´æ˜Ž |\n| --- | --- | --- | --- | --- | --- |\n| text | STRING | æ–‡æœ¬è¾“å…¥æ¡† | ç©º   | ä»»æ„æ–‡æœ¬ | å°±åƒç»™ç”»å®¶çš„è¯¦ç»†è¯´æ˜Žï¼Œåœ¨è¿™é‡Œè¾“å…¥ä½ æƒ³è¦ç”Ÿæˆçš„å›¾åƒçš„æ–‡å­—æè¿°ã€‚æ”¯æŒå¤šè¡Œæ–‡æœ¬ï¼Œå¯ä»¥éžå¸¸è¯¦ç»†åœ°æè¿°ä½ æƒ³è¦çš„æ•ˆæžœã€‚ |\n| clip | CLIP | æ¨¡åž‹é€‰æ‹© | æ—    | å·²åŠ è½½çš„CLIPæ¨¡åž‹ | ç›¸å½“äºŽé€‰æ‹©ç‰¹å®šçš„ç¿»è¯‘å®˜ï¼Œä¸åŒçš„CLIPæ¨¡åž‹å°±åƒä¸åŒçš„ç¿»è¯‘å®˜ï¼Œä»–ä»¬å¯¹è‰ºæœ¯é£Žæ ¼çš„ç†è§£ç•¥æœ‰ä¸åŒã€‚ |\n\n## è¾“å‡º\n\n| è¾“å‡ºåç§° | æ•°æ®ç±»åž‹ | è¯´æ˜Ž  |\n| --- | --- | --- |\n| æ¡ä»¶  | CONDITIONING | è¿™æ˜¯è½¬æ¢åŽçš„â€ç»˜ç”»æŒ‡ä»¤â€ï¼ŒåŒ…å«äº†AIèƒ½å¤Ÿç†è§£çš„è¯¦ç»†åˆ›ä½œæŒ‡å¼•ã€‚è¿™äº›æŒ‡ä»¤ä¼šå‘Šè¯‰AIæ¨¡åž‹åº”è¯¥å¦‚ä½•ç»˜åˆ¶ç¬¦åˆä½ æè¿°çš„å›¾åƒã€‚ |\n\n## ä½¿ç”¨å»ºè®®\n\n1.  **æ–‡æœ¬æç¤ºçš„åŸºæœ¬ç”¨æ³•**\n    *   å¯ä»¥åƒå†™ä½œæ–‡ä¸€æ ·è¯¦ç»†æè¿°ä½ æƒ³è¦çš„å›¾åƒ\n    *   è¶Šå…·ä½“çš„æè¿°ï¼Œç”Ÿæˆçš„å›¾åƒè¶Šç¬¦åˆé¢„æœŸ\n    *   å¯ä»¥ä½¿ç”¨è‹±æ–‡é€—å·åˆ†éš”ä¸åŒçš„æè¿°è¦ç´ \n2.  **ç‰¹æ®ŠåŠŸèƒ½ï¼šä½¿ç”¨Embeddingæ¨¡åž‹**\n    *   Embeddingæ¨¡åž‹å°±åƒé¢„è®¾çš„è‰ºæœ¯é£Žæ ¼åŒ…ï¼Œå¯ä»¥å¿«é€Ÿåº”ç”¨ç‰¹å®šçš„è‰ºæœ¯æ•ˆæžœ\n    *   ç›®å‰æ”¯æŒ .safetensorsã€.ptã€.bin è¿™ä¸‰ç§æ–‡ä»¶æ ¼å¼ï¼Œä½ ä¸ä¸€å®šéœ€è¦åœ¨ä½¿ç”¨çš„æ—¶å€™ç”¨å®Œæ•´çš„æ¨¡åž‹åç§°\n    *   ä½¿ç”¨æ–¹æ³•ï¼š\n        \n        1.  å°†embeddingæ¨¡åž‹æ–‡ä»¶(.ptæ ¼å¼)æ”¾å…¥`ComfyUI/models/embeddings`æ–‡ä»¶å¤¹\n        2.  åœ¨æ–‡æœ¬ä¸­ä½¿ç”¨`embedding:æ¨¡åž‹åç§°`æ¥è°ƒç”¨ ä¾‹å¦‚ï¼šå¦‚æžœä½ æœ‰ä¸€ä¸ªå«`EasyNegative.pt`çš„æ¨¡åž‹ï¼Œå¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š\n        \n        ```\n        a beautiful landscape, embedding:EasyNegative, high quality\n        ```\n        \n3.  **æç¤ºè¯æƒé‡è°ƒæ•´**\n    *   å¯ä»¥ç”¨æ‹¬å·æ¥è°ƒæ•´æŸäº›æè¿°çš„é‡è¦ç¨‹åº¦\n    *   ä¾‹å¦‚ï¼š`(beautiful:1.2)`ä¼šè®©â€beautifulâ€è¿™ä¸ªç‰¹å¾æ›´çªå‡º\n    *   æ™®é€šæ‹¬å·`()`çš„é»˜è®¤æƒé‡æ˜¯1.1\n    *   ä½¿ç”¨é”®ç›˜å¿«æ·é”® `ctrl + ä¸Š/ä¸‹æ–¹å‘é”®` å¤´å¯ä»¥å¿«é€Ÿè°ƒæ•´æƒé‡\n    *   å¯¹åº”æƒé‡å¿«é€Ÿè°ƒæ•´æ­¥é•¿å¯ä»¥åœ¨è®¾ç½®ä¸­è¿›è¡Œä¿®æ”¹\n4.  **æ³¨æ„äº‹é¡¹**\n    *   ç¡®ä¿CLIPæ¨¡åž‹å·²æ­£ç¡®åŠ è½½\n    *   æ–‡æœ¬æè¿°å°½é‡ä½¿ç”¨æ­£é¢ã€æ˜Žç¡®çš„è¯è¯­\n    *   å¦‚æžœä½¿ç”¨embeddingæ¨¡åž‹ï¼Œç¡®ä¿æ–‡ä»¶åç§°è¾“å…¥æ­£ç¡®å¹¶ä¸”å’Œå½“å‰ä¸»æ¨¡åž‹çš„æž¶æž„å»åˆ"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipVisionEncode",
  "markdown": "# CLIPè§†è§‰ç¼–ç  - ComfyUIå†…ç½®èŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\n`CLIPè§†è§‰ç¼–ç ` èŠ‚ç‚¹æ˜¯ ComfyUI ä¸­çš„å›¾åƒç¼–ç èŠ‚ç‚¹ï¼Œç”¨äºŽå°†è¾“å…¥å›¾åƒé€šè¿‡ CLIP Vision æ¨¡åž‹è½¬æ¢ä¸ºè§†è§‰ç‰¹å¾å‘é‡ã€‚è¯¥èŠ‚ç‚¹æ˜¯è¿žæŽ¥å›¾åƒå’Œæ–‡æœ¬ç†è§£çš„é‡è¦æ¡¥æ¢ï¼Œå¹¿æ³›ç”¨äºŽå„ç§ AI å›¾åƒç”Ÿæˆå’Œå¤„ç†å·¥ä½œæµä¸­ã€‚ **èŠ‚ç‚¹åŠŸèƒ½**\n\n*   **å›¾åƒç‰¹å¾æå–**ï¼šå°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºé«˜ç»´ç‰¹å¾å‘é‡\n*   **å¤šæ¨¡æ€æ¡¥æŽ¥**ï¼šä¸ºå›¾åƒå’Œæ–‡æœ¬çš„è”åˆå¤„ç†æä¾›åŸºç¡€\n*   **æ¡ä»¶ç”Ÿæˆ**ï¼šä¸ºåŸºäºŽå›¾åƒçš„æ¡ä»¶ç”Ÿæˆæä¾›è§†è§‰æ¡ä»¶\n\n## è¾“å…¥å‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| `clipè§†è§‰` | CLIP\\_VISION | CLIPè§†è§‰æ¨¡åž‹ï¼Œé€šå¸¸é€šè¿‡ CLIPVisionLoader èŠ‚ç‚¹åŠ è½½ |\n| `å›¾åƒ` | IMAGE | éœ€è¦ç¼–ç çš„è¾“å…¥å›¾åƒ |\n| `è£å‰ª` | ä¸‹æ‹‰é€‰æ‹© | å›¾åƒè£å‰ªæ–¹å¼ï¼Œå¯é€‰å€¼ï¼šcenterï¼ˆå±…ä¸­è£å‰ªï¼‰ã€noneï¼ˆä¸è£å‰ªï¼‰ |\n\n## è¾“å‡ºå‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| CLIPè§†è§‰è¾“å‡º | CLIP\\_VISION\\_OUTPUT | ç¼–ç åŽçš„è§†è§‰ç‰¹å¾ |\n\nè¿™ä¸ªè¾“å‡ºå¯¹è±¡åŒ…å«:\n\n*   `last_hidden_state`: æœ€åŽä¸€å±‚çš„éšè—çŠ¶æ€\n*   `image_embeds`: å›¾åƒåµŒå…¥å‘é‡\n*   `penultimate_hidden_states`: å€’æ•°ç¬¬äºŒå±‚çš„éšè—çŠ¶æ€\n*   `mm_projected`: å¤šæ¨¡æ€æŠ•å½±ç»“æžœï¼ˆå¦‚æžœå¯ç”¨ï¼‰"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipVisionLoader",
  "markdown": "# åŠ è½½CLIPè§†è§‰ - ComfyUIå†…ç½®èŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\nè¯¥èŠ‚ç‚¹ä¼šæ£€æµ‹ä½äºŽ `ComfyUI/models/clip_vision` æ–‡ä»¶å¤¹ä¸‹çš„æ¨¡åž‹ï¼ŒåŒæ—¶ä¹Ÿä¼šè¯»å–åœ¨ `extra_model_paths.yaml` æ–‡ä»¶ä¸­é…ç½®çš„é¢å¤–è·¯å¾„çš„æ¨¡åž‹ï¼Œå¦‚æžœä½ çš„æ¨¡åž‹æ˜¯åœ¨ ComfyUI å¯åŠ¨åŽæ‰æ·»åŠ çš„ï¼Œè¯· **åˆ·æ–° ComfyUI ç•Œé¢** ä¿è¯å‰ç«¯èƒ½å¤ŸèŽ·å–åˆ°æœ€æ–°çš„æ¨¡åž‹æ–‡ä»¶åˆ—è¡¨\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | ä½œç”¨  |\n| --- | --- | --- |\n| `clipåç§°` | COMBO\\[STRING\\] | ä¼šèŽ·å–`ComfyUI/models/clip_vision` æ–‡ä»¶å¤¹ä¸‹å—æ”¯æŒæ ¼å¼çš„æ¨¡åž‹æ–‡ä»¶åˆ—è¡¨ |\n\n## è¾“å‡º\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | ä½œç”¨  |\n| --- | --- | --- |\n| `CLIPè§†è§‰` | CLIP\\_VISION | åŠ è½½çš„CLIPè§†è§‰æ¨¡åž‹ï¼Œå‡†å¤‡ç”¨äºŽç¼–ç å›¾åƒæˆ–æ‰§è¡Œå…¶ä»–è§†è§‰ç›¸å…³ä»»åŠ¡ã€‚ |"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/Load3D",
  "markdown": "# åŠ è½½3D - ComfyUIå†…ç½®èŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\nLoad3D èŠ‚ç‚¹ç”¨äºŽåŠ è½½å’Œå¤„ç† 3D æ¨¡åž‹æ–‡ä»¶çš„æ ¸å¿ƒèŠ‚ç‚¹ï¼Œåœ¨åŠ è½½èŠ‚ç‚¹æ—¶ä¼šè‡ªåŠ¨èŽ·å– `ComfyUI/input/3d/` å¯ç”¨çš„ 3D èµ„æºï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡ä¸Šä¼ åŠŸèƒ½å°†å—æ”¯æŒçš„ 3D æ–‡ä»¶ä¸Šä¼ ç„¶åŽè¿›è¡Œé¢„è§ˆã€‚ **æ”¯æŒæ ¼å¼** ç›®å‰è¯¥èŠ‚ç‚¹æ”¯æŒå¤šç§ 3D æ–‡ä»¶æ ¼å¼ï¼ŒåŒ…æ‹¬ `.gltf`ã€`.glb`ã€`.obj`ã€`.fbx` å’Œ `.stl`ã€‚ **3D èŠ‚ç‚¹é¢„è®¾** 3D èŠ‚ç‚¹çš„ä¸€äº›ç›¸å…³åå¥½è®¾ç½®å¯ä»¥åœ¨ ComfyUI çš„è®¾ç½®èœå•ä¸­è¿›è¡Œè®¾ç½®ï¼Œè¯·å‚è€ƒä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”çš„è®¾ç½® [è®¾ç½®èœå• - 3D](https://docs.comfy.org/zh-CN/interface/settings/3d) é™¤äº†å¸¸è§„çš„èŠ‚ç‚¹è¾“å‡ºä¹‹å¤–ï¼Œ Load3D æœ‰è®¸å¤šç›¸å…³çš„ 3D è§†å›¾ç›¸å…³æ“ä½œæ˜¯ä½äºŽé¢„è§ˆåŒºåŸŸèœå•, 3D èŠ‚ç‚¹\n\n## è¾“å…¥\n\n| å‚æ•°å | ç±»åž‹  | æè¿°  | é»˜è®¤å€¼ | èŒƒå›´  |\n| --- | --- | --- | --- | --- |\n| æ¨¡åž‹æ–‡ä»¶ | æ–‡ä»¶é€‰æ‹© | 3D æ¨¡åž‹æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒä¸Šä¼ ï¼Œé»˜è®¤è¯»å– `ComfyUI/input/3d/` ä¸‹çš„æ¨¡åž‹æ–‡ä»¶ | \\-  | å—æ”¯æŒæ ¼å¼æ–‡ä»¶ |\n| å®½åº¦  | INT | ç”»å¸ƒæ¸²æŸ“å®½åº¦ | 1024 | 1-4096 |\n| é«˜åº¦  | INT | ç”»å¸ƒæ¸²æŸ“é«˜åº¦ | 1024 | 1-4096 |\n\n## è¾“å‡º\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | è¯´æ˜Ž  |\n| --- | --- | --- |\n| image | IMAGE | ç”»å¸ƒæ¸²æŸ“æ¸²æŸ“å›¾åƒ |\n| mask | MASK | åŒ…å«å½“å‰æ¨¡åž‹ä½ç½®çš„é®ç½© |\n| mesh\\_path | STRING | æ¨¡åž‹æ–‡ä»¶è·¯å¾„åœ¨`ComfyUI/input` æ–‡ä»¶å¤¹ä¸‹çš„è·¯å¾„ |\n| normal | IMAGE | æ³•çº¿è´´å›¾ |\n| lineart | IMAGE | çº¿ç¨¿å›¾åƒè¾“å‡ºï¼Œå¯¹åº”çš„ `edge_threshold` å¯åœ¨ç”»å¸ƒçš„æ¨¡åž‹èœå•ä¸­è¿›è¡Œè°ƒèŠ‚ |\n| camera\\_info | LOAD3D\\_CAMERA | ç›¸æœºä¿¡æ¯ |\n| recording\\_video | VIDEO | å½•åˆ¶è§†é¢‘ï¼ˆä»…å½“æœ‰å½•åˆ¶è§†é¢‘å­˜åœ¨æ—¶ï¼‰ |\n\nå¯¹åº”æ‰€æœ‰çš„è¾“å‡ºé¢„è§ˆå¦‚ä¸‹ï¼š ![è§†å›¾æ“ä½œæ¼”ç¤º](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/load3d_outputs.jpg) \n\n## æ¨¡åž‹ç”»å¸ƒ(Canvas)åŒºè¯´æ˜Ž\n\nLoad 3D èŠ‚ç‚¹çš„ Canvas åŒºåŸŸåŒ…å«äº†è¯¸å¤šçš„è§†å›¾æ“ä½œï¼ŒåŒ…æ‹¬ï¼š\n\n*   é¢„è§ˆè§†å›¾è®¾ç½®ï¼ˆç½‘æ ¼ã€èƒŒæ™¯è‰²ã€é¢„è§ˆè§†å›¾ï¼‰\n*   ç›¸æœºæŽ§åˆ¶: æŽ§åˆ¶FOVã€ç›¸æœºç±»åž‹\n*   å…¨å±€å…‰ç…§å¼ºåº¦: è°ƒèŠ‚å…‰ç…§å¼ºåº¦\n*   è§†é¢‘å½•åˆ¶: å½•åˆ¶è§†é¢‘å¹¶å¯¼å‡ºè§†é¢‘\n*   æ¨¡åž‹å¯¼å‡º: æ”¯æŒ`GLB`ã€`OBJ`ã€`STL` æ ¼å¼\n*   ç­‰\n\n![Load 3D èŠ‚ç‚¹UI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/load3d_ui.jpg)\n\n1.  åŒ…å«äº† Load 3D èŠ‚ç‚¹çš„å¤šä¸ªèœå•ä»¥åŠéšè—èœå•\n2.  é‡æ–°`ç¼©æ”¾é¢„è§ˆçª—å£å¤§å°`ä»¥åŠè¿›è¡Œ`ç”»å¸ƒè§†é¢‘å½•åˆ¶`èœå•\n3.  3D è§†å›¾æ“ä½œè½´\n4.  é¢„è§ˆç¼©ç•¥å›¾\n5.  é¢„è§ˆå°ºå¯¸è®¾ç½®ï¼Œé€šè¿‡è®¾ç½®å°ºå¯¸ç„¶åŽå†ç¼©æ”¾çª—å£å¤§å°æ¥ç¼©æ”¾é¢„è§ˆè§†å›¾æ˜¾ç¤º\n\n### 1\\. è§†å›¾æ“ä½œ\n\nè§†å›¾æŽ§åˆ¶æ“ä½œï¼š\n\n*   é¼ æ ‡å·¦é”®ç‚¹å‡» + æ‹–æ‹½ï¼š è§†å›¾æ—‹è½¬æŽ§åˆ¶\n*   é¼ æ ‡å³é”® + æ‹–æ‹½ï¼š å¹³ç§»è§†å›¾\n*   é¼ æ ‡ä¸­é”®ï¼š ç¼©æ”¾æŽ§åˆ¶\n*   åæ ‡è½´ï¼š åˆ‡æ¢è§†å›¾\n\n### 2\\. å·¦ä¾§èœå•åŠŸèƒ½\n\n![Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu.jpg) åœ¨é¢„è§ˆåŒºåŸŸï¼Œæœ‰äº›è§†å›¾æ“ä½œç›¸å…³çš„èœå•è¢«éšè—åœ¨äº†èœå•é‡Œï¼Œç‚¹å‡»èœå•æŒ‰é’®å¯ä»¥å±•å¼€å¯¹åº”ä¸åŒçš„èœå•\n\n*   1.  åœºæ™¯ï¼ˆSceneï¼‰: åŒ…å«é¢„è§ˆçª—å£ç½‘æ ¼ã€èƒŒæ™¯è‰²ã€ç¼©ç•¥å›¾è®¾ç½®\n*   2.  æ¨¡åž‹ï¼ˆModelï¼‰: æ¨¡åž‹æ¸²æŸ“æ¨¡å¼ã€çº¹ç†æè´¨ã€ä¸Šæ–¹å‘è®¾ç½®\n*   3.  æ‘„åƒæœºï¼ˆCameraï¼‰: è½´æµ‹è§†å›¾å’Œé€è§†è§†å›¾åˆ‡æ¢ã€é€è§†è§†è§’å¤§å°è®¾ç½®\n*   4.  ç¯å…‰ï¼ˆLightï¼‰: åœºæ™¯å…¨å±€å…‰ç…§å¼ºåº¦\n*   5.  å¯¼å‡ºï¼ˆExportï¼‰: å¯¼å‡ºæ¨¡åž‹ä¸ºå…¶å®ƒæ ¼å¼ï¼ˆGLBã€OBJã€STLï¼‰\n\n#### åœºæ™¯ï¼ˆSceneï¼‰\n\n![scene menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_scene.jpg) åœºæ™¯èœå•æä¾›äº†å¯¹åœºæ™¯çš„ä¸€äº›åŸºç¡€è®¾ç½®åŠŸèƒ½\n\n1.  æ˜¾ç¤º / éšè—ç½‘æ ¼\n2.  è®¾ç½®èƒŒæ™¯è‰²\n3.  ç‚¹å‡»ä¸Šä¼ è®¾ç½®èƒŒæ™¯å›¾ç‰‡\n4.  éšè—é¢„è§ˆå›¾\n\n#### æ¨¡åž‹(Model)\n\n![Menu_Scene](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_model.jpg) æ¨¡åž‹èœå•æä¾›äº†ä¸€äº›æ¨¡åž‹çš„ç›¸å…³åŠŸèƒ½\n\n1.  **ä¸Šæ–¹å‘(Up direction)**: ç¡®å®šæ¨¡åž‹çš„å“ªä¸ªè½´ä¸ºä¸Šæ–¹å‘\n2.  **æ¸²æŸ“æ¨¡å¼ï¼ˆMaterial modeï¼‰**: æ¨¡åž‹æ¸²æŸ“æ¨¡å¼åˆ‡æ¢ åŽŸå§‹ï¼ˆOriginalï¼‰ã€æ³•çº¿(Normal)ã€çº¿æ¡†(Wireframe)ã€çº¿ç¨¿(Lineart)\n\n#### æ‘„åƒæœºï¼ˆCameraï¼‰\n\n![menu_modelmenu_camera](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_camera.jpg) è¯¥èœå•æä¾›äº†è½´æµ‹è§†å›¾å’Œé€è§†è§†å›¾åˆ‡æ¢ã€é€è§†è§†è§’å¤§å°è®¾ç½®\n\n1.  **ç›¸æœºï¼ˆCameraï¼‰**: åœ¨è½´æµ‹è§†å›¾å’Œæ­£äº¤è§†å›¾ä¹‹é—´å¿«é€Ÿåˆ‡æ¢\n2.  **è§†åœºè§’(FOV)**: è°ƒæ•´ FOV è§†è§’è§’åº¦\n\n#### ç¯å…‰ï¼ˆLightï¼‰\n\n![menu_modelmenu_camera](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_light.jpg) é€šè¿‡è¯¥èœå•å¯ä»¥å¿«é€Ÿè°ƒèŠ‚æ¨¡åž‹åœºæ™¯çš„å…¨å±€å…‰ç…§å¼ºåº¦\n\n#### å¯¼å‡ºï¼ˆExportï¼‰\n\n![menu_export](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/load3d/menu_export.jpg) è¯¥èœå•æä¾›äº†ä¸€ä¸ªå¿«é€Ÿè½¬æ¢æ¨¡åž‹æ ¼å¼å¹¶å¯¼å‡ºçš„èƒ½åŠ›\n\n### 3\\. å³ä¾§èœå•åŠŸèƒ½\n\nå³ä¾§èœå•çš„ä¸¤ä¸ªä¸»è¦åŠŸèƒ½ä¸ºï¼š\n\n1.  **é‡è®¾è§†å›¾æ¯”ä¾‹**ï¼š ç‚¹å‡»æŒ‰é’®åŽè§†å›¾å°†æ ¹æ®è®¾å®šå¥½çš„å®½é«˜æŒ‰æ¯”ä¾‹è°ƒæ•´ç”»å¸ƒæ¸²æŸ“åŒºåŸŸæ¯”ä¾‹\n2.  **è§†é¢‘å½•åˆ¶**ï¼š å…è®¸ä½ å°†å½“å‰çš„ 3D è§†å›¾æ“ä½œå½•åˆ¶ä¸ºè§†é¢‘ï¼Œå…è®¸å¯¼å…¥ï¼Œå¹¶å¯ä»¥ä½œä¸º `recording_video` è¾“å‡ºç»™åŽç»­èŠ‚ç‚¹"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/sampling/ksampler",
  "markdown": "# Ksampler - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\n```\n\ndef common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):\n    latent_image = latent[\"samples\"]\n    latent_image = comfy.sample.fix_empty_latent_channels(model, latent_image)\n\n    if disable_noise:\n        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device=\"cpu\")\n    else:\n        batch_inds = latent[\"batch_index\"] if \"batch_index\" in latent else None\n        noise = comfy.sample.prepare_noise(latent_image, seed, batch_inds)\n\n    noise_mask = None\n    if \"noise_mask\" in latent:\n        noise_mask = latent[\"noise_mask\"]\n\n    callback = latent_preview.prepare_callback(model, steps)\n    disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED\n    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,\n                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n    out = latent.copy()\n    out[\"samples\"] = samples\n    return (out, )\n\n\nclass KSampler:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"model\": (\"MODEL\", {\"tooltip\": \"The model used for denoising the input latent.\"}),\n                \"seed\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 0xffffffffffffffff, \"control_after_generate\": True, \"tooltip\": \"The random seed used for creating the noise.\"}),\n                \"steps\": (\"INT\", {\"default\": 20, \"min\": 1, \"max\": 10000, \"tooltip\": \"The number of steps used in the denoising process.\"}),\n                \"cfg\": (\"FLOAT\", {\"default\": 8.0, \"min\": 0.0, \"max\": 100.0, \"step\":0.1, \"round\": 0.01, \"tooltip\": \"The Classifier-Free Guidance scale balances creativity and adherence to the prompt. Higher values result in images more closely matching the prompt however too high values will negatively impact quality.\"}),\n                \"sampler_name\": (comfy.samplers.KSampler.SAMPLERS, {\"tooltip\": \"The algorithm used when sampling, this can affect the quality, speed, and style of the generated output.\"}),\n                \"scheduler\": (comfy.samplers.KSampler.SCHEDULERS, {\"tooltip\": \"The scheduler controls how noise is gradually removed to form the image.\"}),\n                \"positive\": (\"CONDITIONING\", {\"tooltip\": \"The conditioning describing the attributes you want to include in the image.\"}),\n                \"negative\": (\"CONDITIONING\", {\"tooltip\": \"The conditioning describing the attributes you want to exclude from the image.\"}),\n                \"latent_image\": (\"LATENT\", {\"tooltip\": \"The latent image to denoise.\"}),\n                \"denoise\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01, \"tooltip\": \"The amount of denoising applied, lower values will maintain the structure of the initial image allowing for image to image sampling.\"}),\n            }\n        }\n\n    RETURN_TYPES = (\"LATENT\",)\n    OUTPUT_TOOLTIPS = (\"The denoised latent.\",)\n    FUNCTION = \"sample\"\n\n    CATEGORY = \"sampling\"\n    DESCRIPTION = \"Uses the provided model, positive and negative conditioning to denoise the latent image.\"\n\n    def sample(self, model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0):\n        return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/latent/video/trim-video-latent",
  "markdown": "# TrimVideoLatent èŠ‚ç‚¹ - ComfyUI\n\n![ComfyUI TrimVideoLatent èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/latent/video/trim-video-latent.jpg) TrimVideoLatent èŠ‚ç‚¹ç”¨äºŽåœ¨æ½œåœ¨ç©ºé—´ï¼ˆLATENTï¼‰ä¸­è£å‰ªè§†é¢‘å¸§ã€‚å¸¸ç”¨äºŽå¤„ç†è§†é¢‘æ½œå˜é‡åºåˆ—æ—¶ï¼ŒåŽ»é™¤å‰é¢ä¸éœ€è¦çš„å¸§ï¼Œå®žçŽ°è§†é¢‘çš„â€œå‰å‘è£å‰ªâ€ã€‚ åŸºæœ¬ç”¨æ³•ï¼šå°†éœ€è¦è£å‰ªçš„è§†é¢‘æ½œå˜é‡è¾“å…¥åˆ° samplesï¼Œè®¾ç½® trim\\_amount ä¸ºè¦è£å‰ªçš„å¸§æ•°ã€‚èŠ‚ç‚¹ä¼šä»Žè§†é¢‘çš„å¼€å¤´è£å‰ªæŽ‰æŒ‡å®šæ•°é‡çš„å¸§ï¼Œè¾“å‡ºå‰©ä½™çš„æ½œå˜é‡åºåˆ—ã€‚ å…¸åž‹åœºæ™¯ï¼šç”¨äºŽè§†é¢‘ç”Ÿæˆã€è§†é¢‘ç¼–è¾‘ç­‰åœºæ™¯ä¸‹ï¼ŒåŽ»é™¤ä¸éœ€è¦çš„å‰ç½®å¸§ï¼Œæˆ–é…åˆå…¶ä»–èŠ‚ç‚¹å®žçŽ°è§†é¢‘ç‰‡æ®µçš„æ‹¼æŽ¥ä¸Žå¤„ç†ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### è¾“å…¥å‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | æ˜¯å¦å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- | --- |\n| samples | LATENT | æ˜¯   | æ—    | è¾“å…¥çš„æ½œåœ¨è§†é¢‘æ•°æ® |\n| trim\\_amount | INT | æ˜¯   | 0   | éœ€è¦è£å‰ªæŽ‰çš„å¸§æ•°ï¼ˆä»Žå‰å¾€åŽè£å‰ªï¼‰ |\n\n### è¾“å‡ºå‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| samples | LATENT | è£å‰ªåŽçš„è§†é¢‘æ½œå˜é‡ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Wan2.1 VACE è§†é¢‘ç”Ÿæˆå·¥ä½œæµç¤ºä¾‹\n\nWan2.1 VACE è§†é¢‘ç”Ÿæˆå·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/video/wan/vace)\n\n### æºç \n\n```\nclass TrimVideoLatent:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"samples\": (\"LATENT\",),\n                              \"trim_amount\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 99999}),\n                             }}\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"op\"\n\n    CATEGORY = \"latent/video\"\n\n    EXPERIMENTAL = True\n\n    def op(self, samples, trim_amount):\n        samples_out = samples.copy()\n\n        s1 = samples[\"samples\"]\n        samples_out[\"samples\"] = s1[:, :, trim_amount:]\n        return (samples_out,)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image",
  "markdown": "# Flux 1.1 Pro Ultra Image API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹å·¥ä½œæµ\n\nFLUX 1.1 Pro Ultra æ˜¯ç”± BlackForestLabs æŽ¨å‡ºçš„é«˜æ€§èƒ½ AI å›¾åƒç”Ÿæˆå·¥å…·ï¼Œä¸»æ‰“è¶…é«˜åˆ†è¾¨çŽ‡ä¸Žé«˜æ•ˆç”Ÿæˆèƒ½åŠ›ã€‚å®ƒæ”¯æŒé«˜è¾¾ 4MPï¼ˆæ ‡å‡†ç‰ˆçš„ 4 å€ï¼‰çš„è¶…æ¸…ç”»è´¨ï¼ŒåŒæ—¶å°†å•å¼ å›¾åƒç”Ÿæˆæ—¶é—´æŽ§åˆ¶åœ¨ 10 ç§’ä»¥å†…ï¼Œé€Ÿåº¦æ¯”åŒç±»é«˜åˆ†è¾¨çŽ‡æ¨¡åž‹å¿« 2.5 å€ã€‚ è¯¥å·¥å…·æä¾›ä¸¤ç§æ ¸å¿ƒæ¨¡å¼ï¼š\n\n*   **Ultra æ¨¡å¼**ï¼šä¸“ä¸ºé«˜åˆ†è¾¨çŽ‡éœ€æ±‚è®¾è®¡ï¼Œé€‚åˆå¹¿å‘Šã€ç”µå•†ç­‰éœ€è¦ç»†èŠ‚æ”¾å¤§çš„åœºæ™¯ï¼Œèƒ½ç²¾å‡†è¿˜åŽŸæç¤ºè¯å¹¶ä¿æŒç”Ÿæˆé€Ÿåº¦ã€‚\n*   **Raw æ¨¡å¼**ï¼šä¾§é‡è‡ªç„¶çœŸå®žæ„Ÿï¼Œä¼˜åŒ–äººåƒè‚¤è‰²ã€å…‰å½±åŠè‡ªç„¶æ™¯è§‚çš„ç»†èŠ‚ï¼Œå‡å°‘â€AI å‘³â€ï¼Œé€‚åˆæ‘„å½±è‰ºæœ¯å’ŒçœŸå®žé£Žæ ¼åˆ›ä½œã€‚\n\nç›®å‰åœ¨ ComfyUI ä¸­æˆ‘ä»¬å·²ç»æ”¯æŒäº† Flux 1.1 Pro Ultra Image èŠ‚ç‚¹ï¼Œåœ¨æœ¬ç¯‡æ–‡æ¡£ä¸­æˆ‘ä»¬å°†æ¶‰åŠä»¥ä¸‹å†…å®¹ï¼š\n\n*   Flux 1.1 Pro æ–‡ç”Ÿå›¾\n*   Flux 1.1 Pro å›¾ç”Ÿå›¾ï¼ˆRemixï¼‰\n\nä½ å¯æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”èŠ‚ç‚¹çš„è¯¦ç»†å‚æ•°è®¾ç½®\n\n*   [Flux 1.1 Pro Ultra Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/bfl/flux-1-1-pro-ultra-image)\n\n## Flux 1.1 \\[pro\\] æ–‡ç”Ÿå›¾æ•™ç¨‹\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![Flux 1.1 pro æ–‡ç”Ÿå›¾å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_t2i.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„å·¥ä½œæµè¿è¡Œ\n\n1.  (å¯é€‰)åœ¨ `Flux 1.1 [pro] Ultra Image` èŠ‚ç‚¹çš„ `prompt` ä¿®æ”¹å·¥ä½œæµçš„æç¤ºè¯\n2.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹ `raw` å‚æ•°ä¸º `false`,å¯ä»¥è®©è¾“å‡ºçš„å›¾åƒæ›´çœŸå®ž\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨`Save Image`èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒ, å¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³`ComfyUI/output/` ç›®å½•ä¸‹\n\n## Flux 1.1\\[pro\\] å›¾ç”Ÿå›¾æ•™ç¨‹\n\nå½“æˆ‘ä»¬åœ¨èŠ‚ç‚¹è¾“å…¥ä¸­æ·»åŠ äº† `image_prompt` åˆ™å¯¹åº”è¾“å‡ºç»“æžœå°†ä¼šèžåˆè¾“å…¥å›¾ç‰‡ç‰¹ç‚¹è¿›è¡Œæ··åˆï¼ˆRemixï¼‰ ï¼Œ`image_prompt_strength` çš„å¤§å°å½±å“æ··åˆçš„æ¯”ä¾‹:æ•°å€¼è¶Šå¤§ä¸Žè¾“å…¥å›¾åƒè¶Šç›¸ä¼¼.\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµï¼Œæˆ–è€…åœ¨ **æ–‡ç”Ÿå›¾å·¥ä½œæµ** ä¸­ç´«è‰²çš„èŠ‚ç‚¹ä¸Šå³é”®è®¾ç½® `æ¨¡å¼ï¼ˆmodeï¼‰` ä¸º `æ€»æ˜¯ï¼ˆalwaysï¼‰` æ¥å¯ç”¨ `image_prompt` è¾“å…¥ ![Flux 1.1 pro å›¾ç”Ÿå›¾Remix](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_i2i.png) æˆ‘ä»¬ä¼šå°†ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥ï¼š ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png) \n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„å·¥ä½œæµè¿è¡Œ\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸Šç‚¹å‡» **Upload** ä¸Šä¼ è¾“å…¥å›¾åƒ\n2.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹ `Flux 1.1 [pro] Ultra Image` ä¸­çš„ `image_prompt_strength` çš„å¤§å°ï¼Œæ¥æ”¹å˜æ··åˆçš„æ¯”ä¾‹\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨`Save Image`èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒ, å¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³`ComfyUI/output/` ç›®å½•ä¸‹\n\nä¸‹é¢æ˜¯ä¸åŒ `image_prompt_strength` çš„è¾“å‡ºç»“æžœå¯¹æ¯” ![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/conditioning/video-models/wan-vace-to-video",
  "markdown": "# Wan Vace To Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![Wan Vace To Video](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg) Wan Vace To Video èŠ‚ç‚¹å…è®¸æ‚¨é€šè¿‡æ–‡æœ¬æç¤ºè¯ç”Ÿæˆè§†é¢‘ï¼Œå¹¶æ”¯æŒå¤šç§è¾“å…¥æ–¹å¼ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€é®ç½©å’ŒæŽ§åˆ¶ä¿¡å·ç­‰ã€‚ è¯¥èŠ‚ç‚¹å°†è¾“å…¥çš„æ¡ä»¶ï¼ˆæç¤ºè¯ï¼‰ã€æŽ§åˆ¶è§†é¢‘å’Œé®ç½©ç»„åˆèµ·æ¥ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ã€‚å®ƒé¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œé¢„å¤„ç†å’Œç¼–ç ï¼Œç„¶åŽåº”ç”¨æ¡ä»¶ä¿¡æ¯æ¥ç”Ÿæˆæœ€ç»ˆçš„è§†é¢‘æ½œåœ¨è¡¨ç¤ºã€‚ å½“æä¾›å‚è€ƒå›¾åƒæ—¶ï¼Œå®ƒä¼šè¢«ä½œä¸ºè§†é¢‘çš„èµ·å§‹å‚è€ƒã€‚æŽ§åˆ¶è§†é¢‘å’Œé®ç½©å¯ä»¥ç”¨æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ï¼Œè®©ç”Ÿæˆçš„è§†é¢‘æ›´ç¬¦åˆçš„é¢„æœŸã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…é€‰å‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | é»˜è®¤å€¼ | èŒƒå›´  | è¯´æ˜Ž  |\n| --- | --- | --- | --- | --- |\n| positive | CONDITIONING | \\-  | \\-  | æ­£é¢æç¤ºè¯æ¡ä»¶ |\n| negative | CONDITIONING | \\-  | \\-  | è´Ÿé¢æç¤ºè¯æ¡ä»¶ |\n| vae | VAE | \\-  | \\-  | ç”¨äºŽç¼–ç /è§£ç çš„VAEæ¨¡åž‹ |\n| width | INT | 832 | 16-MAX\\_RESOLUTION | ç”Ÿæˆè§†é¢‘çš„å®½åº¦ï¼Œæ­¥é•¿ä¸º16 |\n| height | INT | 480 | 16-MAX\\_RESOLUTION | ç”Ÿæˆè§†é¢‘çš„é«˜åº¦ï¼Œæ­¥é•¿ä¸º16 |\n| length | INT | 81  | 1-MAX\\_RESOLUTION | ç”Ÿæˆè§†é¢‘çš„å¸§æ•°ï¼Œæ­¥é•¿ä¸º4 |\n| batch\\_size | INT | 1   | 1-4096 | æ‰¹å¤„ç†å¤§å° |\n| strength | FLOAT | 1.0 | 0.0-1000.0 | æ¡ä»¶å¼ºåº¦ï¼Œæ­¥é•¿ä¸º0.01 |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| control\\_video | IMAGE | æŽ§åˆ¶è§†é¢‘ï¼Œç”¨äºŽå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ |\n| control\\_masks | MASK | æŽ§åˆ¶é®ç½©ï¼Œå®šä¹‰è§†é¢‘ä¸­å“ªäº›åŒºåŸŸåº”å—åˆ°æŽ§åˆ¶ |\n| reference\\_image | IMAGE | å‚è€ƒå›¾åƒï¼Œä½œä¸ºè§†é¢‘ç”Ÿæˆçš„èµ·ç‚¹æˆ–å‚è€ƒï¼ˆå•å¼ ï¼‰ |\n\n### è¾“å‡ºå‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| positive | CONDITIONING | å¤„ç†åŽçš„æ­£é¢æç¤ºè¯æ¡ä»¶ |\n| negative | CONDITIONING | å¤„ç†åŽçš„è´Ÿé¢æç¤ºè¯æ¡ä»¶ |\n| latent | LATENT | ç”Ÿæˆçš„è§†é¢‘æ½œåœ¨è¡¨ç¤º |\n| trim\\_latent | INT | è£å‰ªæ½œåœ¨è¡¨ç¤ºçš„å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º0ã€‚å½“æä¾›å‚è€ƒå›¾åƒæ—¶ï¼Œè¯¥å€¼ä¼šè¢«è®¾ç½®ä¸ºå‚è€ƒå›¾åƒåœ¨æ½œåœ¨ç©ºé—´ä¸­çš„å½¢çŠ¶å°ºå¯¸ã€‚å®ƒæŒ‡ç¤ºä¸‹æ¸¸èŠ‚ç‚¹éœ€è¦ä»Žç”Ÿæˆçš„æ½œåœ¨è¡¨ç¤ºä¸­è£å‰ªæŽ‰å¤šå°‘æ¥è‡ªå‚è€ƒå›¾åƒçš„å†…å®¹ï¼Œç¡®ä¿æœ€ç»ˆè§†é¢‘è¾“å‡ºä¸­å‚è€ƒå›¾åƒçš„å½±å“è¢«é€‚å½“æŽ§åˆ¶ã€‚ |\n\n## æºç \n\n\\[æºç æ›´æ–°æ—¶é—´: 2025-05-15\\]\n\n```\nclass WanVaceToVideo:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": {\"positive\": (\"CONDITIONING\", ),\n                             \"negative\": (\"CONDITIONING\", ),\n                             \"vae\": (\"VAE\", ),\n                             \"width\": (\"INT\", {\"default\": 832, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                             \"height\": (\"INT\", {\"default\": 480, \"min\": 16, \"max\": nodes.MAX_RESOLUTION, \"step\": 16}),\n                             \"length\": (\"INT\", {\"default\": 81, \"min\": 1, \"max\": nodes.MAX_RESOLUTION, \"step\": 4}),\n                             \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}),\n                             \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 1000.0, \"step\": 0.01}),\n                },\n                \"optional\": {\"control_video\": (\"IMAGE\", ),\n                             \"control_masks\": (\"MASK\", ),\n                             \"reference_image\": (\"IMAGE\", ),\n                }}\n\n    RETURN_TYPES = (\"CONDITIONING\", \"CONDITIONING\", \"LATENT\", \"INT\")\n    RETURN_NAMES = (\"positive\", \"negative\", \"latent\", \"trim_latent\")\n    FUNCTION = \"encode\"\n\n    CATEGORY = \"conditioning/video_models\"\n\n    EXPERIMENTAL = True\n\n    def encode(self, positive, negative, vae, width, height, length, batch_size, strength, control_video=None, control_masks=None, reference_image=None):\n        latent_length = ((length - 1) // 4) + 1\n        if control_video is not None:\n            control_video = comfy.utils.common_upscale(control_video[:length].movedim(-1, 1), width, height, \"bilinear\", \"center\").movedim(1, -1)\n            if control_video.shape[0] < length:\n                control_video = torch.nn.functional.pad(control_video, (0, 0, 0, 0, 0, 0, 0, length - control_video.shape[0]), value=0.5)\n        else:\n            control_video = torch.ones((length, height, width, 3)) * 0.5\n\n        if reference_image is not None:\n            reference_image = comfy.utils.common_upscale(reference_image[:1].movedim(-1, 1), width, height, \"bilinear\", \"center\").movedim(1, -1)\n            reference_image = vae.encode(reference_image[:, :, :, :3])\n            reference_image = torch.cat([reference_image, comfy.latent_formats.Wan21().process_out(torch.zeros_like(reference_image))], dim=1)\n\n        if control_masks is None:\n            mask = torch.ones((length, height, width, 1))\n        else:\n            mask = control_masks\n            if mask.ndim == 3:\n                mask = mask.unsqueeze(1)\n            mask = comfy.utils.common_upscale(mask[:length], width, height, \"bilinear\", \"center\").movedim(1, -1)\n            if mask.shape[0] < length:\n                mask = torch.nn.functional.pad(mask, (0, 0, 0, 0, 0, 0, 0, length - mask.shape[0]), value=1.0)\n\n        control_video = control_video - 0.5\n        inactive = (control_video * (1 - mask)) + 0.5\n        reactive = (control_video * mask) + 0.5\n\n        inactive = vae.encode(inactive[:, :, :, :3])\n        reactive = vae.encode(reactive[:, :, :, :3])\n        control_video_latent = torch.cat((inactive, reactive), dim=1)\n        if reference_image is not None:\n            control_video_latent = torch.cat((reference_image, control_video_latent), dim=2)\n\n        vae_stride = 8\n        height_mask = height // vae_stride\n        width_mask = width // vae_stride\n        mask = mask.view(length, height_mask, vae_stride, width_mask, vae_stride)\n        mask = mask.permute(2, 4, 0, 1, 3)\n        mask = mask.reshape(vae_stride * vae_stride, length, height_mask, width_mask)\n        mask = torch.nn.functional.interpolate(mask.unsqueeze(0), size=(latent_length, height_mask, width_mask), mode='nearest-exact').squeeze(0)\n\n        trim_latent = 0\n        if reference_image is not None:\n            mask_pad = torch.zeros_like(mask[:, :reference_image.shape[2], :, :])\n            mask = torch.cat((mask_pad, mask), dim=1)\n            latent_length += reference_image.shape[2]\n            trim_latent = reference_image.shape[2]\n\n        mask = mask.unsqueeze(0)\n        positive = node_helpers.conditioning_set_values(positive, {\"vace_frames\": control_video_latent, \"vace_mask\": mask, \"vace_strength\": strength})\n        negative = node_helpers.conditioning_set_values(negative, {\"vace_frames\": control_video_latent, \"vace_mask\": mask, \"vace_strength\": strength})\n\n        latent = torch.zeros([batch_size, 16, latent_length, height // 8, width // 8], device=comfy.model_management.intermediate_device())\n        out_latent = {}\n        out_latent[\"samples\"] = latent\n        return (positive, negative, out_latent, trim_latent)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/ClipSetLastLayer",
  "markdown": "# è®¾ç½®CLIPæœ€åŽä¸€å±‚ - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£ - ComfyUI\n\n`è®¾ç½®CLIPæœ€åŽä¸€å±‚` æ˜¯ ComfyUI ä¸­ç”¨äºŽæŽ§åˆ¶ CLIP æ¨¡åž‹å¤„ç†æ·±åº¦çš„æ ¸å¿ƒèŠ‚ç‚¹ã€‚å®ƒå…è®¸ç”¨æˆ·ç²¾ç¡®æŽ§åˆ¶ CLIP æ–‡æœ¬ç¼–ç å™¨åœ¨å“ªä¸€å±‚åœæ­¢å¤„ç†ï¼Œä»Žè€Œå½±å“æ–‡æœ¬ç†è§£çš„æ·±åº¦å’Œç”Ÿæˆå›¾åƒçš„é£Žæ ¼ã€‚ æƒ³è±¡ CLIP æ¨¡åž‹æ˜¯ä¸€ä¸ª24å±‚çš„æ™ºèƒ½å¤§è„‘ï¼š\n\n*   æµ…å±‚ (1-8å±‚)ï¼šè¯†åˆ«åŸºæœ¬å­—æ¯ã€å•è¯\n*   ä¸­å±‚ (9-16å±‚)ï¼šç†è§£è¯­æ³•ã€å¥å­ç»“æž„\n*   æ·±å±‚ (17-24å±‚)ï¼šæŽŒæ¡æŠ½è±¡æ¦‚å¿µã€å¤æ‚è¯­ä¹‰\n\n`è®¾ç½®CLIPæœ€åŽä¸€å±‚` å°±åƒä¸€ä¸ª **â€œæ€è€ƒæ·±åº¦è°ƒèŠ‚å™¨â€**ï¼š \\-1: ä½¿ç”¨å…¨éƒ¨24å±‚ï¼ˆæœ€å®Œæ•´ç†è§£ï¼‰ -2: åœåœ¨ç¬¬23å±‚ï¼ˆç¨å¾®ç®€åŒ–ï¼‰ -12: åœåœ¨ç¬¬13å±‚ï¼ˆä¸­ç­‰ç†è§£ï¼‰ -24: åªç”¨ç¬¬1å±‚ï¼ˆæœ€åŸºç¡€ç†è§£ï¼‰\n\n## è¾“å…¥\n\n| å‚æ•°åç§° | æ•°æ®ç±»åž‹ | é»˜è®¤å€¼ | å–å€¼èŒƒå›´ | åŠŸèƒ½è¯´æ˜Ž |\n| --- | --- | --- | --- | --- |\n| `clip` | CLIP | \\-  | \\-  | è¦ä¿®æ”¹çš„CLIPæ¨¡åž‹ |\n| `åœæ­¢åœ¨ CLIPå±‚` | INT | \\-1 | \\-24 åˆ° -1 | æŒ‡å®šåœæ­¢å¤„ç†çš„å±‚çº§ï¼Œ-1è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨å±‚çº§ï¼Œ-24è¡¨ç¤ºåªç”¨ç¬¬ä¸€å±‚ |\n\n## è¾“å‡º\n\n| è¾“å‡ºåç§° | æ•°æ®ç±»åž‹ | è¯´æ˜Ž  |\n| --- | --- | --- |\n| clip | CLIP | å·²ä¿®æ”¹çš„CLIPæ¨¡åž‹ï¼ŒæŒ‡å®šçš„å±‚è¢«è®¾ç½®ä¸ºæœ€åŽä¸€å±‚ |\n\n## ä¸ºä»€ä¹ˆéœ€è¦è®¾ç½®æœ€åŽä¸€å±‚\n\n*   **æ€§èƒ½ä¼˜åŒ–**ï¼šå°±åƒä¸éœ€è¦åšå£«å­¦ä½æ¥ç†è§£ç®€å•å¥å­ä¸€æ ·ï¼Œæœ‰æ—¶æµ…å±‚ç†è§£å°±å¤Ÿäº†ï¼Œé€Ÿåº¦æ›´å¿«\n*   **é£Žæ ¼æŽ§åˆ¶**ï¼šä¸åŒå±‚æ¬¡çš„ç†è§£ä¼šäº§ç”Ÿä¸åŒçš„è‰ºæœ¯é£Žæ ¼\n*   **å…¼å®¹æ€§**ï¼šæŸäº›æ¨¡åž‹å¯èƒ½åœ¨ç‰¹å®šå±‚æ¬¡ä¸Šè¡¨çŽ°æ›´å¥½\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-replace-background",
  "markdown": "# Recraft Replace Background - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\n\nclass RecraftReplaceBackgroundNode:\n    \"\"\"\n    Replace background on image, based on provided prompt.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt: str,\n        n: int,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n        )\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/replaceBackground\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-image",
  "markdown": "# Luma Text to Image API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\n[Luma Text to Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-text-to-image) èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨Luma AIçš„å…ˆè¿›æŠ€æœ¯æ ¹æ®æ–‡æœ¬æç¤ºè¯ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œèƒ½å¤Ÿåˆ›å»ºç…§ç‰‡çº§åˆ«çš„é€¼çœŸå†…å®¹å’Œè‰ºæœ¯é£Žæ ¼å›¾åƒã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å¦‚ä½•ä½¿ç”¨å¯¹åº”èŠ‚ç‚¹æ¥è¿›è¡Œæ–‡æœ¬ç”Ÿå›¾çš„å·¥ä½œæµè®¾ç½®ã€‚\n\nä½ å¯æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”èŠ‚ç‚¹çš„è¯¦ç»†å‚æ•°è®¾ç½®ç­‰\n\n[\n\n## Luma Text to Image èŠ‚ç‚¹æ–‡æ¡£\n\nLuma Text to Image API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-text-to-image)[\n\n## Luma Reference èŠ‚ç‚¹æ–‡æ¡£\n\nLuma Reference API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-reference)\n\n## Luma Text to Image API èŠ‚ç‚¹æ–‡æœ¬ç”Ÿå›¾å·¥ä½œæµ\n\nåœ¨ `Luma Text to Image` èŠ‚ç‚¹æ²¡æœ‰ä½¿ç”¨ä»»ä½•çš„å›¾åƒè¾“å…¥æ—¶ï¼Œå¯¹åº”çš„å·¥ä½œæµåˆ™ä¸ºæ–‡ç”Ÿå›¾å·¥ä½œæµï¼Œåœ¨æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬åˆ¶ä½œäº†ä½¿ç”¨`style_image` å’Œ `image_luma_ref` çš„ç¤ºä¾‹ã€‚ èƒ½å¤Ÿè®©ä½ ä½“éªŒåˆ° Luma AI åœ¨ç›¸å…³å›¾åƒå¤„ç†ä¸Šçš„ä¼˜ç§€èƒ½åŠ›ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![Luma æ–‡æœ¬ç”Ÿå›¾å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/luma_t2i.png) è¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä¼šç”¨ä½œè¾“å…¥ï¼š ![è¾“å…¥å›¾ç‰‡-å‚è€ƒ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_ref.png) ![è¾“å…¥å›¾ç‰‡-é£Žæ ¼](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_style.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Luma æ–‡æœ¬ç”Ÿå›¾å·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„å·¥ä½œæµè¿è¡ŒÂ Â \n\n1.  åœ¨ `Load image` èŠ‚ç‚¹ä¸­ä¸Šä¼ å‚è€ƒå›¾åƒ\n2.  åœ¨ `Load imageï¼ˆå·²é‡å‘½åä¸º stylerefï¼‰` èŠ‚ç‚¹ä¸­ä¸Šä¼ é£Žæ ¼å‚è€ƒå›¾åƒ\n3.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹ `Luma Text to Image` èŠ‚ç‚¹çš„æç¤ºè¯\n4.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹ `style_image_weight` çš„æƒé‡ï¼Œæ¥è°ƒæ•´é£Žæ ¼å‚è€ƒå›¾åƒçš„æƒé‡\n5.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n6.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨`Save Image`èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒ, å¯¹åº”çš„å›¾ç‰‡ä¹Ÿä¼šè¢«ä¿å­˜è‡³`ComfyUI/output/` ç›®å½•ä¸‹\n\n![ä¸åŒ style_image_weight æ•ˆæžœå¯¹æ¯”](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg)\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\n*   [å¯¹åº”çš„èŠ‚ç‚¹](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-text-to-image)åŒæ—¶å…è®¸æœ€å¤šåŒæ—¶åˆ†åˆ«è¾“å…¥ 4 å¼ å‚è€ƒå›¾å’Œè§’è‰²å‚è€ƒã€‚\n*   å¦‚æžœè¦å¯ç”¨å¤šå¼ å›¾åƒè¾“å…¥å‚è€ƒï¼Œè¯·åœ¨å¯¹åº”â€œç´«è‰²â€çš„å¤„äºŽ`ç»•è¿‡(Bypass)` çš„èŠ‚ç‚¹ä¸Šå³é”®ï¼Œå°†å¯¹åº”çš„ `æ¨¡å¼ï¼ˆmodeï¼‰` è®¾ç½®ä¸º `æ€»æ˜¯ï¼ˆalwaysï¼‰`"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/save-svg",
  "markdown": "# Save SVG - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Save SVG èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/save-svg.jpg) Save SVG èŠ‚ç‚¹å…è®¸ä½ å°†ä»ŽRecraftçŸ¢é‡ç”ŸæˆèŠ‚ç‚¹èŽ·å–çš„SVGæ•°æ®ä¿å­˜ä¸ºæ–‡ä»¶ç³»ç»Ÿä¸­çš„å¯ç”¨æ–‡ä»¶ã€‚è¿™æ˜¯å¤„ç†å’Œå¯¼å‡ºçŸ¢é‡å›¾å½¢çš„å¿…è¦ç»„ä»¶ã€‚\n\n## èŠ‚ç‚¹åŠŸèƒ½\n\næ­¤èŠ‚ç‚¹æŽ¥æ”¶SVGçŸ¢é‡æ•°æ®ï¼Œå¹¶å°†å…¶ä¿å­˜ä¸ºæ–‡ä»¶ç³»ç»Ÿä¸­çš„æ ‡å‡†SVGæ–‡ä»¶ã€‚å®ƒæ”¯æŒè‡ªåŠ¨æ–‡ä»¶å‘½åå’Œä¿å­˜è·¯å¾„æŒ‡å®šï¼Œä½¿å¾—çŸ¢é‡å›¾å½¢å¯ä»¥è¢«å…¶ä»–è½¯ä»¶æ‰“å¼€å’Œç¼–è¾‘ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| svg | SVG | \\-  | è¦ä¿å­˜çš„ SVG çŸ¢é‡æ•°æ® |\n| filename\\_prefix | å­—ç¬¦ä¸² | â€recraftâ€ | æ–‡ä»¶åå‰ç¼€ |\n| output\\_dir | å­—ç¬¦ä¸² | \\-  | è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º ComfyUI è¾“å‡ºæ–‡ä»¶å¤¹å…·ä½“è·¯å¾„ä¸º `ComfyUI/output/svg/` |\n| index | æ•´æ•°  | \\-1 | ä¿å­˜ç´¢å¼•ï¼Œ-1 è¡¨ç¤ºæ‰€æœ‰ç”Ÿæˆçš„ SVG |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| SVG | SVG | ä¼ é€’è¾“å…¥çš„SVGæ•°æ® |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Recraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\nRecraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass SaveSVGNode:\n    \"\"\"\n    Save SVG files on disk.\n    \"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n        self.prefix_append = \"\"\n\n    RETURN_TYPES = ()\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"save_svg\"\n    CATEGORY = \"api node/image/Recraft\"\n    OUTPUT_NODE = True\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"svg\": (RecraftIO.SVG,),\n                \"filename_prefix\": (\"STRING\", {\"default\": \"svg/ComfyUI\", \"tooltip\": \"The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes.\"})\n            },\n            \"hidden\": {\n                \"prompt\": \"PROMPT\",\n                \"extra_pnginfo\": \"EXTRA_PNGINFO\"\n            }\n        }\n\n    def save_svg(self, svg: SVG, filename_prefix=\"svg/ComfyUI\", prompt=None, extra_pnginfo=None):\n        filename_prefix += self.prefix_append\n        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n        results = list()\n\n        # Prepare metadata JSON\n        metadata_dict = {}\n        if prompt is not None:\n            metadata_dict[\"prompt\"] = prompt\n        if extra_pnginfo is not None:\n            metadata_dict.update(extra_pnginfo)\n\n        # Convert metadata to JSON string\n        metadata_json = json.dumps(metadata_dict, indent=2) if metadata_dict else None\n\n        for batch_number, svg_bytes in enumerate(svg.data):\n            filename_with_batch_num = filename.replace(\"%batch_num%\", str(batch_number))\n            file = f\"{filename_with_batch_num}_{counter:05}_.svg\"\n\n            # Read SVG content\n            svg_bytes.seek(0)\n            svg_content = svg_bytes.read().decode('utf-8')\n\n            # Inject metadata if available\n            if metadata_json:\n                # Create metadata element with CDATA section\n                metadata_element = f\"\"\"  <metadata>\n    <![CDATA[\n{metadata_json}\n    ]]>\n  </metadata>\n\"\"\"\n                # Insert metadata after opening svg tag using regex\n                import re\n                svg_content = re.sub(r'(<svg[^>]*>)', r'\\1\\n' + metadata_element, svg_content)\n\n            # Write the modified SVG to file\n            with open(os.path.join(full_output_folder, file), 'wb') as svg_file:\n                svg_file.write(svg_content.encode('utf-8'))\n\n            results.append({\n                \"filename\": file,\n                \"subfolder\": subfolder,\n                \"type\": self.type\n            })\n            counter += 1\n        return { \"ui\": { \"images\": results } }\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/minimax/minimax-image-to-video",
  "markdown": "# MiniMax Image to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ MiniMax Image to Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg) MiniMax Image to Video èŠ‚ç‚¹ä½¿ç”¨ MiniMax çš„APIï¼ŒåŸºäºŽè¾“å…¥å›¾åƒå’Œæç¤ºè¯åŒæ­¥ç”Ÿæˆè§†é¢‘å†…å®¹ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | ç”¨ä½œè§†é¢‘ç”Ÿæˆç¬¬ä¸€å¸§çš„è¾“å…¥å›¾åƒ |\n| prompt\\_text | å­—ç¬¦ä¸² | \"\"  | å¼•å¯¼è§†é¢‘ç”Ÿæˆçš„æ–‡æœ¬æç¤ºè¯ |\n| model | é€‰æ‹©é¡¹ | â€I2V-01â€ | å¯é€‰æ¨¡åž‹åŒ…æ‹¬â€I2V-01-Directorâ€ã€â€œI2V-01â€ã€â€œI2V-01-liveâ€ |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| seed | æ•´æ•°  | ç”¨äºŽåˆ›å»ºå™ªå£°çš„éšæœºç§å­å€¼ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass MinimaxImageToVideoNode(MinimaxTextToVideoNode):\n    \"\"\"\n    Generates videos synchronously based on an image and prompt, and optional parameters using Minimax's API.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"tooltip\": \"Image to use as first frame of video generation\"\n                    },\n                ),\n                \"prompt_text\": (\n                    \"STRING\",\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt to guide the video generation\",\n                    },\n                ),\n                \"model\": (\n                    [\n                        \"I2V-01-Director\",\n                        \"I2V-01\",\n                        \"I2V-01-live\",\n                    ],\n                    {\n                        \"default\": \"I2V-01\",\n                        \"tooltip\": \"Model to use for video generation\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from an image and prompts using Minimax's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Minimax\"\n    API_NODE = True\n    OUTPUT_NODE = True\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/openai/openai-gpt-image1",
  "markdown": "# OpenAI GPT Image 1 - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”ŸOpenAI GPT Image 1  èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-gpt-image-1.jpg) æ­¤èŠ‚ç‚¹è¿žæŽ¥åˆ°OpenAIçš„ GPT Image 1 APIï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è¯¦ç»†çš„æ–‡æœ¬æç¤ºè¯ç”Ÿæˆå›¾åƒã€‚GPT Image 1ä¸Žä¼ ç»Ÿçš„DALLÂ·Eæ¨¡åž‹ä¸åŒï¼Œå®ƒåˆ©ç”¨äº†GPT-4çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥å¤„ç†æ›´å¤æ‚å’Œè¯­å¢ƒä¸°å¯Œçš„æç¤ºè¯ï¼Œç”Ÿæˆæ›´ç¬¦åˆç”¨æˆ·æ„å›¾çš„å›¾åƒå†…å®¹ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | è¯¦ç»†æè¿°è¦ç”Ÿæˆå†…å®¹çš„æ–‡æœ¬æç¤ºè¯ |\n| quality | é€‰æ‹©é¡¹ | â€lowâ€ | å›¾åƒè´¨é‡çº§åˆ«ï¼Œå¯é€‰å€¼ï¼šâ€œlowâ€, â€œmediumâ€, â€œhighâ€ |\n| size | é€‰æ‹©é¡¹ | â€autoâ€ | è¾“å‡ºå›¾åƒå°ºå¯¸ï¼Œå¯é€‰å€¼ï¼šâ€œautoâ€, â€œ1024x1024â€, â€œ1024x1536â€, â€œ1536x1024â€ |\n\n### å›¾åƒç¼–è¾‘å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| image | å›¾åƒ  | ç”¨äºŽå›¾åƒç¼–è¾‘çš„è¾“å…¥å›¾åƒï¼Œæ”¯æŒæ‰¹é‡è¾“å…¥å¤šå¼ å›¾åƒ |\n| mask | æŽ©ç   | æŒ‡å®šå›¾åƒä¸­è¦ä¿®æ”¹çš„åŒºåŸŸ(å¯é€‰)ï¼Œä½¿ç”¨æŽ©ç æ—¶åªèƒ½è¾“å…¥å•å¼ å›¾åƒ |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| background | é€‰æ‹©é¡¹ | èƒŒæ™¯å¤„ç†é€‰é¡¹ï¼Œå¯é€‰å€¼ï¼šâ€œopaqueâ€(ä¸é€æ˜Ž), â€œtransparentâ€(é€æ˜Ž) |\n| seed | æ•´æ•°  | ç”Ÿæˆçš„éšæœºç§å­ï¼Œå½“å‰åœ¨åŽç«¯å°šæœªå®žçŽ° |\n| n   | æ•´æ•°  | ç”Ÿæˆçš„å›¾åƒæ•°é‡ï¼ŒèŒƒå›´1-8 |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | ç”Ÿæˆçš„å›¾åƒç»“æžœ |\n\n## å·¥ä½œåŽŸç†\n\nOpenAI GPT Image 1 èŠ‚ç‚¹ç»“åˆäº†GPT-4çš„è¯­è¨€ç†è§£èƒ½åŠ›å’Œå›¾åƒç”ŸæˆæŠ€æœ¯ã€‚å®ƒé¦–å…ˆåˆ†æžç”¨æˆ·æä¾›çš„æ–‡æœ¬æç¤ºè¯ï¼Œç†è§£å…¶è¯­ä¹‰å†…å®¹å’Œæ„å›¾ï¼Œç„¶åŽç”Ÿæˆç¬¦åˆæè¿°çš„å›¾åƒã€‚ å½“æä¾›è¾“å…¥å›¾åƒæ—¶ï¼ŒèŠ‚ç‚¹å¯ä»¥åœ¨å›¾åƒç¼–è¾‘æ¨¡å¼ä¸‹å·¥ä½œï¼Œå…è®¸å¯¹çŽ°æœ‰å›¾åƒè¿›è¡Œä¿®æ”¹ã€‚é€šè¿‡é¢å¤–æä¾›æŽ©ç ï¼Œç”¨æˆ·å¯ä»¥ç²¾ç¡®æŽ§åˆ¶å“ªäº›åŒºåŸŸåº”è¯¥è¢«ä¿®æ”¹ï¼Œå“ªäº›åº”è¯¥ä¿æŒä¸å˜ã€‚æ³¨æ„ä½¿ç”¨æŽ©ç æ—¶ï¼Œåªèƒ½æä¾›å•å¼ å›¾åƒè¾“å…¥ã€‚ ç”¨æˆ·å¯ä»¥é€šè¿‡è°ƒæ•´å„ç§å‚æ•°æ¥æŽ§åˆ¶ç”Ÿæˆç»“æžœï¼ŒåŒ…æ‹¬è´¨é‡çº§åˆ«ã€å°ºå¯¸ã€èƒŒæ™¯å¤„ç†å’Œç”Ÿæˆæ•°é‡ã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass OpenAIGPTImage1(ComfyNodeABC):\n    \"\"\"\n    Generates images synchronously via OpenAI's GPT Image 1 endpoint.\n\n    Uses the proxy at /proxy/openai/images/generations. Returned URLs are shortâ€‘lived,\n    so download or cache results if you need to keep them.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls) -> InputTypeDict:\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt for GPT Image 1\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 2**31 - 1,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"not implemented yet in backend\",\n                    },\n                ),\n                \"quality\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"low\", \"medium\", \"high\"],\n                        \"default\": \"low\",\n                        \"tooltip\": \"Image quality, affects cost and generation time.\",\n                    },\n                ),\n                \"background\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"opaque\", \"transparent\"],\n                        \"default\": \"opaque\",\n                        \"tooltip\": \"Return image with or without background\",\n                    },\n                ),\n                \"size\": (\n                    IO.COMBO,\n                    {\n                        \"options\": [\"auto\", \"1024x1024\", \"1024x1536\", \"1536x1024\"],\n                        \"default\": \"auto\",\n                        \"tooltip\": \"Image size\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 8,\n                        \"step\": 1,\n                        \"display\": \"number\",\n                        \"tooltip\": \"How many images to generate\",\n                    },\n                ),\n                \"image\": (\n                    IO.IMAGE,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional reference image for image editing.\",\n                    },\n                ),\n                \"mask\": (\n                    IO.MASK,\n                    {\n                        \"default\": None,\n                        \"tooltip\": \"Optional mask for inpainting (white areas will be replaced)\",\n                    },\n                ),\n            },\n            \"hidden\": {\"auth_token\": \"AUTH_TOKEN_COMFY_ORG\"},\n        }\n\n    RETURN_TYPES = (IO.IMAGE,)\n    FUNCTION = \"api_call\"\n    CATEGORY = \"api node/image/openai\"\n    DESCRIPTION = cleandoc(__doc__ or \"\")\n    API_NODE = True\n\n    def api_call(\n        self,\n        prompt,\n        seed=0,\n        quality=\"low\",\n        background=\"opaque\",\n        image=None,\n        mask=None,\n        n=1,\n        size=\"1024x1024\",\n        auth_token=None,\n    ):\n        model = \"gpt-image-1\"\n        path = \"/proxy/openai/images/generations\"\n        content_type=\"application/json\"\n        request_class = OpenAIImageGenerationRequest\n        img_binaries = []\n        mask_binary = None\n        files = []\n\n        if image is not None:\n            path = \"/proxy/openai/images/edits\"\n            request_class = OpenAIImageEditRequest\n            content_type =\"multipart/form-data\"\n\n            batch_size = image.shape[0]\n\n            for i in range(batch_size):\n                single_image = image[i : i + 1]\n                scaled_image = downscale_image_tensor(single_image).squeeze()\n\n                image_np = (scaled_image.numpy() * 255).astype(np.uint8)\n                img = Image.fromarray(image_np)\n                img_byte_arr = io.BytesIO()\n                img.save(img_byte_arr, format=\"PNG\")\n                img_byte_arr.seek(0)\n                img_binary = img_byte_arr\n                img_binary.name = f\"image_{i}.png\"\n\n                img_binaries.append(img_binary)\n                if batch_size == 1:\n                    files.append((\"image\", img_binary))\n                else:\n                    files.append((\"image[]\", img_binary))\n\n        if mask is not None:\n            if image.shape[0] != 1:\n                raise Exception(\"Cannot use a mask with multiple image\")\n            if image is None:\n                raise Exception(\"Cannot use a mask without an input image\")\n            if mask.shape[1:] != image.shape[1:-1]:\n                raise Exception(\"Mask and Image must be the same size\")\n            batch, height, width = mask.shape\n            rgba_mask = torch.zeros(height, width, 4, device=\"cpu\")\n            rgba_mask[:, :, 3] = 1 - mask.squeeze().cpu()\n\n            scaled_mask = downscale_image_tensor(rgba_mask.unsqueeze(0)).squeeze()\n\n            mask_np = (scaled_mask.numpy() * 255).astype(np.uint8)\n            mask_img = Image.fromarray(mask_np)\n            mask_img_byte_arr = io.BytesIO()\n            mask_img.save(mask_img_byte_arr, format=\"PNG\")\n            mask_img_byte_arr.seek(0)\n            mask_binary = mask_img_byte_arr\n            mask_binary.name = \"mask.png\"\n            files.append((\"mask\", mask_binary))\n\n        # Build the operation\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=path,\n                method=HttpMethod.POST,\n                request_model=request_class,\n                response_model=OpenAIImageGenerationResponse,\n            ),\n            request=request_class(\n                model=model,\n                prompt=prompt,\n                quality=quality,\n                background=background,\n                n=n,\n                seed=seed,\n                size=size,\n            ),\n            files=files if files else None,\n            content_type=content_type,\n            auth_token=auth_token,\n        )\n\n        response = operation.execute()\n\n        img_tensor = validate_and_cast_response(response)\n        return (img_tensor,)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image",
  "markdown": "# Recraft Style - Realistic Image - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Recraft Style - Realistic Image èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg) Recraft Style - Realistic Image èŠ‚ç‚¹ç”¨äºŽè®¾ç½®Recraftå›¾åƒç”Ÿæˆçš„çœŸå®žç…§ç‰‡é£Žæ ¼ï¼Œæä¾›å¤šç§å­é£Žæ ¼é€‰é¡¹ï¼Œä»¥æŽ§åˆ¶ç”Ÿæˆå›¾åƒçš„è§†è§‰ç‰¹æ€§ã€‚\n\n## èŠ‚ç‚¹åŠŸèƒ½\n\næ­¤èŠ‚ç‚¹åˆ›å»ºä¸€ä¸ªé£Žæ ¼é…ç½®å¯¹è±¡ï¼Œç”¨äºŽæŒ‡å¯¼Recraftçš„å›¾åƒç”Ÿæˆè¿‡ç¨‹æœå‘çœŸå®žç…§ç‰‡çš„è§†è§‰æ•ˆæžœã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| substyle | é€‰æ‹©é¡¹ | None | çœŸå®žç…§ç‰‡é£Žæ ¼çš„å…·ä½“å­é£Žæ ¼ï¼ˆå¿…é€‰ï¼‰ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | é£Žæ ¼é…ç½®å¯¹è±¡ï¼Œè¿žæŽ¥åˆ°Recraftç”ŸæˆèŠ‚ç‚¹ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Recraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\nRecraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass RecraftStyleV3RealisticImageNode:\n    \"\"\"\n    Select realistic_image style and optional substyle.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.STYLEV3,)\n    RETURN_NAMES = (\"recraft_style\",)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"create_style\"\n    CATEGORY = \"api node/image/Recraft\"\n\n    RECRAFT_STYLE = RecraftStyleV3.realistic_image\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"substyle\": (get_v3_substyles(s.RECRAFT_STYLE),),\n            }\n        }\n\n    def create_style(self, substyle: str):\n        if substyle == \"None\":\n            substyle = None\n        return (RecraftStyle(self.RECRAFT_STYLE, substyle),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra",
  "markdown": "# Stability Stable Image Ultra - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Stability Stable Image Ultra èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg) Stability Stable Image Ultra èŠ‚ç‚¹ä½¿ç”¨ Stability AI çš„ Stable Diffusion Ultra API ç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚å®ƒæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°å›¾åƒçš„ç”Ÿæˆï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºè¯åˆ›å»ºç»†èŠ‚ä¸°å¯Œã€è‰ºæœ¯è¡¨çŽ°åŠ›å¼ºçš„è§†è§‰å†…å®¹ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | è¯¦ç»†æè¿°è¦ç”Ÿæˆå†…å®¹çš„æ–‡æœ¬æç¤ºè¯ã€‚å¼ºå¤§ã€æè¿°æ€§çš„æç¤ºè¯èƒ½æ˜Žç¡®å®šä¹‰å…ƒç´ ã€é¢œè‰²å’Œä¸»é¢˜ï¼Œä»Žè€ŒèŽ·å¾—æ›´å¥½çš„ç»“æžœã€‚å¯ä»¥ä½¿ç”¨`(è¯:æƒé‡)`æ ¼å¼æŽ§åˆ¶ç‰¹å®šè¯çš„æƒé‡ï¼Œå…¶ä¸­æƒé‡ä¸º0åˆ°1ä¹‹é—´çš„å€¼ã€‚ä¾‹å¦‚ï¼š`å¤©ç©ºæ˜¯æ¸…çˆ½çš„(è“è‰²:0.3)å’Œ(ç»¿è‰²:0.8)`è¡¨ç¤ºå¤©ç©ºæ˜¯è“è‰²å’Œç»¿è‰²çš„ï¼Œä½†ç»¿è‰²æ¯”è“è‰²æ›´æ˜Žæ˜¾ã€‚ |\n| aspect\\_ratio | é€‰æ‹©é¡¹ | â€1:1â€ | è¾“å‡ºå›¾åƒçš„å®½é«˜æ¯” |\n| style\\_preset | é€‰æ‹©é¡¹ | â€Noneâ€ | å¯é€‰çš„ç”Ÿæˆå›¾åƒçš„é¢„è®¾é£Žæ ¼ |\n| seed | æ•´æ•°  | 0   | ç”¨äºŽåˆ›å»ºå™ªå£°çš„éšæœºç§å­ï¼ŒèŒƒå›´0-4294967294 |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | ç”¨äºŽå›¾åƒåˆ°å›¾åƒç”Ÿæˆçš„è¾“å…¥å›¾åƒ |\n| negative\\_prompt | å­—ç¬¦ä¸² | \"\"  | æè¿°ä¸å¸Œæœ›åœ¨è¾“å‡ºå›¾åƒä¸­çœ‹åˆ°çš„å†…å®¹ã€‚è¿™æ˜¯ä¸€ä¸ªé«˜çº§åŠŸèƒ½ |\n| image\\_denoise | æµ®ç‚¹æ•° | 0.5 | è¾“å…¥å›¾åƒçš„åŽ»å™ªå¼ºåº¦ï¼ŒèŒƒå›´0.0-1.0ã€‚0.0ä¼šäº§ç”Ÿä¸Žè¾“å…¥å®Œå…¨ç›¸åŒçš„å›¾åƒï¼Œ1.0åˆ™ç›¸å½“äºŽæ²¡æœ‰æä¾›ä»»ä½•å›¾åƒ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | ç”Ÿæˆçš„å›¾åƒç»“æžœ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\nStability AI Stable Image Ultra å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)\n\n## æ³¨æ„äº‹é¡¹\n\n*   å½“æœªæä¾›è¾“å…¥å›¾åƒæ—¶ï¼Œimage\\_denoiseå‚æ•°ä¸ä¼šç”Ÿæ•ˆ\n*   å¦‚æžœstyle\\_presetè®¾ç½®ä¸ºâ€Noneâ€ï¼Œåˆ™ä¸ä¼šåº”ç”¨ä»»ä½•é¢„è®¾é£Žæ ¼\n*   å½“ä½¿ç”¨å›¾åƒåˆ°å›¾åƒåŠŸèƒ½æ—¶ï¼Œè¾“å…¥å›¾åƒä¼šè½¬æ¢ä¸ºé€‚å½“çš„æ ¼å¼åŽå‘é€åˆ°API\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass StabilityStableImageUltraNode:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/stability\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines\" +\n                                    \"What you wish to see in the output image. A strong, descriptive prompt that clearly defines\" +\n                                    \"elements, colors, and subjects will lead to better results. \" +\n                                    \"To control the weight of a given word use the format `(word:weight)`,\" +\n                                    \"where `word` is the word you'd like to control the weight of and `weight`\" +\n                                    \"is a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`\" +\n                                    \"would convey a sky that was blue and green, but more green than blue.\"\n                    },\n                ),\n                \"aspect_ratio\": ([x.value for x in StabilityAspectRatio],\n                    {\n                        \"default\": StabilityAspectRatio.ratio_1_1,\n                        \"tooltip\": \"Aspect ratio of generated image.\",\n                    },\n                ),\n                \"style_preset\": (get_stability_style_presets(),\n                    {\n                        \"tooltip\": \"Optional desired style of generated image.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 4294967294,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"image\": (IO.IMAGE,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"A blurb of text describing what you do not wish to see in the output image. This is an advanced feature.\"\n                    },\n                ),\n                \"image_denoise\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(self, prompt: str, aspect_ratio: str, style_preset: str, seed: int,\n                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,\n                 auth_token=None):\n        # prepare image binary if image present\n        image_binary = None\n        if image is not None:\n            image_binary = tensor_to_bytesio(image, 1504 * 1504).read()\n        else:\n            image_denoise = None\n\n        if not negative_prompt:\n            negative_prompt = None\n        if style_preset == \"None\":\n            style_preset = None\n\n        files = {\n            \"image\": image_binary\n        }\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/stability/v2beta/stable-image/generate/ultra\",\n                method=HttpMethod.POST,\n                request_model=StabilityStableUltraRequest,\n                response_model=StabilityStableUltraResponse,\n            ),\n            request=StabilityStableUltraRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                aspect_ratio=aspect_ratio,\n                seed=seed,\n                strength=image_denoise,\n                style_preset=style_preset,\n            ),\n            files=files,\n            content_type=\"multipart/form-data\",\n            auth_token=auth_token,\n        )\n        response_api = operation.execute()\n\n        if response_api.finish_reason != \"SUCCESS\":\n            raise Exception(f\"Stable Image Ultra generation failed: {response_api.finish_reason}.\")\n\n        image_data = base64.b64decode(response_api.image)\n        returned_image = bytesio_to_image_tensor(BytesIO(image_data))\n\n        return (returned_image,)\n\n\n```\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-image-inpainting",
  "markdown": "# Recraft Image Inpainting - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\nclass RecraftImageInpaintingNode:\n    \"\"\"\n    Modify image based on prompt and mask.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"mask\": (IO.MASK, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        mask: torch.Tensor,\n        prompt: str,\n        n: int,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n            random_seed=seed,\n        )\n\n        # prepare mask tensor\n        _, H, W, _ = image.shape\n        mask = mask.unsqueeze(-1)\n        mask = mask.movedim(-1,1)\n        mask = common_upscale(mask, width=W, height=H, upscale_method=\"nearest-exact\", crop=\"disabled\")\n        mask = mask.movedim(1,-1)\n        mask = (mask > 0.5).float()\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                mask=mask[i:i+1],\n                path=\"/proxy/recraft/images/inpaint\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-text-to-image",
  "markdown": "# Recraft Text to Image - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Recraft Text to Image èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg) Recraft Text to Image èŠ‚ç‚¹å…è®¸ä½ é€šè¿‡æ–‡æœ¬æç¤ºè¯ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œç›´æŽ¥è¿žæŽ¥ Recraft AI çš„å›¾åƒç”Ÿæˆ APIï¼Œåˆ›å»ºå„ç§é£Žæ ¼çš„å›¾åƒä½œå“ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | ç”Ÿæˆå›¾åƒçš„æ–‡æœ¬æè¿° |\n| size | é€‰æ‹©é¡¹ | 1024x1024 | è¾“å‡ºå›¾åƒå°ºå¯¸ |\n| n   | æ•´æ•°  | 1   | ç”Ÿæˆå›¾åƒæ•°é‡(1-6) |\n| seed | æ•´æ•°  | 0   | éšæœºç§å­å€¼ |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | è®¾ç½®ç”Ÿæˆå›¾åƒçš„é£Žæ ¼ï¼Œé»˜è®¤ä¸ºâ€çœŸå®žç…§ç‰‡â€ |\n| negative\\_prompt | å­—ç¬¦ä¸² | æŒ‡å®šä¸å¸Œæœ›å‡ºçŽ°çš„å…ƒç´  |\n| recraft\\_controls | Recraft Controls | é™„åŠ æŽ§åˆ¶å‚æ•°(é¢œè‰²ç­‰) |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | ç”Ÿæˆçš„å›¾åƒç»“æžœ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n## å·¥ä½œåŽŸç†\n\næœ¬èŠ‚ç‚¹ä¸»è¦é€šè¿‡ä»¥ä¸‹æ­¥éª¤å¤„ç†è¯·æ±‚ï¼š\n\n1.  æ•´åˆè¾“å…¥å‚æ•°ï¼ŒåŒ…æ‹¬æç¤ºè¯ã€å›¾åƒå°ºå¯¸ã€ç”Ÿæˆæ•°é‡å’Œéšæœºç§å­\n2.  å¦‚æœ‰è¿žæŽ¥ï¼Œåˆå¹¶é£Žæ ¼è®¾ç½®å’ŒæŽ§åˆ¶å‚æ•°\n3.  æž„å»ºAPIè¯·æ±‚å¹¶å‘é€åˆ°RecraftæœåŠ¡å™¨\n4.  æŽ¥æ”¶è¿”å›žçš„å›¾åƒURLå¹¶ä¸‹è½½å›¾åƒæ•°æ®\n5.  å°†å›¾åƒæ•°æ®è½¬æ¢ä¸ºComfyUIå¯ç”¨çš„tensoræ ¼å¼å¹¶è¾“å‡º\n\nèŠ‚ç‚¹ä½¿ç”¨åŒæ­¥æ“ä½œæ¨¡å¼ï¼Œä¼šåœ¨å¤„ç†å®Œæˆå‰é˜»å¡žå·¥ä½œæµæ‰§è¡Œï¼Œç›´åˆ°æ‰€æœ‰è¯·æ±‚çš„å›¾åƒéƒ½ç”Ÿæˆå®Œæ¯•ã€‚è¯¥èŠ‚ç‚¹åˆ©ç”¨Recraftçš„V3æ¨¡åž‹è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œèƒ½æ”¯æŒå„ç§è¯¦ç»†çš„æ–‡æœ¬æè¿°å’Œé£Žæ ¼å˜åŒ–ã€‚\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass RecraftTextToImageNode:\n    \"\"\"\n    Generates images synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"size\": (\n                    [res.value for res in RecraftImageSize],\n                    {\n                        \"default\": RecraftImageSize.res_1024x1024,\n                        \"tooltip\": \"The size of the generated image.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        size: str,\n        n: int,\n        seed,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/recraft/image_generation\",\n                method=HttpMethod.POST,\n                request_model=RecraftImageGenerationRequest,\n                response_model=RecraftImageGenerationResponse,\n            ),\n            request=RecraftImageGenerationRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                model=RecraftModel.recraftv3,\n                size=size,\n                n=n,\n                style=recraft_style.style,\n                substyle=recraft_style.substyle,\n                style_id=recraft_style.style_id,\n                controls=controls_api,\n            ),\n            auth_token=auth_token,\n        )\n        response: RecraftImageGenerationResponse = operation.execute()\n        images = []\n        for data in response.data:\n            image = bytesio_to_image_tensor(\n                download_url_to_bytesio(data.url, timeout=1024)\n            )\n            if len(image.shape) < 4:\n                image = image.unsqueeze(0)\n            images.append(image)\n        output_image = torch.cat(images, dim=0)\n\n        return (output_image,)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-vectorize-image",
  "markdown": "# Recraft Vectorize Image - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”ŸRecraft Vectorize ImageèŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg) Recraft Vectorize Image èŠ‚ç‚¹å¯ä»¥å°†é€šè¿‡ Recraft çš„ API å°†æ …æ ¼å›¾åƒ(å¦‚ç…§ç‰‡ã€PNGæˆ–JPEG) è½¬æ¢ä¸ºçŸ¢é‡SVGæ ¼å¼ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | è¦è½¬æ¢ä¸ºçŸ¢é‡çš„è¾“å…¥å›¾åƒ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| SVG | çŸ¢é‡å›¾ | è½¬æ¢åŽçš„SVGçŸ¢é‡å›¾å½¢ï¼Œéœ€è¿žæŽ¥åˆ°SaveSVGèŠ‚ç‚¹ä¿å­˜ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Recraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\nRecraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass RecraftVectorizeImageNode:\n    \"\"\"\n    Generates SVG synchronously from an input image.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.SVG,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        svgs = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/vectorize\",\n                auth_token=auth_token,\n            )\n            svgs.append(SVG(sub_bytes))\n            pbar.update(1)\n\n        return (SVG.combine_all(svgs), )\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-remove-background",
  "markdown": "# Recraft Remove Background - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”ŸRecraft Remove BackgroundèŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg) Recraft Remove Background èŠ‚ç‚¹é€šè¿‡ Recraft çš„ API èƒ½å¤Ÿæ™ºèƒ½è¯†åˆ«å¹¶ç§»é™¤å›¾åƒèƒŒæ™¯ï¼Œç”Ÿæˆå¸¦æœ‰é€æ˜ŽèƒŒæ™¯çš„å›¾åƒå’Œå¯¹åº”çš„Alphaè’™ç‰ˆã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| image | å›¾åƒ  | \\-  | éœ€è¦ç§»é™¤èƒŒæ™¯çš„è¾“å…¥å›¾åƒ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| IMAGE | å›¾åƒ  | ç§»é™¤èƒŒæ™¯åŽçš„å›¾åƒ(å¸¦Alphaé€šé“) |\n| MASK | è’™ç‰ˆ  | ä¸»ä½“å¯¹è±¡çš„è’™ç‰ˆ(ç™½è‰²åŒºåŸŸä¸ºä¿ç•™çš„ä¸»ä½“) |\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass RecraftRemoveBackgroundNode:\n    \"\"\"\n    Remove background from image, and return processed image and mask.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE, IO.MASK)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n            },\n            \"optional\": {\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        auth_token=None,\n        **kwargs,\n    ):\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/removeBackground\",\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        # use alpha channel as masks, in B,H,W format\n        masks_tensor = images_tensor[:,:,:,-1:].squeeze(-1)\n        return (images_tensor, masks_tensor)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-image-to-image",
  "markdown": "# Recraft Image to Image - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\n\nclass RecraftImageToImageNode:\n    \"\"\"\n    Modify image based on prompt and strength.\n    \"\"\"\n\n    RETURN_TYPES = (IO.IMAGE,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"image\": (IO.IMAGE, ),\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"strength\": (\n                    IO.FLOAT,\n                    {\n                        \"default\": 0.5,\n                        \"min\": 0.0,\n                        \"max\": 1.0,\n                        \"step\": 0.01,\n                        \"tooltip\": \"Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity.\"\n                    }\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"recraft_style\": (RecraftIO.STYLEV3,),\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        image: torch.Tensor,\n        prompt: str,\n        n: int,\n        strength: float,\n        seed,\n        auth_token=None,\n        recraft_style: RecraftStyle = None,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        **kwargs,\n    ):\n        default_style = RecraftStyle(RecraftStyleV3.realistic_image)\n        if recraft_style is None:\n            recraft_style = default_style\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        request = RecraftImageGenerationRequest(\n            prompt=prompt,\n            negative_prompt=negative_prompt,\n            model=RecraftModel.recraftv3,\n            n=n,\n            strength=round(strength, 2),\n            style=recraft_style.style,\n            substyle=recraft_style.substyle,\n            style_id=recraft_style.style_id,\n            controls=controls_api,\n            random_seed=seed,\n        )\n\n        images = []\n        total = image.shape[0]\n        pbar = ProgressBar(total)\n        for i in range(total):\n            sub_bytes = handle_recraft_file_request(\n                image=image[i],\n                path=\"/proxy/recraft/images/imageToImage\",\n                request=request,\n                auth_token=auth_token,\n            )\n            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))\n            pbar.update(1)\n\n        images_tensor = torch.cat(images, dim=0)\n        return (images_tensor, )\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-text-to-vector",
  "markdown": "# Recraft Text to Vector - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n```\n\nclass RecraftTextToVectorNode:\n    \"\"\"\n    Generates SVG synchronously based on prompt and resolution.\n    \"\"\"\n\n    RETURN_TYPES = (RecraftIO.SVG,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/image/Recraft\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the image generation.\",\n                    },\n                ),\n                \"substyle\": (get_v3_substyles(RecraftStyleV3.vector_illustration),),\n                \"size\": (\n                    [res.value for res in RecraftImageSize],\n                    {\n                        \"default\": RecraftImageSize.res_1024x1024,\n                        \"tooltip\": \"The size of the generated image.\",\n                    },\n                ),\n                \"n\": (\n                    IO.INT,\n                    {\n                        \"default\": 1,\n                        \"min\": 1,\n                        \"max\": 6,\n                        \"tooltip\": \"The number of images to generate.\",\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"negative_prompt\": (\n                    IO.STRING,\n                    {\n                        \"default\": \"\",\n                        \"forceInput\": True,\n                        \"tooltip\": \"An optional text description of undesired elements on an image.\",\n                    },\n                ),\n                \"recraft_controls\": (\n                    RecraftIO.CONTROLS,\n                    {\n                        \"tooltip\": \"Optional additional controls over the generation via the Recraft Controls node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        substyle: str,\n        size: str,\n        n: int,\n        seed,\n        negative_prompt: str = None,\n        recraft_controls: RecraftControls = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        # create RecraftStyle so strings will be formatted properly (i.e. \"None\" will become None)\n        recraft_style = RecraftStyle(RecraftStyleV3.vector_illustration, substyle=substyle)\n\n        controls_api = None\n        if recraft_controls:\n            controls_api = recraft_controls.create_api_model()\n\n        if not negative_prompt:\n            negative_prompt = None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/recraft/image_generation\",\n                method=HttpMethod.POST,\n                request_model=RecraftImageGenerationRequest,\n                response_model=RecraftImageGenerationResponse,\n            ),\n            request=RecraftImageGenerationRequest(\n                prompt=prompt,\n                negative_prompt=negative_prompt,\n                model=RecraftModel.recraftv3,\n                size=size,\n                n=n,\n                style=recraft_style.style,\n                substyle=recraft_style.substyle,\n                controls=controls_api,\n            ),\n            auth_token=auth_token,\n        )\n        response: RecraftImageGenerationResponse = operation.execute()\n        svg_data = []\n        for data in response.data:\n            svg_data.append(download_url_to_bytesio(data.url, timeout=1024))\n\n        return (SVG(svg_data),)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows",
  "markdown": "# ä¾¿æºç‰ˆ(Windows) - ComfyUI\n\n**ComfyUI Portable(ä¾¿æºç‰ˆ)** æ˜¯ä¸€ä¸ªç‹¬ç«‹å°è£…å®Œæ•´çš„ ComfyUI Windows ç‰ˆæœ¬ï¼Œå†…éƒ¨å·²ç»æ•´åˆäº† ComfyUI è¿è¡Œæ‰€éœ€çš„ç‹¬ç«‹çš„ **Python(python\\_embeded)**,åªéœ€è¦è§£åŽ‹å³å¯ä½¿ç”¨,ç›®å‰ä¾¿æºç‰ˆæœ¬æ”¯æŒé€šè¿‡ **Nvidia** æ˜¾å¡æˆ–è€… **CPU** è¿è¡Œã€‚ æœ¬éƒ¨åˆ†æŒ‡å—å°†å¼•å¯¼ä½ å®Œæˆå¯¹åº”çš„å®‰è£…ã€‚\n\n## ä¸‹è½½ ComfyUI Portable(ä¾¿æºç‰ˆ)\n\næ‚¨å¯é€šè¿‡ç‚¹å‡»ä¸‹é¢çš„é“¾æŽ¥æ¥èŽ·å–æœ€æ–°çš„ **ComfyUI Portable(ä¾¿æºç‰ˆ)** ä¸‹è½½é“¾æŽ¥\n\n[\n\nä¸‹è½½ ComfyUI Portable(ä¾¿æºç‰ˆ)\n\n](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)\n\nä¸‹è½½åŽä½ å¯ä»¥ä½¿ç”¨ç±»ä¼¼è§£åŽ‹è½¯ä»¶å¦‚ [7-ZIP](https://7-zip.org/) å¯¹åŽ‹ç¼©åŒ…è¿›è¡Œè§£åŽ‹ ä¾¿æºç‰ˆè§£åŽ‹åŽå¯¹åº”çš„æ–‡ä»¶ç»“æž„åŠè¯´æ˜Žå¦‚ä¸‹ï¼š\n\n```\nComfyUI_windows_portable\nâ”œâ”€â”€ ðŸ“‚ComfyUI                   // ComfyUI ç¨‹åºä¸»ä½“\nâ”œâ”€â”€ ðŸ“‚python_embeded            // ç‹¬ç«‹çš„ Python çŽ¯å¢ƒ\nâ”œâ”€â”€ ðŸ“‚update                    // ç”¨äºŽå‡çº§ä¾¿æºç‰ˆå®‰è£…åŒ…çš„æ‰¹å¤„ç†è„šæœ¬\nâ”œâ”€â”€ README_VERY_IMPORTANT.txt   // è‹±æ–‡ç‰ˆæœ¬çš„ ComfyUI ä¾¿æºç‰ˆä½¿ç”¨è¯´æ˜Ž\nâ”œâ”€â”€ run_cpu.bat                 // åŒå‡»å¯åŠ¨ ComfyUIï¼ˆä»…æ”¯æŒ CPUï¼‰\nâ””â”€â”€ run_nvidia_gpu.bat          // åŒå‡»å¯åŠ¨ ComfyUIï¼ˆä»…æ”¯æŒ Nvidia æ˜¾å¡ï¼‰\n```\n\næ ¹æ®ä½ çš„ç”µè„‘æƒ…å†µåŒå‡» `run_nvidia_gpu.bat` æˆ–è€… `run_cpu.bat` æ¥å¯åŠ¨ ComfyUIï¼Œä½ ä¼šçœ‹åˆ°å¯¹åº”ä¸‹å›¾æ‰€ç¤ºçš„å‘½ä»¤çš„è¿è¡Œ ![ComfyUIä¾¿æºç‰ˆè¿è¡Œå‘½ä»¤æç¤ºç¬¦](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui-portable-cmd.png) å½“ä½ çœ‹åˆ°ç±»ä¼¼å›¾ç‰‡ä¸­çš„\n\n```\nTo see the GUI go to: http://127.0.0.1:8188\n```\n\næ­¤æ—¶ä½ çš„ ComfyUI æœåŠ¡å·²ç»å¯åŠ¨ï¼Œæ­£å¸¸æƒ…å†µä¸‹ ComfyUI ä¼šè‡ªåŠ¨æ‰“å¼€ä½ çš„é»˜è®¤æµè§ˆå™¨å¹¶è®¿é—® `http://127.0.0.1:8188` åœ°å€ï¼Œå¦‚æžœæ²¡æœ‰è‡ªåŠ¨æ‰“å¼€ï¼Œè¯·æ‰‹åŠ¨æ‰“å¼€æµè§ˆå™¨å¹¶è®¿é—®è¯¥åœ°å€ã€‚\n\n## æ·»åŠ å¤–éƒ¨æ¨¡åž‹è·¯å¾„\n\nå¦‚æžœä½ æƒ³è¦åœ¨ `ComfyUI/models` ä¹‹å¤–ç®¡ç†ä½ çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¯èƒ½å‡ºäºŽä»¥ä¸‹åŽŸå› :\n\n*   ä½ æœ‰å¤šä¸ª ComfyUI å®žä¾‹ï¼Œä½ æƒ³è¦è®©è¿™äº›å®žä¾‹å…±äº«æ¨¡åž‹æ–‡ä»¶ï¼Œä»Žè€Œå‡å°‘ç£ç›˜å ç”¨\n*   ä½ æœ‰å¤šä¸ªä¸åŒçš„ç±»åž‹çš„ GUI ç¨‹åºï¼Œå¦‚ï¼šWebUI, ä½ æƒ³è¦ä»–ä»¬å…±ç”¨æ¨¡åž‹æ–‡ä»¶\n*   æ¨¡åž‹æ–‡ä»¶æ— æ³•è¢«è¯†åˆ«æˆ–è¯»å–åˆ°\n\næˆ‘ä»¬æä¾›äº†é€šè¿‡ `extra_model_paths.yaml` é…ç½®æ–‡ä»¶æ¥æ·»åŠ é¢å¤–æ¨¡åž‹æœç´¢è·¯å¾„çš„æ–¹æ³•ã€‚\n\n### ä¸åŒ ComfyUI ç‰ˆæœ¬é…ç½®æ–‡ä»¶ä½ç½®\n\nå¯¹äºŽ[ä¾¿æºç‰ˆ](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)å’Œ[æ‰‹åŠ¨å®‰è£…](https://docs.comfy.org/zh-CN/installation/manual_install)çš„ ComfyUIç‰ˆæœ¬ï¼Œä½ å¯ä»¥åœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹æ‰¾åˆ° `extra_model_paths.yaml.example` çš„ç¤ºä¾‹æ–‡ä»¶\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nå¤åˆ¶å¹¶é‡å‘½åä¸º `extra_model_paths.yaml` æ¥ä½¿ç”¨, å¹¶ä¿æŒåœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹, è·¯å¾„åº”è¯¥æ˜¯ `ComfyUI/extra_model_paths.yaml`ä½ ä¹Ÿå¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ‰¾åˆ°é…ç½®ç¤ºä¾‹æ–‡ä»¶\n\n### é…ç½®ç¤ºä¾‹\n\næ¯”å¦‚ï¼Œä½ éœ€è¦é¢å¤–è®© ComfyUI è¯†åˆ«çš„æ¨¡åž‹æ–‡ä»¶ä½äºŽä¸‹é¢çš„æ–‡ä»¶å¤¹:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ loras/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\né‚£ä¹ˆä½ å¯ä»¥è¿›è¡Œå¦‚ä¸‹çš„é…ç½®æ¥è®© ComfyUI è¯†åˆ«åˆ°ä½ è®¾å¤‡ä¸Šçš„æ¨¡åž‹è·¯å¾„\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\næˆ–è€…ä½¿ç”¨\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\næˆ–è€…ä½ ä¹Ÿå¯ä»¥å‚è€ƒé»˜è®¤çš„ [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ¥é…ç½®ï¼Œä¿å­˜ä¹‹åŽï¼Œ éœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆã€‚ ä¸‹é¢æ˜¯å®Œæ•´çš„åŽŸå§‹çš„é…ç½®é…ç½®ç¤ºä¾‹:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### æ·»åŠ é¢å¤–è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\né™¤äº†æ·»åŠ å¤–éƒ¨æ¨¡åž‹ä¹‹å¤–ï¼Œä½ åŒæ ·å¯ä»¥æ·»åŠ ä¸åœ¨ ComfyUI é»˜è®¤è·¯å¾„ä¸‹çš„è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\nä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„é…ç½®ç¤ºä¾‹ï¼ˆMac ç³»ç»Ÿï¼‰ï¼Œè¯·æ ¹æ®ä½ çš„å®žé™…æƒ…å†µè¿›è¡Œä¿®æ”¹ï¼Œå¹¶æ–°å¢žåˆ°å¯¹åº”çš„é…ç½®æ–‡ä»¶ä¸­ï¼Œä¿å­˜åŽéœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆ:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆ\n\nå®‰è£…æˆåŠŸåŽï¼Œä½ å¯ä»¥å‚è€ƒè®¿é—®ä¸‹é¢çš„ç« èŠ‚ï¼Œå¼€å§‹ä½ çš„ ComfyUI ä¹‹è·¯ã€‚\n\n[\n\n## è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆ\n\næœ¬æ•™ç¨‹å°†å¼•å¯¼ä½ å®Œæˆç¬¬ä¸€æ¬¡çš„æ¨¡åž‹å®‰è£…ä»¥åŠå¯¹åº”çš„æ–‡æœ¬åˆ°å›¾ç‰‡çš„ç”Ÿæˆ\n\n\n\n](https://docs.comfy.org/zh-CN/get_started/first_generation)\n\n## ç¤¾åŒºåˆ†å‘ç‰ˆæœ¬\n\nåœ¨ä¸­å›½æ—©æœŸæœ‰ç¤¾åŒºä½œè€… [@ç§‹è‘‰aaaki](https://space.bilibili.com/12566101) åˆ¶ä½œè¿‡ç‹¬ç«‹åˆ†å‘çš„ç‰ˆæœ¬-ç§‹å¶æ•´åˆåŒ…ï¼Œæœ‰è¢«å¹¿æ³›ä½¿ç”¨ã€‚ å¦‚æžœä½ æ˜¯åœ¨ä¸­å›½ä½¿ç”¨ï¼Œè¿™ä¸ªç‰ˆæœ¬æ›´æ”¹äº† Github çš„æºåœ°å€ï¼Œå¹¶é…ç½®äº† Pypi åœ°å€ä¸ºä¸ºä¸­å›½å›½å†…é•œåƒåœ°å€ï¼Œè¿™å¯ä»¥è®©ä½ åœ¨å¼€å§‹ä¸Šæ‰‹ ComfyUI æ—¶å¯ä»¥é¿å…ä¸€äº›å› ä¸ºç½‘ç»œå¯¼è‡´çš„ä¾èµ–å’Œæ›´æ–°é—®é¢˜ã€‚\n\n[\n\n## ç§‹å¶ ComfyUI æ•´åˆåŒ…\n\nè®¿é—®ç§‹å¶æ•´åˆåŒ…åŽŸå§‹å‘å¸ƒåœ°å€\n\n\n\n](https://www.bilibili.com/video/BV1Ew411776J)\n\n## å…¶å®ƒ ComfyUI ä¾¿æºç‰ˆç›¸å…³è¯´æ˜Ž\n\n### 1\\. ComfyUI ä¾¿æºç‰ˆå‡çº§\n\nä½ å¯ä»¥ä½¿ç”¨ **update** æ–‡ä»¶å¤¹ä¸‹çš„ç›¸å…³æ‰¹å¤„ç†å‘½ä»¤å®Œæˆ ComfyUI ä¾¿æºç‰ˆçš„å‡çº§\n\n```\nComfyUI_windows_portable\nâ””â”€ ðŸ“‚update\n   â”œâ”€â”€ update.py\n   â”œâ”€â”€ update_comfyui.bat            // æ›´æ–° ComfyUI åˆ°æœ€æ–°çš„ Commit ç‰ˆæœ¬\n   â”œâ”€â”€ update_comfyui_and_python_dependencies.bat  // è¯·ä»…åœ¨ä½ çš„è¿è¡ŒçŽ¯å¢ƒå­˜åœ¨é—®é¢˜æ—¶ä½¿ç”¨\n   â””â”€â”€ update_comfyui_stable.bat       // æ›´æ–° ComfyUI ä¸ºæœ€æ–°çš„ stable ç‰ˆæœ¬\n```\n\n### 2\\. ComfyUI ä¾¿æºç‰ˆè®¾ç½®å±€åŸŸç½‘è®¿é—®\n\nå¦‚æžœä½ çš„ ComfyUI è¿è¡Œåœ¨å±€åŸŸç½‘å†…ï¼Œæƒ³è¦å…¶å®ƒçš„è®¾å¤‡ä¹Ÿå¯ä»¥è®¿é—®åˆ° ComfyUIï¼Œä½ å¯ä»¥é€šè¿‡è®°äº‹æœ¬ä¿®æ”¹ `run_nvidia_gpu.bat` æˆ–è€… `run_cpu.bat` æ–‡ä»¶æ¥å®Œæˆé…ç½®ï¼Œä¸»è¦é€šè¿‡æ·»åŠ `--listen`æ¥æ·»åŠ ç›‘å¬åœ°å€ ä¸‹é¢çš„ç¤ºä¾‹æ˜¯æ·»åŠ äº† `--listen` å‚æ•°çš„ `run_nvidia_gpu.bat` æ–‡ä»¶å‘½ä»¤\n\n```\n.\\python_embeded\\python.exe -s ComfyUI\\main.py --listen --windows-standalone-build\npause\n```\n\nå½“å¯ç”¨ ComfyUI åŽæ‚¨ä¼šå‘çŽ°æœ€åŽçš„è¿è¡Œåœ°å€ä¼šå˜ä¸º\n\n```\nStarting server\n\nTo see the GUI go to: http://0.0.0.0:8188\nTo see the GUI go to: http://[::]:8188\n```\n\nä½ å¯ä»¥é€šè¿‡ `WIN + R` è¾“å…¥`cmd` æ‰“å¼€å‘½ä»¤è¡Œï¼Œè¾“å…¥ `ipconfig` æ¥æŸ¥çœ‹ä½ çš„å±€åŸŸç½‘ IP åœ°å€ï¼Œç„¶åŽåœ¨å…¶å®ƒè®¾å¤‡ä¸Šè¾“å…¥ `http://ä½ çš„å±€åŸŸç½‘IP:8188` æ¥è®¿é—® ComfyUI"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/desktop/linux",
  "markdown": "# Linuxæ¡Œé¢ç‰ˆ - ComfyUI\n\nå½“Linuxé¢„å»ºåŒ…å¯ç”¨æ—¶ï¼Œä½ å¯ä»¥é…ç½®å¤–éƒ¨æ¨¡åž‹è·¯å¾„ï¼š\n\n## æ·»åŠ å¤–éƒ¨æ¨¡åž‹è·¯å¾„\n\nå¦‚æžœä½ åœ¨è®¡ç®—æœºä¸Šçš„ ComfyUI å®‰è£…ç›®å½•ä¹‹å¤–çš„å…¶ä»–ä½ç½®å­˜å‚¨äº†æ¨¡åž‹ï¼Œå¯ä»¥é€šè¿‡é…ç½® `extra_model_paths.yaml` æ–‡ä»¶å°†å®ƒä»¬æ·»åŠ åˆ° ComfyUI ä¸­ã€‚ å¯¹äºŽ ComfyUI æ¡Œé¢ç‰ˆï¼Œå¯¹åº”æ–‡ä»¶è·¯å¾„ä¸ºï¼š\n\n*   Windowsï¼š`C:\\Users\\<ä½ çš„ç”¨æˆ·å>\\AppData\\Roaming\\ComfyUI\\extra_model_paths.yaml`\n*   macOSï¼š`~/Library/Application Support/ComfyUI/extra_model_paths.yaml`\n*   Linuxï¼š`~/.config/ComfyUI/extra_model_paths.yaml`\n\nè¯¦ç»†è¯´æ˜Žè¯·å‚è§[æ¨¡åž‹æ–‡æ¡£](https://docs.comfy.org/zh-CN/development/core-concepts/models#adding-external-model-paths)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster",
  "markdown": "# Recraft Style - Logo Raster - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Recraft Style - Logo Raster èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg) æ­¤èŠ‚ç‚¹åˆ›å»ºä¸€ä¸ªé£Žæ ¼é…ç½®å¯¹è±¡ï¼Œç”¨äºŽæŒ‡å¯¼Recraftçš„å›¾åƒç”Ÿæˆè¿‡ç¨‹æœå‘ä¸“ä¸šæ ‡å¿—è®¾è®¡çš„è§†è§‰æ•ˆæžœã€‚é€šè¿‡é€‰æ‹©ä¸åŒçš„å­é£Žæ ¼ï¼Œå¯ä»¥å®šä¹‰ç”Ÿæˆæ ‡å¿—çš„è®¾è®¡é£Žæ ¼ã€å¤æ‚åº¦å’Œé€‚ç”¨åœºæ™¯ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| substyle | é€‰æ‹©é¡¹ | \\-  | Logoæ …æ ¼é£Žæ ¼çš„å…·ä½“å­é£Žæ ¼(å¿…é€‰) |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | é£Žæ ¼é…ç½®å¯¹è±¡ï¼Œè¿žæŽ¥åˆ°Recraftç”ŸæˆèŠ‚ç‚¹ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Recraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\nRecraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass RecraftStyleV3LogoRasterNode(RecraftStyleV3RealisticImageNode):\n    \"\"\"\n    Select vector_illustration style and optional substyle.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"substyle\": (get_v3_substyles(s.RECRAFT_STYLE, include_none=False),),\n            }\n        }\n\n    RECRAFT_STYLE = RecraftStyleV3.logo_raster\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/desktop/macos",
  "markdown": "# macOS æ¡Œé¢ç‰ˆ - ComfyUI\n\n**ComfyUI æ¡Œé¢ç‰ˆï¼ˆDesktopï¼‰** æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å®‰è£…ç‰ˆæœ¬ï¼Œå¯ä»¥åƒå¸¸è§„è½¯ä»¶ä¸€æ ·å®‰è£…ï¼Œæ”¯æŒå¿«æ·å®‰è£…ã€è‡ªåŠ¨é…ç½® **Python çŽ¯å¢ƒåŠä¾èµ–**ï¼Œå¹¶æ”¯æŒå¯¼å…¥å·²æœ‰çš„ ComfyUI è®¾ç½®ã€æ¨¡åž‹ã€å·¥ä½œæµå’Œæ–‡ä»¶ã€‚ ComfyUI æ¡Œé¢ç‰ˆæ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå®Œæ•´ä»£ç è¯·è®¿é—®[è¿™é‡Œ](https://github.com/Comfy-Org/desktop)ã€‚\n\næœ¬ç¯‡æ•™ç¨‹å°†å¼•å¯¼ä½ å®Œæˆå¯¹åº”çš„è½¯ä»¶å®‰è£…ï¼Œå¹¶æä¾›ç›¸å…³çš„å®‰è£…é…ç½®è¯´æ˜Žã€‚\n\nè¯·ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®ä¸‹è½½å¯¹åº”çš„ macOS ç³»ç»Ÿ **ComfyUI æ¡Œé¢ç‰ˆ** å®‰è£…åŒ…ã€‚\n\n[\n\nDownload for macOS\n\n](https://download.comfy.org/mac/dmg/arm64)\n\n## é€šè¿‡ Homebrew å®‰è£…\n\nComfyUI æ¡Œé¢ç‰ˆä¹Ÿå¯é€šè¿‡ [Homebrew](https://brew.sh/) å®‰è£…ï¼š\n\n## ComfyUI æ¡Œé¢ç‰ˆå®‰è£…æ­¥éª¤\n\n1.  åŒå‡»ä¸‹è½½åˆ°çš„å®‰è£…åŒ…æ–‡ä»¶ã€‚\n2.  å¦‚å›¾æ‰€ç¤ºï¼Œè¯·å°† **ComfyUI** ç¨‹åºæŒ‰ç®­å¤´æ‰€ç¤ºæ‹–å…¥ **Applications** æ–‡ä»¶å¤¹ã€‚ ![ComfyUI å®‰è£…åŒ…](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0.png)\n3.  å¦‚æžœåœ¨æ‰“å¼€å®‰è£…åŒ…åŽï¼Œæ–‡ä»¶å¤¹æ˜¾ç¤ºå¦‚ä¸‹ï¼ˆå›¾æ ‡ä¸Šå‡ºçŽ°ç¦æ­¢ç¬¦å·ï¼‰ï¼Œåˆ™è¯´æ˜Žä½ å½“å‰çš„ç³»ç»Ÿç‰ˆæœ¬ä¸Ž **ComfyUI æ¡Œé¢ç‰ˆ** ä¸å…¼å®¹ã€‚ ![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0-1.png)\n4.  ç„¶åŽåœ¨ **å¯åŠ¨å° (Launchpad)** ä¸­æ‰¾åˆ°å¯¹åº”çš„ **ComfyUI å›¾æ ‡**ï¼Œç‚¹å‡»å®ƒå³å¯è¿›å…¥ ComfyUI çš„åˆå§‹åŒ–è®¾ç½®ã€‚ ![ComfyUI Launchpad](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-1.jpg)\n\n## ComfyUI æ¡Œé¢ç‰ˆåˆå§‹åŒ–æµç¨‹\n\n## è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆ\n\nå®‰è£…æˆåŠŸåŽï¼Œä½ å¯ä»¥å‚è€ƒè®¿é—®ä¸‹é¢çš„ç« èŠ‚ï¼Œå¼€å§‹ä½ çš„ ComfyUI ä¹‹è·¯ã€‚\n\n[\n\n## è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆ\n\næœ¬æ•™ç¨‹å°†å¼•å¯¼ä½ å®Œæˆç¬¬ä¸€æ¬¡çš„æ¨¡åž‹å®‰è£…ä»¥åŠå¯¹åº”çš„æ–‡æœ¬åˆ°å›¾ç‰‡çš„ç”Ÿæˆ\n\n\n\n](https://docs.comfy.org/zh-CN/get_started/first_generation)\n\n## å¦‚ä½•æ›´æ–° ComfyUI æ¡Œé¢ç‰ˆ\n\nç›®å‰ ComfyUI æ¡Œé¢ç‰ˆæ›´æ–°é‡‡ç”¨è‡ªåŠ¨æ£€æµ‹æ›´æ–°ï¼Œè¯·ç¡®ä¿åœ¨è®¾ç½®ä¸­å·²ç»å¯ç”¨è‡ªåŠ¨æ›´æ–° ![ComfyUI æ¡Œé¢ç‰ˆè®¾ç½®](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg) ä½ ä¹Ÿå¯ä»¥åœ¨ `èœå•` â€”> `å¸®åŠ©` â€”> `æ£€æŸ¥æ›´æ–°` ä¸­é€‰æ‹©æ‰‹åŠ¨æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ›´æ–° ![ComfyUI æ¡Œé¢ç‰ˆæ£€æŸ¥æ›´æ–°](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_check_for_updates.jpg)\n\n## æ·»åŠ å¤–éƒ¨æ¨¡åž‹è·¯å¾„\n\nå¦‚æžœä½ æƒ³è¦åœ¨ `ComfyUI/models` ä¹‹å¤–ç®¡ç†ä½ çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¯èƒ½å‡ºäºŽä»¥ä¸‹åŽŸå› :\n\n*   ä½ æœ‰å¤šä¸ª ComfyUI å®žä¾‹ï¼Œä½ æƒ³è¦è®©è¿™äº›å®žä¾‹å…±äº«æ¨¡åž‹æ–‡ä»¶ï¼Œä»Žè€Œå‡å°‘ç£ç›˜å ç”¨\n*   ä½ æœ‰å¤šä¸ªä¸åŒçš„ç±»åž‹çš„ GUI ç¨‹åºï¼Œå¦‚ï¼šWebUI, ä½ æƒ³è¦ä»–ä»¬å…±ç”¨æ¨¡åž‹æ–‡ä»¶\n*   æ¨¡åž‹æ–‡ä»¶æ— æ³•è¢«è¯†åˆ«æˆ–è¯»å–åˆ°\n\næˆ‘ä»¬æä¾›äº†é€šè¿‡ `extra_model_paths.yaml` é…ç½®æ–‡ä»¶æ¥æ·»åŠ é¢å¤–æ¨¡åž‹æœç´¢è·¯å¾„çš„æ–¹æ³•ã€‚\n\n### ä¸åŒ ComfyUI ç‰ˆæœ¬é…ç½®æ–‡ä»¶ä½ç½®\n\nå¯¹äºŽ[ä¾¿æºç‰ˆ](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)å’Œ[æ‰‹åŠ¨å®‰è£…](https://docs.comfy.org/zh-CN/installation/manual_install)çš„ ComfyUIç‰ˆæœ¬ï¼Œä½ å¯ä»¥åœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹æ‰¾åˆ° `extra_model_paths.yaml.example` çš„ç¤ºä¾‹æ–‡ä»¶\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nå¤åˆ¶å¹¶é‡å‘½åä¸º `extra_model_paths.yaml` æ¥ä½¿ç”¨, å¹¶ä¿æŒåœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹, è·¯å¾„åº”è¯¥æ˜¯ `ComfyUI/extra_model_paths.yaml`ä½ ä¹Ÿå¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ‰¾åˆ°é…ç½®ç¤ºä¾‹æ–‡ä»¶\n\n### é…ç½®ç¤ºä¾‹\n\næ¯”å¦‚ï¼Œä½ éœ€è¦é¢å¤–è®© ComfyUI è¯†åˆ«çš„æ¨¡åž‹æ–‡ä»¶ä½äºŽä¸‹é¢çš„æ–‡ä»¶å¤¹:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ loras/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\né‚£ä¹ˆä½ å¯ä»¥è¿›è¡Œå¦‚ä¸‹çš„é…ç½®æ¥è®© ComfyUI è¯†åˆ«åˆ°ä½ è®¾å¤‡ä¸Šçš„æ¨¡åž‹è·¯å¾„\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\næˆ–è€…ä½¿ç”¨\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\næˆ–è€…ä½ ä¹Ÿå¯ä»¥å‚è€ƒé»˜è®¤çš„ [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ¥é…ç½®ï¼Œä¿å­˜ä¹‹åŽï¼Œ éœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆã€‚ ä¸‹é¢æ˜¯å®Œæ•´çš„åŽŸå§‹çš„é…ç½®é…ç½®ç¤ºä¾‹:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### æ·»åŠ é¢å¤–è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\né™¤äº†æ·»åŠ å¤–éƒ¨æ¨¡åž‹ä¹‹å¤–ï¼Œä½ åŒæ ·å¯ä»¥æ·»åŠ ä¸åœ¨ ComfyUI é»˜è®¤è·¯å¾„ä¸‹çš„è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\nä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„é…ç½®ç¤ºä¾‹ï¼ˆMac ç³»ç»Ÿï¼‰ï¼Œè¯·æ ¹æ®ä½ çš„å®žé™…æƒ…å†µè¿›è¡Œä¿®æ”¹ï¼Œå¹¶æ–°å¢žåˆ°å¯¹åº”çš„é…ç½®æ–‡ä»¶ä¸­ï¼Œä¿å­˜åŽéœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆ:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## æ¡Œé¢ç«¯ Python çŽ¯å¢ƒç›¸å…³\n\næ¡Œé¢ç«¯çš„å®‰è£…å°†åœ¨ä½ é€‰æ‹©çš„å®‰è£…ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªpython çš„è™šæ‹ŸçŽ¯å¢ƒï¼Œé€šå¸¸è¿™æ˜¯ä¸€ä¸ªéšè—çš„ `.venv` æ–‡ä»¶å¤¹ã€‚ å¦‚æžœä½ éœ€è¦ä¸º ComfyUI æ’ä»¶å¤„ç†ç›¸å…³çš„ä¾èµ–åˆ™éœ€è¦åœ¨è¿™ä¸ªçŽ¯å¢ƒä¸­è¿›è¡Œå¤„ç†ï¼Œç›´æŽ¥ç³»ç»Ÿç³»ç»Ÿçš„å‘½ä»¤è¡Œä¼šæœ‰å°†å¯¹åº”ä¾èµ–å®‰è£…åˆ°ç³»ç»ŸçŽ¯å¢ƒçš„é£Žé™©ï¼Œè¯·å‚è€ƒä¸‹é¢çš„æŒ‡ç¤ºå®Œæˆå¯¹åº”çŽ¯å¢ƒçš„æ¿€æ´»ã€‚\n\n### å¦‚ä½•ä½¿ç”¨ æ¡Œé¢ç«¯ çš„ python çŽ¯å¢ƒï¼Ÿ\n\nä½ å¯ä»¥ä½¿ç”¨æ¡Œé¢ç«¯è‡ªå¸¦çš„ç»ˆç«¯æ¥ä½¿ç”¨ python çŽ¯å¢ƒã€‚ ![ComfyUI æ¡Œé¢ç‰ˆç»ˆç«¯](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_terminal.jpg) \n\n1.  ç‚¹å‡»èœå•æ çš„ icon æ‰“å¼€åº•éƒ¨é¢æ¿\n2.  ç‚¹å‡» `Terminal` æ‰“å¼€ç»ˆç«¯\n3.  å¦‚æžœä½ æƒ³è¦çœ‹å¯¹åº”çŽ¯å¢ƒçš„ python å®‰è£…ä½ç½®ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤\n\n```\n  python -c \"import sys; print(sys.executable)\"\n```\n\n## å¦‚ä½•å¸è½½ ComfyUI æ¡Œé¢ç‰ˆ\n\nè¦å¸è½½ **ComfyUI æ¡Œé¢ç‰ˆ**ï¼Œä½ å¯ä»¥ç›´æŽ¥åœ¨ **Applications** æ–‡ä»¶å¤¹å†…åˆ é™¤ **ComfyUI** ç¨‹åºã€‚ å¦‚æžœä½ æƒ³è¦**å®Œå…¨åˆ é™¤** **ComfyUI æ¡Œé¢ç‰ˆ** çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¯ä»¥æ‰‹åŠ¨åˆ é™¤ä»¥ä¸‹æ–‡ä»¶å¤¹ï¼š\n\n```\n~/Library/Application Support/ComfyUI\n```\n\nä»¥ä¸Šæ“ä½œ**ä¸ä¼š**åˆ é™¤ä½ çš„ä»¥ä¸‹æ–‡ä»¶å¤¹ã€‚å¦‚æžœéœ€è¦ï¼Œè¯·æ‰‹åŠ¨åˆ é™¤ï¼š\n\n*   Modelsï¼ˆæ¨¡åž‹æ–‡ä»¶ï¼‰\n*   Custom Nodesï¼ˆè‡ªå®šä¹‰èŠ‚ç‚¹ï¼‰\n*   Input/Output ç›®å½•ï¼ˆå›¾ç‰‡è¾“å…¥/è¾“å‡ºç›®å½•ï¼‰\n\n## æ•…éšœæŽ’é™¤\n\n### å¦‚ä½•å®šä½å®‰è£…é”™è¯¯\n\nå¦‚æžœå®‰è£…å¤±è´¥ï¼Œä½ åº”è¯¥å¯ä»¥çœ‹åˆ°ä¸‹é¢çš„ç•Œé¢æ˜¾ç¤º ![ComfyUI å®‰è£…å¤±è´¥](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg) æ­¤æ—¶å»ºè®®ä½ é‡‡å–ä»¥ä¸‹å‡ ç§æ–¹å¼æŸ¥æ‰¾é”™è¯¯åŽŸå› \n\n1.  ç‚¹å‡» `Show Teriminal` æŸ¥çœ‹é”™è¯¯é—®é¢˜è¾“å‡º\n2.  ç‚¹å‡» `Open Logs` æŸ¥çœ‹å®‰è£…è¿‡ç¨‹æ—¥å¿—\n3.  è®¿é—®å®˜æ–¹è®ºå›æŸ¥æ‰¾é”™è¯¯åé¦ˆ\n4.  ç‚¹å‡»`Reinstall`å°è¯•é‡æ–°å®‰è£…\n\nå»ºè®®åœ¨æäº¤åé¦ˆä¹‹å‰ï¼Œä½ å¯ä»¥å°†å¯¹åº”çš„**é”™è¯¯è¾“å‡º**ä»¥åŠ **log æ–‡ä»¶**ä¿¡æ¯æä¾›ç»™ç±»ä¼¼ **GPT**ä¸€ç±»çš„å·¥å…· ![ComfyUI å®‰è£…å¤±è´¥-é”™è¯¯æ—¥å¿—](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg) ![ComfyUI å®‰è£…å¤±è´¥-GPT åé¦ˆ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg) å¦‚ä¸Šå›¾ï¼Œè¯¢é—®å¯¹åº”é”™è¯¯çš„åŽŸå› ï¼Œæˆ–è€…å®Œå…¨åˆ é™¤ ComfyUI åŽè¿›è¡Œå®‰è£…é‡è¯•\n\n### åé¦ˆé”™è¯¯\n\nå¦‚æžœåœ¨å®‰è£…è¿‡ç¨‹ä¸­ï¼Œä½ å‘ç”Ÿäº†ä»»ä½•é”™è¯¯ï¼Œè¯·é€šè¿‡ä»¥ä¸‹ä»»æ„æ–¹å¼æŸ¥çœ‹æ˜¯å¦æœ‰ç±»ä¼¼é”™è¯¯åé¦ˆï¼Œæˆ–è€…å‘æˆ‘ä»¬æäº¤é”™è¯¯\n\n*   Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)\n*   Comfy å®˜æ–¹è®ºå›: [https://forum.comfy.org/](https://forum.comfy.org/)\n\nè¯·åœ¨æäº¤é”™è¯¯æ—¶ç¡®ä¿æäº¤äº†ä»¥ä¸‹æ—¥å¿—ä»¥åŠé…ç½®æ–‡ä»¶ï¼Œæ–¹ä¾¿æˆ‘ä»¬è¿›è¡Œé—®é¢˜çš„å®šä½å’ŒæŸ¥æ‰¾\n\n1.  æ—¥å¿—æ–‡ä»¶\n\n| æ–‡ä»¶å | æè¿°  | ä½ç½®  |\n| --- | --- | --- |\n| main.log | åŒ…å«ä¸Žæ¡Œé¢åº”ç”¨å’ŒæœåŠ¡å™¨å¯åŠ¨ç›¸å…³çš„æ—¥å¿—ï¼Œæ¥è‡ªæ¡Œé¢çš„ Electron è¿›ç¨‹ã€‚ |     |\n| comfyui.log | åŒ…å«ä¸Ž ComfyUI æ­£å¸¸è¿è¡Œç›¸å…³çš„æ—¥å¿—ï¼Œä¾‹å¦‚æ ¸å¿ƒ ComfyUI è¿›ç¨‹çš„ç»ˆç«¯è¾“å‡ºã€‚ |     |\n\n![ComfyUI æ—¥å¿—æ–‡ä»¶è¾“å‡ºä½ç½®](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)\n\n2.  é…ç½®æ–‡ä»¶\n\n| æ–‡ä»¶å | æè¿°  | ä½ç½®  |\n| --- | --- | --- |\n| extra\\_model\\_paths.yaml | åŒ…å« ComfyUI å°†æœç´¢æ¨¡åž‹å’Œè‡ªå®šä¹‰èŠ‚ç‚¹çš„é¢å¤–è·¯å¾„ã€‚ |     |\n| config.json | åŒ…å«åº”ç”¨é…ç½®ã€‚æ­¤æ–‡ä»¶é€šå¸¸ä¸åº”ç›´æŽ¥ç¼–è¾‘ã€‚ |     |\n\n![ComfyUI é…ç½®æ–‡ä»¶ä½ç½®](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/index",
  "markdown": "# ComfyUI å®˜æ–¹æ–‡æ¡£ - ComfyUI\n\n## å…³äºŽ ComfyUI\n\nç”± [comfyanonymous](https://github.com/comfyanonymous) å’Œå…¶ä»–[è´¡çŒ®è€…](https://github.com/comfyanonymous/ComfyUI/graphs/contributors)å¼€å‘ã€‚\n\n*   **ComfyUI** æ˜¯ä¸€ä¸ªåŸºäºŽèŠ‚ç‚¹çš„ç”Ÿæˆå¼ AI ç•Œé¢å’ŒæŽ¨ç†å¼•æ“Ž\n*   ç”¨æˆ·å¯ä»¥é€šè¿‡èŠ‚ç‚¹ç»„åˆå„ç§ AI æ¨¡åž‹å’Œæ“ä½œï¼Œå®žçŽ°é«˜åº¦å¯å®šåˆ¶å’Œå¯æŽ§çš„å†…å®¹ç”Ÿæˆ\n*   ComfyUI å®Œå…¨å¼€æºï¼Œå¯ä»¥åœ¨æœ¬åœ°è®¾å¤‡ä¸Šè¿è¡Œ"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image",
  "markdown": "# Recraft Text to Image API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\næœ¬æ–‡å°†ä»‹ç»å¦‚ä½•åœ¨ ComfyUI ä¸­ä½¿ç”¨ Recraft Text to Image API èŠ‚ç‚¹çš„ç›¸å…³åŠŸèƒ½\n\n[Recraft Text to Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-text-to-image) èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨Recraft AIçš„å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œé€šè¿‡æ–‡æœ¬æè¿°åˆ›å»ºé«˜è´¨é‡ã€é£Žæ ¼å¤šæ ·çš„å›¾åƒå†…å®¹ã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å¦‚ä½•ä½¿ç”¨å¯¹åº”èŠ‚ç‚¹æ¥è¿›è¡Œæ–‡æœ¬åˆ°å›¾åƒçš„å·¥ä½œæµè®¾ç½®ã€‚\n\n## Recraft Text to Image API èŠ‚ç‚¹æ–‡æœ¬åˆ°å›¾åƒå·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![Recraft æ–‡æœ¬åˆ°å›¾åƒå·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/recraft/t2i/recraft_t2i.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Recraft æ–‡æœ¬åˆ°å›¾åƒå·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„å·¥ä½œæµè¿è¡Œï¼š\n\n1.  (å¯é€‰) ä¿®æ”¹ `Color` çš„ `Recraft Color RGB` çš„é¢œè‰²ä¸ºä½ æƒ³è¦çš„é¢œè‰²\n2.  (å¯é€‰) ä¿®æ”¹ `Recraft Style` èŠ‚ç‚¹æ¥æŽ§åˆ¶å›¾åƒçš„è§†è§‰é£Žæ ¼ï¼Œå¦‚æ•°å­—æ’ç”»ã€çœŸå®žç…§ç‰‡æˆ–Logoè®¾è®¡ç­‰ï¼Œè¿™ä¸ªåˆ†ç»„åŒæ—¶æä¾›äº†å…¶å®ƒçš„é£Žæ ¼èŠ‚ç‚¹ï¼Œä½ å¯ä»¥æŒ‰éœ€å¯ç”¨\n3.  (å¯é€‰) ä¿®æ”¹ `Recraft Text to Image` èŠ‚ç‚¹ä¸­çš„ `prompt` å‚æ•°ï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡ä¿®æ”¹`size`å‚æ•°æ¥æ”¹å˜\n4.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç”Ÿæˆ\n5.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹\n\n> (å¯é€‰) æˆ‘ä»¬åœ¨å·¥ä½œæµä¸­æä¾›äº† **Convert to SVG** çš„åˆ†ç»„ï¼Œç”±äºŽè¯¥åˆ†ç»„ä¸­çš„ `Recraft Vectorize Image` èŠ‚ç‚¹ä¹Ÿä¼šé¢å¤–æ¶ˆè€—ç§¯åˆ†ï¼Œä½ å¯æŒ‰éœ€å¯ç”¨ï¼Œå°†ç”Ÿæˆçš„å›¾åƒè½¬æ¢æˆ SVG æ ¼å¼\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\n*   **Recraft Style**ï¼šæä¾›å¤šç§é¢„è®¾é£Žæ ¼ï¼Œå¦‚çœŸå®žç…§ç‰‡ã€æ•°å­—æ’ç”»ã€Logoæ …æ ¼ç­‰\n*   **Seed å‚æ•°**ï¼šä»…ç”¨äºŽç¡®å®šèŠ‚ç‚¹æ˜¯å¦åº”é‡æ–°è¿è¡Œï¼Œä½†å®žé™…ç”Ÿæˆç»“æžœä¸Žç§å­å€¼æ— å…³\n\n## ç›¸å…³èŠ‚ç‚¹è¯¦è§£\n\nä½ å¯æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”èŠ‚ç‚¹çš„è¯¦ç»†å‚æ•°è®¾ç½®ç­‰\n\n[\n\n## Recraft Text to Image èŠ‚ç‚¹æ–‡æ¡£\n\nRecraft Text to Image API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-text-to-image)[\n\n## Recraft Style èŠ‚ç‚¹æ–‡æ¡£\n\nRecraft Style - Realistic Image API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)[\n\n## Recraft Controls èŠ‚ç‚¹æ–‡æ¡£\n\nRecraft Controls API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-controls)\n\nåœ¨æ­¤é¡µé¢\n\n*   [Recraft Text to Image API èŠ‚ç‚¹æ–‡æœ¬åˆ°å›¾åƒå·¥ä½œæµ](#recraft-text-to-image-api-%E8%8A%82%E7%82%B9%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E5%B7%A5%E4%BD%9C%E6%B5%81)\n*   [1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD)\n*   [2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ](#2-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C)\n*   [3\\. è¡¥å……è¯´æ˜Ž](#3-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E)\n*   [ç›¸å…³èŠ‚ç‚¹è¯¦è§£](#%E7%9B%B8%E5%85%B3%E8%8A%82%E7%82%B9%E8%AF%A6%E8%A7%A3)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration",
  "markdown": "# Recraft Style - Digital Illustration - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Recraft Style Digital Illustration èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg) æ­¤èŠ‚ç‚¹åˆ›å»ºä¸€ä¸ªé£Žæ ¼é…ç½®å¯¹è±¡ï¼Œç”¨äºŽæŒ‡å¯¼Recraftçš„å›¾åƒç”Ÿæˆè¿‡ç¨‹æœå‘æ•°å­—æ’ç”»çš„è§†è§‰æ•ˆæžœã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| substyle | é€‰æ‹©é¡¹ | None | æ•°å­—æ’ç”»é£Žæ ¼çš„å…·ä½“å­é£Žæ ¼ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| recraft\\_style | Recraft Style | é£Žæ ¼é…ç½®å¯¹è±¡ï¼Œè¿žæŽ¥åˆ°Recraftç”ŸæˆèŠ‚ç‚¹ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[\n\n## Recraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\nRecraft Text to Image å·¥ä½œæµç¤ºä¾‹\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/recraft/recraft-text-to-image)\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\nclass RecraftStyleV3DigitalIllustrationNode(RecraftStyleV3RealisticImageNode):\n    \"\"\"\n    Select digital_illustration style and optional substyle.\n    \"\"\"\n\n    RECRAFT_STYLE = RecraftStyleV3.digital_illustration\n\n```\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image",
  "markdown": "# Stability AI Stable Diffusion 3.5 API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\n[Stability AI Stable Diffusion 3.5 Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image) èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨ Stability AI çš„ Stable Diffusion 3.5 æ¨¡åž‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºè¯æˆ–å‚è€ƒå›¾åƒåˆ›å»ºé«˜è´¨é‡ã€ç»†èŠ‚ä¸°å¯Œçš„å›¾åƒå†…å®¹ã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å¦‚ä½•ä½¿ç”¨å¯¹åº”èŠ‚ç‚¹æ¥è¿›è¡Œæ–‡ç”Ÿå›¾å’Œå›¾ç”Ÿå›¾çš„å·¥ä½œæµè®¾ç½®ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![Stability AI Stable Diffusion 3.5 æ–‡ç”Ÿå›¾å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_diffusion_3-5-t2i.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Stability AI Stable Diffusion 3.5 æ–‡ç”Ÿå›¾æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  (å¯é€‰)ä¿®æ”¹ `Stability AI Stable Diffusion 3.5 Image` èŠ‚ç‚¹ä¸­çš„ `prompt` å‚æ•°ï¼Œè¾“å…¥ä½ æƒ³è¦ç”Ÿæˆçš„å›¾åƒæè¿°ã€‚æç¤ºè¯è¶Šè¯¦ç»†ï¼Œç”Ÿæˆçš„å›¾åƒè´¨é‡å¾€å¾€è¶Šå¥½ã€‚\n2.  (å¯é€‰)é€‰æ‹© `model` å‚æ•°æ¥é€‰æ‹©ä½¿ç”¨çš„SD 3.5æ¨¡åž‹ç‰ˆæœ¬ã€‚\n3.  (å¯é€‰)é€‰æ‹© `style_preset` å‚æ•°æ¥æŽ§åˆ¶å›¾åƒçš„è§†è§‰é£Žæ ¼ã€‚ä¸åŒçš„é¢„è®¾é£Žæ ¼ä¼šäº§ç”Ÿä¸åŒé£Žæ ¼ç‰¹ç‚¹çš„å›¾åƒï¼Œå¦‚â€cinematicâ€ï¼ˆç”µå½±æ„Ÿï¼‰ã€â€œanimeâ€ï¼ˆåŠ¨æ¼«é£Žæ ¼ï¼‰ç­‰ã€‚é€‰æ‹©â€Noneâ€åˆ™ä¸åº”ç”¨ä»»ä½•ç‰¹å®šé£Žæ ¼ã€‚\n4.  (å¯é€‰)ç¼–è¾‘ `String(Multiline)` æ¥ä¿®æ”¹è´Ÿå‘æç¤ºè¯ï¼Œç”¨äºŽæŒ‡å®šä¸å¸Œæœ›åœ¨ç”Ÿæˆå›¾åƒä¸­å‡ºçŽ°çš„å…ƒç´ ã€‚\n5.  ç‚¹å‡» `Runï¼ˆè¿è¡Œï¼‰` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç”Ÿæˆã€‚\n6.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\n*   **æç¤ºè¯(Prompt)**ï¼šæç¤ºè¯æ˜¯ç”Ÿæˆè¿‡ç¨‹ä¸­æœ€é‡è¦çš„å‚æ•°ä¹‹ä¸€ï¼Œè¯¦ç»†ã€æ¸…æ™°çš„æè¿°ä¼šå¸¦æ¥æ›´å¥½çš„æ•ˆæžœã€‚å¯ä»¥åŒ…å«åœºæ™¯ã€ä¸»ä½“ã€é¢œè‰²ã€å…‰ç…§ã€é£Žæ ¼ç­‰å…ƒç´ ã€‚\n*   **CFG Scale**ï¼šæŽ§åˆ¶ç”Ÿæˆå™¨å¯¹æç¤ºè¯çš„éµå¾ªç¨‹åº¦ï¼Œå€¼è¶Šé«˜ç”Ÿæˆçš„å›¾åƒè¶ŠæŽ¥è¿‘æç¤ºè¯æè¿°ï¼Œä½†å¤ªé«˜å¯èƒ½ä¼šå¯¼è‡´è¿‡åº¦é¥±å’Œæˆ–ä¸è‡ªç„¶çš„ç»“æžœã€‚\n*   **é£Žæ ¼é¢„è®¾(Style Preset)**ï¼šæä¾›å¤šç§é¢„è®¾é£Žæ ¼ï¼Œèƒ½å¤Ÿå¿«é€Ÿå®šä¹‰å›¾åƒçš„æ•´ä½“é£Žæ ¼ã€‚\n*   **è´Ÿé¢æç¤ºè¯(Negative Prompt)**ï¼šç”¨äºŽæŒ‡å®šä¸å¸Œæœ›åœ¨ç”Ÿæˆå›¾åƒä¸­å‡ºçŽ°çš„å…ƒç´ \n*   **Seed å‚æ•°**ï¼šå¯ä»¥ç”¨äºŽå¤çŽ°æˆ–å¾®è°ƒç”Ÿæˆç»“æžœï¼Œå¯¹äºŽåˆ›ä½œè¿‡ç¨‹ä¸­çš„è¿­ä»£å¾ˆæœ‰å¸®åŠ©ã€‚\n*   å½“å‰ `Load Image` èŠ‚ç‚¹ä¸º â€œç»•è¿‡ï¼ˆBypassï¼‰â€ æ¨¡å¼ï¼Œå¦‚éœ€å¯ç”¨å¯ä»¥å‚è€ƒæ­¥éª¤å›¾åœ¨å¯¹åº”èŠ‚ç‚¹ä¸Šå³é”®ç„¶åŽå°†â€æ¨¡å¼ï¼ˆModeï¼‰â€œè®¾ç½®ä¸ºâ€æ€»æ˜¯ï¼ˆAlwaysï¼‰â€ æ¥å¯ç”¨è¾“å…¥,å³å¯è½¬ä¸ºå›¾ç”Ÿå›¾æ¨¡å¼ã€‚\n*   `image_denoise` åœ¨æ²¡æœ‰è¾“å…¥å›¾åƒæ—¶ï¼Œè¯¥å‚æ•°ä¸èµ·ä½œç”¨ã€‚\n\n## Stability AI Stable Diffusion 3.5 å›¾ç”Ÿå›¾å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![Stability AI Stable Diffusion 3.5 å›¾ç”Ÿå›¾å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/stable_diffusion_3_5-i2i.png) ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡æˆ‘ä»¬å°†ç”¨äºŽè¾“å…¥å›¾ç‰‡ ![Stability AI Stable Diffusion 3.5 å›¾ç”Ÿå›¾å·¥ä½œæµè¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/input.jpg) \n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Stability AI Stable Diffusion 3.5 å›¾ç”Ÿå›¾æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆå›¾ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  é€šè¿‡ `Load Image` èŠ‚ç‚¹åŠ è½½ä¸€å¼ å‚è€ƒå›¾åƒï¼Œè¯¥å›¾åƒå°†ä½œä¸ºç”Ÿæˆçš„åŸºç¡€ã€‚\n2.  (å¯é€‰)ä¿®æ”¹ `Stability AI Stable Diffusion 3.5 Image` èŠ‚ç‚¹ä¸­çš„ `prompt` å‚æ•°ï¼Œæè¿°ä½ å¸Œæœ›åœ¨å‚è€ƒå›¾åƒåŸºç¡€ä¸Šæ”¹å˜æˆ–å¢žå¼ºçš„å…ƒç´ ã€‚\n3.  (å¯é€‰)é€‰æ‹© `style_preset` å‚æ•°æ¥æŽ§åˆ¶å›¾åƒçš„è§†è§‰é£Žæ ¼ï¼Œä¸åŒçš„é¢„è®¾é£Žæ ¼ä¼šäº§ç”Ÿä¸åŒé£Žæ ¼ç‰¹ç‚¹çš„å›¾åƒã€‚\n4.  (å¯é€‰ï½œé‡è¦)è°ƒæ•´ `image_denoise` å‚æ•°ï¼ˆèŒƒå›´0.0-1.0ï¼‰æ¥æŽ§åˆ¶å¯¹åŽŸå§‹å›¾åƒçš„ä¿®æ”¹ç¨‹åº¦ï¼š\n    *   å€¼è¶ŠæŽ¥è¿‘0.0ï¼Œç”Ÿæˆçš„å›¾åƒè¶ŠæŽ¥è¿‘è¾“å…¥çš„å‚è€ƒå›¾åƒï¼ˆå½“ 0.0 æ—¶ï¼ŒåŸºæœ¬å’ŒåŽŸå§‹å›¾åƒä¸€è‡´ï¼‰\n    *   å€¼è¶ŠæŽ¥è¿‘1.0ï¼Œç”Ÿæˆçš„å›¾åƒè¶ŠæŽ¥è¿‘çº¯æ–‡æœ¬ç”Ÿæˆçš„æ•ˆæžœï¼ˆå½“ 1.0 æ—¶ï¼Œç›¸å½“äºŽæ²¡æœ‰æä¾›å‚è€ƒå›¾åƒï¼‰\n5.  (å¯é€‰)ç¼–è¾‘ `String(Multiline)` æ¥ä¿®æ”¹è´Ÿå‘æç¤ºè¯ï¼Œç”¨äºŽæŒ‡å®šä¸å¸Œæœ›åœ¨ç”Ÿæˆå›¾åƒä¸­å‡ºçŽ°çš„å…ƒç´ ã€‚\n6.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç”Ÿæˆã€‚\n7.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\nä¸‹å›¾å±•ç¤ºäº†ä½¿ç”¨ç›¸åŒå‚æ•°è®¾ç½®ä¸‹ï¼Œæœ‰æ— è¾“å…¥å›¾åƒçš„å¯¹æ¯”æ•ˆæžœï¼š ![Stability AI Stable Diffusion 3.5 æœ‰æ— å›¾åƒè¾“å…¥å¯¹æ¯”](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg) **å›¾åƒåŽ»å™ªå¼ºåº¦(Image Denoise)**ï¼šè¿™ä¸ªå‚æ•°å†³å®šäº†ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿ç•™åŽŸå§‹å›¾åƒç‰¹å¾çš„ç¨‹åº¦ï¼Œæ˜¯å›¾ç”Ÿå›¾æ¨¡å¼ä¸­æœ€å…³é”®çš„è°ƒèŠ‚å‚æ•°,ä¸‹å›¾æ˜¯ä¸åŒçš„åŽ»å™ªå¼ºåº¦ä¸‹ç”Ÿæˆçš„å›¾åƒæ•ˆæžœï¼š ![Stability AI Stable Diffusion 3.5 å›¾ç”Ÿå›¾åŽ»å™ªå¼ºåº¦è¯´æ˜Ž](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg)\n\n*   **å‚è€ƒå›¾åƒé€‰æ‹©**ï¼šé€‰æ‹©å…·æœ‰æ¸…æ™°ä¸»ä½“å’Œè‰¯å¥½æž„å›¾çš„å›¾åƒé€šå¸¸èƒ½èŽ·å¾—æ›´å¥½çš„ç»“æžœã€‚\n*   **æç¤ºè¯æŠ€å·§**ï¼šåœ¨å›¾ç”Ÿå›¾æ¨¡å¼ä¸­ï¼Œæç¤ºè¯åº”è¯¥æ›´å¤šåœ°å…³æ³¨ä½ å¸Œæœ›æ”¹å˜æˆ–å¢žå¼ºçš„éƒ¨åˆ†ï¼Œè€Œä¸éœ€è¦æè¿°å·²ç»å­˜åœ¨äºŽå›¾åƒä¸­çš„æ‰€æœ‰å…ƒç´ ã€‚\n*   **æ¨¡å¼åˆ‡æ¢**ï¼šå½“æä¾›è¾“å…¥å›¾åƒæ—¶ï¼ŒèŠ‚ç‚¹ä¼šè‡ªåŠ¨ä»Žæ–‡æœ¬åˆ°å›¾åƒæ¨¡å¼åˆ‡æ¢åˆ°å›¾åƒåˆ°å›¾åƒæ¨¡å¼ï¼Œå¹¶ä¸”ä¼šå¿½ç•¥å®½é«˜æ¯”å‚æ•°è®¾ç½®ã€‚\n\n## ç›¸å…³èŠ‚ç‚¹è¯¦è§£\n\nä½ å¯æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”èŠ‚ç‚¹çš„è¯¦ç»†å‚æ•°è®¾ç½®ç­‰\n\n[\n\n## Stability Stable Diffusion 3.5 Image èŠ‚ç‚¹æ–‡æ¡£\n\nStability Stable Diffusion 3.5 Image API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2Fapi-node%2Fimage%2Fopenai%2Fopenai-dalle2",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/desktop/windows",
  "markdown": "# Windowsæ¡Œé¢ç‰ˆ - ComfyUI\n\n**ComfyUI æ¡Œé¢ç‰ˆï¼ˆDesktopï¼‰** æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å®‰è£…ç‰ˆæœ¬ï¼Œå¯ä»¥åƒå¸¸è§„è½¯ä»¶ä¸€æ ·è¿›è¡Œå®‰è£…ï¼Œæ”¯æŒå¿«æ·å®‰è£…è‡ªåŠ¨é…ç½® **PythonçŽ¯å¢ƒåŠä¾èµ–** ï¼Œæ”¯æŒå¯¼å…¥å·²æœ‰çš„ ComfyUI è®¾ç½®ã€æ¨¡åž‹ã€å·¥ä½œæµå’Œæ–‡ä»¶ï¼Œå¯ä»¥å¿«é€Ÿä»Žå·²æœ‰çš„[ComfyUI ä¾¿æºç‰ˆ](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)è¿ç§»åˆ°æ¡Œé¢ç‰ˆ ComfyUI æ¡Œé¢ç‰ˆæ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå®Œæ•´ä»£ç è¯·è®¿é—® [è¿™é‡Œ](https://github.com/Comfy-Org/desktop) ComfyUI æ¡Œé¢ç‰ˆ(Windows)ç¡¬ä»¶è¦æ±‚ï¼š\n\n*   NVIDIA æ˜¾å¡\n\næœ¬ç¯‡æ•™ç¨‹å°†å¼•å¯¼ä½ å®Œæˆå¯¹åº”çš„è½¯ä»¶å®‰è£…ï¼Œå¹¶è¯´æ˜Žç›¸å…³å®‰è£…é…ç½®è¯´æ˜Žã€‚\n\nè¯·ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®ä¸‹è½½å¯¹åº”çš„é’ˆå¯¹ Windows ç³»ç»Ÿçš„ **ComfyUI æ¡Œé¢ç‰ˆ** å®‰è£…åŒ…\n\n[\n\nDownload for Windows (NVIDIA)\n\n](https://download.comfy.org/windows/nsis/x64)\n\n## ComfyUI æ¡Œé¢ç‰ˆå®‰è£…æ­¥éª¤\n\nåŒå‡»ä¸‹è½½åˆ°çš„å®‰è£…åŒ…æ–‡ä»¶ï¼Œé¦–å…ˆå°†ä¼šæ‰§è¡Œä¸€æ¬¡è‡ªåŠ¨å®‰è£…ï¼Œå¹¶åœ¨æ¡Œé¢ç”Ÿæˆä¸€ä¸ª **ComfyUI æ¡Œé¢ç‰ˆ** çš„å¿«æ·æ–¹å¼ ![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-shortcut.jpg) åŒå‡»å¯¹åº”çš„å¿«æ·ï¼Œè¿›å…¥ ComfyUI çš„åˆå§‹åŒ–è®¾ç½®\n\n### ComfyUI æ¡Œé¢ç‰ˆåˆå§‹åŒ–æµç¨‹\n\n## è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆ\n\nå®‰è£…æˆåŠŸåŽï¼Œä½ å¯ä»¥å‚è€ƒè®¿é—®ä¸‹é¢çš„ç« èŠ‚ï¼Œå¼€å§‹ä½ çš„ ComfyUI ä¹‹è·¯ã€‚\n\n[\n\n## è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆ\n\næœ¬æ•™ç¨‹å°†å¼•å¯¼ä½ å®Œæˆç¬¬ä¸€æ¬¡çš„æ¨¡åž‹å®‰è£…ä»¥åŠå¯¹åº”çš„æ–‡æœ¬åˆ°å›¾ç‰‡çš„ç”Ÿæˆ\n\n\n\n](https://docs.comfy.org/zh-CN/get_started/first_generation)\n\n## å¦‚ä½•æ›´æ–° ComfyUI æ¡Œé¢ç‰ˆ\n\nç›®å‰ ComfyUI æ¡Œé¢ç‰ˆæ›´æ–°é‡‡ç”¨è‡ªåŠ¨æ£€æµ‹æ›´æ–°ï¼Œè¯·ç¡®ä¿åœ¨è®¾ç½®ä¸­å·²ç»å¯ç”¨è‡ªåŠ¨æ›´æ–° ![ComfyUI æ¡Œé¢ç‰ˆè®¾ç½®](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg) ä½ ä¹Ÿå¯ä»¥åœ¨ `èœå•` â€”> `å¸®åŠ©` â€”> `æ£€æŸ¥æ›´æ–°` ä¸­é€‰æ‹©æ‰‹åŠ¨æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ›´æ–° ![ComfyUI æ¡Œé¢ç‰ˆæ£€æŸ¥æ›´æ–°](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_check_for_updates.jpg)\n\n## æ·»åŠ å¤–éƒ¨æ¨¡åž‹è·¯å¾„\n\nå¦‚æžœä½ æƒ³è¦åœ¨ `ComfyUI/models` ä¹‹å¤–ç®¡ç†ä½ çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¯èƒ½å‡ºäºŽä»¥ä¸‹åŽŸå› :\n\n*   ä½ æœ‰å¤šä¸ª ComfyUI å®žä¾‹ï¼Œä½ æƒ³è¦è®©è¿™äº›å®žä¾‹å…±äº«æ¨¡åž‹æ–‡ä»¶ï¼Œä»Žè€Œå‡å°‘ç£ç›˜å ç”¨\n*   ä½ æœ‰å¤šä¸ªä¸åŒçš„ç±»åž‹çš„ GUI ç¨‹åºï¼Œå¦‚ï¼šWebUI, ä½ æƒ³è¦ä»–ä»¬å…±ç”¨æ¨¡åž‹æ–‡ä»¶\n*   æ¨¡åž‹æ–‡ä»¶æ— æ³•è¢«è¯†åˆ«æˆ–è¯»å–åˆ°\n\næˆ‘ä»¬æä¾›äº†é€šè¿‡ `extra_model_paths.yaml` é…ç½®æ–‡ä»¶æ¥æ·»åŠ é¢å¤–æ¨¡åž‹æœç´¢è·¯å¾„çš„æ–¹æ³•ã€‚\n\n### ä¸åŒ ComfyUI ç‰ˆæœ¬é…ç½®æ–‡ä»¶ä½ç½®\n\nå¯¹äºŽ[ä¾¿æºç‰ˆ](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)å’Œ[æ‰‹åŠ¨å®‰è£…](https://docs.comfy.org/zh-CN/installation/manual_install)çš„ ComfyUIç‰ˆæœ¬ï¼Œä½ å¯ä»¥åœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹æ‰¾åˆ° `extra_model_paths.yaml.example` çš„ç¤ºä¾‹æ–‡ä»¶\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nå¤åˆ¶å¹¶é‡å‘½åä¸º `extra_model_paths.yaml` æ¥ä½¿ç”¨, å¹¶ä¿æŒåœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹, è·¯å¾„åº”è¯¥æ˜¯ `ComfyUI/extra_model_paths.yaml`ä½ ä¹Ÿå¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ‰¾åˆ°é…ç½®ç¤ºä¾‹æ–‡ä»¶\n\n### é…ç½®ç¤ºä¾‹\n\næ¯”å¦‚ï¼Œä½ éœ€è¦é¢å¤–è®© ComfyUI è¯†åˆ«çš„æ¨¡åž‹æ–‡ä»¶ä½äºŽä¸‹é¢çš„æ–‡ä»¶å¤¹:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ loras/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\né‚£ä¹ˆä½ å¯ä»¥è¿›è¡Œå¦‚ä¸‹çš„é…ç½®æ¥è®© ComfyUI è¯†åˆ«åˆ°ä½ è®¾å¤‡ä¸Šçš„æ¨¡åž‹è·¯å¾„\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\næˆ–è€…ä½¿ç”¨\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\næˆ–è€…ä½ ä¹Ÿå¯ä»¥å‚è€ƒé»˜è®¤çš„ [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ¥é…ç½®ï¼Œä¿å­˜ä¹‹åŽï¼Œ éœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆã€‚ ä¸‹é¢æ˜¯å®Œæ•´çš„åŽŸå§‹çš„é…ç½®é…ç½®ç¤ºä¾‹:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### æ·»åŠ é¢å¤–è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\né™¤äº†æ·»åŠ å¤–éƒ¨æ¨¡åž‹ä¹‹å¤–ï¼Œä½ åŒæ ·å¯ä»¥æ·»åŠ ä¸åœ¨ ComfyUI é»˜è®¤è·¯å¾„ä¸‹çš„è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\nä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„é…ç½®ç¤ºä¾‹ï¼ˆMac ç³»ç»Ÿï¼‰ï¼Œè¯·æ ¹æ®ä½ çš„å®žé™…æƒ…å†µè¿›è¡Œä¿®æ”¹ï¼Œå¹¶æ–°å¢žåˆ°å¯¹åº”çš„é…ç½®æ–‡ä»¶ä¸­ï¼Œä¿å­˜åŽéœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆ:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n## æ¡Œé¢ç«¯ Python çŽ¯å¢ƒç›¸å…³\n\næ¡Œé¢ç«¯çš„å®‰è£…å°†åœ¨ä½ é€‰æ‹©çš„å®‰è£…ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªpython çš„è™šæ‹ŸçŽ¯å¢ƒï¼Œé€šå¸¸è¿™æ˜¯ä¸€ä¸ªéšè—çš„ `.venv` æ–‡ä»¶å¤¹ã€‚ å¦‚æžœä½ éœ€è¦ä¸º ComfyUI æ’ä»¶å¤„ç†ç›¸å…³çš„ä¾èµ–åˆ™éœ€è¦åœ¨è¿™ä¸ªçŽ¯å¢ƒä¸­è¿›è¡Œå¤„ç†ï¼Œç›´æŽ¥ç³»ç»Ÿç³»ç»Ÿçš„å‘½ä»¤è¡Œä¼šæœ‰å°†å¯¹åº”ä¾èµ–å®‰è£…åˆ°ç³»ç»ŸçŽ¯å¢ƒçš„é£Žé™©ï¼Œè¯·å‚è€ƒä¸‹é¢çš„æŒ‡ç¤ºå®Œæˆå¯¹åº”çŽ¯å¢ƒçš„æ¿€æ´»ã€‚\n\n### å¦‚ä½•ä½¿ç”¨ æ¡Œé¢ç«¯ çš„ python çŽ¯å¢ƒï¼Ÿ\n\nä½ å¯ä»¥ä½¿ç”¨æ¡Œé¢ç«¯è‡ªå¸¦çš„ç»ˆç«¯æ¥ä½¿ç”¨ python çŽ¯å¢ƒã€‚ ![ComfyUI æ¡Œé¢ç‰ˆç»ˆç«¯](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/desktop_terminal.jpg) \n\n1.  ç‚¹å‡»èœå•æ çš„ icon æ‰“å¼€åº•éƒ¨é¢æ¿\n2.  ç‚¹å‡» `Terminal` æ‰“å¼€ç»ˆç«¯\n3.  å¦‚æžœä½ æƒ³è¦çœ‹å¯¹åº”çŽ¯å¢ƒçš„ python å®‰è£…ä½ç½®ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤\n\n```\n  python -c \"import sys; print(sys.executable)\"\n```\n\n## å¦‚ä½•å¸è½½ ComfyUI æ¡Œé¢ç‰ˆ\n\nå¯¹äºŽ **ComfyUI æ¡Œé¢ç‰ˆ** ä½ å¯ä»¥åœ¨ Windows çš„ç³»ç»Ÿè®¾ç½®ä¸­ä½¿ç”¨ç³»ç»Ÿçš„å¸è½½åŠŸèƒ½æ¥å®Œæˆå¯¹åº”è½¯ä»¶çš„å¸è½½æ“ä½œ ![ComfyUI æ¡Œé¢ç‰ˆå¸è½½](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-uninstall-comfyui.jpg) å¦‚æžœä½ æƒ³è¦å®Œå…¨åˆ é™¤ **ComfyUI æ¡Œé¢ç‰ˆ** çš„æ‰€æœ‰æ–‡ä»¶ï¼Œä½ å¯ä»¥æ‰‹åŠ¨åˆ é™¤ä»¥ä¸‹æ–‡ä»¶å¤¹ï¼š\n\n*   C:\\\\Users<ä½ çš„ç”¨æˆ·å>\\\\AppData\\\\Local@comfyorgcomfyui-electron-updater\n*   C:\\\\Users<ä½ çš„ç”¨æˆ·å>\\\\AppData\\\\Local\\\\Programs@comfyorgcomfyui-electron\n*   C:\\\\Users<ä½ çš„ç”¨æˆ·å>\\\\AppData\\\\Roaming\\\\ComfyUI\n\nä»¥ä¸Šçš„æ“ä½œå¹¶ä¸ä¼šåˆ é™¤ä»¥ä¸‹ä½ çš„ä»¥ä¸‹æ–‡ä»¶å¤¹ï¼Œå¦‚æžœä½ éœ€è¦åˆ é™¤å¯¹åº”æ–‡ä»¶çš„è¯ï¼Œè¯·æ‰‹åŠ¨åˆ é™¤ï¼š\n\n*   models æ¨¡åž‹æ–‡ä»¶\n*   custom nodes è‡ªå®šä¹‰èŠ‚ç‚¹\n*   input/output directories. å›¾ç‰‡è¾“å…¥/è¾“å‡ºç›®å½•\n\n## æ•…éšœæŽ’é™¤\n\n### æ˜¾ç¤ºä¸æ”¯æŒçš„è®¾å¤‡\n\n![ComfyUI å®‰è£…æ­¥éª¤ - ä¸æ”¯æŒçš„è®¾å¤‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-0.jpg) ç”±äºŽ ComfyUI æ¡Œé¢ç‰ˆï¼ˆWindowsï¼‰ä»…æ”¯æŒå¯ä»¥ä½¿ç”¨ **CUDA çš„ Nvdia æ˜¾å¡** æ‰€ä»¥å¦‚æžœä½ çš„è®¾å¤‡ä¸æ”¯æŒï¼Œå¯èƒ½ä¼šå‡ºçŽ°æ­¤ç•Œé¢\n\n*   è¯·æ›´æ¢ä½¿ç”¨æ”¯æŒçš„è®¾å¤‡\n*   æˆ–è€…è€ƒè™‘ä½¿ç”¨ [ComfyUIä¾¿æºç‰ˆ](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows) æˆ–è€…é€šè¿‡[æ‰‹åŠ¨å®‰è£…](https://docs.comfy.org/zh-CN/installation/manual_install)æ¥ä½¿ç”¨ ComfyUI\n\n### å¦‚ä½•å®šä½å®‰è£…é”™è¯¯\n\nå¦‚æžœå®‰è£…å¤±è´¥ï¼Œä½ åº”è¯¥å¯ä»¥çœ‹åˆ°ä¸‹é¢çš„ç•Œé¢æ˜¾ç¤º ![ComfyUI å®‰è£…å¤±è´¥](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg) æ­¤æ—¶å»ºè®®ä½ é‡‡å–ä»¥ä¸‹å‡ ç§æ–¹å¼æŸ¥æ‰¾é”™è¯¯åŽŸå› \n\n1.  ç‚¹å‡» `Show Teriminal` æŸ¥çœ‹é”™è¯¯é—®é¢˜è¾“å‡º\n2.  ç‚¹å‡» `Open Logs` æŸ¥çœ‹å®‰è£…è¿‡ç¨‹æ—¥å¿—\n3.  è®¿é—®å®˜æ–¹è®ºå›æŸ¥æ‰¾é”™è¯¯åé¦ˆ\n4.  ç‚¹å‡»`Reinstall`å°è¯•é‡æ–°å®‰è£…\n\nå»ºè®®åœ¨æäº¤åé¦ˆä¹‹å‰ï¼Œä½ å¯ä»¥å°†å¯¹åº”çš„**é”™è¯¯è¾“å‡º**ä»¥åŠ **log æ–‡ä»¶**ä¿¡æ¯æä¾›ç»™ç±»ä¼¼ **GPT**ä¸€ç±»çš„å·¥å…· ![ComfyUI å®‰è£…å¤±è´¥-é”™è¯¯æ—¥å¿—](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg) ![ComfyUI å®‰è£…å¤±è´¥-GPT åé¦ˆ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg) å¦‚ä¸Šå›¾ï¼Œè¯¢é—®å¯¹åº”é”™è¯¯çš„åŽŸå› ï¼Œæˆ–è€…å®Œå…¨åˆ é™¤ ComfyUI åŽè¿›è¡Œå®‰è£…é‡è¯•\n\n### åé¦ˆé”™è¯¯\n\nå¦‚æžœåœ¨å®‰è£…è¿‡ç¨‹ä¸­ï¼Œä½ å‘ç”Ÿäº†ä»»ä½•é”™è¯¯ï¼Œè¯·é€šè¿‡ä»¥ä¸‹ä»»æ„æ–¹å¼æŸ¥çœ‹æ˜¯å¦æœ‰ç±»ä¼¼é”™è¯¯åé¦ˆï¼Œæˆ–è€…å‘æˆ‘ä»¬æäº¤é”™è¯¯\n\n*   Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)\n*   Comfy å®˜æ–¹è®ºå›: [https://forum.comfy.org/](https://forum.comfy.org/)\n\nè¯·åœ¨æäº¤é”™è¯¯æ—¶ç¡®ä¿æäº¤äº†ä»¥ä¸‹æ—¥å¿—ä»¥åŠé…ç½®æ–‡ä»¶ï¼Œæ–¹ä¾¿æˆ‘ä»¬è¿›è¡Œé—®é¢˜çš„å®šä½å’ŒæŸ¥æ‰¾\n\n1.  æ—¥å¿—æ–‡ä»¶\n\n| æ–‡ä»¶å | æè¿°  | ä½ç½®  |\n| --- | --- | --- |\n| main.log | åŒ…å«ä¸Žæ¡Œé¢åº”ç”¨å’ŒæœåŠ¡å™¨å¯åŠ¨ç›¸å…³çš„æ—¥å¿—ï¼Œæ¥è‡ªæ¡Œé¢çš„ Electron è¿›ç¨‹ã€‚ |     |\n| comfyui.log | åŒ…å«ä¸Ž ComfyUI æ­£å¸¸è¿è¡Œç›¸å…³çš„æ—¥å¿—ï¼Œä¾‹å¦‚æ ¸å¿ƒ ComfyUI è¿›ç¨‹çš„ç»ˆç«¯è¾“å‡ºã€‚ |     |\n\n![ComfyUI æ—¥å¿—æ–‡ä»¶è¾“å‡ºä½ç½®](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)\n\n2.  é…ç½®æ–‡ä»¶\n\n| æ–‡ä»¶å | æè¿°  | ä½ç½®  |\n| --- | --- | --- |\n| extra\\_model\\_paths.yaml | åŒ…å« ComfyUI å°†æœç´¢æ¨¡åž‹å’Œè‡ªå®šä¹‰èŠ‚ç‚¹çš„é¢å¤–è·¯å¾„ã€‚ |     |\n| config.json | åŒ…å«åº”ç”¨é…ç½®ã€‚æ­¤æ–‡ä»¶é€šå¸¸ä¸åº”ç›´æŽ¥ç¼–è¾‘ã€‚ |     |\n\n![ComfyUI é…ç½®æ–‡ä»¶ä½ç½®](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-video",
  "markdown": "# Luma Text to Video API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\n[Luma Text to Video](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-text-to-video) èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨Luma AIçš„åˆ›æ–°è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œé€šè¿‡æ–‡æœ¬æè¿°åˆ›å»ºé«˜è´¨é‡ã€æµç•…çš„è§†é¢‘å†…å®¹ã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å¦‚ä½•ä½¿ç”¨å¯¹åº”èŠ‚ç‚¹æ¥è¿›è¡Œæ–‡æœ¬åˆ°è§†é¢‘çš„å·¥ä½œæµè®¾ç½®ã€‚\n\nä½ å¯æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”èŠ‚ç‚¹çš„è¯¦ç»†å‚æ•°è®¾ç½®ç­‰\n\n[\n\n## Luma Text to Video èŠ‚ç‚¹æ–‡æ¡£\n\nLuma Text to Video API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-text-to-video)[\n\n## Luma Concepts èŠ‚ç‚¹æ–‡æ¡£\n\nLuma Concepts API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-concepts)\n\n## Luma Text to Video API èŠ‚ç‚¹æ–‡æœ¬åˆ°è§†é¢‘å·¥ä½œæµ\n\nLuma Text to Video èŠ‚ç‚¹éœ€è¦æä¾›æ–‡æœ¬æç¤ºè¯æ¥æè¿°ç”Ÿæˆè§†é¢‘å†…å®¹ã€‚åœ¨æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬åˆ¶ä½œäº†ä½¿ç”¨`prompt`å’Œ`luma_concepts`çš„ç¤ºä¾‹ï¼Œè®©ä½ ä½“éªŒLuma AIåœ¨è§†é¢‘ç”Ÿæˆä¸Šçš„ä¼˜ç§€èƒ½åŠ›ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„è§†é¢‘çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![Luma æ–‡æœ¬åˆ°è§†é¢‘å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2v/luma_t2v.mp4)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Luma æ–‡æœ¬åˆ°è§†é¢‘å·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Luma Text to Video` èŠ‚ç‚¹ä¸­ç¼–å†™æç¤ºè¯ï¼Œæè¿°ä½ å¸Œæœ›ç”Ÿæˆçš„è§†é¢‘å†…å®¹\n2.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘çš„ç”Ÿæˆ\n3.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Video` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„è§†é¢‘ï¼Œå¯¹åº”çš„è§†é¢‘ä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹\n\n> (å¯é€‰)ä¿®æ”¹ `Luma Concepts` èŠ‚ç‚¹æ¥æŽ§åˆ¶ç›¸æœºè¿åŠ¨æ•ˆæžœï¼Œä¸ºè§†é¢‘æ·»åŠ ä¸“ä¸šçš„é•œå¤´è¯­è¨€\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\n*   **æç¤ºè¯æ’°å†™**ï¼šå°½å¯èƒ½è¯¦ç»†åœ°æè¿°åœºæ™¯ã€ä¸»ä½“ã€åŠ¨ä½œå’Œæ°›å›´ï¼Œä»¥èŽ·å¾—æœ€ä½³ç”Ÿæˆæ•ˆæžœ\n*   **Luma Concepts**ï¼šä¸»è¦ç”¨äºŽæŽ§åˆ¶ç›¸æœºè¿åŠ¨ï¼Œæä¾›æ›´ä¸“ä¸šçš„è§†é¢‘é•œå¤´æ•ˆæžœ\n*   **Seed å‚æ•°**ï¼šä»…ç”¨äºŽç¡®å®šèŠ‚ç‚¹æ˜¯å¦åº”é‡æ–°è¿è¡Œï¼Œä½†å®žé™…ç”Ÿæˆç»“æžœä¸Žç§å­å€¼æ— å…³\n*   **æ¨¡åž‹é€‰æ‹©**ï¼šä¸åŒçš„è§†é¢‘ç”Ÿæˆæ¨¡åž‹æœ‰ä¸åŒçš„ç‰¹ç‚¹ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´ model å‚æ•°æ¥é€‰æ‹©\n*   **åˆ†è¾¨çŽ‡ä¸Žæ—¶é•¿**ï¼šå¯ä»¥é€šè¿‡ resolution å’Œ duration å‚æ•°æ¥è°ƒæ•´è¾“å‡ºè§†é¢‘çš„åˆ†è¾¨çŽ‡å’Œæ—¶é•¿\n*   **Ray 1.6 æ¨¡åž‹æ³¨æ„äº‹é¡¹**ï¼šå½“ä½¿ç”¨ Ray 1.6 æ¨¡åž‹æ—¶ï¼Œduration å’Œ resolution å‚æ•°å°†ä¸ä¼šç”Ÿæ•ˆ\n\nåœ¨æ­¤é¡µé¢\n\n*   [Luma Text to Video èŠ‚ç‚¹æ–‡æ¡£](#luma-text-to-video-%E8%8A%82%E7%82%B9%E6%96%87%E6%A1%A3)\n*   [Luma Text to Video API èŠ‚ç‚¹æ–‡æœ¬åˆ°è§†é¢‘å·¥ä½œæµ](#luma-text-to-video-api-%E8%8A%82%E7%82%B9%E6%96%87%E6%9C%AC%E5%88%B0%E8%A7%86%E9%A2%91%E5%B7%A5%E4%BD%9C%E6%B5%81)\n*   [1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD)\n*   [2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ](#2-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C)\n*   [3\\. è¡¥å……è¯´æ˜Ž](#3-%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E)"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-text-to-video",
  "markdown": "# Luma Text to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Luma Text to Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg) Luma Text to Video èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨Luma AIçš„åˆ›æ–°è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œé€šè¿‡æ–‡æœ¬æè¿°åˆ›å»ºé«˜è´¨é‡ã€æµç•…çš„è§†é¢‘å†…å®¹ã€‚\n\n## èŠ‚ç‚¹åŠŸèƒ½\n\næ­¤èŠ‚ç‚¹è¿žæŽ¥åˆ°Luma AIçš„æ–‡æœ¬åˆ°è§†é¢‘APIï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è¯¦ç»†çš„æ–‡æœ¬æç¤ºè¯ç”ŸæˆåŠ¨æ€è§†é¢‘å†…å®¹ã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### åŸºæœ¬å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt | å­—ç¬¦ä¸² | \"\"  | æè¿°è¦ç”Ÿæˆè§†é¢‘å†…å®¹çš„æ–‡æœ¬æç¤ºè¯ |\n| model | é€‰æ‹©é¡¹ | \\-  | ä½¿ç”¨çš„è§†é¢‘ç”Ÿæˆæ¨¡åž‹ |\n| aspect\\_ratio | é€‰æ‹©é¡¹ | â€ratio\\_16\\_9â€ | è§†é¢‘å®½é«˜æ¯” |\n| resolution | é€‰æ‹©é¡¹ | â€res\\_540pâ€ | è§†é¢‘åˆ†è¾¨çŽ‡ |\n| duration | é€‰æ‹©é¡¹ | \\-  | è§†é¢‘æ—¶é•¿é€‰é¡¹ |\n| loop | å¸ƒå°”å€¼ | False | æ˜¯å¦å¾ªçŽ¯æ’­æ”¾è§†é¢‘ |\n| seed | æ•´æ•°  | 0   | éšæœºç§å­ï¼Œç”¨äºŽå†³å®šèŠ‚ç‚¹æ˜¯å¦éœ€è¦é‡æ–°è¿è¡Œï¼›å®žé™…ç»“æžœä¸Žç§å­æ— å…³ |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| luma\\_concepts | LUMA\\_CONCEPTS | å¯é€‰çš„æ‘„åƒæœºæ¦‚å¿µï¼Œé€šè¿‡Luma ConceptsèŠ‚ç‚¹æŽ§åˆ¶æ‘„åƒæœºè¿åŠ¨ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n[](https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-text-to-video)\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass LumaTextToVideoGenerationNode(ComfyNodeABC):\n    \"\"\"\n    Generates videos synchronously based on prompt and output_size.\n    \"\"\"\n\n    RETURN_TYPES = (IO.VIDEO,)\n    DESCRIPTION = cleandoc(__doc__ or \"\")  # Handle potential None value\n    FUNCTION = \"api_call\"\n    API_NODE = True\n    CATEGORY = \"api node/video/Luma\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt\": (\n                    IO.STRING,\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Prompt for the video generation\",\n                    },\n                ),\n                \"model\": ([model.value for model in LumaVideoModel],),\n                \"aspect_ratio\": (\n                    [ratio.value for ratio in LumaAspectRatio],\n                    {\n                        \"default\": LumaAspectRatio.ratio_16_9,\n                    },\n                ),\n                \"resolution\": (\n                    [resolution.value for resolution in LumaVideoOutputResolution],\n                    {\n                        \"default\": LumaVideoOutputResolution.res_540p,\n                    },\n                ),\n                \"duration\": ([dur.value for dur in LumaVideoModelOutputDuration],),\n                \"loop\": (\n                    IO.BOOLEAN,\n                    {\n                        \"default\": False,\n                    },\n                ),\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"luma_concepts\": (\n                    LumaIO.LUMA_CONCEPTS,\n                    {\n                        \"tooltip\": \"Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\"\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    def api_call(\n        self,\n        prompt: str,\n        model: str,\n        aspect_ratio: str,\n        resolution: str,\n        duration: str,\n        loop: bool,\n        seed,\n        luma_concepts: LumaConceptChain = None,\n        auth_token=None,\n        **kwargs,\n    ):\n        duration = duration if model != LumaVideoModel.ray_1_6 else None\n        resolution = resolution if model != LumaVideoModel.ray_1_6 else None\n\n        operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/luma/generations\",\n                method=HttpMethod.POST,\n                request_model=LumaGenerationRequest,\n                response_model=LumaGeneration,\n            ),\n            request=LumaGenerationRequest(\n                prompt=prompt,\n                model=model,\n                resolution=resolution,\n                aspect_ratio=aspect_ratio,\n                duration=duration,\n                loop=loop,\n                concepts=luma_concepts.create_api_model() if luma_concepts else None,\n            ),\n            auth_token=auth_token,\n        )\n        response_api: LumaGeneration = operation.execute()\n\n        operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=f\"/proxy/luma/generations/{response_api.id}\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=LumaGeneration,\n            ),\n            completed_statuses=[LumaState.completed],\n            failed_statuses=[LumaState.failed],\n            status_extractor=lambda x: x.state,\n            auth_token=auth_token,\n        )\n        response_poll = operation.execute()\n\n        vid_response = requests.get(response_poll.assets.video)\n        return (VideoFromFile(BytesIO(vid_response.content)),)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/minimax/minimax-text-to-video",
  "markdown": "# MiniMax Text to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ MiniMax Text to Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg) MiniMax Text to Video èŠ‚ç‚¹é€šè¿‡è¿žæŽ¥ MiniMax çš„APIï¼Œå…è®¸ç”¨æˆ·åˆ©ç”¨æ–‡æœ¬æç¤ºè¯ç”Ÿæˆé«˜è´¨é‡ã€æµç•…çš„è§†é¢‘å†…å®¹ã€‚è¯¥èŠ‚ç‚¹æ”¯æŒä¸åŒçš„è§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼Œå¯ä»¥åˆ›å»ºå„ç§é£Žæ ¼å’Œç±»åž‹çš„çŸ­è§†é¢‘ç‰‡æ®µã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt\\_text | å­—ç¬¦ä¸² | \"\"  | ç”¨äºŽæŒ‡å¯¼è§†é¢‘ç”Ÿæˆçš„æ–‡æœ¬æç¤ºè¯ |\n| model | é€‰æ‹©é¡¹ | â€T2V-01â€ | ä½¿ç”¨çš„è§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼Œå¯é€‰å€¼åŒ…æ‹¬â€T2V-01â€å’Œâ€T2V-01-Directorâ€ |\n| seed | æ•´æ•°  | 0   | ç”Ÿæˆçš„éšæœºç§å­ï¼Œå½±å“åˆå§‹å™ªå£°åˆ›å»ºï¼Œé»˜è®¤å€¼ä¸º0 |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-03)\\]\n\n```\n\nclass MinimaxTextToVideoNode:\n    \"\"\"\n    Generates videos synchronously based on a prompt, and optional parameters using Minimax's API.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"prompt_text\": (\n                    \"STRING\",\n                    {\n                        \"multiline\": True,\n                        \"default\": \"\",\n                        \"tooltip\": \"Text prompt to guide the video generation\",\n                    },\n                ),\n                \"model\": (\n                    [\n                        \"T2V-01\",\n                        \"T2V-01-Director\",\n                    ],\n                    {\n                        \"default\": \"T2V-01\",\n                        \"tooltip\": \"Model to use for video generation\",\n                    },\n                ),\n            },\n            \"optional\": {\n                \"seed\": (\n                    IO.INT,\n                    {\n                        \"default\": 0,\n                        \"min\": 0,\n                        \"max\": 0xFFFFFFFFFFFFFFFF,\n                        \"control_after_generate\": True,\n                        \"tooltip\": \"The random seed used for creating the noise.\",\n                    },\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Generates videos from prompts using Minimax's API\"\n    FUNCTION = \"generate_video\"\n    CATEGORY = \"api node/video/Minimax\"\n    API_NODE = True\n    OUTPUT_NODE = True\n\n    def generate_video(\n        self,\n        prompt_text,\n        seed=0,\n        model=\"T2V-01\",\n        image: torch.Tensor=None, # used for ImageToVideo\n        subject: torch.Tensor=None, # used for SubjectToVideo\n        auth_token=None,\n    ):\n        '''\n        Function used between Minimax nodes - supports T2V, I2V, and S2V, based on provided arguments.\n        '''\n        # upload image, if passed in\n        image_url = None\n        if image is not None:\n            image_url = upload_images_to_comfyapi(image, max_images=1, auth_token=auth_token)[0]\n\n        # TODO: figure out how to deal with subject properly, API returns invalid params when using S2V-01 model\n        subject_reference = None\n        if subject is not None:\n            subject_url = upload_images_to_comfyapi(subject, max_images=1, auth_token=auth_token)[0]\n            subject_reference = [SubjectReferenceItem(image=subject_url)]\n\n\n        video_generate_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/video_generation\",\n                method=HttpMethod.POST,\n                request_model=MinimaxVideoGenerationRequest,\n                response_model=MinimaxVideoGenerationResponse,\n            ),\n            request=MinimaxVideoGenerationRequest(\n                model=Model(model),\n                prompt=prompt_text,\n                callback_url=None,\n                first_frame_image=image_url,\n                subject_reference=subject_reference,\n                prompt_optimizer=None,\n            ),\n            auth_token=auth_token,\n        )\n        response = video_generate_operation.execute()\n\n        task_id = response.task_id\n        if not task_id:\n            raise Exception(f\"Minimax generation failed: {response.base_resp}\")\n\n        video_generate_operation = PollingOperation(\n            poll_endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/query/video_generation\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=MinimaxTaskResultResponse,\n                query_params={\"task_id\": task_id},\n            ),\n            completed_statuses=[\"Success\"],\n            failed_statuses=[\"Fail\"],\n            status_extractor=lambda x: x.status.value,\n            auth_token=auth_token,\n        )\n        task_result = video_generate_operation.execute()\n\n        file_id = task_result.file_id\n        if file_id is None:\n            raise Exception(\"Request was not successful. Missing file ID.\")\n        file_retrieve_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=\"/proxy/minimax/files/retrieve\",\n                method=HttpMethod.GET,\n                request_model=EmptyRequest,\n                response_model=MinimaxFileRetrieveResponse,\n                query_params={\"file_id\": int(file_id)},\n            ),\n            request=EmptyRequest(),\n            auth_token=auth_token,\n        )\n        file_result = file_retrieve_operation.execute()\n\n        file_url = file_result.file.download_url\n        if file_url is None:\n            raise Exception(\n                f\"No video was found in the response. Full response: {file_result.model_dump()}\"\n            )\n        logging.info(f\"Generated video URL: {file_url}\")\n\n        video_io = download_url_to_bytesio(file_url)\n        if video_io is None:\n            error_msg = f\"Failed to download video from {file_url}\"\n            logging.error(error_msg)\n            raise Exception(error_msg)\n        return (VideoFromFile(video_io),)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/credits",
  "markdown": "# ComfyUI ç§¯åˆ†ç®¡ç† - ComfyUI\n\nç§¯åˆ†ç³»ç»Ÿæ˜¯ä¸ºäº†æ”¯æŒ `API Nodes` èŠ‚ç‚¹è€Œæ–°å¢žçš„ï¼Œç”±äºŽè°ƒç”¨é—­æº AI æ¨¡åž‹éœ€è¦æ¶ˆè€—Tokenï¼Œæ‰€ä»¥å¯¹åº”çš„ç§¯åˆ†ç®¡ç†æ˜¯å¾ˆæœ‰å¿…è¦çš„ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹ç§¯åˆ†ç•Œé¢å¹¶ä¸ä¼šå±•ç¤ºï¼Œè¯·é¦–å…ˆåœ¨`è®¾ç½®` -> `ç”¨æˆ·`ä¸­ç™»å½•å¯¹åº”çš„ ComfyUI è´¦å·ï¼Œç„¶åŽä½ å°±å¯ä»¥åœ¨ `è®¾ç½®` -> `ç§¯åˆ†` ä¸­æŸ¥çœ‹å…³è”è´¦å·çš„ç§¯åˆ†ä¿¡æ¯äº†ã€‚ ![ComfyUI ç§¯åˆ†ç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/menu-credits.jpg)\n\n## å¦‚ä½•è´­ä¹°ç§¯åˆ†?\n\nä¸‹é¢æ˜¯å¯¹åº”çš„ç§¯åˆ†è´­ä¹°æ¼”ç¤ºè§†é¢‘ï¼š\n\nè¯¦ç»†æ“ä½œæ­¥éª¤å¦‚ä¸‹ï¼š\n\n## å¸¸è§é—®é¢˜"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/appearance",
  "markdown": "# ComfyUI å¤–è§‚è®¾ç½® - ComfyUI\n\nè¿™éƒ¨åˆ†çš„è®¾ç½®ä¸»è¦ç”¨äºŽè‡ªå®šä¹‰ ComfyUI çš„å¤–è§‚ï¼ŒåŒ…æ‹¬è‰²å½©ä¸»é¢˜ã€èƒŒæ™¯å›¾ç‰‡ã€èŠ‚ç‚¹æ ·å¼ç­‰ã€‚\n\n## è‰²å½©ä¸»é¢˜\n\nè‡ªå®šä¹‰ ComfyUI å¤–è§‚çš„ä¸»è¦æ–¹å¼æ˜¯é€šè¿‡å†…ç½®çš„è°ƒè‰²æ¿ç³»ç»Ÿã€‚ ![è‰²å½©ä¸»é¢˜](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/color-palette.jpg)\n\n1.  åˆ‡æ¢ ComfyUI ä¸»é¢˜\n2.  å°†å½“å‰é€‰ä¸­çš„ä¸»é¢˜å¯¼å‡ºä¸º JSON æ ¼å¼\n3.  ä»ŽJsonæ–‡ä»¶ä¸­è½½å…¥è‡ªå®šä¹‰ä¸»é¢˜é…ç½®\n4.  åˆ é™¤è‡ªå®šä¹‰ä¸»é¢˜é…ç½®\n\n### å¦‚ä½•è‡ªå®šä¹‰é¢œè‰²ä¸»é¢˜\n\nè°ƒè‰²æ¿å…è®¸æ‚¨ä¿®æ”¹è®¸å¤šç‰¹å®šå±žæ€§ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æœ€å¸¸è‡ªå®šä¹‰çš„å…ƒç´ ï¼Œé¢œè‰²é‡‡ç”¨åå…­è¿›åˆ¶è¡¨ç¤ºï¼š\n\n```\n{\n  \"id\": \"dark\",                     // å¿…é¡»æ˜¯å”¯ä¸€çš„ï¼Œä¸èƒ½å’Œå…¶å®ƒä¸»é¢˜çš„idç›¸åŒ\n  \"name\": \"Dark (Default)\",         // ä¸»é¢˜åç§°,æ˜¾ç¤ºåœ¨ä¸»é¢˜é€‰æ‹©å™¨ä¸­\n  \"colors\": {\n    \"node_slot\": {                  // èŠ‚ç‚¹è¿žæŽ¥æ§½çš„é¢œè‰²é…ç½®\n      \"CLIP\": \"#FFD500\",            // CLIP æ¨¡åž‹è¿žæŽ¥æ§½é¢œè‰²\n      \"CLIP_VISION\": \"#A8DADC\",     // CLIP Vision æ¨¡åž‹è¿žæŽ¥æ§½é¢œè‰²\n      \"CLIP_VISION_OUTPUT\": \"#ad7452\", // CLIP Vision è¾“å‡ºè¿žæŽ¥æ§½é¢œè‰²\n      \"CONDITIONING\": \"#FFA931\",     // æ¡ä»¶æŽ§åˆ¶è¿žæŽ¥æ§½é¢œè‰²\n      \"CONTROL_NET\": \"#6EE7B7\",     // ControlNet æ¨¡åž‹è¿žæŽ¥æ§½é¢œè‰²\n      \"IMAGE\": \"#64B5F6\",           // å›¾åƒæ•°æ®è¿žæŽ¥æ§½é¢œè‰²\n      \"LATENT\": \"#FF9CF9\",          // æ½œåœ¨ç©ºé—´è¿žæŽ¥æ§½é¢œè‰²\n      \"MASK\": \"#81C784\",            // è’™ç‰ˆæ•°æ®è¿žæŽ¥æ§½é¢œè‰²\n      \"MODEL\": \"#B39DDB\",           // æ¨¡åž‹è¿žæŽ¥æ§½é¢œè‰²\n      \"STYLE_MODEL\": \"#C2FFAE\",     // é£Žæ ¼æ¨¡åž‹è¿žæŽ¥æ§½é¢œè‰²\n      \"VAE\": \"#FF6E6E\",             // VAE æ¨¡åž‹è¿žæŽ¥æ§½é¢œè‰²\n      \"NOISE\": \"#B0B0B0\",           // å™ªå£°æ•°æ®è¿žæŽ¥æ§½é¢œè‰²\n      \"GUIDER\": \"#66FFFF\",          // å¼•å¯¼å™¨è¿žæŽ¥æ§½é¢œè‰²\n      \"SAMPLER\": \"#ECB4B4\",         // é‡‡æ ·å™¨è¿žæŽ¥æ§½é¢œè‰²\n      \"SIGMAS\": \"#CDFFCD\",          // Sigmas æ•°æ®è¿žæŽ¥æ§½é¢œè‰²\n      \"TAESD\": \"#DCC274\"            // TAESD æ¨¡åž‹è¿žæŽ¥æ§½é¢œè‰²\n    },\n    \"litegraph_base\": {             // LiteGraph åŸºç¡€ç•Œé¢é…ç½®\n      \"BACKGROUND_IMAGE\": \"\",        // èƒŒæ™¯å›¾ç‰‡,é»˜è®¤ä¸ºç©º\n      \"CLEAR_BACKGROUND_COLOR\": \"#222\", // ä¸»ç”»å¸ƒèƒŒæ™¯è‰²\n      \"NODE_TITLE_COLOR\": \"#999\",    // èŠ‚ç‚¹æ ‡é¢˜æ–‡æœ¬é¢œè‰²\n      \"NODE_SELECTED_TITLE_COLOR\": \"#FFF\", // é€‰ä¸­èŠ‚ç‚¹çš„æ ‡é¢˜é¢œè‰²\n      \"NODE_TEXT_SIZE\": 14,          // èŠ‚ç‚¹æ–‡æœ¬å¤§å°\n      \"NODE_TEXT_COLOR\": \"#AAA\",     // èŠ‚ç‚¹æ–‡æœ¬é¢œè‰²\n      \"NODE_TEXT_HIGHLIGHT_COLOR\": \"#FFF\", // èŠ‚ç‚¹æ–‡æœ¬é«˜äº®é¢œè‰²\n      \"NODE_SUBTEXT_SIZE\": 12,       // èŠ‚ç‚¹å­æ–‡æœ¬å¤§å°\n      \"NODE_DEFAULT_COLOR\": \"#333\",   // èŠ‚ç‚¹é»˜è®¤é¢œè‰²\n      \"NODE_DEFAULT_BGCOLOR\": \"#353535\", // èŠ‚ç‚¹é»˜è®¤èƒŒæ™¯è‰²\n      \"NODE_DEFAULT_BOXCOLOR\": \"#666\", // èŠ‚ç‚¹é»˜è®¤è¾¹æ¡†é¢œè‰²\n      \"NODE_DEFAULT_SHAPE\": 2,        // èŠ‚ç‚¹é»˜è®¤å½¢çŠ¶\n      \"NODE_BOX_OUTLINE_COLOR\": \"#FFF\", // èŠ‚ç‚¹è¾¹æ¡†è½®å»“é¢œè‰²\n      \"NODE_BYPASS_BGCOLOR\": \"#FF00FF\", // èŠ‚ç‚¹æ—è·¯èƒŒæ™¯è‰²\n      \"NODE_ERROR_COLOUR\": \"#E00\",    // èŠ‚ç‚¹é”™è¯¯çŠ¶æ€é¢œè‰²\n      \"DEFAULT_SHADOW_COLOR\": \"rgba(0,0,0,0.5)\", // é»˜è®¤é˜´å½±é¢œè‰²\n      \"DEFAULT_GROUP_FONT\": 24,       // åˆ†ç»„é»˜è®¤å­—ä½“å¤§å°\n      \"WIDGET_BGCOLOR\": \"#222\",       // å°éƒ¨ä»¶èƒŒæ™¯è‰²\n      \"WIDGET_OUTLINE_COLOR\": \"#666\", // å°éƒ¨ä»¶è½®å»“é¢œè‰²\n      \"WIDGET_TEXT_COLOR\": \"#DDD\",    // å°éƒ¨ä»¶æ–‡æœ¬é¢œè‰²\n      \"WIDGET_SECONDARY_TEXT_COLOR\": \"#999\", // å°éƒ¨ä»¶æ¬¡è¦æ–‡æœ¬é¢œè‰²\n      \"WIDGET_DISABLED_TEXT_COLOR\": \"#666\", // å°éƒ¨ä»¶ç¦ç”¨çŠ¶æ€æ–‡æœ¬é¢œè‰²\n      \"LINK_COLOR\": \"#9A9\",          // è¿žæŽ¥çº¿é¢œè‰²\n      \"EVENT_LINK_COLOR\": \"#A86\",    // äº‹ä»¶è¿žæŽ¥çº¿é¢œè‰²\n      \"CONNECTING_LINK_COLOR\": \"#AFA\", // æ­£åœ¨è¿žæŽ¥æ—¶çš„è¿žæŽ¥çº¿é¢œè‰²\n      \"BADGE_FG_COLOR\": \"#FFF\",      // å¾½ç« å‰æ™¯è‰²\n      \"BADGE_BG_COLOR\": \"#0F1F0F\"    // å¾½ç« èƒŒæ™¯è‰²\n    },\n    \"comfy_base\": {                  // ComfyUI åŸºç¡€ç•Œé¢é…ç½®\n      \"fg-color\": \"#fff\",            // å‰æ™¯è‰²\n      \"bg-color\": \"#202020\",         // èƒŒæ™¯è‰²\n      \"comfy-menu-bg\": \"#353535\",    // èœå•èƒŒæ™¯è‰²\n      \"comfy-menu-secondary-bg\": \"#303030\", // æ¬¡çº§èœå•èƒŒæ™¯è‰²\n      \"comfy-input-bg\": \"#222\",      // è¾“å…¥æ¡†èƒŒæ™¯è‰²\n      \"input-text\": \"#ddd\",          // è¾“å…¥æ–‡æœ¬é¢œè‰²\n      \"descrip-text\": \"#999\",        // æè¿°æ–‡æœ¬é¢œè‰²\n      \"drag-text\": \"#ccc\",           // æ‹–æ‹½æ–‡æœ¬é¢œè‰²\n      \"error-text\": \"#ff4444\",       // é”™è¯¯æ–‡æœ¬é¢œè‰²\n      \"border-color\": \"#4e4e4e\",     // è¾¹æ¡†é¢œè‰²\n      \"tr-even-bg-color\": \"#222\",    // è¡¨æ ¼å¶æ•°è¡ŒèƒŒæ™¯è‰²\n      \"tr-odd-bg-color\": \"#353535\",  // è¡¨æ ¼å¥‡æ•°è¡ŒèƒŒæ™¯è‰²\n      \"content-bg\": \"#4e4e4e\",       // å†…å®¹åŒºèƒŒæ™¯è‰²\n      \"content-fg\": \"#fff\",          // å†…å®¹åŒºå‰æ™¯è‰²\n      \"content-hover-bg\": \"#222\",    // å†…å®¹åŒºæ‚¬åœèƒŒæ™¯è‰²\n      \"content-hover-fg\": \"#fff\",    // å†…å®¹åŒºæ‚¬åœå‰æ™¯è‰²\n      \"bar-shadow\": \"rgba(16, 16, 16, 0.5) 0 0 0.5rem\" // æ é˜´å½±æ•ˆæžœ\n    }\n  }\n}\n```\n\n## ç”»å¸ƒ\n\n### èƒŒæ™¯å›¾ç‰‡\n\n*   ç‰ˆæœ¬è¦æ±‚ï¼šComfyUI å‰ç«¯ç‰ˆæœ¬ 1.20.5 æˆ–æ›´æ–°ç‰ˆæœ¬\n*   åŠŸèƒ½ï¼šä¸ºç”»å¸ƒè®¾ç½®è‡ªå®šä¹‰èƒŒæ™¯å›¾ç‰‡ï¼Œæä¾›æ›´åŠ ä¸ªæ€§åŒ–çš„å·¥ä½œç©ºé—´ï¼Œä½ å¯ä»¥ä¸Šä¼ å›¾ç‰‡æˆ–è€…ä½¿ç”¨ç½‘ç»œå›¾ç‰‡æ¥ä¸ºç”»å¸ƒè®¾ç½®èƒŒæ™¯å›¾ç‰‡\n\n![è®¾ç½®èƒŒæ™¯å›¾ç‰‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/set-as-bg.jpg)\n\n## èŠ‚ç‚¹\n\n### èŠ‚ç‚¹ä¸é€æ˜Žåº¦\n\n*   åŠŸèƒ½ï¼šè®¾ç½®èŠ‚ç‚¹çš„ä¸é€æ˜Žåº¦ï¼Œ0è¡¨ç¤ºå®Œå…¨é€æ˜Žï¼Œ1è¡¨ç¤ºå®Œå…¨ä¸é€æ˜Ž\n\n![èŠ‚ç‚¹ä¸é€æ˜Žåº¦](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/node-opacity.jpg)\n\n## èŠ‚ç‚¹ç»„ä»¶\n\n### æ–‡æœ¬åŸŸå°éƒ¨ä»¶å­—ä½“å¤§å°\\*\\*\n\n*   **èŒƒå›´**ï¼š8 - 24\n*   **åŠŸèƒ½**ï¼šè®¾ç½®æ–‡æœ¬åŸŸå°éƒ¨ä»¶ä¸­çš„å­—ä½“å¤§å°ï¼Œè°ƒæ•´æ–‡æœ¬è¾“å…¥æ¡†ä¸­æ–‡å­—çš„æ˜¾ç¤ºå¤§å°ï¼Œæå‡å¯è¯»æ€§ ![æ–‡æœ¬åŸŸå°éƒ¨ä»¶å­—ä½“å¤§å°](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/textarea-font-size.jpg)\n\n## ä¾§è¾¹æ \n\n### ç»Ÿä¸€ä¾§è¾¹æ å®½åº¦\n\n*   **åŠŸèƒ½**ï¼šå¯ç”¨åŽï¼Œå½“ä½ åœ¨ä¸åŒçš„ä¾§è¾¹æ ä¹‹é—´åˆ‡æ¢æ—¶ï¼Œä¾§è¾¹æ çš„å®½åº¦å°†ç»Ÿä¸€ä¸ºä¸€è‡´çš„å®½åº¦ï¼Œå¦‚æžœç¦ç”¨ï¼Œä¸åŒçš„ä¾§è¾¹æ çš„å®½åº¦åœ¨åˆ‡æ¢æ—¶å¯ä»¥ä¿æŒè‡ªå®šä¹‰çš„å®½åº¦\n\n### ä¾§è¾¹æ å¤§å°\n\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶ä¾§è¾¹æ çš„å°ºå¯¸å¤§å°ï¼Œå¯ä»¥è®¾ç½®ä¸ºæ­£å¸¸æˆ–è€…å°\n\n### ä¾§è¾¹æ ä½ç½®\n\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶ä¾§è¾¹æ æ˜¾ç¤ºåœ¨ç•Œé¢çš„å·¦ä¾§è¿˜æ˜¯å³ä¾§ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®ä½¿ç”¨ä¹ æƒ¯è°ƒæ•´ä¾§è¾¹æ ä½ç½®\n\n## æ ‘å½¢æµè§ˆå™¨\n\n### æ ‘å½¢æµè§ˆå™¨é¡¹ç›®å†…è¾¹è·\n\n*   **åŠŸèƒ½**ï¼šè®¾ç½®æ ‘å½¢æµè§ˆå™¨ï¼ˆä¾§è¾¹æ é¢æ¿ï¼‰ä¸­é¡¹ç›®çš„å†…è¾¹è·ï¼Œè°ƒæ•´æ ‘å½¢ç»“æž„ä¸­å„é¡¹ç›®ä¹‹é—´çš„é—´è·\n\n![æ ‘å½¢æµè§ˆå™¨é¡¹ç›®å†…è¾¹è·](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/appearance/padding.jpg)\n\n## ä½¿ç”¨user.cssè¿›è¡Œé«˜çº§å¤–è§‚è‡ªå®šä¹‰\n\nå¯¹äºŽè°ƒè‰²æ¿ä¸èƒ½æä¾›è¶³å¤ŸæŽ§åˆ¶çš„æƒ…å†µï¼Œæ‚¨å¯ä»¥é€šè¿‡ user.css æ–‡ä»¶ä½¿ç”¨è‡ªå®šä¹‰ CSSã€‚æ­¤æ–¹æ³•æŽ¨èç»™éœ€è¦è‡ªå®šä¹‰è°ƒè‰²æ¿ç³»ç»Ÿä¸­ä¸å¯ç”¨å…ƒç´ çš„é«˜çº§ç”¨æˆ·ã€‚\n\n### è¦æ±‚\n\n*   ComfyUI å‰ç«¯ç‰ˆæœ¬ 1.20.5 æˆ–æ›´æ–°ç‰ˆæœ¬\n\n### è®¾ç½® user.css\n\n1.  åœ¨ ComfyUI ç”¨æˆ·ç›®å½•ï¼ˆä¸Žå·¥ä½œæµå’Œè®¾ç½®ç›¸åŒä½ç½® - è¯·å‚é˜…ä¸‹é¢çš„ä½ç½®è¯¦ç»†ä¿¡æ¯ï¼‰ä¸­åˆ›å»ºä¸€ä¸ªåä¸º `user.css` çš„æ–‡ä»¶\n2.  åœ¨æ­¤æ–‡ä»¶ä¸­æ·»åŠ æ‚¨çš„è‡ªå®šä¹‰ CSS è§„åˆ™\n3.  é‡å¯ ComfyUI æˆ–åˆ·æ–°é¡µé¢ä»¥åº”ç”¨æ›´æ”¹\n\n### ç”¨æˆ·ç›®å½•ä½ç½®\n\nComfyUI ç”¨æˆ·ç›®å½•æ˜¯å­˜å‚¨æ‚¨çš„ä¸ªäººè®¾ç½®ã€å·¥ä½œæµå’Œè‡ªå®šä¹‰å†…å®¹çš„åœ°æ–¹ã€‚ä½ç½®å–å†³äºŽæ‚¨çš„å®‰è£…ç±»åž‹ï¼š\n\næ­¤ä½ç½®åŒ…å«æ‚¨çš„å·¥ä½œæµã€è®¾ç½®å’Œå…¶ä»–ç”¨æˆ·ç‰¹å®šæ–‡ä»¶ã€‚ æ‰¾åˆ°ä¸Šè¿°æ–‡ä»¶å¤¹ä½ç½®åŽï¼Œè¯·å°†å¯¹åº”çš„ Css æ–‡ä»¶å¤åˆ¶åˆ°å¯¹åº”çš„ç”¨æˆ·ç›®å½•ä¸­å¦‚é»˜è®¤ç”¨æˆ·æ–‡ä»¶å¤¹ä¸º`ComfyUI/user/default`ï¼Œç„¶åŽé‡å¯ ComfyUI æˆ–åˆ·æ–°é¡µé¢ä»¥åº”ç”¨æ›´æ”¹\n\n### user.css ç¤ºä¾‹åŠç›¸å…³è¯´æ˜Ž\n\n`user.css` æ–‡ä»¶ä¼šåœ¨å¯åŠ¨çš„æ—©æœŸå°±è¿›è¡ŒåŠ è½½ã€‚æ‰€ä»¥èƒ½éœ€è¦åœ¨ CSS è§„åˆ™ä¸­ä½¿ç”¨ `!important` æ¥ç¡®ä¿å®ƒä»¬è¦†ç›–é»˜è®¤æ ·å¼ã€‚ **user.css è‡ªå®šä¹‰ç¤ºä¾‹**\n\n```\n/* å¢žåŠ è¾“å…¥æ¡†å’Œèœå•ä¸­çš„å­—ä½“å¤§å°ä»¥æé«˜å¯è¯»æ€§ */\n.comfy-multiline-input, .litecontextmenu .litemenu-entry {\n    font-size: 20px !important;\n}\n\n/* ä½¿ä¸Šä¸‹æ–‡èœå•é¡¹æ›´å¤§ï¼Œä¾¿äºŽé€‰æ‹© */\n.litegraph .litemenu-entry,\n.litemenu-title {\n  font-size: 24px !important; \n}\n\n/* ä¸ºè°ƒè‰²æ¿ä¸­ä¸å¯ç”¨çš„ç‰¹å®šå…ƒç´ è‡ªå®šä¹‰æ ·å¼ */\n.comfy-menu {\n    border: 1px solid rgb(126, 179, 189) !important;\n    border-radius: 0px 0px 0px 10px !important;\n    backdrop-filter: blur(2px);\n}\n```\n\n**æœ€ä½³å®žè·µ**\n\n1.  **é¦–å…ˆä½¿ç”¨è°ƒè‰²æ¿**è¿›è¡Œå¤§å¤šæ•°è‡ªå®šä¹‰\n2.  **ä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨ user.css**ï¼Œç”¨äºŽè°ƒè‰²æ¿æœªæ¶µç›–çš„å…ƒç´ \n3.  **åœ¨è¿›è¡Œé‡å¤§æ›´æ”¹å‰å¯¼å‡ºæ‚¨çš„ä¸»é¢˜**ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶æ¢å¤\n4.  **ä¸Žç¤¾åŒºåˆ†äº«æ‚¨çš„ä¸»é¢˜**ï¼Œä»¥å¯å‘ä»–äºº\n\n**æ•…éšœæŽ’é™¤**\n\n*   å¦‚æžœæ‚¨çš„è°ƒè‰²æ¿æ›´æ”¹æ²¡æœ‰æ˜¾ç¤ºï¼Œå°è¯•åˆ·æ–°é¡µé¢\n*   å¦‚æžœ CSS è‡ªå®šä¹‰ä¸èµ·ä½œç”¨ï¼Œæ£€æŸ¥æ‚¨æ˜¯å¦ä½¿ç”¨å‰ç«¯ç‰ˆæœ¬ 1.20.5+\n*   å°è¯•åœ¨æœªåº”ç”¨çš„ user.css è§„åˆ™ä¸­æ·»åŠ  `!important`\n*   ä¿ç•™æ‚¨çš„è‡ªå®šä¹‰å¤‡ä»½ï¼Œä»¥ä¾¿è½»æ¾æ¢å¤"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-image-to-video",
  "markdown": "# Luma Image to Video API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\n[Luma Image to Video](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-image-to-video) èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨Luma AIçš„å…ˆè¿›æŠ€æœ¯å°†é™æ€å›¾åƒè½¬æ¢ä¸ºæµç•…ã€åŠ¨æ€çš„è§†é¢‘å†…å®¹ï¼Œä¸ºå›¾åƒèµ‹äºˆç”Ÿå‘½åŠ›å’ŒåŠ¨æ€ç‰¹æ€§ã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å¦‚ä½•ä½¿ç”¨å¯¹åº”èŠ‚ç‚¹æ¥è¿›è¡Œå›¾åƒåˆ°è§†é¢‘çš„å·¥ä½œæµè®¾ç½®ã€‚\n\nä½ å¯æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”èŠ‚ç‚¹çš„è¯¦ç»†å‚æ•°è®¾ç½®ç­‰\n\n[\n\n## Luma Image to Video èŠ‚ç‚¹æ–‡æ¡£\n\nLuma Image to Video API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-image-to-video)[\n\n## Luma Concepts èŠ‚ç‚¹æ–‡æ¡£\n\nLuma Concepts API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/luma/luma-concepts)\n\n## Luma Image to Video API èŠ‚ç‚¹å›¾åƒåˆ°è§†é¢‘å·¥ä½œæµ\n\nLuma Image to Video èŠ‚ç‚¹éœ€è¦è‡³å°‘æä¾›ä¸€ä¸ªå›¾åƒè¾“å…¥ï¼ˆ`first_image`æˆ–`last_image`ï¼‰ï¼Œç»“åˆæ–‡æœ¬æç¤ºè¯æ¥ç¡®å®šè§†é¢‘çš„åŠ¨æ€æ•ˆæžœã€‚åœ¨æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬åˆ¶ä½œäº†ä½¿ç”¨`first_image`å’Œ`luma_concepts`çš„ç¤ºä¾‹ï¼Œè®©ä½ ä½“éªŒLuma AIåœ¨è§†é¢‘ç”Ÿæˆä¸Šçš„ä¼˜ç§€èƒ½åŠ›ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„è§†é¢‘çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![Luma å›¾åƒåˆ°è§†é¢‘å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/luma_i2v.mp4) è¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä¼šç”¨ä½œè¾“å…¥ï¼š ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/input.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Luma å›¾åƒåˆ°è§†é¢‘å·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `first_image` èŠ‚ç‚¹ä¸­ä¸Šä¼ ä½ çš„è¾“å…¥å›¾åƒ\n2.  (å¯é€‰)åœ¨ Luma Image to Video èŠ‚ç‚¹ä¸­ç¼–å†™æç¤ºè¯ï¼Œæè¿°ä½ å¸Œæœ›è§†é¢‘å¦‚ä½•åŠ¨æ€å±•ç¤ºå›¾åƒ\n3.  (å¯é€‰)ä¿®æ”¹ `Luma Concepts` èŠ‚ç‚¹æ¥æŽ§åˆ¶ç›¸æœºè¿åŠ¨æ•ˆæžœï¼Œä¸ºè§†é¢‘æ·»åŠ ä¸“ä¸šçš„é•œå¤´è¯­è¨€\n4.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘çš„ç”Ÿæˆ\n5.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Video` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„è§†é¢‘ï¼Œå¯¹åº”çš„è§†é¢‘ä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\n*   **è¾“å…¥å›¾åƒè¦æ±‚**ï¼š`first_image` å’Œ `last_image` è‡³å°‘éœ€è¦æä¾›ä¸€ä¸ªï¼Œæ¯ä¸ªè¾“å…¥æœ€å¤šåªæŽ¥å—1å¼ å›¾ç‰‡\n*   **Luma Concepts**ï¼šä¸»è¦ç”¨äºŽæŽ§åˆ¶ç›¸æœºè¿åŠ¨ï¼Œæä¾›æ›´ä¸“ä¸šçš„è§†é¢‘é•œå¤´æ•ˆæžœ\n*   **Seed å‚æ•°**ï¼šä»…ç”¨äºŽç¡®å®šèŠ‚ç‚¹æ˜¯å¦åº”é‡æ–°è¿è¡Œï¼Œä½†å®žé™…ç”Ÿæˆç»“æžœä¸Žç§å­å€¼æ— å…³\n*   **å¯ç”¨è¾“å…¥èŠ‚ç‚¹**ï¼šè¦å¯ç”¨å¯¹åº”çš„è¾“å…¥è¯·åœ¨ç›®å‰ç´«è‰²â€ç»•è¿‡ï¼ˆBypassï¼‰â€œæ¨¡å¼çš„èŠ‚ç‚¹ä¸Šå³é”®ï¼Œè®¾ç½®å¯¹åº”çš„â€æ¨¡å¼ï¼ˆmodeï¼‰â€œä¸ºâ€æ€»æ˜¯ï¼ˆalwaysï¼‰â€\n*   **æ¨¡åž‹é€‰æ‹©**ï¼šä¸åŒçš„è§†é¢‘ç”Ÿæˆæ¨¡åž‹æœ‰ä¸åŒçš„ç‰¹ç‚¹ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´ model å‚æ•°æ¥é€‰æ‹©\n*   **åˆ†è¾¨çŽ‡ä¸Žæ—¶é•¿**ï¼šå¯ä»¥é€šè¿‡ resolution å’Œ duration å‚æ•°æ¥è°ƒæ•´è¾“å‡ºè§†é¢‘çš„åˆ†è¾¨çŽ‡å’Œæ—¶é•¿"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pika/pika-text-to-video",
  "markdown": "# Pika 2.2 Text to Video - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Pika 2.2 Text to Video èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg) Pika 2.2 Text to Video èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨Pikaçš„2.2ç‰ˆæœ¬APIï¼Œé€šè¿‡æ–‡æœ¬æè¿°åˆ›å»ºè§†é¢‘å†…å®¹ã€‚æ­¤èŠ‚ç‚¹è¿žæŽ¥åˆ°Pikaçš„æ–‡æœ¬åˆ°è§†é¢‘APIï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡æ–‡æœ¬æç¤ºè¯ç”Ÿæˆè§†é¢‘ï¼Œå¹¶æä¾›å¤šç§å‚æ•°æŽ§åˆ¶ç”Ÿæˆæ•ˆæžœã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- |\n| prompt\\_text | å­—ç¬¦ä¸² | \"\"  | æè¿°è¦ç”Ÿæˆè§†é¢‘å†…å®¹çš„æ–‡æœ¬æç¤ºè¯ |\n| negative\\_prompt | å­—ç¬¦ä¸² | \"\"  | æŒ‡å®šä¸å¸Œæœ›åœ¨è§†é¢‘ä¸­å‡ºçŽ°çš„å…ƒç´  |\n| seed | æ•´æ•°  | 0   | ç”Ÿæˆè¿‡ç¨‹çš„éšæœºç§å­ |\n| resolution | é€‰æ‹©é¡¹ | â€1080pâ€ | ç”Ÿæˆè§†é¢‘çš„åˆ†è¾¨çŽ‡ |\n| duration | é€‰æ‹©é¡¹ | â€5sâ€ | ç”Ÿæˆè§†é¢‘çš„æŒç»­æ—¶é—´ |\n| aspect\\_ratio | æµ®ç‚¹æ•° | 1.7777777777777777 | è¾“å‡ºè§†é¢‘çš„å®½é«˜æ¯”ï¼ŒèŒƒå›´0.4-2.5ï¼Œæ­¥é•¿0.001 |\n\n### è¾“å‡ºå‚æ•°\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| VIDEO | è§†é¢‘  | ç”Ÿæˆçš„è§†é¢‘ç»“æžœ |\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-05)\\]\n\n```\n\nclass PikaTextToVideoNodeV2_2(PikaNodeBase):\n    \"\"\"Pika 2.2 Text to Video Node.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                **cls.get_base_inputs_types(PikaBodyGenerate22T2vGenerate22T2vPost),\n                \"aspect_ratio\": model_field_to_node_input(\n                    IO.FLOAT,\n                    PikaBodyGenerate22T2vGenerate22T2vPost,\n                    \"aspectRatio\",\n                    step=0.001,\n                    min=0.4,\n                    max=2.5,\n                    default=1.7777777777777777,\n                ),\n            },\n            \"hidden\": {\n                \"auth_token\": \"AUTH_TOKEN_COMFY_ORG\",\n            },\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    DESCRIPTION = \"Sends a text prompt to the Pika API v2.2 to generate a video.\"\n\n    def api_call(\n        self,\n        prompt_text: str,\n        negative_prompt: str,\n        seed: int,\n        resolution: str,\n        duration: int,\n        aspect_ratio: float,\n        auth_token: Optional[str] = None,\n    ) -> tuple[VideoFromFile]:\n        \"\"\"API call for Pika 2.2 Text to Video.\"\"\"\n        initial_operation = SynchronousOperation(\n            endpoint=ApiEndpoint(\n                path=PATH_TEXT_TO_VIDEO,\n                method=HttpMethod.POST,\n                request_model=PikaBodyGenerate22T2vGenerate22T2vPost,\n                response_model=PikaGenerateResponse,\n            ),\n            request=PikaBodyGenerate22T2vGenerate22T2vPost(\n                promptText=prompt_text,\n                negativePrompt=negative_prompt,\n                seed=seed,\n                resolution=resolution,\n                duration=duration,\n                aspectRatio=aspect_ratio,\n            ),\n            auth_token=auth_token,\n            content_type=\"application/x-www-form-urlencoded\",\n        )\n\n        return self.execute_task(initial_operation, auth_token)\n\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/built-in-nodes/api-node/video/pixverse/pixverse-template",
  "markdown": "# Pixverse Template - ComfyUI åŽŸç”ŸèŠ‚ç‚¹æ–‡æ¡£\n\n![ComfyUI åŽŸç”Ÿ Pixverse Template èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg) Pixverse Template èŠ‚ç‚¹å…è®¸ä½ ä»Žé¢„å®šä¹‰çš„è§†é¢‘ç”Ÿæˆæ¨¡æ¿ä¸­é€‰æ‹©ï¼Œç”¨äºŽæŽ§åˆ¶Pixverseè§†é¢‘ç”ŸæˆèŠ‚ç‚¹çš„è¾“å‡ºé£Žæ ¼å’Œæ•ˆæžœã€‚è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©èŠ‚ç‚¹ï¼Œå¯ä»¥è¿žæŽ¥åˆ°Pixverseçš„è§†é¢‘ç”ŸæˆèŠ‚ç‚¹ï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿå¿«é€Ÿåº”ç”¨é¢„è®¾çš„è§†é¢‘é£Žæ ¼ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´å¤æ‚çš„å‚æ•°ç»„åˆã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| template | é€‰æ‹©é¡¹ | ä»Žå¯ç”¨çš„é¢„è®¾è§†é¢‘ç”Ÿæˆæ¨¡æ¿åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæ¨¡æ¿ |\n\n### è¾“å‡º\n\n| è¾“å‡º  | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| pixverse\\_template | PixverseIO.TEMPLATE | åŒ…å«æ‰€é€‰æ¨¡æ¿IDçš„é…ç½®å¯¹è±¡ |\n\n## æºç å‚è€ƒ\n\n\\[èŠ‚ç‚¹æºç  (æ›´æ–°äºŽ2025-05-05)\\]\n\n```\n\nclass PixverseTemplateNode:\n    \"\"\"\n    Select template for Pixverse Video generation.\n    \"\"\"\n\n    RETURN_TYPES = (PixverseIO.TEMPLATE,)\n    RETURN_NAMES = (\"pixverse_template\",)\n    FUNCTION = \"create_template\"\n    CATEGORY = \"api node/video/Pixverse\"\n\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\n            \"required\": {\n                \"template\": (list(pixverse_templates.keys()), ),\n            }\n        }\n\n    def create_template(self, template: str):\n        template_id = pixverse_templates.get(template, None)\n        if template_id is None:\n            raise Exception(f\"Template '{template}' is not recognized.\")\n        # just return the integer\n        return (template_id,)\n\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/about",
  "markdown": "# å…³äºŽé¡µé¢ - ComfyUI\n\nAbout é¡µé¢æ˜¯ ComfyUI è®¾ç½®ç³»ç»Ÿä¸­çš„ä¸€ä¸ªä¿¡æ¯å±•ç¤ºé¢æ¿ï¼Œç”¨äºŽæ˜¾ç¤ºåº”ç”¨ç¨‹åºç‰ˆæœ¬ä¿¡æ¯ã€ç›¸å…³é“¾æŽ¥å’Œç³»ç»Ÿç»Ÿè®¡æ•°æ®ï¼Œè¿™äº›è®¾ç½®åœ¨å‘æˆ‘ä»¬æäº¤åé¦ˆé—®é¢˜æ—¶ï¼Œå¯ä»¥æä¾›ç»™æˆ‘ä»¬ä¸€äº›éžå¸¸å…³é”®çš„ä¿¡æ¯ã€‚ ![about](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/settings-about.jpg)\n\n### ç‰ˆæœ¬ä¿¡æ¯å¾½ç« \n\nAbout é¡µé¢æ˜¾ç¤ºä»¥ä¸‹æ ¸å¿ƒç‰ˆæœ¬ä¿¡æ¯ï¼š\n\n*   **ComfyUI ç‰ˆæœ¬**ï¼šæ˜¾ç¤ºåŽç«¯ ComfyUI çš„ç‰ˆæœ¬å·ï¼Œé“¾æŽ¥åˆ°å®˜æ–¹ GitHub ä»“åº“\n*   **ComfyUI\\_frontend ç‰ˆæœ¬**ï¼šæ˜¾ç¤ºå‰ç«¯ç•Œé¢çš„ç‰ˆæœ¬å·ï¼Œé“¾æŽ¥åˆ°å‰ç«¯ GitHub ä»“åº“\n*   **Discord ç¤¾åŒº**ï¼šæä¾› ComfyOrg Discord æœåŠ¡å™¨çš„é“¾æŽ¥\n*   **å®˜æ–¹ç½‘ç«™**ï¼šé“¾æŽ¥åˆ° ComfyOrg å®˜æ–¹ç½‘ç«™\n\n### è‡ªå®šä¹‰èŠ‚ç‚¹å¾½ç« \n\nå¦‚æžœå®‰è£…äº†è‡ªå®šä¹‰èŠ‚ç‚¹ï¼ŒAbout é¡µé¢è¿˜ä¼šæ˜¾ç¤ºè‡ªå®šä¹‰èŠ‚ç‚¹æä¾›çš„é¢å¤–å¾½ç« ä¿¡æ¯ã€‚è¿™äº›å¾½ç« ç”±å„ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹é€šè¿‡ `aboutPageBadges` å±žæ€§æ³¨å†Œã€‚\n\n### ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯\n\né¡µé¢åº•éƒ¨æ˜¾ç¤ºè¯¦ç»†çš„ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š\n\n*   ç¡¬ä»¶é…ç½®ä¿¡æ¯\n*   è½¯ä»¶çŽ¯å¢ƒä¿¡æ¯\n*   ç³»ç»Ÿæ€§èƒ½æ•°æ®\n\n## æ‰©å±•å¼€å‘è€…æŒ‡å—\n\næ‰©å±•å¼€å‘è€…å¯ä»¥é€šè¿‡åœ¨æ‰©å±•é…ç½®ä¸­æ·»åŠ  `aboutPageBadges` å±žæ€§æ¥å‘ About é¡µé¢æ·»åŠ è‡ªå®šä¹‰å¾½ç« ï¼š\n\n```\napp.registerExtension({\n  name: 'MyExtension',\n  aboutPageBadges: [\n    {\n      label: 'My Extension v1.0.0',\n      url: 'https://github.com/myuser/myextension',\n      icon: 'pi pi-github'\n    }\n  ]\n})\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/overview",
  "markdown": "# æ¦‚è¿° - ComfyUI\n\nComfyUI æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ GenAI æŽ¨ç†å¼•æ“Žï¼Œå¯ç”¨äºŽæœ¬åœ°è¿è¡Œ AI æ¨¡åž‹ã€åˆ›å»ºå·¥ä½œæµã€å¼€å‘è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œä»¥åŠéƒ¨ç½²ä¸ºæœåŠ¡å™¨ã€‚ ComfyUI çš„ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š\n\n*   **[åˆ›å»ºå·¥ä½œæµ](https://docs.comfy.org/zh-CN/development/core-concepts/workflow)**ï¼šå·¥ä½œæµæ˜¯ä¸€ç§ç¼–æŽ’ AI æ¨¡åž‹å’Œè‡ªåŠ¨åŒ–ä»»åŠ¡çš„æ–¹å¼ã€‚å®ƒä»¬æ˜¯ä¸€ç³»åˆ—ç›¸äº’è¿žæŽ¥å½¢æˆç®¡é“çš„èŠ‚ç‚¹ã€‚\n*   **[è‡ªå®šä¹‰èŠ‚ç‚¹](https://docs.comfy.org/zh-CN/development/core-concepts/custom-nodes)**ï¼šä»»ä½•äººéƒ½å¯ä»¥ç¼–å†™è‡ªå®šä¹‰èŠ‚ç‚¹æ¥æ‰©å±• ComfyUI çš„åŠŸèƒ½ã€‚èŠ‚ç‚¹ä½¿ç”¨ Python ç¼–å†™ï¼Œå¹¶ç”±ç¤¾åŒºå‘å¸ƒã€‚\n*   **æ‰©å±•**ï¼šæ‰©å±•æ˜¯æ”¹è¿› ComfyUI ç”¨æˆ·ç•Œé¢çš„ç¬¬ä¸‰æ–¹åº”ç”¨ç¨‹åºã€‚\n*   **[éƒ¨ç½²](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_overview)**ï¼šComfyUI å¯ä»¥åœ¨æ‚¨è‡ªå·±çš„çŽ¯å¢ƒä¸­éƒ¨ç½²ä¸º API ç«¯ç‚¹ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/comms_messages",
  "markdown": "# æ¶ˆæ¯ä¼ é€’ - ComfyUI\n\n## æ¶ˆæ¯ä¼ é€’æœºåˆ¶\n\nåœ¨å·¥ä½œæµæ‰§è¡ŒæœŸé—´ï¼ˆæˆ–å½“æ‰§è¡Œé˜Ÿåˆ—çŠ¶æ€å‘ç”Ÿå˜åŒ–æ—¶ï¼‰ï¼Œ`PromptExecutor` ä¼š é€šè¿‡ `PromptServer` å®žä¾‹çš„ `send_sync` æ–¹æ³•å‘å®¢æˆ·ç«¯å›žä¼ æ¶ˆæ¯ã€‚ è¿™äº›æ¶ˆæ¯ç”± `api.js` æ–‡ä»¶ä¸­å®šä¹‰çš„ `socket` äº‹ä»¶ç›‘å¬å™¨è´Ÿè´£æŽ¥æ”¶ï¼ˆæˆªè‡³æœ¬æ–‡æ’°å†™æ—¶ï¼Œè¯¥ç›‘å¬å™¨å¤§è‡´ä½äºŽç¬¬ 90 è¡Œï¼Œæ‚¨ä¹Ÿå¯ä»¥é€šè¿‡æœç´¢ `this.socket.addEventListener` æ‰¾åˆ°å®ƒï¼‰ã€‚ è¯¥ç›‘å¬å™¨ä¼šä¸ºæ¯ç§å·²çŸ¥çš„æ¶ˆæ¯ç±»åž‹åˆ›å»ºä¸€ä¸ª `CustomEvent` å¯¹è±¡ï¼Œå¹¶å°†å…¶æ´¾å‘ç»™æ‰€æœ‰å·²æ³¨å†Œçš„ç›¸åº”ç›‘å¬å™¨ã€‚ æ‰©å±•ç¨‹åºå¯ä»¥éµå¾ªæ ‡å‡†çš„ Javascript æ¨¡å¼æ¥æ³¨å†Œäº‹ä»¶æŽ¥æ”¶ï¼ˆæ­¤æ“ä½œé€šå¸¸åœ¨ `setup()` å‡½æ•°ä¸­å®Œæˆï¼‰ï¼š\n\n```\napi.addEventListener(message_type, messageHandler);\n```\n\nå¦‚æžœ `message_type` å¹¶éžå†…ç½®æ¶ˆæ¯ç±»åž‹ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°†å…¶æ·»åŠ è‡³å·²çŸ¥æ¶ˆæ¯ç±»åž‹åˆ—è¡¨ã€‚ æ³¨å†Œçš„ `messageHandler` å‡½æ•°åœ¨è¢«è°ƒç”¨æ—¶ï¼Œä¼šæŽ¥æ”¶åˆ°ä¸€ä¸ª `CustomEvent` å¯¹è±¡ã€‚ è¯¥å¯¹è±¡æ˜¯å¯¹ `socket` äº‹ä»¶çš„æ‰©å±•ï¼Œé¢å¤–å¢žåŠ äº†ä¸€ä¸ª `.detail` å±žæ€§ï¼Œæ­¤å±žæ€§æ˜¯ä¸€ä¸ªåŒ…å«äº†æœåŠ¡å™¨æ‰€å‘é€æ•°æ®çš„å­—å…¸ã€‚å› æ­¤ï¼Œé€šå¸¸çš„ä½¿ç”¨æ–¹å¼å¦‚ä¸‹ï¼š\n\n```\nfunction messageHandler(event) {\n    if (event.detail.node == aNodeIdThatIsInteresting) { // åˆ¤æ–­æ˜¯å¦ä¸ºç›®æ ‡èŠ‚ç‚¹\n        // åˆ©ç”¨ event.detail.other_things ä¸­çš„æ•°æ®æ‰§è¡Œç›¸åº”æ“ä½œ\n    }\n}\n```\n\n### å†…ç½®æ¶ˆæ¯ç±»åž‹\n\nåœ¨å·¥ä½œæµæ‰§è¡ŒæœŸé—´ï¼ˆæˆ–å½“æ‰§è¡Œé˜Ÿåˆ—çŠ¶æ€å‘ç”Ÿå˜åŒ–æ—¶ï¼‰ï¼Œ`PromptExecutor` ä¼šé€šè¿‡ `PromptServer` å®žä¾‹çš„ `send_sync` æ–¹æ³•å‘å®¢æˆ·ç«¯å‘é€ä»¥ä¸‹ç±»åž‹çš„æ¶ˆæ¯ã€‚ æ‰©å±•ç¨‹åºå¯ä»¥æ³¨å†Œç›‘å¬è¿™äº›æ¶ˆæ¯ä¸­çš„ä»»æ„ä¸€ç§ã€‚\n\n| äº‹ä»¶ç±»åž‹ (event) | è§¦å‘æ—¶æœº | æ•°æ®å†…å®¹ (data) |\n| --- | --- | --- |\n| `execution_start` | å½“ä¸€ä¸ªæç¤º (prompt) å³å°†å¼€å§‹æ‰§è¡Œæ—¶ | `prompt_id` (æç¤ºID) |\n| `execution_error` | å½“æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯æ—¶ | `prompt_id` (æç¤ºID)ï¼Œä»¥åŠå…¶ä»–é™„åŠ é”™è¯¯ä¿¡æ¯ |\n| `execution_interrupted` | å½“æŸä¸ªèŠ‚ç‚¹æŠ›å‡º `InterruptProcessingException` å¼‚å¸¸å¯¼è‡´æ‰§è¡Œä¸­æ–­æ—¶ | `prompt_id` (æç¤ºID)ã€`node_id` (èŠ‚ç‚¹ID)ã€`node_type` (èŠ‚ç‚¹ç±»åž‹) ä»¥åŠ `executed` (ä¸€ä¸ªåŒ…å«å·²æ‰§è¡ŒèŠ‚ç‚¹IDçš„åˆ—è¡¨) |\n| `execution_cached` | åœ¨æ‰§è¡Œå¼€å§‹é˜¶æ®µ | `prompt_id` (æç¤ºID)ã€`nodes` (ä¸€ä¸ªèŠ‚ç‚¹IDåˆ—è¡¨ï¼Œè¿™äº›èŠ‚ç‚¹çš„ç¼“å­˜è¾“å‡ºå°†è¢«å¤ç”¨ï¼Œå› æ­¤è¿™äº›èŠ‚ç‚¹ä¼šè¢«è·³è¿‡æ‰§è¡Œ) |\n| `execution_success` | å½“æç¤ºä¸­çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½å·²æˆåŠŸæ‰§è¡Œæ—¶ | `prompt_id`, `timestamp`(æ—¶é—´æˆ³) |\n| `executing` | å½“ä¸€ä¸ªæ–°èŠ‚ç‚¹å³å°†å¼€å§‹æ‰§è¡Œæ—¶ | `node` (å½“å‰æ‰§è¡Œçš„èŠ‚ç‚¹IDï¼Œè‹¥ä¸º `None` åˆ™è¡¨ç¤ºæ•´ä¸ªæç¤ºæ‰§è¡Œå®Œæ¯•)ã€`prompt_id` (æç¤ºID) |\n| `executed` | å½“ä¸€ä¸ªèŠ‚ç‚¹æ‰§è¡Œå®Œæ¯•å¹¶è¿”å›žäº†ç”¨æˆ·ç•Œé¢ (UI) å…ƒç´ æ—¶ | `node` (èŠ‚ç‚¹ID)ã€`prompt_id` (æç¤ºID)ã€`output` (èŠ‚ç‚¹è¿”å›žçš„UIæ•°æ®) |\n| `progress` | åœ¨æ‰§è¡ŒæŸä¸ªå®žçŽ°äº†ç‰¹å®šè¿›åº¦æŠ¥å‘Šé’©å­ (hook) çš„èŠ‚ç‚¹æœŸé—´ | `node` (èŠ‚ç‚¹ID)ã€`prompt_id` (æç¤ºID)ã€`value` (å½“å‰è¿›åº¦å€¼)ã€`max` (æœ€å¤§è¿›åº¦å€¼) |\n| `status` | å½“æ‰§è¡Œé˜Ÿåˆ—çš„çŠ¶æ€å‘ç”Ÿå˜åŒ–æ—¶ | `exec_info` (ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å« `queue_remaining`ï¼Œè¡¨ç¤ºé˜Ÿåˆ—ä¸­å‰©ä½™çš„ä»»åŠ¡æ•°é‡) |\n\n### å…³äºŽ `executed` æ¶ˆæ¯çš„ä½¿ç”¨\n\nå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ`executed` æ¶ˆæ¯å¹¶éžåœ¨æ¯ä¸ªèŠ‚ç‚¹å®Œæˆæ‰§è¡Œæ—¶éƒ½ä¼šå‘é€ï¼ˆè¿™ä¸€ç‚¹ä¸Ž `executing` æ¶ˆæ¯ä¸åŒï¼‰ï¼Œ å®ƒä»…åœ¨èŠ‚ç‚¹æ‰§è¡ŒåŽéœ€è¦æ›´æ–°ç”¨æˆ·ç•Œé¢æ—¶æ‰ä¼šè§¦å‘ã€‚ è¦å®žçŽ°è¿™ä¸€ç‚¹ï¼ŒèŠ‚ç‚¹çš„Pythonä¸»æ‰§è¡Œå‡½æ•°éœ€è¦è¿”å›žä¸€ä¸ªå­—å…¸ï¼Œè€Œéžé€šå¸¸çš„å…ƒç»„ï¼š\n\n```\n# åœ¨ä¸»æ‰§è¡Œå‡½æ•°çš„æœ«å°¾\n        return { \"ui\": a_new_dictionary, \"result\": the_tuple_of_output_values }\n```\n\nè¿™æ ·ï¼Œ`a_new_dictionary` çš„å†…å®¹ä¾¿ä¼šä½œä¸º `executed` æ¶ˆæ¯ä¸­ `output` å­—æ®µçš„å€¼å‘é€ç»™å®¢æˆ·ç«¯ã€‚ å¦‚æžœèŠ‚ç‚¹æœ¬èº«æ²¡æœ‰è¾“å‡ºï¼ˆå³ä¸äº§ç”Ÿä¼ é€’ç»™ä¸‹æ¸¸èŠ‚ç‚¹çš„æ•°æ®ï¼‰ï¼Œé‚£ä¹ˆè¿”å›žå­—å…¸ä¸­çš„ `result` é”®å¯ä»¥çœç•¥ï¼ˆä¾‹å¦‚ï¼Œå¯ä»¥å‚è€ƒ `nodes.py` æ–‡ä»¶ä¸­ `SaveImage` èŠ‚ç‚¹çš„å®žçŽ°æ–¹å¼ï¼‰ã€‚\n\n### è‡ªå®šä¹‰æ¶ˆæ¯ç±»åž‹\n\nå¦‚å‰æ‰€è¿°ï¼Œåœ¨å®¢æˆ·ç«¯ï¼Œåªéœ€ä¸ºè‡ªå®šä¹‰çš„æ¶ˆæ¯ç±»åž‹åç§°æ³¨å†Œä¸€ä¸ªç›‘å¬å™¨ï¼Œå³å¯è½»æ¾æ·»åŠ å¯¹æ–°æ¶ˆæ¯ç±»åž‹çš„å¤„ç†ã€‚\n\n```\napi.addEventListener(\"my.custom.message\", messageHandler);\n```\n\nåœ¨æœåŠ¡å™¨ç«¯ï¼Œå®žçŽ°æ–¹å¼åŒæ ·ç®€æ´ï¼š\n\n```\nfrom server import PromptServer\n# ç„¶åŽï¼Œï¼ˆé€šå¸¸ï¼‰åœ¨æ‚¨çš„èŠ‚ç‚¹ä¸»æ‰§è¡Œå‡½æ•°ä¸­\n        PromptServer.instance.send_sync(\"my.custom.message\", a_dictionary)\n```\n\n#### èŽ·å–å½“å‰èŠ‚ç‚¹ ID (node\\_id)\n\nå¤§å¤šæ•°å†…ç½®æ¶ˆæ¯çš„ `node` å­—æ®µéƒ½åŒ…å«äº†å½“å‰æ­£åœ¨æ‰§è¡Œçš„èŠ‚ç‚¹ IDã€‚åœ¨è‡ªå®šä¹‰æ¶ˆæ¯ä¸­ï¼Œæ‚¨å¾ˆå¯èƒ½ä¹Ÿéœ€è¦åŒ…å«æ­¤ä¿¡æ¯ã€‚ åœ¨æœåŠ¡å™¨ç«¯ï¼Œå¯ä»¥é€šè¿‡ä¸€ä¸ªéšè—è¾“å…¥æ¥èŽ·å–èŠ‚ç‚¹ IDã€‚è¿™éœ€è¦åœ¨èŠ‚ç‚¹çš„ `INPUT_TYPES` å­—å…¸ä¸­æ·»åŠ ä¸€ä¸ª `hidden` é”®æ¥å®žçŽ°ï¼š\n\n```\n    @classmethod    \n    def INPUT_TYPES(s):\n        return {\"required\" : { }, # æ­¤å¤„å¡«å†™æ‚¨èŠ‚ç‚¹æ‰€éœ€çš„å¸¸è§„è¾“å…¥\n                \"hidden\": { \"node_id\": \"UNIQUE_ID\" } } # æ·»åŠ æ­¤ hidden é”®ä»¥èŽ·å–èŠ‚ç‚¹ID\n\n    def my_main_function(self, required_inputs, node_id): # node_id ä¼šä½œä¸ºå‚æ•°ä¼ å…¥\n        # æ‰§è¡ŒæŸäº›æ“ä½œ...\n        PromptServer.instance.send_sync(\"my.custom.message\", {\"node\": node_id, \"other_things\": etc}) # åœ¨æ¶ˆæ¯ä¸­åŒ…å«èŠ‚ç‚¹ID\n```"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2Fapi-node%2Fvideo%2Fkwai_vgi%2Fkling-camera-control-i2v",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/comms_routes",
  "markdown": "# è·¯ç”± - ComfyUI\n\n## è·¯ç”±\n\næœåŠ¡å™¨å®šä¹‰äº†ä¸€ç³»åˆ— `get` å’Œ `post` æ–¹æ³•ï¼Œ è¿™äº›æ–¹æ³•å¯ä»¥é€šè¿‡åœ¨ `server.py` ä¸­æœç´¢ `@routes` æ‰¾åˆ°ã€‚å½“ä½ åœ¨ç½‘é¡µå®¢æˆ·ç«¯æäº¤å·¥ä½œæµæ—¶ï¼Œ å®ƒä¼šè¢«å‘é€åˆ° `/prompt` ç«¯ç‚¹ï¼Œè¯¥ç«¯ç‚¹ä¼šéªŒè¯æç¤ºå¹¶å°†å…¶æ·»åŠ åˆ°æ‰§è¡Œé˜Ÿåˆ—ä¸­ï¼Œ è¿”å›ž `prompt_id` å’Œ `number`ï¼ˆé˜Ÿåˆ—ä¸­çš„ä½ç½®ï¼‰ï¼Œå¦‚æžœéªŒè¯å¤±è´¥åˆ™è¿”å›ž `error` å’Œ `node_errors`ã€‚ æç¤ºé˜Ÿåˆ—å®šä¹‰åœ¨ `execution.py` ä¸­ï¼Œè¯¥æ–‡ä»¶è¿˜å®šä¹‰äº† `PromptExecutor` ç±»ã€‚\n\n### å†…ç½®è·¯ç”±\n\n`server.py` å®šä¹‰äº†ä»¥ä¸‹è·¯ç”±ï¼š\n\n| è·¯å¾„  | get/post | ç”¨é€”  |\n| --- | --- | --- |\n| `/` | get | åŠ è½½ Comfy ç½‘é¡µ |\n| `/embeddings` | get | èŽ·å–å¯ç”¨çš„åµŒå…¥æ¨¡åž‹åç§°åˆ—è¡¨ |\n| `/extensions` | get | èŽ·å–æ³¨å†Œäº† `WEB_DIRECTORY` çš„æ‰©å±•åˆ—è¡¨ |\n| `/workflow_templates` | get | èŽ·å–è‡ªå®šä¹‰èŠ‚ç‚¹æ¨¡å—åŠå…¶å…³è”æ¨¡æ¿å·¥ä½œæµçš„æ˜ å°„ |\n| `/upload/image` | post | ä¸Šä¼ å›¾ç‰‡ |\n| `/upload/mask` | post | ä¸Šä¼ è’™ç‰ˆ |\n| `/view` | get | æŸ¥çœ‹å›¾ç‰‡ã€‚æ›´å¤šé€‰é¡¹è¯·å‚è§ `server.py` ä¸­çš„ `@routes.get(\"/view\")` |\n| `/view_metadata`/ | get | èŽ·å–æ¨¡åž‹çš„å…ƒæ•°æ® |\n| `/system_stats` | get | èŽ·å–ç³»ç»Ÿä¿¡æ¯ï¼ˆPython ç‰ˆæœ¬ã€è®¾å¤‡ã€æ˜¾å­˜ç­‰ï¼‰ |\n| `/prompt` | get | èŽ·å–å½“å‰çŠ¶æ€ |\n| `/prompt` | post | æäº¤æç¤ºåˆ°é˜Ÿåˆ— |\n| `/object_info` | get | èŽ·å–æ‰€æœ‰èŠ‚ç‚¹ç±»åž‹çš„è¯¦ç»†ä¿¡æ¯ |\n| `/object_info/{node_class}` | get | èŽ·å–ç‰¹å®šèŠ‚ç‚¹ç±»åž‹çš„è¯¦ç»†ä¿¡æ¯ |\n| `/history` | get | èŽ·å–é˜Ÿåˆ—åŽ†å²è®°å½• |\n| `/history/{prompt_id}` | get | èŽ·å–ç‰¹å®šæç¤ºçš„é˜Ÿåˆ—åŽ†å²è®°å½• |\n| `/history` | post | æ¸…é™¤åŽ†å²è®°å½•æˆ–åˆ é™¤åŽ†å²è®°å½•é¡¹ |\n| `/queue` | get | èŽ·å–é˜Ÿåˆ—çŠ¶æ€ |\n| `/interrupt` | post | åœæ­¢å½“å‰å·¥ä½œæµ |\n| `/free` | post | é€šè¿‡å¸è½½æŒ‡å®šæ¨¡åž‹é‡Šæ”¾å†…å­˜ |\n\n### è‡ªå®šä¹‰è·¯ç”±\n\nå¦‚æžœä½ æƒ³åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ä»Žå®¢æˆ·ç«¯å‘æœåŠ¡å™¨å‘é€æ¶ˆæ¯ï¼Œä½ éœ€è¦åœ¨æœåŠ¡å™¨ä¸­æ·»åŠ ä¸€ä¸ªè‡ªå®šä¹‰è·¯ç”±ã€‚ å¯¹äºŽå¤æ‚çš„æƒ…å†µï¼Œä½ éœ€è¦æ·±å…¥ç ”ç©¶ [aiohttp æ¡†æž¶æ–‡æ¡£](https://docs.aiohttp.org/)ï¼Œä½†å¤§å¤šæ•°æƒ…å†µå¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼å¤„ç†ï¼š\n\n```\nfrom server import PromptServer\nfrom aiohttp import web\nroutes = PromptServer.instance.routes\n@routes.post('/my_new_path')\nasync def my_function(request):\n    the_data = await request.post()\n    # the_data now holds a dictionary of the values sent\n    MyClass.handle_my_message(the_data)\n    return web.json_response({})\n```\n\nå®¢æˆ·ç«¯å¯ä»¥é€šè¿‡å‘é€ `FormData` å¯¹è±¡æ¥ä½¿ç”¨è¿™ä¸ªæ–°è·¯ç”±ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼Œ è¿™å°†å¯¼è‡´ä¸Šé¢ä»£ç ä¸­çš„ `the_data` åŒ…å« `message` å’Œ `node_id` é”®ï¼š\n\n```\nimport { api } from \"../../scripts/api.js\";\nfunction send_message(node_id, message) {\n    const body = new FormData();\n    body.append('message',message);\n    body.append('node_id', node_id);\n    api.fetchApi(\"/my_new_path\", { method: \"POST\", body, });\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/api-key-integration",
  "markdown": "# é€šè¿‡ API Key é›†æˆæ¥ä½¿ç”¨ ComfyUI API èŠ‚ç‚¹\n\nä»Ž[PR #8041](https://github.com/comfyanonymous/ComfyUI/pull/8041)å¼€å§‹ï¼ŒComfyUI æ”¯æŒé€šè¿‡åˆ›å»º API Key æ¥ç›´æŽ¥ä½¿ç”¨ ComfyUI å†…ç½®çš„ API èŠ‚ç‚¹ï¼Œæ— éœ€ç‰¹å®šçš„å‰ç«¯ç•Œé¢ï¼ˆç”šè‡³å¯ä»¥å®Œå…¨ä¸ä½¿ç”¨å‰ç«¯ï¼‰ã€‚ è¿™æ„å‘³ç€ä½ å¯ä»¥åˆ›å»ºå·¥ä½œæµæ¥ç»„åˆï¼š\n\n*   æœ¬åœ°æ“ä½œç³»ç»Ÿæ¨¡åž‹\n*   è‡ªå®šä¹‰èŠ‚ç‚¹ç¤¾åŒºçš„å·¥å…·\n*   æµè¡Œçš„ä»˜è´¹æ¨¡åž‹\n\nå¹¶é€šè¿‡æœ¬åœ° Comfy webserver API ä¸€èµ·è¿è¡Œæ‰€æœ‰å†…å®¹ï¼Œè®©å®ƒå¤„ç†æ‰€æœ‰çš„åè°ƒå·¥ä½œã€‚ è¿™å¯¹äºŽå°† Comfy ç”¨ä½œåŽç«¯æœåŠ¡ã€é€šè¿‡å‘½ä»¤è¡Œè¿è¡Œ Comfyã€æ‹¥æœ‰è‡ªå·±çš„å‰ç«¯ç­‰ç”¨æˆ·éƒ½å¾ˆæœ‰å¸®åŠ©ã€‚\n\n## å‰ææ¡ä»¶\n\nä½¿ç”¨ API Key æ¥è°ƒç”¨ ComfyUI å†…ç½®çš„ API èŠ‚ç‚¹éœ€è¦ï¼š\n\n*   ç¡®ä¿ä½ çš„ ComfyUI ç‰ˆæœ¬ >= [PR #8041](https://github.com/comfyanonymous/ComfyUI/pull/8041)\n*   å¯¹åº”è´¦æˆ·çš„ API Key\n*   è¶³å¤Ÿçš„è´¦æˆ·ç§¯åˆ†\n\nä½¿ç”¨ API Key æ¥è°ƒç”¨ ComfyUI å†…ç½®çš„ API èŠ‚ç‚¹éœ€è¦å…ˆåœ¨ [ComfyUI Platform](https://platform.comfy.org/login) ä¸Šæ³¨å†Œä¸€ä¸ªè´¦æˆ·ï¼Œç„¶åŽåˆ›å»º API key\n\n[\n\nè¯·å‚è€ƒç”¨æˆ·ç•Œé¢ç« èŠ‚äº†è§£å¦‚ä½•ä½¿ç”¨ API Key è¿›è¡Œç™»å½•\n\n\n\n](https://docs.comfy.org/zh-CN/interface/user#%E4%BD%BF%E7%94%A8-api-key-%E8%BF%9B%E8%A1%8C%E7%99%BB%E5%BD%95)\n\nä½ éœ€è¦ç¡®ä¿ä½ çš„ ComfyUI è´¦æˆ·æœ‰è¶³å¤Ÿçš„ç§¯åˆ†æ¥æµ‹è¯•å¯¹åº”çš„åŠŸèƒ½ã€‚\n\n[](https://docs.comfy.org/zh-CN/interface/credits)\n\n## Python ç¤ºä¾‹\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªå¦‚ä½•é€šè¿‡ Python ä»£ç å‘ ComfyUI API å‘é€åŒ…å« APIèŠ‚ç‚¹çš„å·¥ä½œæµçš„ç¤ºä¾‹ï¼š\n\n```\n\"\"\"åœ¨æ— å¤´æ¨¡å¼æˆ–ä½¿ç”¨æ›¿ä»£å‰ç«¯è¿è¡Œ ComfyUI æ—¶ä½¿ç”¨ API èŠ‚ç‚¹\n\nä½ å¯ä»¥é€šè¿‡åœ¨ prompt ä¸­åŒ…å« API key æ¥æ‰§è¡ŒåŒ…å« API èŠ‚ç‚¹çš„ ComfyUI å·¥ä½œæµã€‚\nAPI key éœ€è¦æ·»åŠ åˆ° payload çš„ `extra_data` å­—æ®µä¸­ã€‚\nä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªå¦‚ä½•å®žçŽ°çš„ç¤ºä¾‹ã€‚\n\næ›´å¤šä¿¡æ¯è¯·å‚è€ƒï¼š\n\n- API èŠ‚ç‚¹æ¦‚è¿°: https://docs.comfy.org/zh-CN/tutorials/api-nodes/overview\n- è¦ç”Ÿæˆ API keyï¼Œè¯·ç™»å½•è¿™é‡Œ: https://platform.comfy.org/login\n\"\"\"\n\nimport json\nfrom urllib import request\n\nSERVER_URL = \"http://127.0.0.1:8188\"\n\n# æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å« API èŠ‚ç‚¹çš„ prompt/jobï¼ˆAPI æ ¼å¼çš„å·¥ä½œæµï¼‰ã€‚\nworkflow_with_api_nodes = \"\"\"{\n  \"11\": {\n    \"inputs\": {\n      \"prompt\": \"A dreamy, surreal half-body portrait of a young woman meditating. She has a short, straight bob haircut dyed in pastel pink, with soft bangs covering her forehead. Her eyes are gently closed, and her hands are raised in a calm, open-palmed meditative pose, fingers slightly curved, as if levitating or in deep concentration. She wears a colorful dress made of patchwork-like pastel tiles, featuring clouds, stars, and rainbows. Around her float translucent, iridescent soap bubbles reflecting the rainbow hues. The background is a fantastical sky filled with cotton-candy clouds and vivid rainbow waves, giving the entire scene a magical, dreamlike atmosphere. Emphasis on youthful serenity, whimsical ambiance, and vibrant soft lighting.\",\n      \"prompt_upsampling\": false,\n      \"seed\": 589991183902375,\n      \"aspect_ratio\": \"1:1\",\n      \"raw\": false,\n      \"image_prompt_strength\": 0.4000000000000001,\n      \"image_prompt\": [\n        \"14\",\n        0\n      ]\n    },\n    \"class_type\": \"FluxProUltraImageNode\",\n    \"_meta\": {\n      \"title\": \"Flux 1.1 [pro] Ultra Image\"\n    }\n  },\n  \"12\": {\n    \"inputs\": {\n      \"filename_prefix\": \"ComfyUI\",\n      \"images\": [\n        \"11\",\n        0\n      ]\n    },\n    \"class_type\": \"SaveImage\",\n    \"_meta\": {\n      \"title\": \"Save Image\"\n    }\n  },\n  \"14\": {\n    \"inputs\": {\n      \"image\": \"example.png\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"Load Image\"\n    }\n  }\n}\"\"\"\n\n\nprompt = json.loads(workflow_with_api_nodes)\npayload = {\n    \"prompt\": prompt,\n    # å°† `api_key_comfy_org` æ·»åŠ åˆ° payload ä¸­ã€‚\n    # å¦‚æžœä½ éœ€è¦å¤„ç†å¤šä¸ªå®¢æˆ·ç«¯ï¼Œå¯ä»¥å…ˆä»Žå…³è”çš„ç”¨æˆ·èŽ·å– keyã€‚\n    \"extra_data\": {\n        \"api_key_comfy_org\": \"comfyui-87d01e28d*******************************************************\"  # æ›¿æ¢ä¸ºå®žé™…çš„ key\n    },\n}\ndata = json.dumps(payload).encode(\"utf-8\")\nreq = request.Request(f\"{SERVER_URL}/prompt\", data=data)\n\n# å‘é€è¯·æ±‚\nrequest.urlopen(req)\n\n```\n\n## ç›¸å…³æ–‡æ¡£\n\n*   [APIèŠ‚ç‚¹æ¦‚è¿°](https://docs.comfy.org/zh-CN/tutorials/api-nodes/overview)\n*   [è´¦æˆ·ç®¡ç†](https://docs.comfy.org/zh-CN/interface/user)\n*   [ç§¯åˆ†](https://docs.comfy.org/zh-CN/interface/credits)"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/overview",
  "markdown": "# æ¦‚è¿° - ComfyUI\n\nè‡ªå®šä¹‰èŠ‚ç‚¹å…è®¸ä½ å®žçŽ°æ–°åŠŸèƒ½å¹¶ä¸Žæ›´å¹¿æ³›çš„ç¤¾åŒºåˆ†äº«ã€‚ è‡ªå®šä¹‰èŠ‚ç‚¹å°±åƒä»»ä½• Comfy èŠ‚ç‚¹ä¸€æ ·ï¼šå®ƒæŽ¥æ”¶è¾“å…¥ï¼Œå¯¹å…¶è¿›è¡Œå¤„ç†ï¼Œç„¶åŽäº§ç”Ÿè¾“å‡ºã€‚ è™½ç„¶æœ‰äº›è‡ªå®šä¹‰èŠ‚ç‚¹æ‰§è¡Œéžå¸¸å¤æ‚çš„ä»»åŠ¡ï¼Œä½†è®¸å¤šèŠ‚ç‚¹åªåšä¸€ä»¶äº‹ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•èŠ‚ç‚¹çš„ä¾‹å­ï¼Œå®ƒæŽ¥æ”¶ä¸€å¼ å›¾ç‰‡å¹¶è¿›è¡Œåè‰²å¤„ç†ã€‚ ![å”¯ä¸€å›¾ç‰‡èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/invert_image_node.png)\n\n## å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¨¡åž‹\n\nComfy è¿è¡Œåœ¨å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¨¡åž‹ä¸‹ã€‚æœåŠ¡å™¨ç«¯ç”± Python ç¼–å†™ï¼Œè´Ÿè´£æ‰€æœ‰å®žé™…å·¥ä½œï¼šæ•°æ®å¤„ç†ã€æ¨¡åž‹ã€å›¾åƒæ‰©æ•£ç­‰ã€‚å®¢æˆ·ç«¯ç”± Javascript ç¼–å†™ï¼Œè´Ÿè´£ç”¨æˆ·ç•Œé¢ã€‚ Comfy ä¹Ÿå¯ä»¥ä»¥ API æ¨¡å¼ä½¿ç”¨ï¼Œåœ¨è¯¥æ¨¡å¼ä¸‹ï¼Œå·¥ä½œæµç”±éž Comfy å®¢æˆ·ç«¯ï¼ˆå¦‚å…¶ä»– UI æˆ–å‘½ä»¤è¡Œè„šæœ¬ï¼‰å‘é€åˆ°æœåŠ¡å™¨ã€‚ è‡ªå®šä¹‰èŠ‚ç‚¹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å››ç±»ï¼š\n\n### ä»…æœåŠ¡å™¨ç«¯\n\nå¤§å¤šæ•°è‡ªå®šä¹‰èŠ‚ç‚¹ä»…åœ¨æœåŠ¡å™¨ç«¯è¿è¡Œï¼Œé€šè¿‡å®šä¹‰ä¸€ä¸ª Python ç±»æ¥æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºç±»åž‹ï¼Œå¹¶æä¾›ä¸€ä¸ªå¯è°ƒç”¨çš„å‡½æ•°æ¥å¤„ç†è¾“å…¥å¹¶ç”Ÿæˆè¾“å‡ºã€‚\n\n### ä»…å®¢æˆ·ç«¯\n\nå°‘æ•°è‡ªå®šä¹‰èŠ‚ç‚¹ä»…å¯¹å®¢æˆ·ç«¯ UI è¿›è¡Œä¿®æ”¹ï¼Œä½†ä¸æ·»åŠ æ ¸å¿ƒåŠŸèƒ½ã€‚å°½ç®¡åå­—å¦‚æ­¤ï¼Œå®ƒä»¬ç”šè‡³å¯èƒ½ä¸ä¼šå‘ç³»ç»Ÿæ·»åŠ æ–°èŠ‚ç‚¹ã€‚\n\n### å®¢æˆ·ç«¯ä¸ŽæœåŠ¡å™¨ç«¯ç‹¬ç«‹\n\nè‡ªå®šä¹‰èŠ‚ç‚¹å¯ä»¥åŒæ—¶æä¾›é¢å¤–çš„æœåŠ¡å™¨åŠŸèƒ½å’Œé¢å¤–ï¼ˆç›¸å…³çš„ï¼‰UI åŠŸèƒ½ï¼ˆä¾‹å¦‚ç”¨äºŽæ–°æ•°æ®ç±»åž‹çš„æ–°å°éƒ¨ä»¶ï¼‰ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´çš„é€šä¿¡å¯ä»¥é€šè¿‡ Comfy çš„æ•°æ®æµæŽ§åˆ¶æ¥å¤„ç†ã€‚\n\n### å®¢æˆ·ç«¯ä¸ŽæœåŠ¡å™¨ç«¯è”åŠ¨\n\nåœ¨å°‘æ•°æƒ…å†µä¸‹ï¼ŒUI åŠŸèƒ½å’ŒæœåŠ¡å™¨éœ€è¦ç›´æŽ¥ç›¸äº’é€šä¿¡ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/walkthrough",
  "markdown": "# å¿«é€Ÿå…¥é—¨ - ComfyUI\n\næœ¬é¡µå°†å¸¦ä½ ä¸€æ­¥æ­¥å®Œæˆè‡ªå®šä¹‰èŠ‚ç‚¹çš„åˆ›å»ºè¿‡ç¨‹ã€‚ æˆ‘ä»¬çš„ç¤ºä¾‹å°†æŽ¥æ”¶ä¸€æ‰¹å›¾ç‰‡ï¼Œå¹¶è¿”å›žå…¶ä¸­ä¸€å¼ å›¾ç‰‡ã€‚æœ€åˆï¼Œè¿™ä¸ªèŠ‚ç‚¹ä¼šè¿”å›žå¹³å‡é¢œè‰²æœ€äº®çš„å›¾ç‰‡ï¼›éšåŽæˆ‘ä»¬ä¼šæ‰©å±•å®ƒï¼Œæ”¯æŒå¤šç§é€‰æ‹©æ ‡å‡†ï¼Œæœ€åŽè¿˜ä¼šæ·»åŠ ä¸€äº›å®¢æˆ·ç«¯ä»£ç ã€‚ æœ¬é¡µå‡è®¾ä½ å¯¹ Python æˆ– Javascript çš„äº†è§£å¾ˆå°‘ã€‚ å®Œæˆæœ¬æ•™ç¨‹åŽï¼Œå¯ä»¥æ·±å…¥äº†è§£ [åŽç«¯ä»£ç ](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview) å’Œ [å‰ç«¯ä»£ç ](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview)ã€‚\n\n## ç¼–å†™åŸºç¡€èŠ‚ç‚¹\n\n### å‰ç½®æ¡ä»¶\n\n*   ä¸€ä¸ªå¯ç”¨çš„ ComfyUI [å®‰è£…çŽ¯å¢ƒ](https://docs.comfy.org/zh-CN/installation/manual_install)ã€‚å¼€å‘å»ºè®®æ‰‹åŠ¨å®‰è£… ComfyUIã€‚\n*   ä¸€ä¸ªå¯ç”¨çš„ comfy-cli [å®‰è£…çŽ¯å¢ƒ](https://docs.comfy.org/zh-CN/comfy-cli/getting-started)ã€‚\n\n### çŽ¯å¢ƒæ­å»º\n\n```\ncd ComfyUI/custom_nodes\ncomfy node scaffold\n```\n\nå›žç­”å‡ ä¸ªé—®é¢˜åŽï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæ–°çš„ç›®å½•ã€‚\n\n```\n ~  % comfy node scaffold\nYou've downloaded .cookiecutters/cookiecutter-comfy-extension before. Is it okay to delete and re-download it? [y/n] (y): y\n  [1/9] full_name (): Comfy\n  [2/9] email (you@gmail.com): me@comfy.org\n  [3/9] github_username (your_github_username): comfy\n  [4/9] project_name (My Custom Nodepack): FirstComfyNode\n  [5/9] project_slug (firstcomfynode): \n  [6/9] project_short_description (A collection of custom nodes for ComfyUI): \n  [7/9] version (0.0.1): \n  [8/9] Select open_source_license\n    1 - GNU General Public License v3\n    2 - MIT license\n    3 - BSD license\n    4 - ISC license\n    5 - Apache Software License 2.0\n    6 - Not open source\n    Choose from [1/2/3/4/5/6] (1): 1\n  [9/9] include_web_directory_for_custom_javascript [y/n] (n): y\nInitialized empty Git repository in firstcomfynode/.git/\nâœ“ Custom node project created successfully!\n```\n\n### å®šä¹‰èŠ‚ç‚¹\n\nå°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ° `src/nodes.py` æœ«å°¾ï¼š\n\n```\nclass ImageSelector:\n    CATEGORY = \"example\"\n    @classmethod    \n    def INPUT_TYPES(s):\n        return { \"required\":  { \"images\": (\"IMAGE\",), } }\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"choose_image\"\n```\n\nè‡ªå®šä¹‰èŠ‚ç‚¹é€šè¿‡ Python ç±»å®šä¹‰ï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹å››é¡¹ï¼š`CATEGORY`ï¼ˆæŒ‡å®šæ–°èŠ‚ç‚¹åœ¨æ·»åŠ èŠ‚ç‚¹èœå•ä¸­çš„ä½ç½®ï¼‰ã€`INPUT_TYPES`ï¼ˆç±»æ–¹æ³•ï¼Œå®šä¹‰èŠ‚ç‚¹è¾“å…¥ï¼Œè¯¦è§[åŽæ–‡](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#input-types)ï¼‰ã€`RETURN_TYPES`ï¼ˆå®šä¹‰èŠ‚ç‚¹è¾“å‡ºï¼‰ã€`FUNCTION`ï¼ˆèŠ‚ç‚¹æ‰§è¡Œæ—¶è°ƒç”¨çš„å‡½æ•°åï¼‰ã€‚\n\n### ä¸»å‡½æ•°\n\nä¸»å‡½æ•° `choose_image` ä¼šæ”¶åˆ°åœ¨ `INPUT_TYPES` ä¸­å®šä¹‰çš„å‘½åå‚æ•°ï¼Œå¹¶è¿”å›žä¸€ä¸ªä¸Ž `RETURN_TYPES` åŒ¹é…çš„ `tuple`ã€‚ç”±äºŽæˆ‘ä»¬å¤„ç†çš„æ˜¯å›¾ç‰‡ï¼Œå›¾ç‰‡åœ¨å†…éƒ¨ä»¥ `torch.Tensor` å­˜å‚¨ï¼Œ\n\nç„¶åŽå°†å‡½æ•°æ·»åŠ åˆ°ä½ çš„ç±»ä¸­ã€‚å›¾ç‰‡çš„æ•°æ®ç±»åž‹æ˜¯å½¢çŠ¶ä¸º `[B,H,W,C]` çš„ `torch.Tensor`ï¼Œå…¶ä¸­ `B` æ˜¯æ‰¹é‡å¤§å°ï¼Œ`C` æ˜¯é€šé“æ•°ï¼ˆRGB ä¸º 3ï¼‰ã€‚éåŽ†è¯¥å¼ é‡ä¼šå¾—åˆ° `B` ä¸ªå½¢çŠ¶ä¸º `[H,W,C]` çš„å¼ é‡ã€‚`.flatten()` æ–¹æ³•å°†å…¶å˜ä¸ºä¸€ç»´å¼ é‡ï¼Œé•¿åº¦ä¸º `H*W*C`ï¼Œ`torch.mean()` æ±‚å‡å€¼ï¼Œ`.item()` å°†å•å€¼å¼ é‡è½¬ä¸º Python æµ®ç‚¹æ•°ã€‚\n\n```\ndef choose_image(self, images):\n    brightness = list(torch.mean(image.flatten()).item() for image in images)\n    brightest = brightness.index(max(brightness))\n    result = images[brightest].unsqueeze(0)\n    return (result,)\n```\n\næœ€åŽä¸¤è¡Œè¯´æ˜Žï¼š\n\n*   `images[brightest]` è¿”å›žå½¢çŠ¶ä¸º `[H,W,C]` çš„å¼ é‡ã€‚`unsqueeze` ç”¨äºŽåœ¨ç¬¬ 0 ç»´æ’å…¥ä¸€ä¸ªé•¿åº¦ä¸º 1 çš„ç»´åº¦ï¼Œå¾—åˆ° `[B,H,W,C]`ï¼Œå…¶ä¸­ `B=1`ï¼Œå³å•å¼ å›¾ç‰‡ã€‚\n*   `return (result,)` æœ«å°¾çš„é€—å·å¾ˆé‡è¦ï¼Œç¡®ä¿è¿”å›žçš„æ˜¯å…ƒç»„ã€‚\n\n### æ³¨å†ŒèŠ‚ç‚¹\n\nè¦è®© Comfy è¯†åˆ«æ–°èŠ‚ç‚¹ï¼Œå¿…é¡»åœ¨åŒ…çº§åˆ«å¯ç”¨ã€‚ä¿®æ”¹ `src/nodes.py` æœ«å°¾çš„ `NODE_CLASS_MAPPINGS` å˜é‡ã€‚ä½ éœ€è¦é‡å¯ ComfyUI æ‰èƒ½çœ‹åˆ°æ›´æ”¹ã€‚\n\n```\n\nNODE_CLASS_MAPPINGS = {\n    \"Example\" : Example,\n    \"Image Selector\" : ImageSelector,\n}\n\n# å¯é€‰ï¼šä½ å¯ä»¥åœ¨ `NODE_DISPLAY_NAME_MAPPINGS` å­—å…¸ä¸­é‡å‘½åèŠ‚ç‚¹ã€‚\nNODE_DISPLAY_NAME_MAPPINGS = {\n    \"Example\": \"Example Node\",\n    \"Image Selector\": \"Image Selector\",\n}\n```\n\n## æ·»åŠ é€‰é¡¹\n\nè¿™ä¸ªèŠ‚ç‚¹å¯èƒ½æœ‰ç‚¹æ— èŠï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åŠ ä¸€äº›é€‰é¡¹ï¼›æ¯”å¦‚ä¸€ä¸ªå°éƒ¨ä»¶ï¼Œè®©ä½ é€‰æ‹©æœ€äº®ã€æœ€çº¢ã€æœ€ç»¿æˆ–æœ€è“çš„å›¾ç‰‡ã€‚å°†ä½ çš„ `INPUT_TYPES` ä¿®æ”¹ä¸ºï¼š\n\n```\n@classmethod    \ndef INPUT_TYPES(s):\n    return { \"required\":  { \"images\": (\"IMAGE\",), \n                            \"mode\": ([\"brightest\", \"reddest\", \"greenest\", \"bluest\"],)} }\n```\n\nç„¶åŽæ›´æ–°ä¸»å‡½æ•°ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªå¾ˆç®€å•çš„â€œæœ€çº¢â€å®šä¹‰ï¼Œå³åƒç´ çš„å¹³å‡ R å€¼é™¤ä»¥ä¸‰è‰²å¹³å‡å€¼ã€‚æ‰€ä»¥ï¼š\n\n```\ndef choose_image(self, images, mode):\n    batch_size = images.shape[0]\n    brightness = list(torch.mean(image.flatten()).item() for image in images)\n    if (mode==\"brightest\"):\n        scores = brightness\n    else:\n        channel = 0 if mode==\"reddest\" else (1 if mode==\"greenest\" else 2)\n        absolute = list(torch.mean(image[:,:,channel].flatten()).item() for image in images)\n        scores = list( absolute[i]/(brightness[i]+1e-8) for i in range(batch_size) )\n    best = scores.index(max(scores))\n    result = images[best].unsqueeze(0)\n    return (result,)\n```\n\n## è°ƒæ•´ UI\n\nä¹Ÿè®¸æˆ‘ä»¬æƒ³è¦ä¸€äº›å¯è§†åŒ–åé¦ˆï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å‘é€ä¸€æ¡æ–‡æœ¬æ¶ˆæ¯è¿›è¡Œæ˜¾ç¤ºã€‚\n\n### ä»ŽæœåŠ¡å™¨å‘é€æ¶ˆæ¯\n\nåªéœ€åœ¨ Python ä»£ç ä¸­æ·»åŠ ä¸¤è¡Œï¼š\n\n```\nfrom server import PromptServer\n```\n\nåœ¨ `choose_image` æ–¹æ³•æœ«å°¾æ·»åŠ ä¸€è¡Œï¼Œå°†æ¶ˆæ¯å‘é€åˆ°å‰ç«¯ï¼ˆ`send_sync` éœ€è¦ä¸€ä¸ªå”¯ä¸€çš„æ¶ˆæ¯ç±»åž‹å’Œä¸€ä¸ªå­—å…¸ï¼‰ï¼š\n\n```\nPromptServer.instance.send_sync(\"example.imageselector.textmessage\", {\"message\":f\"Picked image {best+1}\"})\nreturn (result,)\n```\n\n### ç¼–å†™å®¢æˆ·ç«¯æ‰©å±•\n\nè¦ä¸ºå®¢æˆ·ç«¯æ·»åŠ  Javascriptï¼Œåœ¨ä½ çš„è‡ªå®šä¹‰èŠ‚ç‚¹ç›®å½•ä¸‹åˆ›å»º `web/js` å­ç›®å½•ï¼Œå¹¶åœ¨ `__init__.py` æœ«å°¾å¯¼å‡º `WEB_DIRECTORY`ï¼š\n\n```\nWEB_DIRECTORY = \"./web/js\"\n__all__ = ['NODE_CLASS_MAPPINGS', 'WEB_DIRECTORY']\n```\n\nå®¢æˆ·ç«¯æ‰©å±•ä»¥ `.js` æ–‡ä»¶ä¿å­˜åœ¨ `web/js` å­ç›®å½•ä¸‹ï¼Œæ‰€ä»¥åˆ›å»º `image_selector/web/js/imageSelector.js`ï¼Œå†…å®¹å¦‚ä¸‹ã€‚ï¼ˆæ›´å¤šå†…å®¹è§ [å®¢æˆ·ç«¯å¼€å‘](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_overview)ï¼‰\n\n```\napp.registerExtension({\n\tname: \"example.imageselector\",\n    async setup() {\n        function messageHandler(event) { alert(event.detail.message); }\n        app.api.addEventListener(\"example.imageselector.textmessage\", messageHandler);\n    },\n})\n```\n\næˆ‘ä»¬æ‰€åšçš„å°±æ˜¯æ³¨å†Œä¸€ä¸ªæ‰©å±•ï¼Œå¹¶åœ¨ `setup()` æ–¹æ³•ä¸­ä¸ºæˆ‘ä»¬å‘é€çš„æ¶ˆæ¯ç±»åž‹æ·»åŠ ç›‘å¬å™¨ã€‚å®ƒä¼šè¯»å–æˆ‘ä»¬å‘é€çš„å­—å…¸ï¼ˆå­˜å‚¨åœ¨ `event.detail` ä¸­ï¼‰ã€‚ åœæ­¢ Comfy æœåŠ¡å™¨ï¼Œé‡æ–°å¯åŠ¨ï¼Œåˆ·æ–°ç½‘é¡µï¼Œè¿è¡Œä½ çš„å·¥ä½œæµã€‚\n\n### å®Œæ•´ç¤ºä¾‹\n\nå®Œæ•´ç¤ºä¾‹è§[è¿™é‡Œ](https://gist.github.com/robinjhuang/fbf54b7715091c7b478724fc4dffbd03)ã€‚ä½ å¯ä»¥ä¸‹è½½ç¤ºä¾‹å·¥ä½œæµ [JSON æ–‡ä»¶](https://github.com/Comfy-Org/docs/blob/main/public/workflow.json) æˆ–åœ¨ä¸‹æ–¹æŸ¥çœ‹ï¼š\n\n![Image Selector Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/firstnodeworkflow.png)"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/workflow_templates",
  "markdown": "# å·¥ä½œæµæ¨¡æ¿ - ComfyUI\n\nå¦‚æžœä½ çš„è‡ªå®šä¹‰èŠ‚ç‚¹åŒ…å«ç¤ºä¾‹å·¥ä½œæµæ–‡ä»¶ï¼ŒComfyUI å¯ä»¥åœ¨æ¨¡æ¿æµè§ˆå™¨ï¼ˆ`å·¥ä½œæµ`/`æµè§ˆæ¨¡æ¿`èœå•ï¼‰ä¸­å‘ç”¨æˆ·å±•ç¤ºè¿™äº›æ–‡ä»¶ã€‚å·¥ä½œæµæ¨¡æ¿æ˜¯å¸®åŠ©ç”¨æˆ·å¿«é€Ÿä¸Šæ‰‹ä½ çš„èŠ‚ç‚¹çš„å¥½æ–¹æ³•ã€‚ ä½œä¸ºèŠ‚ç‚¹å¼€å‘è€…ï¼Œä½ åªéœ€è¦åˆ›å»ºä¸€ä¸ª `example_workflows` æ–‡ä»¶å¤¹å¹¶åœ¨å…¶ä¸­æ”¾ç½® `json` æ–‡ä»¶å³å¯ã€‚ä½ è¿˜å¯ä»¥é€‰æ‹©æ€§åœ°æ”¾ç½®åŒåçš„ `jpg` æ–‡ä»¶ä½œä¸ºæ¨¡æ¿ç¼©ç•¥å›¾ã€‚ åœ¨åº•å±‚ï¼ŒComfyUI ä¼šé™æ€æä¾›è¿™äº›æ–‡ä»¶ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªç«¯ç‚¹ï¼ˆ`/api/workflow_templates`ï¼‰è¿”å›žå·¥ä½œæµæ¨¡æ¿é›†åˆã€‚\n\n## ç¤ºä¾‹\n\nåœ¨ `ComfyUI-MyCustomNodeModule/example_workflows/` ç›®å½•ä¸‹ï¼š\n\n*   `My_example_workflow_1.json`\n*   `My_example_workflow_1.jpg`\n*   `My_example_workflow_2.json`\n\nåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒComfyUI çš„æ¨¡æ¿æµè§ˆå™¨ä¼šæ˜¾ç¤ºä¸€ä¸ªåä¸º `ComfyUI-MyCustomNodeModule` çš„ç±»åˆ«ï¼Œå…¶ä¸­åŒ…å«ä¸¤ä¸ªé¡¹ç›®ï¼Œå…¶ä¸­ä¸€ä¸ªå¸¦æœ‰ç¼©ç•¥å›¾ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/help_page",
  "markdown": "# å¸®åŠ©é¡µé¢ - ComfyUI\n\n## ä½¿ç”¨ Markdown åˆ›å»ºèŠ‚ç‚¹æ–‡æ¡£\n\nè‡ªå®šä¹‰èŠ‚ç‚¹å¯ä»¥ä½¿ç”¨ Markdown æ¥åˆ›å»ºå¯Œæ–‡æœ¬æ–‡æ¡£ï¼Œè¿™äº›æ–‡æ¡£ä¿¡æ¯å°†åœ¨ UI ä¸­æ˜¾ç¤ºï¼Œå–ä»£å¸¸è§çš„èŠ‚ç‚¹æè¿°ä¿¡æ¯ã€‚å¯ä»¥ä¸ºç”¨æˆ·æä¾›å…³äºŽèŠ‚ç‚¹åŠŸèƒ½ã€å‚æ•°å’Œä½¿ç”¨ç¤ºä¾‹çš„è¯¦ç»†ä¿¡æ¯ã€‚\n\n## è®¾ç½®\n\nä¸ºä½ çš„èŠ‚ç‚¹æ·»åŠ èŠ‚ç‚¹æ–‡æ¡£ï¼š\n\n1.  åœ¨ä½ çš„ `WEB_DIRECTORY` ä¸­åˆ›å»º `docs` æ–‡ä»¶å¤¹\n2.  æ·»åŠ ä»¥èŠ‚ç‚¹åç§°å‘½åçš„ Markdown æ–‡ä»¶ï¼ˆæ‚¨çš„èŠ‚ç‚¹åç§°æ˜¯ç”¨äºŽæ³¨å†ŒèŠ‚ç‚¹çš„ `NODE_CLASS_MAPPINGS` å­—å…¸ä¸­çš„å­—å…¸é”®ï¼‰ï¼š\n    *   `WEB_DIRECTORY/docs/NodeName.md` - é»˜è®¤æ–‡æ¡£\n    *   `WEB_DIRECTORY/docs/NodeName/en.md` - è‹±æ–‡æ–‡æ¡£\n    *   `WEB_DIRECTORY/docs/NodeName/zh.md` - ä¸­æ–‡æ–‡æ¡£\n    *   æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–è¯­è¨€ç‰ˆæœ¬ï¼ˆä¾‹å¦‚ `fr.md`ã€`de.md` ç­‰ï¼‰\n\nç³»ç»Ÿå°†æ ¹æ®ç”¨æˆ·çš„è¯­è¨€è®¾ç½®è‡ªåŠ¨åŠ è½½ç›¸åº”çš„æ–‡æ¡£ï¼Œå¦‚æžœæ²¡æœ‰æœ¬åœ°åŒ–ç‰ˆæœ¬ï¼Œåˆ™å›žé€€åˆ° `NodeName.md`ã€‚\n\n## æ”¯æŒçš„ Markdown åŠŸèƒ½\n\n*   æ ‡å‡† Markdown è¯­æ³•ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€ä»£ç å—ç­‰ï¼‰\n*   ä½¿ç”¨ Markdown è¯­æ³•çš„å›¾ç‰‡ï¼š`![æ›¿ä»£æ–‡æœ¬](image.png)`\n*   å…·æœ‰ç‰¹å®šå±žæ€§çš„ HTML åª’ä½“å…ƒç´ ï¼š\n    *   `<video>` å’Œ `<source>` æ ‡ç­¾\n    *   å…è®¸çš„å±žæ€§ï¼š`controls`ã€`autoplay`ã€`loop`ã€`muted`ã€`preload`ã€`poster`\n\n## ç¤ºä¾‹ç»“æž„\n\n```\nmy-custom-node/\nâ”œâ”€â”€ __init__.py\nâ”œâ”€â”€ web/              # WEB_DIRECTORY\nâ”‚   â”œâ”€â”€ js/\nâ”‚   â”‚   â””â”€â”€ my-node.js\nâ”‚   â””â”€â”€ docs/\nâ”‚       â”œâ”€â”€ MyNode.md           # é»˜è®¤æ–‡æ¡£\nâ”‚       â””â”€â”€ MyNode/\nâ”‚           â”œâ”€â”€ en.md           # è‹±æ–‡ç‰ˆæœ¬\nâ”‚           â””â”€â”€ zh.md           # ä¸­æ–‡ç‰ˆæœ¬\n```\n\n## ç¤ºä¾‹ Markdown æ–‡ä»¶\n\n```\n# æˆ‘çš„è‡ªå®šä¹‰èŠ‚ç‚¹\n\næ­¤èŠ‚ç‚¹ä½¿ç”¨é«˜çº§ç®—æ³•å¤„ç†å›¾åƒã€‚\n\n## å‚æ•°\n\n- **image**: è¦å¤„ç†çš„è¾“å…¥å›¾åƒ\n- **strength**: å¤„ç†å¼ºåº¦ (0.0 - 1.0)\n\n## ç”¨æ³•\n\n![ä½¿ç”¨ç¤ºä¾‹](example.png)\n\n<video controls loop muted>\n  <source src=\"demo.mp4\" type=\"video/mp4\">\n</video>\n```\n\nåœ¨æ­¤é¡µé¢\n\n*   [ä½¿ç”¨ Markdown åˆ›å»ºèŠ‚ç‚¹æ–‡æ¡£](#%E4%BD%BF%E7%94%A8-markdown-%E5%88%9B%E5%BB%BA%E8%8A%82%E7%82%B9%E6%96%87%E6%A1%A3)\n*   [è®¾ç½®](#%E8%AE%BE%E7%BD%AE)\n*   [æ”¯æŒçš„ Markdown åŠŸèƒ½](#%E6%94%AF%E6%8C%81%E7%9A%84-markdown-%E5%8A%9F%E8%83%BD)\n*   [ç¤ºä¾‹ç»“æž„](#%E7%A4%BA%E4%BE%8B%E7%BB%93%E6%9E%84)\n*   [ç¤ºä¾‹ Markdown æ–‡ä»¶](#%E7%A4%BA%E4%BE%8B-markdown-%E6%96%87%E4%BB%B6)"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/comms_overview",
  "markdown": "# æœåŠ¡å™¨æ¦‚è§ˆ - ComfyUI\n\n## æ¦‚è§ˆ\n\nComfy æœåŠ¡å™¨æž„å»ºäºŽ [aiohttp æ¡†æž¶](https://docs.aiohttp.org/) åŸºç¡€ä¹‹ä¸Šï¼Œè¯¥æ¡†æž¶åˆ™ä¾èµ–äºŽ [asyncio](https://pypi.org/project/asyncio/) åº“ã€‚ æœåŠ¡å™¨å‘å®¢æˆ·ç«¯å‘é€æ¶ˆæ¯æ—¶ï¼Œä¼šé€šè¿‡å…¶ `send_sync` æ–¹æ³•ï¼ˆè¯¥æœåŠ¡å™¨æ˜¯ `server.py` æ–‡ä»¶ä¸­å®šä¹‰çš„ `PromptServer` ç±»çš„ä¸€ä¸ªå®žä¾‹ï¼‰ä»¥ `socket` æ¶ˆæ¯çš„å½¢å¼è¿›è¡Œã€‚è¿™äº›æ¶ˆæ¯ç”±æ³¨å†Œåœ¨ `api.js` æ–‡ä»¶ä¸­çš„ `socket` äº‹ä»¶ç›‘å¬å™¨è´Ÿè´£å¤„ç†ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚é˜…[æ¶ˆæ¯ä¼ é€’](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_messages)ã€‚ å®¢æˆ·ç«¯å‘æœåŠ¡å™¨å‘é€æ¶ˆæ¯æ—¶ï¼Œåˆ™é€šè¿‡ `api.js` æ–‡ä»¶ä¸­å®šä¹‰çš„ `api.fetchApi()` æ–¹æ³•è¿›è¡Œï¼Œè¿™äº›è¯·æ±‚ç”±æœåŠ¡å™¨ç«¯è®¾å®šçš„ HTTP è·¯ç”±è´Ÿè´£å¤„ç†ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚é˜…[è·¯ç”±æœºåˆ¶](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_routes)éƒ¨åˆ†ã€‚ python3 .github/scripts/validate-links.py\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/publishing",
  "markdown": "# å‘å¸ƒèŠ‚ç‚¹ - ComfyUI\n\n## è®¾ç½®æ³¨å†Œè¡¨è´¦æˆ·\n\næŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è®¾ç½®æ³¨å†Œè¡¨è´¦æˆ·å¹¶å‘å¸ƒæ‚¨çš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ã€‚\n\n### è§‚çœ‹æ•™ç¨‹\n\n### åˆ›å»ºå‘å¸ƒè€…\n\nå‘å¸ƒè€…æ˜¯ä¸€ä¸ªå¯ä»¥å‘æ³¨å†Œè¡¨(registry)å‘å¸ƒè‡ªå®šä¹‰èŠ‚ç‚¹çš„èº«ä»½ã€‚æ¯ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹éƒ½éœ€è¦åœ¨ pyproject.toml æ–‡ä»¶ ä¸­åŒ…å«å‘å¸ƒè€…æ ‡è¯†ç¬¦ã€‚ è®¿é—® [Comfy Registry](https://registry.comfy.org/)ï¼Œåˆ›å»ºä¸€ä¸ªå‘å¸ƒè€…è´¦æˆ·ã€‚æ‚¨çš„å‘å¸ƒè€… ID æ˜¯å…¨çƒå”¯ä¸€çš„ï¼Œå¹¶ä¸”ä¹‹åŽä¸èƒ½æ›´æ”¹ï¼Œå› ä¸ºå®ƒç”¨äºŽæ‚¨çš„è‡ªå®šä¹‰èŠ‚ç‚¹çš„ URL ä¸­ã€‚ æ‚¨çš„å‘å¸ƒè€… ID å¯ä»¥åœ¨ä¸ªäººèµ„æ–™é¡µé¢ä¸Š `@` ç¬¦å·åŽé¢æ‰¾åˆ°ã€‚ ![Hero Dark](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/publisherid.png)\n\n### åˆ›å»ºç”¨äºŽå‘å¸ƒçš„ API å¯†é’¥\n\nè®¿é—®[è¿™é‡Œ](https://registry.comfy.org/nodes)å¹¶ç‚¹å‡»ä½ æƒ³è¦ä¸ºå…¶åˆ›å»º API å¯†é’¥çš„å‘å¸ƒè€…ã€‚è¿™å°†ç”¨äºŽé€šè¿‡ CLI å‘å¸ƒè‡ªå®šä¹‰èŠ‚ç‚¹ã€‚ ![ä¸ºç‰¹å®šå‘å¸ƒè€…åˆ›å»ºå¯†é’¥](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-1.png) ä¸º API å¯†é’¥å‘½åå¹¶å°†å…¶å®‰å…¨ä¿å­˜ã€‚å¦‚æžœå¯†é’¥ä¸¢å¤±äº†å®ƒï¼Œè¯·é‡æ–°åˆ›å»ºä¸€ä¸ªæ–°çš„å¯†é’¥ã€‚ ![åˆ›å»º API å¯†é’¥](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-2.png)\n\n### æ·»åŠ å…ƒæ•°æ®\n\nè¿™ä¸ªå‘½ä»¤å°†ä¼šç”Ÿæˆä¸‹é¢è¿™æ ·çš„å…ƒæ•°æ®ï¼š\n\n```\n# pyproject.toml\n[project]\nname = \"\" # Unique identifier for your node. Immutable after creation.\ndescription = \"\"\nversion = \"1.0.0\" # Custom Node version. Must be semantically versioned.\nlicense = { file = \"LICENSE.txt\" }\ndependencies  = [] # Filled in from requirements.txt\n\n[project.urls]\nRepository = \"https://github.com/...\"\n\n[tool.comfy]\nPublisherId = \"\" # TODO (fill in Publisher ID from Comfy Registry Website).\nDisplayName = \"\" # Display name for the Custom Node. Can be changed later.\nIcon = \"https://example.com/icon.png\" # SVG, PNG, JPG or GIF (MAX. 800x400px)\n```\n\nå°†æ­¤æ–‡ä»¶æ·»åŠ åˆ°æ‚¨çš„ä»“åº“ä¸­ã€‚æŸ¥çœ‹[è§„èŒƒ](https://docs.comfy.org/zh-CN/registry/specifications)ä»¥èŽ·å–æœ‰å…³ pyproject.toml æ–‡ä»¶çš„æ›´å¤šä¿¡æ¯ã€‚\n\n## å‘å¸ƒåˆ°æ³¨å†Œè¡¨(registry)\n\n### é€‰é¡¹ 1: Comfy CLI\n\nè¿è¡Œä¸‹é¢çš„å‘½ä»¤æ‰‹åŠ¨å°†æ‚¨çš„èŠ‚ç‚¹å‘å¸ƒåˆ°æ³¨å†Œè¡¨ã€‚\n\nä¼šè¢«æç¤ºè¦æ±‚è¾“å…¥ API å¯†é’¥ã€‚\n\n```\nAPI Key for publisher '<publisher id>': ****************************************************\n\n...Version 1.0.0 Published. \nSee it here: https://registry.comfy.org/publisherId/your-node\n```\n\n### é€‰é¡¹ 2: Github Actions\n\né€šè¿‡ Github Actions è‡ªåŠ¨å‘å¸ƒæ‚¨çš„èŠ‚ç‚¹ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/overview",
  "markdown": "# æ¦‚è¿° - ComfyUI\n\n## ç®€ä»‹\n\næ³¨å†Œè¡¨ï¼ˆRegistryï¼‰æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹çš„å…¬å…±é›†åˆã€‚å¼€å‘è€…å¯ä»¥å‘å¸ƒã€ç‰ˆæœ¬æŽ§åˆ¶ã€å¼ƒç”¨å’Œè·Ÿè¸ªä¸Žå…¶è‡ªå®šä¹‰èŠ‚ç‚¹ç›¸å…³çš„æŒ‡æ ‡ã€‚ComfyUI ç”¨æˆ·å¯ä»¥ä»Žæ³¨å†Œè¡¨ä¸­å‘çŽ°ã€å®‰è£…å’Œè¯„ä»·è‡ªå®šä¹‰èŠ‚ç‚¹ã€‚\n\n## ä¸ºä»€ä¹ˆä½¿ç”¨ Registryï¼Ÿ\n\næ³¨å†Œè¡¨é€šè¿‡æ ‡å‡†åŒ–è‡ªå®šä¹‰èŠ‚ç‚¹çš„å¼€å‘æ¥å¸®åŠ©ç¤¾åŒºï¼š Â  **èŠ‚ç‚¹ç‰ˆæœ¬æŽ§åˆ¶ï¼š** å¼€å‘è€…ç»å¸¸å‘å¸ƒå…¶è‡ªå®šä¹‰èŠ‚ç‚¹çš„æ–°ç‰ˆæœ¬ï¼Œè¿™å¾€å¾€ä¼šç ´åä¾èµ–å®ƒä»¬çš„å·¥ä½œæµã€‚é€šè¿‡ä½¿ç”¨[è¯­ä¹‰åŒ–ç‰ˆæœ¬æŽ§åˆ¶](https://semver.org/)ï¼Œç”¨æˆ·çŽ°åœ¨å¯ä»¥é€‰æ‹©å®‰å…¨åœ°å‡çº§ã€å¼ƒç”¨æˆ–é”å®šå…¶èŠ‚ç‚¹ç‰ˆæœ¬ï¼Œæå‰äº†è§£å…¶æ“ä½œå°†å¦‚ä½•å½±å“å…¶å·¥ä½œæµã€‚å·¥ä½œæµ JSON å°†å­˜å‚¨æ‰€ä½¿ç”¨çš„èŠ‚ç‚¹ç‰ˆæœ¬ï¼Œå› æ­¤æ‚¨å¯ä»¥å§‹ç»ˆå¯é åœ°é‡çŽ°æ‚¨çš„å·¥ä½œæµã€‚ Â  **èŠ‚ç‚¹å®‰å…¨æ€§ï¼š** æ³¨å†Œè¡¨å°†ä½œä¸º [ComfyUI-manager](https://github.com/comfy-org/ComfyUI-Manager) çš„åŽç«¯ã€‚æ‰€æœ‰èŠ‚ç‚¹éƒ½å°†è¢«æ‰«ææ˜¯å¦å­˜åœ¨æ¶æ„è¡Œä¸ºï¼Œå¦‚è‡ªå®šä¹‰ pip åŒ…ã€ä»»æ„ç³»ç»Ÿè°ƒç”¨ç­‰ã€‚é€šè¿‡è¿™äº›æ£€æŸ¥çš„èŠ‚ç‚¹å°†åœ¨ UI-manager ä¸Šå…¶åç§°æ—è¾¹æ˜¾ç¤ºéªŒè¯æ ‡å¿—ï¼ˆï¼‰ã€‚æœ‰å…³å®‰å…¨æ ‡å‡†åˆ—è¡¨ï¼Œè¯·å‚é˜…[æ ‡å‡†](https://docs.comfy.org/zh-CN/registry/standards)ã€‚ Â  **æœç´¢ï¼š** åœ¨ Registry ä¸Šæœç´¢æ‰€æœ‰èŠ‚ç‚¹ï¼Œä¸ºæ‚¨çš„å·¥ä½œæµæ‰¾åˆ°çŽ°æœ‰èŠ‚ç‚¹ã€‚\n\n## å‘å¸ƒèŠ‚ç‚¹\n\næŒ‰ç…§[æ•™ç¨‹](https://docs.comfy.org/zh-CN/registry/publishing)å¼€å§‹å‘å¸ƒæ‚¨çš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ã€‚\n\n## å¸¸è§é—®é¢˜"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/tips",
  "markdown": "# Tips - ComfyUI\n\n### \n\n[â€‹](#recommended-development-lifecycle)\n\nRecommended Development Lifecycle"
},
{
  "url": "https://docs.comfy.org/zh-CN/specs/workflow_json",
  "markdown": "# å·¥ä½œæµ JSON - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyWorkflow1_0\",\n  \"definitions\": {\n    \"ComfyWorkflow1_0\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"version\": {\n          \"type\": \"number\",\n          \"const\": 1\n        },\n        \"config\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"links_ontop\": {\n                      \"type\": \"boolean\"\n                    },\n                    \"align_to_grid\": {\n                      \"type\": \"boolean\"\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"state\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"lastGroupid\": {\n              \"type\": \"number\"\n            },\n            \"lastNodeId\": {\n              \"type\": \"number\"\n            },\n            \"lastLinkId\": {\n              \"type\": \"number\"\n            },\n            \"lastRerouteId\": {\n              \"type\": \"number\"\n            }\n          },\n          \"additionalProperties\": true\n        },\n        \"groups\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"title\": {\n                \"type\": \"string\"\n              },\n              \"bounding\": {\n                \"type\": \"array\",\n                \"minItems\": 4,\n                \"maxItems\": 4,\n                \"items\": [\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"font_size\": {\n                \"type\": \"number\"\n              },\n              \"locked\": {\n                \"type\": \"boolean\"\n              }\n            },\n            \"required\": [\n              \"title\",\n              \"bounding\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"nodes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"size\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"flags\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"collapsed\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"pinned\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"allow_interaction\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"horizontal\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"skip_repeated_outputs\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"order\": {\n                \"type\": \"number\"\n              },\n              \"mode\": {\n                \"type\": \"number\"\n              },\n              \"inputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"link\": {\n                      \"type\": [\n                        \"number\",\n                        \"null\"\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"outputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"links\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        {\n                          \"type\": \"null\"\n                        }\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"properties\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"Node name for S&R\": {\n                    \"type\": \"string\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"widgets_values\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\"\n                  },\n                  {\n                    \"type\": \"object\",\n                    \"additionalProperties\": {}\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"bgcolor\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"type\",\n              \"pos\",\n              \"size\",\n              \"flags\",\n              \"order\",\n              \"mode\",\n              \"properties\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"links\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"type\": \"number\"\n              },\n              \"origin_id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"origin_slot\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"target_id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"target_slot\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"string\"\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    }\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"parentId\": {\n                \"type\": \"number\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"origin_id\",\n              \"origin_slot\",\n              \"target_id\",\n              \"target_slot\",\n              \"type\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"reroutes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"type\": \"number\"\n              },\n              \"parentId\": {\n                \"type\": \"number\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"linkIds\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"number\"\n                    }\n                  },\n                  {\n                    \"type\": \"null\"\n                  }\n                ]\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"pos\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"extra\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"ds\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"scale\": {\n                          \"type\": \"number\"\n                        },\n                        \"offset\": {\n                          \"anyOf\": [\n                            {\n                              \"type\": \"object\",\n                              \"properties\": {\n                                \"0\": {\n                                  \"type\": \"number\"\n                                },\n                                \"1\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              \"required\": [\n                                \"0\",\n                                \"1\"\n                              ],\n                              \"additionalProperties\": true\n                            },\n                            {\n                              \"type\": \"array\",\n                              \"minItems\": 2,\n                              \"maxItems\": 2,\n                              \"items\": [\n                                {\n                                  \"type\": \"number\"\n                                },\n                                {\n                                  \"type\": \"number\"\n                                }\n                              ]\n                            }\n                          ]\n                        }\n                      },\n                      \"required\": [\n                        \"scale\",\n                        \"offset\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"info\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"name\": {\n                          \"type\": \"string\"\n                        },\n                        \"author\": {\n                          \"type\": \"string\"\n                        },\n                        \"description\": {\n                          \"type\": \"string\"\n                        },\n                        \"version\": {\n                          \"type\": \"string\"\n                        },\n                        \"created\": {\n                          \"type\": \"string\"\n                        },\n                        \"modified\": {\n                          \"type\": \"string\"\n                        },\n                        \"software\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"required\": [\n                        \"name\",\n                        \"author\",\n                        \"description\",\n                        \"version\",\n                        \"created\",\n                        \"modified\",\n                        \"software\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"linkExtensions\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"parentId\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    },\n                    \"reroutes\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          },\n                          \"pos\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"0\": {\n                                    \"type\": \"number\"\n                                  },\n                                  \"1\": {\n                                    \"type\": \"number\"\n                                  }\n                                },\n                                \"required\": [\n                                  \"0\",\n                                  \"1\"\n                                ],\n                                \"additionalProperties\": true\n                              },\n                              {\n                                \"type\": \"array\",\n                                \"minItems\": 2,\n                                \"maxItems\": 2,\n                                \"items\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"number\"\n                                  }\n                                ]\n                              }\n                            ]\n                          },\n                          \"linkIds\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              {\n                                \"type\": \"null\"\n                              }\n                            ]\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"pos\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"models\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"url\": {\n                \"type\": \"string\",\n                \"format\": \"uri\"\n              },\n              \"hash\": {\n                \"type\": \"string\"\n              },\n              \"hash_type\": {\n                \"type\": \"string\"\n              },\n              \"directory\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"name\",\n              \"url\",\n              \"directory\"\n            ],\n            \"additionalProperties\": false\n          }\n        }\n      },\n      \"required\": [\n        \"version\",\n        \"state\",\n        \"nodes\"\n      ],\n      \"additionalProperties\": true\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/specs/workflow_json_0.4",
  "markdown": "# å·¥ä½œæµ JSON 0.4 - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyWorkflow0_4\",\n  \"definitions\": {\n    \"ComfyWorkflow0_4\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"last_node_id\": {\n          \"anyOf\": [\n            {\n              \"type\": \"integer\"\n            },\n            {\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"last_link_id\": {\n          \"type\": \"number\"\n        },\n        \"nodes\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"id\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"pos\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"size\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"0\": {\n                        \"type\": \"number\"\n                      },\n                      \"1\": {\n                        \"type\": \"number\"\n                      }\n                    },\n                    \"required\": [\n                      \"0\",\n                      \"1\"\n                    ],\n                    \"additionalProperties\": true\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"number\"\n                      }\n                    ]\n                  }\n                ]\n              },\n              \"flags\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"collapsed\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"pinned\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"allow_interaction\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"horizontal\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"skip_repeated_outputs\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"order\": {\n                \"type\": \"number\"\n              },\n              \"mode\": {\n                \"type\": \"number\"\n              },\n              \"inputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"link\": {\n                      \"type\": [\n                        \"number\",\n                        \"null\"\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"outputs\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"name\": {\n                      \"type\": \"string\"\n                    },\n                    \"type\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"string\"\n                        },\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"string\"\n                          }\n                        },\n                        {\n                          \"type\": \"number\"\n                        }\n                      ]\n                    },\n                    \"links\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"array\",\n                          \"items\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        {\n                          \"type\": \"null\"\n                        }\n                      ]\n                    },\n                    \"slot_index\": {\n                      \"anyOf\": [\n                        {\n                          \"type\": \"integer\"\n                        },\n                        {\n                          \"type\": \"string\"\n                        }\n                      ]\n                    }\n                  },\n                  \"required\": [\n                    \"name\",\n                    \"type\"\n                  ],\n                  \"additionalProperties\": true\n                }\n              },\n              \"properties\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"Node name for S&R\": {\n                    \"type\": \"string\"\n                  }\n                },\n                \"additionalProperties\": true\n              },\n              \"widgets_values\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\"\n                  },\n                  {\n                    \"type\": \"object\",\n                    \"additionalProperties\": {}\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"bgcolor\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"id\",\n              \"type\",\n              \"pos\",\n              \"size\",\n              \"flags\",\n              \"order\",\n              \"mode\",\n              \"properties\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"links\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"array\",\n            \"minItems\": 6,\n            \"maxItems\": 6,\n            \"items\": [\n              {\n                \"type\": \"number\"\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"integer\"\n                  },\n                  {\n                    \"type\": \"string\"\n                  }\n                ]\n              },\n              {\n                \"anyOf\": [\n                  {\n                    \"type\": \"string\"\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": \"string\"\n                    }\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              }\n            ]\n          }\n        },\n        \"groups\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"title\": {\n                \"type\": \"string\"\n              },\n              \"bounding\": {\n                \"type\": \"array\",\n                \"minItems\": 4,\n                \"maxItems\": 4,\n                \"items\": [\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  },\n                  {\n                    \"type\": \"number\"\n                  }\n                ]\n              },\n              \"color\": {\n                \"type\": \"string\"\n              },\n              \"font_size\": {\n                \"type\": \"number\"\n              },\n              \"locked\": {\n                \"type\": \"boolean\"\n              }\n            },\n            \"required\": [\n              \"title\",\n              \"bounding\"\n            ],\n            \"additionalProperties\": true\n          }\n        },\n        \"config\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"links_ontop\": {\n                      \"type\": \"boolean\"\n                    },\n                    \"align_to_grid\": {\n                      \"type\": \"boolean\"\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"extra\": {\n          \"anyOf\": [\n            {\n              \"anyOf\": [\n                {\n                  \"not\": {}\n                },\n                {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"ds\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"scale\": {\n                          \"type\": \"number\"\n                        },\n                        \"offset\": {\n                          \"anyOf\": [\n                            {\n                              \"type\": \"object\",\n                              \"properties\": {\n                                \"0\": {\n                                  \"type\": \"number\"\n                                },\n                                \"1\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              \"required\": [\n                                \"0\",\n                                \"1\"\n                              ],\n                              \"additionalProperties\": true\n                            },\n                            {\n                              \"type\": \"array\",\n                              \"minItems\": 2,\n                              \"maxItems\": 2,\n                              \"items\": [\n                                {\n                                  \"type\": \"number\"\n                                },\n                                {\n                                  \"type\": \"number\"\n                                }\n                              ]\n                            }\n                          ]\n                        }\n                      },\n                      \"required\": [\n                        \"scale\",\n                        \"offset\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"info\": {\n                      \"type\": \"object\",\n                      \"properties\": {\n                        \"name\": {\n                          \"type\": \"string\"\n                        },\n                        \"author\": {\n                          \"type\": \"string\"\n                        },\n                        \"description\": {\n                          \"type\": \"string\"\n                        },\n                        \"version\": {\n                          \"type\": \"string\"\n                        },\n                        \"created\": {\n                          \"type\": \"string\"\n                        },\n                        \"modified\": {\n                          \"type\": \"string\"\n                        },\n                        \"software\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"required\": [\n                        \"name\",\n                        \"author\",\n                        \"description\",\n                        \"version\",\n                        \"created\",\n                        \"modified\",\n                        \"software\"\n                      ],\n                      \"additionalProperties\": true\n                    },\n                    \"linkExtensions\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"parentId\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    },\n                    \"reroutes\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                          \"id\": {\n                            \"type\": \"number\"\n                          },\n                          \"parentId\": {\n                            \"type\": \"number\"\n                          },\n                          \"pos\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"0\": {\n                                    \"type\": \"number\"\n                                  },\n                                  \"1\": {\n                                    \"type\": \"number\"\n                                  }\n                                },\n                                \"required\": [\n                                  \"0\",\n                                  \"1\"\n                                ],\n                                \"additionalProperties\": true\n                              },\n                              {\n                                \"type\": \"array\",\n                                \"minItems\": 2,\n                                \"maxItems\": 2,\n                                \"items\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"number\"\n                                  }\n                                ]\n                              }\n                            ]\n                          },\n                          \"linkIds\": {\n                            \"anyOf\": [\n                              {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": \"number\"\n                                }\n                              },\n                              {\n                                \"type\": \"null\"\n                              }\n                            ]\n                          }\n                        },\n                        \"required\": [\n                          \"id\",\n                          \"pos\"\n                        ],\n                        \"additionalProperties\": true\n                      }\n                    }\n                  },\n                  \"additionalProperties\": true\n                }\n              ]\n            },\n            {\n              \"type\": \"null\"\n            }\n          ]\n        },\n        \"version\": {\n          \"type\": \"number\"\n        },\n        \"models\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"url\": {\n                \"type\": \"string\",\n                \"format\": \"uri\"\n              },\n              \"hash\": {\n                \"type\": \"string\"\n              },\n              \"hash_type\": {\n                \"type\": \"string\"\n              },\n              \"directory\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"name\",\n              \"url\",\n              \"directory\"\n            ],\n            \"additionalProperties\": false\n          }\n        }\n      },\n      \"required\": [\n        \"last_node_id\",\n        \"last_link_id\",\n        \"nodes\",\n        \"links\",\n        \"version\"\n      ],\n      \"additionalProperties\": true\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/comfyui-server/execution_model_inversion_guide",
  "markdown": "# æ‰§è¡Œæ¨¡åž‹åè½¬æŒ‡å— - ComfyUI\n\n[PR #2666](https://github.com/comfyanonymous/ComfyUI/pull/2666) å°†æ‰§è¡Œæ¨¡åž‹ä»ŽåŽŸå…ˆçš„â€œåŽç«¯åˆ°å‰ç«¯â€é€’å½’æ–¹å¼ï¼Œè½¬å˜ä¸ºâ€œå‰ç«¯åˆ°åŽç«¯â€çš„æ‹“æ‰‘æŽ’åºæ–¹å¼ã€‚å°½ç®¡å¤šæ•°è‡ªå®šä¹‰èŠ‚ç‚¹é¢„è®¡ä»èƒ½ç…§å¸¸å·¥ä½œï¼Œæœ¬æŒ‡å—æ—¨åœ¨å¸®åŠ©è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘è€…è¯†åˆ«é‚£äº›_å¯èƒ½_å› æ­¤å˜æ›´è€Œå‡ºçŽ°é—®é¢˜çš„æƒ…å†µã€‚\n\n## ä¸å…¼å®¹å˜æ›´\n\n### Monkey Patching\n\nä»»ä½•æ›¾å¯¹æ‰§è¡Œæ¨¡åž‹è¿›è¡Œ `Monkey Patching` ï¼ˆçŒ´å­è¡¥ä¸ï¼‰çš„ä»£ç ï¼Œåœ¨æ–°æ¨¡åž‹ä¸‹å¾ˆå¯èƒ½å¤±æ•ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ­¤ `PR` å¸¦æ¥çš„æ‰§è¡Œæ€§èƒ½å·²è¶…è¶Šå¤šæ•°ä¸»æµ `Monkey Patching` æ–¹æ¡ˆï¼Œå› æ­¤è®¸å¤šæ­¤ç±»è¡¥ä¸å·²æ— å¿…è¦ã€‚\n\n### å¯é€‰è¾“å…¥éªŒè¯\n\nåœ¨æ­¤ `PR` æ›´æ–°å‰ï¼Œç³»ç»Ÿä»…å¯¹é‚£äº›å®Œå…¨é€šè¿‡ä¸€è¿žä¸² `\\\"required\\\"` ï¼ˆå¿…éœ€ï¼‰è¾“å…¥è¿žæŽ¥åˆ°è¾“å‡ºèŠ‚ç‚¹çš„èŠ‚ç‚¹è¿›è¡ŒéªŒè¯ã€‚å¦‚æžœæ‚¨çš„è‡ªå®šä¹‰èŠ‚ç‚¹ä»¥å¾€ä»…é€šè¿‡ `\\\"optional\\\"` ï¼ˆå¯é€‰ï¼‰è¾“å…¥è¿žæŽ¥ï¼Œé‚£ä¹ˆä¹‹å‰å¯èƒ½å¹¶æœªå‘çŽ°å…¶éªŒè¯å¤±è´¥çš„æƒ…å†µã€‚\n\nä»¥ä¸‹åˆ—å‡ºäº†ä¸€äº›å¯èƒ½å¯¼è‡´éªŒè¯å¤±è´¥çš„æƒ…å½¢åŠå»ºè®®è§£å†³æ–¹æ¡ˆï¼š\n\n*   ä¸ºäº†é…ç½®è‡ªå®šä¹‰å°éƒ¨ä»¶ï¼ˆwidgetï¼‰ï¼Œåœ¨ä¸é€‚åˆè¿›è¡Œæ¯”è¾ƒçš„ç±»åž‹ï¼ˆå¦‚å­—å…¸ï¼‰ä¸Šä½¿ç”¨äº†ä¿ç•™çš„[é™„åŠ å‚æ•°](https://docs.comfy.org/zh-CN/custom-nodes/backend/datatypes#%E9%99%84%E5%8A%A0%E5%8F%82%E6%95%B0)ï¼ˆä¾‹å¦‚ `min` å’Œ `max`ï¼‰ã€‚\n    *   å°†æ‰€ç”¨çš„é™„åŠ å‚æ•°æ›´æ”¹ä¸ºéžä¿ç•™å…³é”®å­—ï¼Œä¾‹å¦‚ `uiMin` å’Œ `uiMax`ã€‚_ï¼ˆæŽ¨èæ–¹æ¡ˆï¼‰_\n        \n        ```\n        @classmethod\n        def INPUT_TYPES(cls):\n            return {\n                \"required\": {\n                    \"my_size\": (\"VEC2\", {\"uiMin\": 0.0, \"uiMax\": 1.0}),\n                }\n            }\n        ```\n        \n    *   ä¸ºè¯¥è¾“å…¥å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„ [VALIDATE\\_INPUTS](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#validate-inputs) å‡½æ•°ï¼Œä»Žè€Œè·³è¿‡å¯¹å…¶çš„éªŒè¯ã€‚_ï¼ˆå¿«é€Ÿæ–¹æ¡ˆï¼‰_\n        \n        ```\n        @classmethod\n        def VALIDATE_INPUTS(cls, my_size):\n            return True\n        ```\n        \n*   ä½¿ç”¨äº†å¤åˆç±»åž‹ï¼ˆä¾‹å¦‚ `CUSTOM_A,CUSTOM_B`ï¼‰\n    *   ï¼ˆä½œä¸ºè¾“å‡ºæ—¶ï¼‰å®šä¹‰å¹¶ä½¿ç”¨ç±»ä¼¼ `MakeSmartType` çš„åŒ…è£…å™¨ [è§äºŽæ­¤ PR çš„å•å…ƒæµ‹è¯•](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R2)\n        \n        ```\n        class MyCustomNode:\n        \n            @classmethod\n            def INPUT_TYPES(cls):\n                return {\n                    \"required\": {\n                        \"input\": (MakeSmartType(\"FOO,BAR\"), {}),\n                    }\n                }\n        \n            RETURN_TYPES = (MakeSmartType(\"FOO,BAR\"),)\n        \n            # ...\n        ```\n        \n    *   ï¼ˆä½œä¸ºè¾“å…¥æ—¶ï¼‰å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„ [VALIDATE\\_INPUTS](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#validate-inputs) å‡½æ•°ï¼Œä½¿å…¶æŽ¥å— `input_types` å‚æ•°ï¼Œä»Žè€Œè·³è¿‡ç±»åž‹éªŒè¯ã€‚\n        \n        ```\n        @classmethod\n        def VALIDATE_INPUTS(cls, input_types):\n            return True\n        ```\n        \n    *   ï¼ˆè¾“å…¥è¾“å‡ºå‡é€‚ç”¨ï¼Œä¸”ä¾¿æ·ï¼‰å®šä¹‰å¹¶ä½¿ç”¨ `@VariantSupport` è£…é¥°å™¨ [è§äºŽæ­¤ PR çš„å•å…ƒæµ‹è¯•](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R15)\n        \n        ```\n        @VariantSupport\n        class MyCustomNode:\n        \n            @classmethod\n            def INPUT_TYPES(cls):\n                return {\n                    \"required\": {\n                        \"input\": (\"FOO,BAR\", {}),\n                    }\n                }\n            \n            RETURN_TYPES = (MakeSmartType(\"FOO,BAR\"),)\n        \n            # ...\n        ```\n        \n*   åœ¨å›¾ï¼ˆgraphï¼‰å®šä¹‰ä¸­å°†åˆ—è¡¨ï¼ˆä¾‹å¦‚ `[1, 2, 3]`ï¼‰ç”¨ä½œå¸¸é‡ï¼ˆä¾‹å¦‚ï¼Œä»£è¡¨ä¸€ä¸ª `VEC3` ç±»åž‹çš„å¸¸é‡è¾“å…¥ï¼‰ã€‚æ­¤ç”¨æ³•åœ¨æ—§ç‰ˆä¸­éœ€é…åˆå‰ç«¯æ‰©å±•ã€‚å¹¶ä¸”ï¼Œæ­¤å‰å¤§å°æ°å¥½ä¸º `2` çš„åˆ—è¡¨æœ¬èº«å°±ä¼šå¯¼è‡´å¤±è´¥â€”â€”å®ƒä»¬ä¼šè¢«è§†ä¸ºæ— æ•ˆé“¾æŽ¥ã€‚\n    *   å°†åˆ—è¡¨åŒ…è£…åœ¨å½¢å¦‚ `{ \"value\": [1, 2, 3] }` çš„å­—å…¸ä¸­ã€‚\n\n### æ‰§è¡Œé¡ºåº\n\næ‰§è¡Œé¡ºåºä»¥å¾€ä¾¿ä¼šå› èŠ‚ç‚¹çš„ `ID` ä¸åŒè€Œå˜åŒ–ï¼Œå¦‚ä»Šï¼Œç¼“å­˜å€¼çš„ä¸åŒä¹Ÿå¯èƒ½å¯¼è‡´æ‰§è¡Œé¡ºåºçš„æ”¹å˜ã€‚é€šå¸¸è€Œè¨€ï¼Œé™¤äº†å›¾ç»“æž„æ‰€å›ºæœ‰çš„çº¦æŸå¤–ï¼Œæ‰§è¡Œé¡ºåºåº”è¢«è§†ä¸ºä¸ç¡®å®šçš„ï¼Œå¹¶å¯èƒ½éšæ—¶è°ƒæ•´ã€‚ åˆ‡å‹¿ä¾èµ–ç‰¹å®šçš„æ‰§è¡Œé¡ºåºã€‚ _HIC SUNT DRACONES_\n\n## æ–°å¢žåŠŸèƒ½\n\n### éªŒè¯æ›´æ”¹\n\nä¸ºäº†ç¼“è§£å‰è¿°[å¯é€‰è¾“å…¥éªŒè¯](#%E5%8F%AF%E9%80%89%E8%BE%93%E5%85%A5%E9%AA%8C%E8%AF%81)å˜æ›´å¸¦æ¥çš„å½±å“ï¼Œ`VALIDATE_INPUTS` å‡½æ•°æ–°å¢žäº†è‹¥å¹²ç‰¹æ€§ã€‚\n\n*   å¯¹äºŽç”± `VALIDATE_INPUTS` å‡½æ•°æŽ¥æ”¶çš„è¾“å…¥ï¼Œç³»ç»Ÿå°†ä¸å†æ‰§è¡Œé»˜è®¤éªŒè¯æµç¨‹ã€‚\n*   `VALIDATE_INPUTS` å‡½æ•°çŽ°æ”¯æŒæŽ¥æ”¶ `**kwargs` å‚æ•°ã€‚ä¸€æ—¦ä½¿ç”¨ï¼ŒèŠ‚ç‚¹åˆ›å»ºè€…å°†è¢«è§†ä¸ºå·²è‡ªè¡Œå¤„ç†æ‰€æœ‰è¾“å…¥çš„éªŒè¯ã€‚\n*   `VALIDATE_INPUTS` å‡½æ•°å¯ä»¥æŽ¥æ”¶ä¸€ä¸ªåä¸º `input_types` çš„å‚æ•°ã€‚è¯¥å‚æ•°æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«äº†æ¯ä¸ªé€šè¿‡é“¾æŽ¥æŽ¥å…¥çš„è¾“å…¥åŠå…¶å¯¹åº”è¿žæŽ¥è¾“å‡ºçš„ç±»åž‹ã€‚è‹¥å®šä¹‰äº†æ­¤å‚æ•°ï¼Œç³»ç»Ÿå°†è·³è¿‡å¯¹è¯¥èŠ‚ç‚¹æ‰€æœ‰è¾“å…¥çš„ç±»åž‹éªŒè¯ã€‚\n\næ›´å¤šè¯¦æƒ…è¯·å‚é˜… [VALIDATE\\_INPUTS](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#validate-inputs) æ–‡æ¡£ã€‚\n\n### Lazy Evaluation\n\nè¾“å…¥çŽ°æ”¯æŒ `Lazy Evaluation` ï¼ˆæƒ°æ€§æ±‚å€¼ï¼‰ï¼Œå³å¯ä»¥å…ˆåˆ¤æ–­æ˜¯å¦ç¡®å®žéœ€è¦æŸä¸ªè¾“å…¥å€¼ï¼Œå†å†³å®šæ˜¯å¦æ‰§è¡Œå…¶è¿žæŽ¥çš„ä¸Šæ¸¸èŠ‚ç‚¹åŠå…¶æ‰€æœ‰ä¾èµ–èŠ‚ç‚¹ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§[æƒ°æ€§æ±‚å€¼](https://docs.comfy.org/zh-CN/custom-nodes/backend/lazy_evaluation)ã€‚\n\n### Node Expansion\n\nåœ¨è¿è¡Œæ—¶ï¼ŒèŠ‚ç‚¹å¯ä»¥åŠ¨æ€æ‰©å±•ä¸ºä¸€ä¸ªå­å›¾ï¼ˆsubgraphï¼‰ã€‚è¯¥æœºåˆ¶ä½¿å¾—é€šè¿‡å°¾é€’å½’ï¼ˆtail-recursionï¼‰å®žçŽ°å¾ªçŽ¯ç­‰å¤æ‚é€»è¾‘æˆä¸ºå¯èƒ½ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§[èŠ‚ç‚¹æ‰©å±•](https://docs.comfy.org/zh-CN/custom-nodes/backend/expansion)ã€‚\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/specs/nodedef_json_1_0",
  "markdown": "# èŠ‚ç‚¹å®šä¹‰ JSON 1.0 - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyNodeDefV1\",\n  \"definitions\": {\n    \"ComfyNodeDefV1\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"input\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"required\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"INT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"FLOAT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"round\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"boolean\",\n                                    \"const\": false\n                                  }\n                                ]\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"BOOLEAN\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"label_on\": {\n                                \"type\": \"string\"\n                              },\n                              \"label_off\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"STRING\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"string\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"multiline\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"dynamicPrompts\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultVal\": {\n                                \"type\": \"string\"\n                              },\n                              \"placeholder\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": [\n                            \"string\",\n                            \"number\"\n                          ]\n                        }\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"COMBO\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            \"optional\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {\n                \"anyOf\": [\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"INT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"FLOAT\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"array\",\n                                    \"items\": {\n                                      \"type\": \"number\"\n                                    }\n                                  }\n                                ]\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"min\": {\n                                \"type\": \"number\"\n                              },\n                              \"max\": {\n                                \"type\": \"number\"\n                              },\n                              \"step\": {\n                                \"type\": \"number\"\n                              },\n                              \"display\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"slider\",\n                                  \"number\",\n                                  \"knob\"\n                                ]\n                              },\n                              \"round\": {\n                                \"anyOf\": [\n                                  {\n                                    \"type\": \"number\"\n                                  },\n                                  {\n                                    \"type\": \"boolean\",\n                                    \"const\": false\n                                  }\n                                ]\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"BOOLEAN\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"label_on\": {\n                                \"type\": \"string\"\n                              },\n                              \"label_off\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"STRING\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {\n                                \"type\": \"string\"\n                              },\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"multiline\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"dynamicPrompts\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"defaultVal\": {\n                                \"type\": \"string\"\n                              },\n                              \"placeholder\": {\n                                \"type\": \"string\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": [\n                            \"string\",\n                            \"number\"\n                          ]\n                        }\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\",\n                        \"const\": \"COMBO\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"control_after_generate\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"image_folder\": {\n                                \"type\": \"string\",\n                                \"enum\": [\n                                  \"input\",\n                                  \"output\",\n                                  \"temp\"\n                                ]\n                              },\n                              \"allow_batch\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"video_upload\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"remote\": {\n                                \"type\": \"object\",\n                                \"properties\": {\n                                  \"route\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"string\",\n                                        \"format\": \"uri\"\n                                      },\n                                      {\n                                        \"type\": \"string\",\n                                        \"pattern\": \"^\\\\/\"\n                                      }\n                                    ]\n                                  },\n                                  \"refresh\": {\n                                    \"anyOf\": [\n                                      {\n                                        \"type\": \"number\",\n                                        \"minimum\": -9007199254740991,\n                                        \"maximum\": 9007199254740991\n                                      },\n                                      {\n                                        \"type\": \"number\",\n                                        \"maximum\": 9007199254740991,\n                                        \"minimum\": -9007199254740991\n                                      }\n                                    ]\n                                  },\n                                  \"response_key\": {\n                                    \"type\": \"string\"\n                                  },\n                                  \"query_params\": {\n                                    \"type\": \"object\",\n                                    \"additionalProperties\": {\n                                      \"type\": \"string\"\n                                    }\n                                  },\n                                  \"refresh_button\": {\n                                    \"type\": \"boolean\"\n                                  },\n                                  \"control_after_refresh\": {\n                                    \"type\": \"string\",\n                                    \"enum\": [\n                                      \"first\",\n                                      \"last\"\n                                    ]\n                                  },\n                                  \"timeout\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  },\n                                  \"max_retries\": {\n                                    \"type\": \"number\",\n                                    \"minimum\": 0\n                                  }\n                                },\n                                \"required\": [\n                                  \"route\"\n                                ],\n                                \"additionalProperties\": false\n                              },\n                              \"options\": {\n                                \"type\": \"array\",\n                                \"items\": {\n                                  \"type\": [\n                                    \"string\",\n                                    \"number\"\n                                  ]\n                                }\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  },\n                  {\n                    \"type\": \"array\",\n                    \"minItems\": 2,\n                    \"maxItems\": 2,\n                    \"items\": [\n                      {\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"anyOf\": [\n                          {\n                            \"not\": {}\n                          },\n                          {\n                            \"type\": \"object\",\n                            \"properties\": {\n                              \"default\": {},\n                              \"defaultInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"forceInput\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"tooltip\": {\n                                \"type\": \"string\"\n                              },\n                              \"hidden\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"advanced\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"rawLink\": {\n                                \"type\": \"boolean\"\n                              },\n                              \"lazy\": {\n                                \"type\": \"boolean\"\n                              }\n                            },\n                            \"additionalProperties\": true\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                ]\n              }\n            },\n            \"hidden\": {\n              \"type\": \"object\",\n              \"additionalProperties\": {}\n            }\n          },\n          \"additionalProperties\": false\n        },\n        \"output\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"anyOf\": [\n              {\n                \"type\": \"string\"\n              },\n              {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": [\n                    \"string\",\n                    \"number\"\n                  ]\n                }\n              }\n            ]\n          }\n        },\n        \"output_is_list\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"output_name\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"output_tooltips\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"string\"\n          }\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"display_name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"category\": {\n          \"type\": \"string\"\n        },\n        \"output_node\": {\n          \"type\": \"boolean\"\n        },\n        \"python_module\": {\n          \"type\": \"string\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\"\n        },\n        \"experimental\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"name\",\n        \"display_name\",\n        \"description\",\n        \"category\",\n        \"output_node\",\n        \"python_module\"\n      ],\n      \"additionalProperties\": false\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/standards",
  "markdown": "# æ ‡å‡† - ComfyUI\n\n## åŸºæœ¬æ ‡å‡†\n\n### 1\\. ç¤¾åŒºä»·å€¼\n\nè‡ªå®šä¹‰èŠ‚ç‚¹å¿…é¡»ä¸º ComfyUI ç¤¾åŒºæä¾›æœ‰ä»·å€¼çš„åŠŸèƒ½ é¿å…ï¼š\n\n*   è¿‡åº¦è‡ªæˆ‘å®£ä¼ \n*   å†’å……æˆ–è¯¯å¯¼è¡Œä¸º\n*   æ¶æ„è¡Œä¸º\n*   è‡ªæˆ‘å®£ä¼ ä»…å…è®¸åœ¨æŒ‡å®šçš„è®¾ç½®èœå•éƒ¨åˆ†å†…\n*   é¡¶éƒ¨å’Œä¾§è¾¹èœå•åº”ä»…åŒ…å«å®žç”¨åŠŸèƒ½\n\n### 2\\. èŠ‚ç‚¹å…¼å®¹æ€§\n\nä¸è¦å¹²æ‰°å…¶ä»–è‡ªå®šä¹‰èŠ‚ç‚¹çš„æ“ä½œï¼ˆå®‰è£…ã€æ›´æ–°ã€åˆ é™¤ï¼‰\n\n*   å¯¹äºŽå…¶ä»–è‡ªå®šä¹‰èŠ‚ç‚¹çš„ä¾èµ–ï¼š\n    *   ä½¿ç”¨ä¾èµ–åŠŸèƒ½æ—¶æ˜¾ç¤ºæ¸…æ™°çš„è­¦å‘Š\n    *   æä¾›å±•ç¤ºæ‰€éœ€èŠ‚ç‚¹çš„ç¤ºä¾‹å·¥ä½œæµ\n\n### 3\\. æ³•å¾‹åˆè§„æ€§\n\nå¿…é¡»éµå®ˆæ‰€æœ‰é€‚ç”¨çš„æ³•å¾‹å’Œæ³•è§„\n\n### 5\\. è´¨é‡è¦æ±‚\n\nèŠ‚ç‚¹å¿…é¡»åŠŸèƒ½å®Œæ•´ã€æ–‡æ¡£å®Œå–„ä¸”ç§¯æžç»´æŠ¤ã€‚\n\n### 6\\. åˆ†å‰æŒ‡å—\n\nåˆ†å‰çš„èŠ‚ç‚¹å¿…é¡»ï¼š\n\n*   ä¸ŽåŽŸå§‹èŠ‚ç‚¹åç§°æœ‰æ˜Žæ˜¾åŒºåˆ«\n*   åœ¨åŠŸèƒ½æˆ–ä»£ç ä¸Šæä¾›æ˜¾è‘—å·®å¼‚\n\nä»¥ä¸‹æ˜¯å‘å¸ƒè‡ªå®šä¹‰èŠ‚ç‚¹åˆ°æ³¨å†Œè¡¨å¿…é¡»æ»¡è¶³çš„æ ‡å‡†ã€‚\n\n## å®‰å…¨æ ‡å‡†\n\nè‡ªå®šä¹‰èŠ‚ç‚¹åº”è¯¥æ˜¯å®‰å…¨çš„ã€‚æˆ‘ä»¬å°†å¼€å§‹ä¸Žè¿åè¿™äº›æ ‡å‡†çš„è‡ªå®šä¹‰èŠ‚ç‚¹åˆä½œè¿›è¡Œé‡å†™ã€‚å¦‚æžœæœ‰ä¸€äº›åº”è¯¥ç”±æ ¸å¿ƒæš´éœ²çš„ä¸»è¦åŠŸèƒ½ï¼Œè¯·åœ¨ [rfcs ä»“åº“](https://github.com/comfy-org/rfcs) ä¸­æå‡ºè¯·æ±‚ã€‚\n\n### eval/exec è°ƒç”¨\n\n#### æ”¿ç­–\n\nç”±äºŽå®‰å…¨è€ƒè™‘ï¼Œç¦æ­¢åœ¨è‡ªå®šä¹‰èŠ‚ç‚¹ä¸­ä½¿ç”¨ `eval` å’Œ `exec` å‡½æ•°ã€‚\n\n#### åŽŸå› \n\nè¿™äº›å‡½æ•°å¯ä»¥å¯ç”¨ä»»æ„ä»£ç æ‰§è¡Œï¼Œåœ¨å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶åˆ›å»ºæ½œåœ¨çš„è¿œç¨‹ä»£ç æ‰§è¡Œï¼ˆRCEï¼‰æ¼æ´žã€‚åŒ…å«å°†ç”¨æˆ·è¾“å…¥ä¼ é€’ç»™ `eval` æˆ– `exec` çš„èŠ‚ç‚¹çš„å·¥ä½œæµå¯èƒ½è¢«åˆ©ç”¨è¿›è¡Œå„ç§ç½‘ç»œæ”»å‡»ï¼ŒåŒ…æ‹¬ï¼š\n\n*   é”®ç›˜è®°å½•\n*   å‹’ç´¢è½¯ä»¶\n*   å…¶ä»–æ¶æ„ä»£ç æ‰§è¡Œ\n\n### ç”¨äºŽ pip install çš„ subprocess\n\n#### æ”¿ç­–\n\nä¸å…è®¸é€šè¿‡ subprocess è°ƒç”¨è¿›è¡Œè¿è¡Œæ—¶åŒ…å®‰è£…ã€‚\n\n#### åŽŸå› \n\n*   ComfyUI manager å°†ä¸Ž ComfyUI ä¸€èµ·å‘å¸ƒï¼Œå¹¶å…è®¸ç”¨æˆ·å®‰è£…ä¾èµ–é¡¹\n*   é›†ä¸­å¼ä¾èµ–ç®¡ç†æé«˜äº†å®‰å…¨æ€§å’Œç”¨æˆ·ä½“éªŒ\n*   æœ‰åŠ©äºŽé˜²æ­¢æ½œåœ¨çš„ä¾›åº”é“¾æ”»å‡»\n*   æ¶ˆé™¤äº†å¤šæ¬¡é‡æ–°åŠ è½½ ComfyUI çš„éœ€è¦\n\n### ä»£ç æ··æ·†\n\n#### æ”¿ç­–\n\nç¦æ­¢åœ¨è‡ªå®šä¹‰èŠ‚ç‚¹ä¸­è¿›è¡Œä»£ç æ··æ·†ã€‚\n\n#### åŽŸå› \n\næ··æ·†çš„ä»£ç ï¼š\n\n*   æ— æ³•å®¡æŸ¥ï¼Œå¾ˆå¯èƒ½å…·æœ‰æ¶æ„æ€§"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/google/gemini",
  "markdown": "# Google Gemini API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\nGoogle Gemini æ˜¯ Google æŽ¨å‡ºçš„ä¸€æ¬¾å¼ºå¤§çš„ AI æ¨¡åž‹ï¼Œæ”¯æŒå¯¹è¯ã€æ–‡æœ¬ç”Ÿæˆç­‰å¤šç§åŠŸèƒ½ã€‚ç›®å‰ ComfyUI å·²é›†æˆ Google Gemini APIï¼Œä½ å¯ä»¥ç›´æŽ¥åœ¨ ComfyUI ä¸­ä½¿ç”¨ç›¸å…³èŠ‚ç‚¹æ¥å®Œæˆå¯¹è¯åŠŸèƒ½ã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å®Œæˆå¯¹åº”å¯¹è¯åŠŸèƒ½ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„ Json æ–‡ä»¶å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/google/api_google_gemini.json)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![OpenAI Chat Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg)\n\nä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½ä½ éœ€è¦ AI çš„è§£è¯»å›¾ç‰‡\n2.  (å¯é€‰) å¦‚æžœéœ€è¦ä½ å¯ä»¥ä¿®æ”¹`Google Gemini` ä¸­çš„æç¤ºè¯ï¼Œä»Žè€Œè®© AI æ¥æ‰§è¡Œç‰¹å®šä»»åŠ¡\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå¯¹è¯ã€‚\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Preview Any` èŠ‚ç‚¹ä¸­æŸ¥çœ‹å¯¹åº” AI è¿”å›žçš„å†…å®¹ã€‚\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\n*   ç›®å‰æ–‡ä»¶è¾“å…¥èŠ‚ç‚¹ `Gemini Input Files` éœ€è¦å…ˆå°†æ–‡ä»¶ä¸Šä¼ è‡³`ComfyUI/input/` ç›®å½•ä¸‹ï¼Œ æ­¤èŠ‚ç‚¹æ­£åœ¨æ”¹è¿›ï¼Œæˆ‘ä»¬ä¼šåœ¨æ›´æ–°åŽä¿®æ”¹æ¨¡æ¿\n*   å·¥ä½œæµä¸­æä¾›äº†ä½¿ç”¨ `Batch Images` æ¥è¾“å…¥çš„ç¤ºä¾‹ï¼Œå¦‚æžœä½ æœ‰å¤šå¼ å›¾ç‰‡éœ€è¦ AI è§£è¯»ï¼Œå¯å‚è€ƒæ­¥éª¤å›¾åœ¨ä½¿ç”¨å³é”®æ¥å°†å¯¹åº”çš„èŠ‚ç‚¹æ¨¡å¼è®¾ç½®ä¸º `æ€»æ˜¯ï¼ˆalwaysï¼‰` æ¥å¯ç”¨"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/cicd",
  "markdown": "# è‡ªå®šä¹‰èŠ‚ç‚¹ CI/CD - ComfyUI\n\n## ç®€ä»‹\n\nåœ¨å¯¹è‡ªå®šä¹‰èŠ‚ç‚¹è¿›è¡Œæ›´æ”¹æ—¶ï¼Œåœ¨ Comfy æˆ–å…¶ä»–è‡ªå®šä¹‰èŠ‚ç‚¹ä¸­å‡ºçŽ°é—®é¢˜å¹¶ä¸ç½•è§ã€‚åœ¨æ¯ç§æ“ä½œç³»ç»Ÿå’Œä¸åŒçš„ Pytorch é…ç½®ä¸Šè¿›è¡Œæµ‹è¯•é€šå¸¸æ˜¯ä¸çŽ°å®žçš„ã€‚\n\n### ä½¿ç”¨ Github Actions è¿è¡Œ Comfy å·¥ä½œæµ\n\n[Comfy-Action](https://github.com/Comfy-Org/comfy-action) å…è®¸æ‚¨åœ¨ Github Actions ä¸Šè¿è¡Œ Comfy workflow.json æ–‡ä»¶ã€‚å®ƒæ”¯æŒä¸‹è½½æ¨¡åž‹ã€è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œå¹¶å¯åœ¨ Linux/Mac/Windows ä¸Šè¿è¡Œã€‚\n\n### ç»“æžœ\n\nè¾“å‡ºæ–‡ä»¶ä¼šä¸Šä¼ åˆ° [CI/CD ä»ªè¡¨æ¿](https://comfyci.org/)ï¼Œå¯ä»¥åœ¨æäº¤æ–°æ›´æ”¹æˆ–å‘å¸ƒè‡ªå®šä¹‰èŠ‚ç‚¹çš„æ–°ç‰ˆæœ¬ä¹‹å‰ä½œä¸ºæœ€åŽä¸€æ­¥æŸ¥çœ‹ã€‚ ![ComfyCI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyci.png)"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/manager",
  "markdown": "# å‘å¸ƒåˆ° Manager - ComfyUI\n\n### ä½¿ç”¨ ComfyUI Manager\n\nè¦è®©ä½ çš„è‡ªå®šä¹‰èŠ‚ç‚¹é€šè¿‡ **ComfyUI Manager** æä¾›ï¼Œä½ éœ€è¦å°†å…¶ä¿å­˜ä¸º git ä»“åº“ï¼ˆé€šå¸¸åœ¨ `github.com`ï¼‰ï¼Œ ç„¶åŽåœ¨ **ComfyUI Manager** çš„ git ä»“åº“æäº¤ä¸€ä¸ª Pull Requestï¼Œåœ¨å…¶ä¸­ç¼–è¾‘ `custom-node-list.json` ä»¥æ·»åŠ ä½ çš„èŠ‚ç‚¹ã€‚ [è¯¦ç»†è¯´æ˜Ž](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#how-to-register-your-custom-node-into-comfyui-manager)ã€‚ å½“ç”¨æˆ·å®‰è£…èŠ‚ç‚¹æ—¶ï¼Œ**ComfyUI Manager** ä¼šï¼š\n\n### ComfyUI Manager æ–‡ä»¶\n\nå¦‚ä¸Šæ‰€è¿°ï¼Œ**ComfyUI Manager** ä¼šä½¿ç”¨ä¸€äº›æ–‡ä»¶å’Œè„šæœ¬æ¥ç®¡ç†è‡ªå®šä¹‰èŠ‚ç‚¹çš„ç”Ÿå‘½å‘¨æœŸã€‚è¿™äº›éƒ½æ˜¯å¯é€‰çš„ã€‚\n\n*   `requirements.txt` - å¦‚ä¸Šæ‰€è¿°çš„ Python ä¾èµ–\n*   `install.py`, `uninstall.py` - å®‰è£…æˆ–å¸è½½è‡ªå®šä¹‰èŠ‚ç‚¹æ—¶æ‰§è¡Œ\n*   `disable.py`, `enable.py` - ç¦ç”¨æˆ–é‡æ–°å¯ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹æ—¶æ‰§è¡Œ\n*   `node_list.json` - ä»…å½“è‡ªå®šä¹‰èŠ‚ç‚¹çš„ NODE\\_CLASS\\_MAPPINGS æ¨¡å¼ä¸ç¬¦åˆå¸¸è§„æ—¶æ‰éœ€è¦ã€‚\n\nå®˜æ–¹è¯¦æƒ…è¯·å‚è§ [ComfyUI Manager æŒ‡å—](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#custom-node-support-guide)ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/lifecycle",
  "markdown": "# ç”Ÿå‘½å‘¨æœŸ - ComfyUI\n\n## Comfy å¦‚ä½•åŠ è½½è‡ªå®šä¹‰èŠ‚ç‚¹\n\nå½“ Comfy å¯åŠ¨æ—¶ï¼Œå®ƒä¼šæ‰«æ `custom_nodes` ç›®å½•ä¸‹çš„ Python æ¨¡å—ï¼Œå¹¶å°è¯•åŠ è½½å®ƒä»¬ã€‚ å¦‚æžœæ¨¡å—å¯¼å‡ºäº† `NODE_CLASS_MAPPINGS`ï¼Œå®ƒå°±ä¼šè¢«è§†ä¸ºè‡ªå®šä¹‰èŠ‚ç‚¹ã€‚\n\n### **init**.py\n\nå½“ Comfy å°è¯•å¯¼å…¥æ¨¡å—æ—¶ï¼Œä¼šæ‰§è¡Œ `__init__.py` æ–‡ä»¶ã€‚è¦è®©æ¨¡å—è¢«è¯†åˆ«ä¸ºåŒ…å«è‡ªå®šä¹‰èŠ‚ç‚¹å®šä¹‰ï¼Œå¿…é¡»å¯¼å‡º `NODE_CLASS_MAPPINGS`ã€‚å¦‚æžœå¯¼å‡ºäº†ï¼ˆå¹¶ä¸”å¯¼å…¥è¿‡ç¨‹ä¸­æ²¡æœ‰å‡ºé”™ï¼‰ï¼Œæ¨¡å—ä¸­å®šä¹‰çš„èŠ‚ç‚¹å°±ä¼šåœ¨ Comfy ä¸­å¯ç”¨ã€‚å¦‚æžœä½ çš„ä»£ç æœ‰é”™è¯¯ï¼ŒComfy ä¼šç»§ç»­è¿è¡Œï¼Œä½†ä¼šæŠ¥å‘Šè¯¥æ¨¡å—åŠ è½½å¤±è´¥ã€‚æ‰€ä»¥è¯·æ£€æŸ¥ Python æŽ§åˆ¶å°ï¼ ä¸€ä¸ªéžå¸¸ç®€å•çš„ `__init__.py` æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```\nfrom .python_file import MyCustomNode\nNODE_CLASS_MAPPINGS = { \"My Custom Node\" : MyCustomNode }\n__all__ = [\"NODE_CLASS_MAPPINGS\"]\n```\n\n#### NODE\\_CLASS\\_MAPPINGS\n\n`NODE_CLASS_MAPPINGS` å¿…é¡»æ˜¯ä¸€ä¸ª `dict`ï¼Œå°†è‡ªå®šä¹‰èŠ‚ç‚¹çš„å”¯ä¸€åç§°ï¼ˆåœ¨æ•´ä¸ª Comfy å®‰è£…ä¸­å”¯ä¸€ï¼‰æ˜ å°„åˆ°å¯¹åº”çš„èŠ‚ç‚¹ç±»ã€‚\n\n#### NODE\\_DISPLAY\\_NAME\\_MAPPINGS\n\n`__init__.py` è¿˜å¯ä»¥å¯¼å‡º `NODE_DISPLAY_NAME_MAPPINGS`ï¼Œå®ƒå°†åŒæ ·çš„å”¯ä¸€åç§°æ˜ å°„ä¸ºèŠ‚ç‚¹çš„æ˜¾ç¤ºåç§°ã€‚ å¦‚æžœæ²¡æœ‰æä¾› `NODE_DISPLAY_NAME_MAPPINGS`ï¼ŒComfy ä¼šä½¿ç”¨å”¯ä¸€åç§°ä½œä¸ºæ˜¾ç¤ºåç§°ã€‚\n\n#### WEB\\_DIRECTORY\n\nå¦‚æžœä½ éœ€è¦éƒ¨ç½²å®¢æˆ·ç«¯ä»£ç ï¼Œè¿˜éœ€è¦å¯¼å‡º JavaScript æ–‡ä»¶æ‰€åœ¨è·¯å¾„ï¼ˆç›¸å¯¹äºŽæ¨¡å—çš„è·¯å¾„ï¼‰ã€‚é€šå¸¸å°†è¿™äº›æ–‡ä»¶æ”¾åœ¨è‡ªå®šä¹‰èŠ‚ç‚¹çš„ `js` å­ç›®å½•ä¸‹ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview",
  "markdown": "# å±žæ€§ - ComfyUI\n\n### ç®€å•ç¤ºä¾‹\n\nä¸‹é¢æ˜¯â€œåè½¬å›¾ç‰‡èŠ‚ç‚¹â€çš„ä»£ç ï¼Œæ¦‚è¿°äº†è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘ä¸­çš„å…³é”®æ¦‚å¿µã€‚\n\n```\nclass InvertImageNode:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": { \"image_in\" : (\"IMAGE\", {}) },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"image_out\",)\n    CATEGORY = \"examples\"\n    FUNCTION = \"invert\"\n\n    def invert(self, image_in):\n        image_out = 1 - image_in\n        return (image_out,)\n```\n\n### ä¸»è¦å±žæ€§\n\næ¯ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹éƒ½æ˜¯ä¸€ä¸ª Python ç±»ï¼Œå…·æœ‰ä»¥ä¸‹å…³é”®å±žæ€§ï¼š\n\n#### INPUT\\_TYPES\n\né¡¾åæ€ä¹‰ï¼Œ`INPUT_TYPES` å®šä¹‰äº†èŠ‚ç‚¹çš„è¾“å…¥ã€‚è¯¥æ–¹æ³•è¿”å›žä¸€ä¸ª `dict`ï¼Œ**å¿…é¡»**åŒ…å« `required` é”®ï¼Œä¹Ÿ**å¯ä»¥**åŒ…å« `optional` å’Œ/æˆ– `hidden` é”®ã€‚`required` å’Œ `optional` è¾“å…¥çš„å”¯ä¸€åŒºåˆ«åœ¨äºŽï¼Œ`optional` è¾“å…¥å¯ä»¥ä¸è¿žæŽ¥ã€‚  \nå…³äºŽ `hidden` è¾“å…¥çš„æ›´å¤šä¿¡æ¯ï¼Œå‚è§ [éšè—è¾“å…¥](https://docs.comfy.org/zh-CN/custom-nodes/backend/more_on_inputs#hidden-inputs)ã€‚ æ¯ä¸ªé”®çš„å€¼åˆæ˜¯ä¸€ä¸ª `dict`ï¼Œå…¶ä¸­çš„é”®å€¼å¯¹æŒ‡å®šè¾“å…¥çš„åç§°å’Œç±»åž‹ã€‚ç±»åž‹ç”±ä¸€ä¸ª `tuple` å®šä¹‰ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯æ•°æ®ç±»åž‹ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯åŒ…å«é™„åŠ å‚æ•°çš„ `dict`ã€‚ è¿™é‡Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªå¿…éœ€è¾“å…¥ï¼Œåä¸º `image_in`ï¼Œç±»åž‹ä¸º `IMAGE`ï¼Œæ²¡æœ‰é¢å¤–å‚æ•°ã€‚ æ³¨æ„ï¼Œä¸ŽæŽ¥ä¸‹æ¥å‡ ä¸ªå±žæ€§ä¸åŒï¼Œ`INPUT_TYPES` æ˜¯ä¸€ä¸ª `@classmethod`ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯è®©ä¸‹æ‹‰å°éƒ¨ä»¶ä¸­çš„é€‰é¡¹ï¼ˆæ¯”å¦‚è¦åŠ è½½çš„ checkpoint åç§°ï¼‰å¯ä»¥åœ¨è¿è¡Œæ—¶ç”± Comfy åŠ¨æ€è®¡ç®—ã€‚æˆ‘ä»¬ç¨åŽä¼šè¯¦ç»†ä»‹ç»è¿™ä¸€ç‚¹ã€‚\n\n#### RETURN\\_TYPES\n\nä¸€ä¸ªç”± `str` ç»„æˆçš„ `tuple`ï¼Œå®šä¹‰äº†èŠ‚ç‚¹è¿”å›žçš„æ•°æ®ç±»åž‹ã€‚å¦‚æžœèŠ‚ç‚¹æ²¡æœ‰è¾“å‡ºï¼Œä¹Ÿå¿…é¡»æä¾› `RETURN_TYPES = ()`ã€‚\n\n#### RETURN\\_NAMES\n\nç”¨äºŽæ ‡è®°è¾“å‡ºçš„åç§°ã€‚æ­¤é¡¹ä¸ºå¯é€‰ï¼›å¦‚æžœçœç•¥ï¼Œåç§°å°†ç›´æŽ¥ä½¿ç”¨ `RETURN_TYPES` çš„å°å†™å½¢å¼ã€‚\n\n#### CATEGORY\n\nèŠ‚ç‚¹åœ¨ ComfyUI **æ·»åŠ èŠ‚ç‚¹** èœå•ä¸­çš„åˆ†ç±»ã€‚å¯ä»¥ç”¨è·¯å¾„æŒ‡å®šå­èœå•ï¼Œä¾‹å¦‚ `examples/trivial`ã€‚\n\n#### FUNCTION\n\nèŠ‚ç‚¹æ‰§è¡Œæ—¶åº”è°ƒç”¨çš„ Python å‡½æ•°åã€‚ è¯¥å‡½æ•°ä»¥å‘½åå‚æ•°çš„æ–¹å¼è¢«è°ƒç”¨ã€‚æ‰€æœ‰ `required`ï¼ˆå’Œ `hidden`ï¼‰è¾“å…¥éƒ½ä¼šåŒ…å«åœ¨å†…ï¼›`optional` è¾“å…¥åªæœ‰åœ¨è¿žæŽ¥æ—¶æ‰ä¼šåŒ…å«ï¼Œå› æ­¤ä½ åº”åœ¨å‡½æ•°å®šä¹‰ä¸­ä¸ºå®ƒä»¬æä¾›é»˜è®¤å€¼ï¼ˆæˆ–ç”¨ `**kwargs` æ•èŽ·ï¼‰ã€‚ è¯¥å‡½æ•°è¿”å›žä¸€ä¸ªä¸Ž `RETURN_TYPES` å¯¹åº”çš„å…ƒç»„ã€‚å³ä½¿æ²¡æœ‰è¿”å›žå†…å®¹ï¼Œä¹Ÿå¿…é¡»è¿”å›žå…ƒç»„ï¼ˆ`return ()`ï¼‰ã€‚åŒæ ·ï¼Œå¦‚æžœåªæœ‰ä¸€ä¸ªè¾“å‡ºï¼Œè®°å¾—åŠ ä¸Šé€—å· `return (image_out,)`ï¼\n\n### æ‰§è¡ŒæŽ§åˆ¶æ‰©å±•\n\nComfy çš„ä¸€ä¸ªå¾ˆæ£’çš„ç‰¹æ€§æ˜¯å®ƒä¼šç¼“å­˜è¾“å‡ºï¼Œå¹¶ä¸”åªä¼šæ‰§è¡Œé‚£äº›ç»“æžœå¯èƒ½ä¸Žä¸Šæ¬¡è¿è¡Œä¸åŒçš„èŠ‚ç‚¹ã€‚è¿™å¯ä»¥æžå¤§åœ°åŠ å¿«è®¸å¤šå·¥ä½œæµçš„é€Ÿåº¦ã€‚ æœ¬è´¨ä¸Šï¼Œè¿™é€šè¿‡è¯†åˆ«å“ªäº›èŠ‚ç‚¹ä¼šäº§ç”Ÿè¾“å‡ºï¼ˆæ¯”å¦‚ Image Preview å’Œ Save Image èŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹æ€»æ˜¯ä¼šè¢«æ‰§è¡Œï¼‰ï¼Œç„¶åŽåå‘è¿½è¸ªå“ªäº›èŠ‚ç‚¹æä¾›äº†è‡ªä¸Šæ¬¡è¿è¡Œä»¥æ¥å¯èƒ½å·²æ›´æ”¹çš„æ•°æ®ã€‚ è‡ªå®šä¹‰èŠ‚ç‚¹æœ‰ä¸¤ä¸ªå¯é€‰ç‰¹æ€§å¯ä»¥ååŠ©è¿™ä¸€è¿‡ç¨‹ã€‚\n\n#### OUTPUT\\_NODE\n\né»˜è®¤æƒ…å†µä¸‹ï¼ŒèŠ‚ç‚¹ä¸ä¼šè¢«è§†ä¸ºè¾“å‡ºèŠ‚ç‚¹ã€‚è®¾ç½® `OUTPUT_NODE = True` å¯ä»¥æŒ‡å®šè¯¥èŠ‚ç‚¹ä¸ºè¾“å‡ºèŠ‚ç‚¹ã€‚\n\n#### IS\\_CHANGED\n\né»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æžœèŠ‚ç‚¹çš„ä»»ä½•è¾“å…¥æˆ–å°éƒ¨ä»¶å‘ç”Ÿå˜åŒ–ï¼ŒComfy ä¼šè®¤ä¸ºè¯¥èŠ‚ç‚¹å·²æ›´æ”¹ã€‚è¿™é€šå¸¸æ˜¯æ­£ç¡®çš„ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ä½ å¯èƒ½éœ€è¦é‡å†™æ­¤è¡Œä¸ºï¼Œä¾‹å¦‚èŠ‚ç‚¹ä½¿ç”¨äº†éšæœºæ•°ï¼ˆä¸”æœªæŒ‡å®šç§å­â€”â€”æ­¤æ—¶æœ€å¥½æä¾›ä¸€ä¸ªç§å­è¾“å…¥ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥æŽ§åˆ¶å¯å¤çŽ°æ€§å¹¶é¿å…ä¸å¿…è¦çš„æ‰§è¡Œï¼‰ã€åŠ è½½äº†å¯èƒ½å·²åœ¨å¤–éƒ¨æ›´æ”¹çš„è¾“å…¥ï¼Œæˆ–æœ‰æ—¶ä¼šå¿½ç•¥æŸäº›è¾“å…¥ï¼ˆå› æ­¤ä¸éœ€è¦ä»…å› è¿™äº›è¾“å…¥å˜åŒ–è€Œæ‰§è¡Œï¼‰ã€‚\n\n`IS_CHANGED` æŽ¥æ”¶ä¸Žä¸»å‡½æ•°ï¼ˆç”± `FUNCTION` æŒ‡å®šï¼‰ç›¸åŒçš„å‚æ•°ï¼Œå¹¶å¯ä»¥è¿”å›žä»»æ„ Python å¯¹è±¡ã€‚è¯¥å¯¹è±¡ä¼šä¸Žä¸Šæ¬¡è¿è¡Œæ—¶è¿”å›žçš„å¯¹è±¡è¿›è¡Œæ¯”è¾ƒï¼Œå¦‚æžœ `is_changed != is_changed_old`ï¼Œåˆ™è®¤ä¸ºèŠ‚ç‚¹å·²æ›´æ”¹ï¼ˆç›¸å…³ä»£ç åœ¨ `execution.py` ä¸­ï¼‰ã€‚ ç”±äºŽ `True == True`ï¼Œå¦‚æžœèŠ‚ç‚¹è¿”å›ž `True` è¡¨ç¤ºå·²æ›´æ”¹ï¼Œå®žé™…ä¸Šä¼šè¢«è®¤ä¸ºæœªæ›´æ”¹ï¼å¦‚æžœä¸æ˜¯ä¸ºäº†å…¼å®¹çŽ°æœ‰èŠ‚ç‚¹ï¼Œè¿™ä¸€è¡Œä¸ºæœ¬å¯ä»¥åœ¨ Comfy ä»£ç ä¸­ä¿®æ­£ã€‚ å¦‚æžœä½ å¸Œæœ›èŠ‚ç‚¹å§‹ç»ˆè¢«è®¤ä¸ºå·²æ›´æ”¹ï¼ˆä¸æŽ¨èï¼Œå› ä¸ºè¿™ä¼šé˜»æ­¢ Comfy ä¼˜åŒ–æ‰§è¡Œæµç¨‹ï¼‰ï¼Œå¯ä»¥ `return float(\"NaN\")`ã€‚è¿™ä¼šè¿”å›žä¸€ä¸ª `NaN`ï¼Œå®ƒä¸Žä»»ä½•å€¼éƒ½ä¸ç›¸ç­‰ï¼Œç”šè‡³ä¸Žå¦ä¸€ä¸ª `NaN` ä¹Ÿä¸ç›¸ç­‰ã€‚ ä¸€ä¸ªå®žé™…æ£€æŸ¥å˜åŒ–çš„å¥½ä¾‹å­æ˜¯å†…ç½®çš„ LoadImage èŠ‚ç‚¹çš„ä»£ç ï¼Œå®ƒä¼šåŠ è½½å›¾ç‰‡å¹¶è¿”å›žå“ˆå¸Œå€¼ï¼š\n\n```\n    @classmethod\n    def IS_CHANGED(s, image):\n        image_path = folder_paths.get_annotated_filepath(image)\n        m = hashlib.sha256()\n        with open(image_path, 'rb') as f:\n            m.update(f.read())\n        return m.digest().hex()\n```\n\n### å…¶ä»–å±žæ€§\n\nè¿˜æœ‰ä¸‰ä¸ªå±žæ€§å¯ä»¥ç”¨æ¥ä¿®æ”¹ Comfy å¯¹èŠ‚ç‚¹çš„é»˜è®¤å¤„ç†æ–¹å¼ã€‚\n\n#### INPUT\\_IS\\_LIST, OUTPUT\\_IS\\_LIST\n\nç”¨äºŽæŽ§åˆ¶æ•°æ®çš„é¡ºåºå¤„ç†ï¼Œè¯¦è§[åŽæ–‡](https://docs.comfy.org/zh-CN/custom-nodes/backend/lists)ã€‚\n\n### VALIDATE\\_INPUTS\n\nå¦‚æžœå®šä¹‰äº†ç±»æ–¹æ³• `VALIDATE_INPUTS`ï¼Œåˆ™åœ¨å·¥ä½œæµå¼€å§‹æ‰§è¡Œå‰ä¼šè¢«è°ƒç”¨ã€‚  \n`VALIDATE_INPUTS` å¦‚æžœè¾“å…¥æœ‰æ•ˆåº”è¿”å›ž `True`ï¼Œå¦åˆ™è¿”å›žä¸€ä¸ªæè¿°é”™è¯¯çš„å­—ç¬¦ä¸²ï¼ˆè¿™ä¼šé˜»æ­¢æ‰§è¡Œï¼‰ã€‚\n\n#### å¸¸é‡æ ¡éªŒ\n\n`VALIDATE_INPUTS` åªä¼šæ”¶åˆ°å…¶ç­¾åä¸­è¯·æ±‚çš„è¾“å…¥ï¼ˆå³ `inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args` è¿”å›žçš„å‚æ•°ï¼‰ã€‚é€šè¿‡è¿™ç§æ–¹å¼æŽ¥æ”¶çš„è¾“å…¥ä¸ä¼šç»è¿‡é»˜è®¤æ ¡éªŒè§„åˆ™ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œå‰ç«¯ä¼šä½¿ç”¨ `foo` è¾“å…¥æŒ‡å®šçš„ `min` å’Œ `max`ï¼Œä½†åŽç«¯ä¸ä¼šå¼ºåˆ¶æ ¡éªŒã€‚\n\n```\nclass CustomNode:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": { \"foo\" : (\"INT\", {\"min\": 0, \"max\": 10}) },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, foo):\n        # YOLOï¼Œå•¥éƒ½è¡Œï¼\n        return True\n```\n\næ­¤å¤–ï¼Œå¦‚æžœè¯¥å‡½æ•°æŽ¥æ”¶ `**kwargs`ï¼Œåˆ™ä¼šæ”¶åˆ°æ‰€æœ‰å¯ç”¨è¾“å…¥ï¼Œå¹¶ä¸”æ‰€æœ‰è¿™äº›è¾“å…¥éƒ½å°†è·³è¿‡æ ¡éªŒï¼Œå°±åƒæ˜¾å¼æŒ‡å®šä¸€æ ·ã€‚\n\n#### ç±»åž‹æ ¡éªŒ\n\nå¦‚æžœ `VALIDATE_INPUTS` æ–¹æ³•æŽ¥æ”¶ä¸€ä¸ªåä¸º `input_types` çš„å‚æ•°ï¼Œåˆ™ä¼šä¼ å…¥ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºæ¯ä¸ªè¿žæŽ¥åˆ°å…¶ä»–èŠ‚ç‚¹è¾“å‡ºçš„è¾“å…¥åï¼Œå€¼ä¸ºè¯¥è¾“å‡ºçš„ç±»åž‹ã€‚ å½“å­˜åœ¨æ­¤å‚æ•°æ—¶ï¼Œæ‰€æœ‰è¾“å…¥ç±»åž‹çš„é»˜è®¤æ ¡éªŒéƒ½ä¼šè¢«è·³è¿‡ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªåˆ©ç”¨å‰ç«¯å…è®¸æŒ‡å®šå¤šç§ç±»åž‹çš„ä¾‹å­ï¼š\n\n```\nclass AddNumbers:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"input1\" : (\"INT,FLOAT\", {\"min\": 0, \"max\": 1000}),\n                \"input2\" : (\"INT,FLOAT\", {\"min\": 0, \"max\": 1000})\n            },\n        }\n\n    @classmethod\n    def VALIDATE_INPUTS(cls, input_types):\n        # input1 å’Œ input2 çš„ min/max ä»ç„¶ä¼šè¢«æ ¡éªŒï¼Œå› ä¸º\n        # æˆ‘ä»¬æ²¡æœ‰å°† `input1` æˆ– `input2` ä½œä¸ºå‚æ•°æŽ¥æ”¶\n        if input_types[\"input1\"] not in (\"INT\", \"FLOAT\"):\n            return \"input1 å¿…é¡»æ˜¯ INT æˆ– FLOAT ç±»åž‹\"\n        if input_types[\"input2\"] not in (\"INT\", \"FLOAT\"):\n            return \"input2 å¿…é¡»æ˜¯ INT æˆ– FLOAT ç±»åž‹\"\n        return True\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/registry/specifications",
  "markdown": "# pyproject.toml - ComfyUI\n\n## è§„èŒƒ\n\n`pyproject.toml` æ–‡ä»¶åŒ…å«ä¸¤ä¸ªä¸»è¦çš„ ComfyUI è‡ªå®šä¹‰èŠ‚ç‚¹éƒ¨åˆ†ï¼š`[project]` å’Œ `[tool.comfy]`ã€‚ä»¥ä¸‹æ˜¯æ¯ä¸ªéƒ¨åˆ†çš„è§„èŒƒã€‚\n\n## \\[project\\] éƒ¨åˆ†\n\n### nameï¼ˆå¿…éœ€ï¼‰\n\nèŠ‚ç‚¹ ID å”¯ä¸€æ ‡è¯†è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œå¹¶å°†ç”¨äºŽæ³¨å†Œè¡¨ä¸­çš„ URLã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡å¼•ç”¨æ­¤åç§°æ¥å®‰è£…èŠ‚ç‚¹ï¼š\n\n```\ncomfy node install <node-id>\n```\n\n**è¦æ±‚:**\n\n*   å¿…é¡»å°äºŽ 100 ä¸ªå­—ç¬¦\n*   åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€è¿žå­—ç¬¦ã€ä¸‹åˆ’çº¿å’Œå¥ç‚¹\n*   ä¸èƒ½æœ‰è¿žç»­çš„ç‰¹æ®Šå­—ç¬¦\n*   ä¸èƒ½ä»¥æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦å¼€å¤´\n*   ä¸åŒºåˆ†å¤§å°å†™æ¯”è¾ƒ\n\n**æœ€ä½³å®žè·µ:**\n\n*   ä½¿ç”¨ç®€çŸ­ã€æè¿°æ€§çš„åç§°\n*   ä¸è¦åœ¨åç§°ä¸­åŒ…å« â€œComfyUIâ€\n*   ä½¿å…¶æ˜“äºŽè®°å¿†å’Œè¾“å…¥\n\n**Examples:**\n\n```\nname = \"image-processor\"      # âœ… Good: Simple and clear\nname = \"super-resolution\"     # âœ… Good: Describes functionality\nname = \"ComfyUI-enhancer\"    # âŒ Bad: Includes ComfyUI\nname = \"123-tool\"            # âŒ Bad: Starts with number\n```\n\næ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[å®˜æ–¹ Python æ–‡æ¡£](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#name)ã€‚\n\n### versionï¼ˆå¿…éœ€ï¼‰\n\nä½¿ç”¨ [è¯­ä¹‰åŒ–ç‰ˆæœ¬æŽ§åˆ¶](https://semver.org/) å¹¶åŒ…å«ä¸‰ä¸ªæ•°å­—çš„ç‰ˆæœ¬å· X.Y.Zï¼š\n\n*   Xï¼ˆ**MAJOR**ï¼‰ï¼šé‡å¤§æ›´æ”¹\n*   Yï¼ˆ**MINOR**ï¼‰ï¼šæ–°åŠŸèƒ½ï¼ˆå‘åŽå…¼å®¹ï¼‰\n*   Z (**PATCH**): Bug fixes\n\n**Examples:**\n\n```\nversion = \"1.0.0\"    # åˆå§‹ç‰ˆæœ¬\nversion = \"1.1.0\"    # æ·»åŠ æ–°åŠŸèƒ½\nversion = \"1.1.1\"    # ä¿®å¤é”™è¯¯\nversion = \"2.0.0\"    # é‡å¤§æ›´æ”¹\n```\n\n### licenseï¼ˆå¯é€‰ï¼‰\n\næŒ‡å®šè‡ªå®šä¹‰èŠ‚ç‚¹çš„è®¸å¯è¯ã€‚å¯ä»¥ä»¥ä¸¤ç§æ–¹å¼æŒ‡å®šï¼š\n\n1.  **æ–‡ä»¶å¼•ç”¨ï¼š**\n\n```\nlicense = { file = \"LICENSE\" }     # âœ… æŒ‡å‘ LICENSE æ–‡ä»¶\nlicense = { file = \"LICENSE.txt\" } # âœ… æŒ‡å‘ LICENSE.txt æ–‡ä»¶\nlicense = \"LICENSE\"                # âŒ æ ¼å¼é”™è¯¯\n```\n\n2.  **è®¸å¯è¯åç§°ï¼š**\n\n```\nlicense = { text = \"MIT License\" }  # âœ… æ­£ç¡®æ ¼å¼\nlicense = { text = \"Apache-2.0\" }   # âœ… æ­£ç¡®æ ¼å¼\nlicense = \"MIT LICENSE\"             # âŒ æ ¼å¼é”™è¯¯\n```\n\nå¸¸è§è®¸å¯è¯ï¼š[MIT](https://opensource.org/license/mit), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html), [Apache](https://www.apache.org/licenses/LICENSE-2.0)\n\n### descriptionï¼ˆæŽ¨èï¼‰\n\nè‡ªå®šä¹‰èŠ‚ç‚¹çš„ç®€è¦æè¿°ã€‚\n\n```\ndescription = \"A super resolution node for enhancing image quality\"\n```\n\n### repository (å¿…éœ€)\n\nç›¸å…³èµ„æºçš„é“¾æŽ¥ï¼š\n\n```\n[project.urls]\nRepository = \"https://github.com/username/repository\"\n```\n\n### urlsï¼ˆæŽ¨èï¼‰\n\nç›¸å…³èµ„æºçš„é“¾æŽ¥ï¼š\n\n```\n[project.urls]\nDocumentation = \"https://github.com/username/repository/wiki\"\n\"Bug Tracker\" = \"https://github.com/username/repository/issues\"\n```\n\n### requires-pythonï¼ˆæŽ¨èï¼‰\n\næŒ‡å®šè‡ªå®šä¹‰èŠ‚ç‚¹æ”¯æŒçš„ Python ç‰ˆæœ¬ï¼š\n\n```\nrequires-python = \">=3.8\"        # Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬\nrequires-python = \">=3.8,<3.11\"  # Python 3.8 åˆ° 3.11 ä¹‹é—´ï¼ˆä¸åŒ…æ‹¬ 3.11ï¼‰\n```\n\n### å‰ç«¯ç‰ˆæœ¬å…¼å®¹æ€§ï¼ˆå¯é€‰ï¼‰\n\nå¦‚æžœä½ çš„èŠ‚ç‚¹å¯¹ ComfyUI å‰ç«¯ç‰ˆæœ¬æœ‰ç‰¹å®šè¦æ±‚ï¼Œä½ å¯ä»¥ä½¿ç”¨ `comfyui-frontend-package` ä¾èµ–é¡¹æ¥æŒ‡å®šã€‚è¯¥åŒ…å‘å¸ƒåœ¨ [PyPI](https://pypi.org/project/comfyui-frontend-package/) ä¸Šã€‚ åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ä½¿ç”¨æ­¤å­—æ®µï¼š\n\n*   ä½ çš„è‡ªå®šä¹‰èŠ‚ç‚¹ä½¿ç”¨äº†ç‰¹å®šç‰ˆæœ¬ä¸­å¼•å…¥çš„å‰ç«¯ API\n*   ä½ å‘çŽ°äº†ä½ çš„èŠ‚ç‚¹ä¸ŽæŸäº›å‰ç«¯ç‰ˆæœ¬ä¹‹é—´çš„ä¸å…¼å®¹æ€§\n*   ä½ çš„èŠ‚ç‚¹éœ€è¦ä»…åœ¨è¾ƒæ–°å‰ç«¯ç‰ˆæœ¬ä¸­å¯ç”¨çš„ç‰¹å®š UI åŠŸèƒ½\n\n```\n[project]\ndependencies = [\n    \"comfyui-frontend-package>=1.20.0\"       # éœ€è¦å‰ç«¯ 1.20.0 æˆ–æ›´æ–°ç‰ˆæœ¬\n    \"comfyui-frontend-package<=1.21.6\"       # é™åˆ¶å‰ç«¯ç‰ˆæœ¬æœ€é«˜åˆ° 1.21.6\n    \"comfyui-frontend-package>=1.19,<1.22\"   # é€‚ç”¨äºŽå‰ç«¯ 1.19 åˆ° 1.21.x\n    \"comfyui-frontend-package~=1.20.0\"       # å…¼å®¹ 1.20.x ä½†ä¸åŒ…æ‹¬ 1.21.0\n    \"comfyui-frontend-package!=1.21.3\"       # é€‚ç”¨äºŽä»»ä½•ç‰ˆæœ¬ï¼Œé™¤äº† 1.21.3\n]\n```\n\n### classifiersï¼ˆæŽ¨èï¼‰\n\nä½¿ç”¨åˆ†ç±»å™¨æŒ‡å®šæ“ä½œç³»ç»Ÿçš„å…¼å®¹æ€§å’ŒGPUåŠ é€Ÿå™¨ã€‚è¿™ä¸ªä¿¡æ¯ç”¨äºŽå¸®åŠ©ç”¨æˆ·æ‰¾åˆ°é€‚åˆä»–ä»¬ç³»ç»Ÿçš„èŠ‚ç‚¹ã€‚\n\n```\n[project]\nclassifiers = [\n    # é€‚ç”¨äºŽæ‰€æœ‰æ“ä½œç³»ç»Ÿçš„èŠ‚ç‚¹\n    \"Operating System :: OS Independent\",\n\n    # æˆ–è€…å¯¹äºŽç‰¹å®šæ“ä½œç³»ç»Ÿçš„èŠ‚ç‚¹ï¼ŒæŒ‡å®šæ”¯æŒçš„ç³»ç»Ÿï¼š\n    \"Operating System :: Microsoft :: Windows\",  # Windows specific\n    \"Operating System :: POSIX :: Linux\",  # Linux specific\n    \"Operating System :: MacOS\",  # macOS specific\n    \n    # GPU åŠ é€Ÿå™¨æ”¯æŒ\n    \"Environment :: GPU :: NVIDIA CUDA\",    # NVIDIA CUDA æ”¯æŒ\n    \"Environment :: GPU :: AMD ROCm\",       # AMD ROCm æ”¯æŒ\n    \"Environment :: GPU :: Intel Arc\",      # Intel Arc æ”¯æŒ\n    \"Environment :: NPU :: Huawei Ascend\",  # åŽä¸ºæ˜‡è…¾æ”¯æŒ\n    \"Environment :: GPU :: Apple Metal\",    # Apple Metal æ”¯æŒ\n]\n```\n\n### PublisherIdï¼ˆå¿…éœ€ï¼‰\n\nä½ çš„å”¯ä¸€å‘å¸ƒè€…æ ‡è¯†ç¬¦ï¼Œé€šå¸¸ä¸Žæ‚¨çš„ GitHub ç”¨æˆ·ååŒ¹é…ã€‚ **Examples:**\n\n```\nPublisherId = \"john-doe\"        # âœ… åŒ¹é… GitHub ç”¨æˆ·å\nPublisherId = \"image-wizard\"    # âœ… å”¯ä¸€æ ‡è¯†ç¬¦\n```\n\n### DisplayNameï¼ˆå¯é€‰ï¼‰\n\nä½ çš„è‡ªå®šä¹‰èŠ‚ç‚¹çš„ç”¨æˆ·å‹å¥½åç§°ã€‚\n\n```\nDisplayName = \"Super Resolution Node\"\n```\n\n### Iconï¼ˆå¯é€‰ï¼‰\n\nä½ çš„è‡ªå®šä¹‰èŠ‚ç‚¹çš„å›¾æ ‡ URLï¼Œå°†åœ¨ ComfyUI Registry å’Œ ComfyUI-Manager ä¸­æ˜¾ç¤ºã€‚ **è¦æ±‚ï¼š**\n\n*   æ–‡ä»¶ç±»åž‹ï¼šSVG, PNG, JPG, æˆ– GIF\n*   æœ€å¤§åˆ†è¾¨çŽ‡ï¼š400px Ã— 400px\n*   é•¿å®½æ¯”åº”è¯¥æ˜¯æ­£æ–¹å½¢\n\n```\nIcon = \"https://raw.githubusercontent.com/username/repo/main/icon.png\"\n```\n\nURL æŒ‡å‘ä¸€ä¸ªè¾ƒå¤§çš„æ¨ªå¹…å›¾åƒï¼Œå°†åœ¨ ComfyUI Registry å’Œ ComfyUI-Manager ä¸­æ˜¾ç¤ºã€‚ **è¦æ±‚ï¼š**\n\n*   æ–‡ä»¶ç±»åž‹ï¼šSVG, PNG, JPG, æˆ– GIF\n*   é•¿å®½æ¯”ï¼š21:9\n\n```\nBanner = \"https://raw.githubusercontent.com/username/repo/main/banner.png\"\n```\n\n### requires-comfyuiï¼ˆå¯é€‰ï¼‰\n\næŒ‡å®šä½ çš„èŠ‚ç‚¹å…¼å®¹çš„ ComfyUI ç‰ˆæœ¬ã€‚è¿™æœ‰åŠ©äºŽç”¨æˆ·ç¡®ä¿ä»–ä»¬å®‰è£…äº†æ­£ç¡®ç‰ˆæœ¬çš„ ComfyUIã€‚ **æ”¯æŒçš„æ“ä½œç¬¦ï¼š** `<`, `>`, `<=`, `>=`, `~=`, `<>`, `!=` å’ŒèŒƒå›´\n\n```\nrequires-comfyui = \">=1.0.0\"        # ComfyUI 1.0.0 æˆ–æ›´é«˜ç‰ˆæœ¬\nrequires-comfyui = \">=1.0.0,<2.0.0\"  # ComfyUI 1.0.0 åˆ° 2.0.0 ä¹‹é—´ï¼ˆä¸åŒ…æ‹¬ 2.0.0ï¼‰\nrequires-comfyui = \"~=1.0.0\"         # å…¼å®¹ç‰ˆæœ¬ï¼š1.0.0 æˆ–æ›´æ–°ç‰ˆæœ¬ï¼Œä½†ä¸åŒ…æ‹¬ 2.0.0\nrequires-comfyui = \"!=1.2.3\"         # ä»»ä½•ç‰ˆæœ¬ï¼Œé™¤äº† 1.2.3\nrequires-comfyui = \">0.1.3,<1.0.0\"   # å¤§äºŽ 0.1.3 ä¸”å°äºŽ 1.0.0\n```\n\n### includesï¼ˆå¯é€‰ï¼‰\n\næŒ‡å®šæ˜¯å¦å¼ºåˆ¶åŒ…å«æŸäº›ç‰¹å®šæ–‡ä»¶å¤¹ã€‚å¯¹äºŽä¸€äº›æƒ…å†µï¼Œä¾‹å¦‚åœ¨ frontend é¡¹ç›®ä¸­çš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œæœ€ç»ˆæ‰“åŒ…è¾“å‡ºçš„æ–‡ä»¶å¤¹å¯èƒ½ä¼šè¢«åŒ…å«åœ¨ .gitignore ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å¼ºåˆ¶åŒ…å«å®ƒä»¥ç”¨äºŽæ³¨å†Œè¡¨ä½¿ç”¨ã€‚\n\n## å®Œæ•´ç¤ºä¾‹\n\n```\n[project]\nname = \"super-resolution-node\"\nversion = \"1.0.0\"\ndescription = \"Enhance image quality using advanced super resolution techniques\"\nlicense = { file = \"LICENSE\" }\nrequires-python = \">=3.8\"\ndependencies = [\n    \"comfyui-frontend-package<=1.21.6\"  # å‰ç«¯ç‰ˆæœ¬å…¼å®¹æ€§\n]\nclassifiers = [\n    \"Operating System :: OS Independent\"  # é€‚ç”¨äºŽæ‰€æœ‰æ“ä½œç³»ç»Ÿ\n]\ndynamic = [\"dependencies\"]\n\n[tool.setuptools.dynamic]\ndependencies = {file = [\"requirements.txt\"]}\n\n[project.urls]\nRepository = \"https://github.com/username/super-resolution-node\"\nDocumentation = \"https://github.com/username/super-resolution-node/wiki\"\n\"Bug Tracker\" = \"https://github.com/username/super-resolution-node/issues\"\n\n[tool.comfy]\nPublisherId = \"image-wizard\"\nDisplayName = \"Super Resolution Node\"\nIcon = \"https://raw.githubusercontent.com/username/super-resolution-node/main/icon.png\"\nBanner = \"https://raw.githubusercontent.com/username/super-resolution-node/main/banner.png\"\nrequires-comfyui = \">=1.0.0\"  # ComfyUI ç‰ˆæœ¬å…¼å®¹æ€§\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/specs/nodedef_json",
  "markdown": "# èŠ‚ç‚¹å®šä¹‰ JSON - ComfyUI\n\n```\n{\n  \"$ref\": \"#/definitions/ComfyNodeDefV2\",\n  \"definitions\": {\n    \"ComfyNodeDefV2\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"inputs\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {\n            \"anyOf\": [\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": \"number\"\n                        }\n                      }\n                    ]\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"min\": {\n                    \"type\": \"number\"\n                  },\n                  \"max\": {\n                    \"type\": \"number\"\n                  },\n                  \"step\": {\n                    \"type\": \"number\"\n                  },\n                  \"display\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"slider\",\n                      \"number\",\n                      \"knob\"\n                    ]\n                  },\n                  \"control_after_generate\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"INT\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"array\",\n                        \"items\": {\n                          \"type\": \"number\"\n                        }\n                      }\n                    ]\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"min\": {\n                    \"type\": \"number\"\n                  },\n                  \"max\": {\n                    \"type\": \"number\"\n                  },\n                  \"step\": {\n                    \"type\": \"number\"\n                  },\n                  \"display\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"slider\",\n                      \"number\",\n                      \"knob\"\n                    ]\n                  },\n                  \"round\": {\n                    \"anyOf\": [\n                      {\n                        \"type\": \"number\"\n                      },\n                      {\n                        \"type\": \"boolean\",\n                        \"const\": false\n                      }\n                    ]\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"FLOAT\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"label_on\": {\n                    \"type\": \"string\"\n                  },\n                  \"label_off\": {\n                    \"type\": \"string\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"BOOLEAN\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {\n                    \"type\": \"string\"\n                  },\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"multiline\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"dynamicPrompts\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"defaultVal\": {\n                    \"type\": \"string\"\n                  },\n                  \"placeholder\": {\n                    \"type\": \"string\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"STRING\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {},\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"control_after_generate\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"image_upload\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"image_folder\": {\n                    \"type\": \"string\",\n                    \"enum\": [\n                      \"input\",\n                      \"output\",\n                      \"temp\"\n                    ]\n                  },\n                  \"allow_batch\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"video_upload\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"remote\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                      \"route\": {\n                        \"anyOf\": [\n                          {\n                            \"type\": \"string\",\n                            \"format\": \"uri\"\n                          },\n                          {\n                            \"type\": \"string\",\n                            \"pattern\": \"^\\\\/\"\n                          }\n                        ]\n                      },\n                      \"refresh\": {\n                        \"anyOf\": [\n                          {\n                            \"type\": \"number\",\n                            \"minimum\": -9007199254740991,\n                            \"maximum\": 9007199254740991\n                          },\n                          {\n                            \"type\": \"number\",\n                            \"maximum\": 9007199254740991,\n                            \"minimum\": -9007199254740991\n                          }\n                        ]\n                      },\n                      \"response_key\": {\n                        \"type\": \"string\"\n                      },\n                      \"query_params\": {\n                        \"type\": \"object\",\n                        \"additionalProperties\": {\n                          \"type\": \"string\"\n                        }\n                      },\n                      \"refresh_button\": {\n                        \"type\": \"boolean\"\n                      },\n                      \"control_after_refresh\": {\n                        \"type\": \"string\",\n                        \"enum\": [\n                          \"first\",\n                          \"last\"\n                        ]\n                      },\n                      \"timeout\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0\n                      },\n                      \"max_retries\": {\n                        \"type\": \"number\",\n                        \"minimum\": 0\n                      }\n                    },\n                    \"required\": [\n                      \"route\"\n                    ],\n                    \"additionalProperties\": false\n                  },\n                  \"options\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                      \"type\": [\n                        \"string\",\n                        \"number\"\n                      ]\n                    }\n                  },\n                  \"type\": {\n                    \"type\": \"string\",\n                    \"const\": \"COMBO\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              },\n              {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"default\": {},\n                  \"defaultInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"forceInput\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"tooltip\": {\n                    \"type\": \"string\"\n                  },\n                  \"hidden\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"advanced\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"rawLink\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"lazy\": {\n                    \"type\": \"boolean\"\n                  },\n                  \"type\": {\n                    \"type\": \"string\"\n                  },\n                  \"name\": {\n                    \"type\": \"string\"\n                  },\n                  \"isOptional\": {\n                    \"type\": \"boolean\"\n                  }\n                },\n                \"required\": [\n                  \"type\",\n                  \"name\"\n                ],\n                \"additionalProperties\": true\n              }\n            ]\n          }\n        },\n        \"outputs\": {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"index\": {\n                \"type\": \"number\"\n              },\n              \"name\": {\n                \"type\": \"string\"\n              },\n              \"type\": {\n                \"type\": \"string\"\n              },\n              \"is_list\": {\n                \"type\": \"boolean\"\n              },\n              \"options\": {\n                \"type\": \"array\"\n              },\n              \"tooltip\": {\n                \"type\": \"string\"\n              }\n            },\n            \"required\": [\n              \"index\",\n              \"name\",\n              \"type\",\n              \"is_list\"\n            ],\n            \"additionalProperties\": false\n          }\n        },\n        \"hidden\": {\n          \"type\": \"object\",\n          \"additionalProperties\": {}\n        },\n        \"name\": {\n          \"type\": \"string\"\n        },\n        \"display_name\": {\n          \"type\": \"string\"\n        },\n        \"description\": {\n          \"type\": \"string\"\n        },\n        \"category\": {\n          \"type\": \"string\"\n        },\n        \"output_node\": {\n          \"type\": \"boolean\"\n        },\n        \"python_module\": {\n          \"type\": \"string\"\n        },\n        \"deprecated\": {\n          \"type\": \"boolean\"\n        },\n        \"experimental\": {\n          \"type\": \"boolean\"\n        }\n      },\n      \"required\": [\n        \"inputs\",\n        \"outputs\",\n        \"name\",\n        \"display_name\",\n        \"description\",\n        \"category\",\n        \"output_node\",\n        \"python_module\"\n      ],\n      \"additionalProperties\": false\n    }\n  },\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/lists",
  "markdown": "# æ•°æ®åˆ—è¡¨ - ComfyUI\n\n## é•¿åº¦ä¸ºä¸€çš„å¤„ç†\n\nåœ¨å†…éƒ¨ï¼ŒComfy æœåŠ¡å™¨å°†ä»Žä¸€ä¸ªèŠ‚ç‚¹æµå‘ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„æ•°æ®è¡¨ç¤ºä¸º Python `list`ï¼Œé€šå¸¸é•¿åº¦ä¸º 1ï¼Œç±»åž‹ä¸ºç›¸å…³çš„æ•°æ®ç±»åž‹ã€‚ åœ¨æ­£å¸¸æ“ä½œä¸­ï¼Œå½“ä¸€ä¸ªèŠ‚ç‚¹è¿”å›žè¾“å‡ºæ—¶ï¼Œè¾“å‡º `tuple` ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½ä¼šè¢«å•ç‹¬åŒ…è£¹åœ¨ä¸€ä¸ªé•¿åº¦ä¸º 1 çš„åˆ—è¡¨ä¸­ï¼›ç„¶åŽå½“ä¸‹ä¸€ä¸ªèŠ‚ç‚¹è¢«è°ƒç”¨æ—¶ï¼Œæ•°æ®ä¼šè¢«è§£åŒ…å¹¶ä¼ é€’ç»™ä¸»å‡½æ•°ã€‚\n\n## åˆ—è¡¨å¤„ç†\n\nåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå•ä¸ªå·¥ä½œæµä¼šå¤„ç†å¤šä¸ªæ•°æ®å®žä¾‹ï¼Œæ­¤æ—¶å†…éƒ¨æ•°æ®å°†æ˜¯åŒ…å«å¤šä¸ªæ•°æ®å®žä¾‹çš„åˆ—è¡¨ã€‚ ä¾‹å¦‚ï¼Œé€ä¸ªå¤„ç†ä¸€ç³»åˆ—å›¾åƒä»¥é¿å… VRAM ä¸è¶³ï¼Œæˆ–å¤„ç†ä¸åŒå°ºå¯¸çš„å›¾åƒã€‚ é»˜è®¤æƒ…å†µä¸‹ï¼ŒComfy ä¼šæŒ‰é¡ºåºå¤„ç†åˆ—è¡¨ä¸­çš„å€¼ï¼š\n\n*   å¦‚æžœè¾“å…¥æ˜¯ä¸åŒé•¿åº¦çš„ `list`ï¼Œè¾ƒçŸ­çš„ä¼šé€šè¿‡é‡å¤æœ€åŽä¸€ä¸ªå€¼è¿›è¡Œå¡«å……\n*   ä¸»æ–¹æ³•ä¼šé’ˆå¯¹è¾“å…¥åˆ—è¡¨ä¸­çš„æ¯ä¸ªå€¼è°ƒç”¨ä¸€æ¬¡\n*   è¾“å‡ºä¹Ÿæ˜¯ `list`ï¼Œæ¯ä¸ªè¾“å‡ºçš„é•¿åº¦ä¸Žæœ€é•¿çš„è¾“å…¥ç›¸åŒ\n\nç›¸å…³ä»£ç å¯åœ¨ `execution.py` çš„ `map_node_over_list` æ–¹æ³•ä¸­æ‰¾åˆ°ã€‚ ç„¶è€Œï¼Œç”±äºŽ Comfy ä¼šå°†èŠ‚ç‚¹è¾“å‡ºåŒ…è£¹ä¸ºé•¿åº¦ä¸º 1 çš„ `list`ï¼Œå¦‚æžœè‡ªå®šä¹‰èŠ‚ç‚¹è¿”å›žçš„ `tuple` ä¸­åŒ…å«ä¸€ä¸ª `list`ï¼Œè¯¥ `list` ä¼šè¢«åŒ…è£¹å¹¶ä½œä¸ºå•ä¸ªæ•°æ®å¤„ç†ã€‚ ä¸ºäº†å‘Šè¯‰ Comfy è¿”å›žçš„åˆ—è¡¨ä¸åº”è¢«åŒ…è£¹ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ç³»åˆ—æ•°æ®è¿›è¡Œé¡ºåºå¤„ç†ï¼ŒèŠ‚ç‚¹åº”æä¾›ä¸€ä¸ªç±»å±žæ€§ `OUTPUT_IS_LIST`ï¼Œå®ƒæ˜¯ä¸€ä¸ªä¸Ž `RETURN_TYPES` é•¿åº¦ç›¸åŒçš„ `tuple[bool]`ï¼Œç”¨äºŽæŒ‡å®šå“ªäº›è¾“å‡ºåº”å¦‚æ­¤å¤„ç†ã€‚ èŠ‚ç‚¹ä¹Ÿå¯ä»¥é‡å†™é»˜è®¤çš„è¾“å…¥è¡Œä¸ºï¼Œåœ¨ä¸€æ¬¡è°ƒç”¨ä¸­æŽ¥æ”¶æ•´ä¸ªåˆ—è¡¨ã€‚è¿™å¯ä»¥é€šè¿‡è®¾ç½®ç±»å±žæ€§ `INPUT_IS_LIST` ä¸º `True` å®žçŽ°ã€‚ ä»¥ä¸‹æ˜¯å†…ç½®èŠ‚ç‚¹çš„ä¸€ä¸ªï¼ˆå¸¦æ³¨é‡Šçš„ï¼‰ç¤ºä¾‹â€”â€”`ImageRebatch` æŽ¥æ”¶ä¸€ä¸ªæˆ–å¤šä¸ªå›¾åƒæ‰¹æ¬¡ï¼ˆä½œä¸ºåˆ—è¡¨æŽ¥æ”¶ï¼Œå› ä¸º `INPUT_IS_LIST = True`ï¼‰ï¼Œå¹¶å°†å®ƒä»¬é‡æ–°åˆ†æ‰¹ä¸ºæ‰€éœ€å¤§å°çš„æ‰¹æ¬¡ã€‚\n\n```\n\nclass ImageRebatch:\n    @classmethod\n    def INPUT_TYPES(s):\n        return {\"required\": { \"images\": (\"IMAGE\",),\n                              \"batch_size\": (\"INT\", {\"default\": 1, \"min\": 1, \"max\": 4096}) }}\n    RETURN_TYPES = (\"IMAGE\",)\n    INPUT_IS_LIST = True\n    OUTPUT_IS_LIST = (True, )\n    FUNCTION = \"rebatch\"\n    CATEGORY = \"image/batch\"\n\n    def rebatch(self, images, batch_size):\n        batch_size = batch_size[0]    # æ‰€æœ‰è¾“å…¥éƒ½æ˜¯åˆ—è¡¨ï¼Œæ‰€ä»¥ batch_size æ˜¯ list[int]\n\n        output_list = []\n        all_images = []\n        for img in images:                    # æ¯ä¸ª img æ˜¯ä¸€ä¸ªå›¾åƒæ‰¹æ¬¡\n            for i in range(img.shape[0]):     # æ¯ä¸ª i æ˜¯ä¸€å¼ å•ç‹¬çš„å›¾åƒ\n                all_images.append(img[i:i+1])\n\n        for i in range(0, len(all_images), batch_size): # æŒ‰ batch_size åˆ†å—ï¼Œæ¯å—ç»„æˆä¸€ä¸ªæ–°æ‰¹æ¬¡\n            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))  # å¦‚æžœå›¾åƒæ‰¹æ¬¡å®½é«˜ä¸åŒä¼šæŠ¥é”™ï¼\n\n        return (output_list,)\n```\n\n#### INPUT\\_IS\\_LIST"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/snippets",
  "markdown": "# å¸¦æ³¨é‡Šçš„ç¤ºä¾‹ - ComfyUI\n\nä¸æ–­å¢žé•¿çš„ç¤ºä¾‹ä»£ç ç‰‡æ®µé›†åˆâ€¦â€¦\n\n## å›¾åƒä¸Žè’™ç‰ˆ\n\n### åŠ è½½å›¾åƒ\n\nå°†å›¾åƒåŠ è½½ä¸ºæ‰¹é‡å¤§å°ä¸º1ï¼ˆåŸºäºŽ `nodes.py` ä¸­çš„ `LoadImage` æºä»£ç ï¼‰\n\n```\ni = Image.open(image_path)\ni = ImageOps.exif_transpose(i)\nif i.mode == 'I':\n    i = i.point(lambda i: i * (1 / 255))\nimage = i.convert(\"RGB\")\nimage = np.array(image).astype(np.float32) / 255.0\nimage = torch.from_numpy(image)[None,]\n```\n\n### ä¿å­˜å›¾åƒæ‰¹é‡\n\nä¿å­˜ä¸€æ‰¹å›¾åƒï¼ˆåŸºäºŽ `nodes.py` ä¸­çš„ `SaveImage` æºä»£ç ï¼‰\n\n```\nfor (batch_number, image) in enumerate(images):\n    i = 255. * image.cpu().numpy()\n    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n    filepath = # some path that takes the batch number into account\n    img.save(filepath)\n```\n\n### åè½¬è’™ç‰ˆ\n\nåè½¬è’™ç‰ˆæ˜¯ä¸€ä¸ªç®€å•çš„è¿‡ç¨‹ã€‚ç”±äºŽè’™ç‰ˆå·²è¢«å½’ä¸€åŒ–åˆ° \\[0,1\\] åŒºé—´ï¼š\n\n### å°†è’™ç‰ˆè½¬æ¢ä¸ºå›¾åƒå½¢çŠ¶\n\n```\n# æˆ‘ä»¬éœ€è¦ [B,H,W,C]ï¼Œå…¶ä¸­ C = 1\nif len(mask.shape)==2: # å½“å‰ä¸º [H,W]ï¼Œæ’å…¥ B å’Œ C ä½œä¸ºç¬¬1ç»´\n    mask = mask[None,:,:,None]\nelif len(mask.shape)==3 and mask.shape[2]==1: # å½“å‰ä¸º [H,W,C]\n    mask = mask[None,:,:,:]\nelif len(mask.shape)==3:                      # å½“å‰ä¸º [B,H,W]\n    mask = mask[:,:,:,None]\n```\n\n### å°†è’™ç‰ˆç”¨ä½œé€æ˜Žå±‚\n\nå½“ç”¨äºŽä¿®å¤æˆ–åˆ†å‰²ç­‰ä»»åŠ¡æ—¶ï¼Œè’™ç‰ˆçš„å€¼æœ€ç»ˆä¼šè¢«å››èˆäº”å…¥ä¸ºæœ€æŽ¥è¿‘çš„æ•´æ•°ï¼Œä½¿å…¶ä¸ºäºŒå€¼â€”â€”0è¡¨ç¤ºè¦å¿½ç•¥çš„åŒºåŸŸï¼Œ1è¡¨ç¤ºè¦å¤„ç†çš„åŒºåŸŸã€‚ä½†åœ¨è’™ç‰ˆä¼ é€’åˆ°è¿™äº›èŠ‚ç‚¹ä¹‹å‰ï¼Œè¿™ä¸€æ­¥ä¸ä¼šå‘ç”Ÿã€‚è¿™ç§çµæ´»æ€§å…è®¸ä½ åƒåœ¨æ•°ç æ‘„å½±ä¸­é‚£æ ·ï¼Œå°†è’™ç‰ˆç”¨ä½œé€æ˜Žå±‚ï¼š\n\n```\n# å°†è’™ç‰ˆåè½¬å›žåŽŸå§‹é€æ˜Žå±‚\nmask = 1.0 - mask\n\n# æ‰©å±• `C`ï¼ˆé€šé“ï¼‰ç»´åº¦\nmask = mask.unsqueeze(-1)\n\n# æ²¿ `C` ç»´æ‹¼æŽ¥ï¼ˆcatï¼‰\nrgba_image = torch.cat((rgb_image, mask), dim=-1)\n```\n\n## å™ªå£°\n\n### åˆ›å»ºå™ªå£°å˜ä½“\n\nä»¥ä¸‹æ˜¯ä¸€ä¸ªåˆ›å»ºæ··åˆä¸¤ä¸ªå™ªå£°æºçš„å™ªå£°å¯¹è±¡çš„ç¤ºä¾‹ã€‚é€šè¿‡è°ƒæ•´ `weight2`ï¼Œå¯ä»¥ç”¨æ¥ç”Ÿæˆè½»å¾®ä¸åŒçš„å™ªå£°å˜ä½“ã€‚\n\n```\nclass Noise_MixedNoise:\n    def __init__(self, nosie1, noise2, weight2):\n        self.noise1  = noise1\n        self.noise2  = noise2\n        self.weight2 = weight2\n\n    @property\n    def seed(self): return self.noise1.seed\n\n    def generate_noise(self, input_latent:torch.Tensor) -> torch.Tensor:\n        noise1 = self.noise1.generate_noise(input_latent)\n        noise2 = self.noise2.generate_noise(input_latent)\n        return noise1 * (1.0-self.weight2) + noise2 * (self.weight2)\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN",
  "markdown": "# ComfyUI å®˜æ–¹æ–‡æ¡£ - ComfyUI\n\n## å…³äºŽ ComfyUI\n\nç”± [comfyanonymous](https://github.com/comfyanonymous) å’Œå…¶ä»–[è´¡çŒ®è€…](https://github.com/comfyanonymous/ComfyUI/graphs/contributors)å¼€å‘ã€‚\n\n*   **ComfyUI** æ˜¯ä¸€ä¸ªåŸºäºŽèŠ‚ç‚¹çš„ç”Ÿæˆå¼ AI ç•Œé¢å’ŒæŽ¨ç†å¼•æ“Ž\n*   ç”¨æˆ·å¯ä»¥é€šè¿‡èŠ‚ç‚¹ç»„åˆå„ç§ AI æ¨¡åž‹å’Œæ“ä½œï¼Œå®žçŽ°é«˜åº¦å¯å®šåˆ¶å’Œå¯æŽ§çš„å†…å®¹ç”Ÿæˆ\n*   ComfyUI å®Œå…¨å¼€æºï¼Œå¯ä»¥åœ¨æœ¬åœ°è®¾å¤‡ä¸Šè¿è¡Œ"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/tensors",
  "markdown": "# ä½¿ç”¨ torch.Tensor - ComfyUI\n\n## pytorchã€å¼ é‡ä¸Ž torch.Tensor\n\nComfy çš„æ‰€æœ‰æ ¸å¿ƒæ•°å€¼è®¡ç®—éƒ½æ˜¯ç”± [pytorch](https://pytorch.org/) å®Œæˆçš„ã€‚å¦‚æžœä½ çš„è‡ªå®šä¹‰èŠ‚ç‚¹éœ€è¦æ·±å…¥ stable diffusion çš„åº•å±‚ï¼Œä½ å°±éœ€è¦ç†Ÿæ‚‰è¿™ä¸ªåº“ï¼Œè¿™è¿œè¶…æœ¬ç®€ä»‹çš„èŒƒå›´ã€‚ ä¸è¿‡ï¼Œè®¸å¤šè‡ªå®šä¹‰èŠ‚ç‚¹éƒ½éœ€è¦æ“ä½œå›¾åƒã€æ½œå˜é‡å’Œè’™ç‰ˆï¼Œè¿™äº›åœ¨å†…éƒ¨éƒ½è¡¨ç¤ºä¸º `torch.Tensor`ï¼Œå› æ­¤ä½ å¯èƒ½éœ€è¦æ”¶è— [torch.Tensor çš„å®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/tensors.html)ã€‚\n\n### ä»€ä¹ˆæ˜¯å¼ é‡ï¼Ÿ\n\n`torch.Tensor` è¡¨ç¤ºå¼ é‡ï¼Œå¼ é‡æ˜¯å‘é‡æˆ–çŸ©é˜µåœ¨ä»»æ„ç»´åº¦ä¸Šçš„æ•°å­¦æ³›åŒ–ã€‚å¼ é‡çš„ _ç§©_ï¼ˆrankï¼‰æ˜¯å®ƒçš„ç»´åº¦æ•°é‡ï¼ˆæ‰€ä»¥å‘é‡ç§©ä¸º 1ï¼ŒçŸ©é˜µç§©ä¸º 2ï¼‰ï¼›å®ƒçš„ _å½¢çŠ¶_ï¼ˆshapeï¼‰æè¿°äº†æ¯ä¸ªç»´åº¦çš„å¤§å°ã€‚ å› æ­¤ï¼Œä¸€ä¸ª RGB å›¾åƒï¼ˆé«˜ä¸º Hï¼Œå®½ä¸º Wï¼‰å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸‰ç»„æ•°ç»„ï¼ˆæ¯ä¸ªé¢œè‰²é€šé“ä¸€ç»„ï¼‰ï¼Œæ¯ç»„å¤§å°ä¸º H x Wï¼Œå¯ä»¥è¡¨ç¤ºä¸ºå½¢çŠ¶ä¸º `[H,W,3]` çš„å¼ é‡ã€‚åœ¨ Comfy ä¸­ï¼Œå›¾åƒå‡ ä¹Žæ€»æ˜¯ä»¥æ‰¹é‡ï¼ˆbatchï¼‰å½¢å¼å‡ºçŽ°ï¼ˆå³ä½¿æ‰¹é‡ä¸­åªæœ‰ä¸€å¼ å›¾ï¼‰ã€‚`torch` æ€»æ˜¯å°†æ‰¹é‡ç»´æ”¾åœ¨ç¬¬ä¸€ä½ï¼Œæ‰€ä»¥ Comfy çš„å›¾åƒå½¢çŠ¶ä¸º `[B,H,W,3]`ï¼Œé€šå¸¸å†™ä½œ `[B,H,W,C]`ï¼Œå…¶ä¸­ C ä»£è¡¨é€šé“æ•°ï¼ˆChannelsï¼‰ã€‚\n\n### squeezeã€unsqueeze ä¸Ž reshape\n\nå¦‚æžœå¼ é‡çš„æŸä¸ªç»´åº¦å¤§å°ä¸º 1ï¼ˆç§°ä¸ºæŠ˜å ç»´åº¦ï¼‰ï¼Œé‚£ä¹ˆåŽ»æŽ‰è¿™ä¸ªç»´åº¦åŽçš„å¼ é‡ä¸ŽåŽŸå¼ é‡ç­‰ä»·ï¼ˆæ¯”å¦‚åªæœ‰ä¸€å¼ å›¾ç‰‡çš„æ‰¹é‡å…¶å®žå°±æ˜¯ä¸€å¼ å›¾ç‰‡ï¼‰ã€‚åŽ»é™¤è¿™ç§æŠ˜å ç»´åº¦ç§°ä¸º squeezeï¼Œæ’å…¥ä¸€ä¸ªè¿™æ ·çš„ç»´åº¦ç§°ä¸º unsqueezeã€‚\n\nå°†åŒæ ·çš„æ•°æ®ä»¥ä¸åŒçš„å½¢çŠ¶è¡¨ç¤ºç§°ä¸º reshapeã€‚é€šå¸¸ä½ éœ€è¦äº†è§£åº•å±‚æ•°æ®ç»“æž„ï¼Œå› æ­¤è¯·è°¨æ…Žæ“ä½œï¼\n\n### é‡è¦ç¬¦å·è¯´æ˜Ž\n\n`torch.Tensor` æ”¯æŒå¤§å¤šæ•° Python çš„åˆ‡ç‰‡ç¬¦å·ã€è¿­ä»£å’Œå…¶ä»–å¸¸è§çš„ç±»åˆ—è¡¨æ“ä½œã€‚å¼ é‡è¿˜æœ‰ä¸€ä¸ª `.shape` å±žæ€§ï¼Œè¿”å›žå…¶å¤§å°ï¼Œç±»åž‹ä¸º `torch.Size`ï¼ˆå®ƒæ˜¯ `tuple` çš„å­ç±»ï¼Œå¯ä»¥å½“ä½œå…ƒç»„ä½¿ç”¨ï¼‰ã€‚ è¿˜æœ‰ä¸€äº›ä½ ç»å¸¸ä¼šè§åˆ°çš„é‡è¦ç¬¦å·ï¼ˆå…¶ä¸­å‡ ä¸ªåœ¨æ ‡å‡† Python é‡Œä¸å¸¸è§ï¼Œä½†åœ¨å¤„ç†å¼ é‡æ—¶å¾ˆå¸¸ç”¨ï¼‰ï¼š\n\n*   `torch.Tensor` æ”¯æŒåœ¨åˆ‡ç‰‡ç¬¦å·ä¸­ä½¿ç”¨ `None`ï¼Œè¡¨ç¤ºæ’å…¥ä¸€ä¸ªå¤§å°ä¸º 1 çš„æ–°ç»´åº¦ã€‚\n*   `:` åœ¨åˆ‡ç‰‡å¼ é‡æ—¶å¸¸ç”¨ï¼Œè¡¨ç¤ºâ€ä¿ç•™æ•´ä¸ªç»´åº¦â€ã€‚å°±åƒ Python é‡Œçš„ `a[start:end]`ï¼Œä½†çœç•¥äº†èµ·æ­¢ç‚¹ã€‚\n*   `...` è¡¨ç¤ºâ€æœªæŒ‡å®šæ•°é‡çš„æ‰€æœ‰ç»´åº¦â€ã€‚æ‰€ä»¥ `a[0, ...]` ä¼šæå–æ‰¹é‡ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œæ— è®ºæœ‰å¤šå°‘ç»´åº¦ã€‚\n*   åœ¨éœ€è¦ä¼ é€’å½¢çŠ¶çš„å‡½æ•°ä¸­ï¼Œå½¢çŠ¶é€šå¸¸ä»¥ `tuple` å½¢å¼ä¼ é€’ï¼Œå…¶ä¸­æŸä¸ªç»´åº¦å¯ä»¥ç”¨ `-1`ï¼Œè¡¨ç¤ºè¯¥ç»´åº¦çš„å¤§å°ç”±æ•°æ®æ€»é‡è‡ªåŠ¨æŽ¨ç®—ã€‚\n\n```\n>>> a = torch.Tensor((1,2))\n>>> a.shape\ntorch.Size([2])\n>>> a[:,None].shape \ntorch.Size([2, 1])\n>>> a.reshape((1,-1)).shape\ntorch.Size([1, 2])\n```\n\n### å…ƒç´ çº§æ“ä½œ\n\nè®¸å¤š `torch.Tensor` çš„äºŒå…ƒæ“ä½œï¼ˆåŒ…æ‹¬ â€™+â€™, â€™-â€™, â€™\\*â€™, â€™/â€™ å’Œ â€™==â€˜ï¼‰éƒ½æ˜¯å…ƒç´ çº§çš„ï¼ˆå³å¯¹æ¯ä¸ªå…ƒç´ ç‹¬ç«‹æ“ä½œï¼‰ã€‚æ“ä½œæ•°å¿…é¡»æ˜¯å½¢çŠ¶ç›¸åŒçš„ä¸¤ä¸ªå¼ é‡ï¼Œæˆ–ä¸€ä¸ªå¼ é‡å’Œä¸€ä¸ªæ ‡é‡ã€‚æ‰€ä»¥ï¼š\n\n```\n>>> import torch\n>>> a = torch.Tensor((1,2))\n>>> b = torch.Tensor((3,2))\n>>> a*b\ntensor([3., 4.])\n>>> a/b\ntensor([0.3333, 1.0000])\n>>> a==b\ntensor([False,  True])\n>>> a==1\ntensor([ True, False])\n>>> c = torch.Tensor((3,2,1)) \n>>> a==c\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n```\n\n### å¼ é‡çš„å¸ƒå°”å€¼\n\nä½ å¯èƒ½ç†Ÿæ‚‰ Python åˆ—è¡¨çš„çœŸå€¼ï¼šéžç©ºåˆ—è¡¨ä¸º `True`ï¼Œ`None` æˆ– `[]` ä¸º `False`ã€‚è€Œ `torch.Tensor`ï¼ˆåªè¦æœ‰å¤šä¸ªå…ƒç´ ï¼‰æ²¡æœ‰å®šä¹‰çš„çœŸå€¼ã€‚ä½ éœ€è¦ç”¨ `.all()` æˆ– `.any()` æ¥åˆå¹¶å…ƒç´ çº§çš„çœŸå€¼ï¼š\n\n```\n>>> a = torch.Tensor((1,2))\n>>> print(\"yes\" if a else \"no\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n>>> a.all()\ntensor(False)\n>>> a.any()\ntensor(True)\n```\n\nè¿™ä¹Ÿæ„å‘³ç€ä½ éœ€è¦ç”¨ `if a is not None:` è€Œä¸æ˜¯ `if a:` æ¥åˆ¤æ–­ä¸€ä¸ªå¼ é‡å˜é‡æ˜¯å¦å·²è¢«èµ‹å€¼ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/get_started/first_generation",
  "markdown": "# å¼€å§‹ ComfyUI çš„ AI ç»˜å›¾ä¹‹æ—…\n\næœ¬ç¯‡çš„ä¸»è¦ç›®çš„æ˜¯å¸¦ä½ åˆæ­¥äº†è§£ ComfyUI ç†Ÿæ‚‰ ComfyUI çš„ä¸€äº›åŸºç¡€æ“ä½œï¼Œå¹¶å¼•å¯¼ä½ é¦–æ¬¡çš„å›¾ç‰‡ç”Ÿæˆ\n\n1.  åŠ è½½ç¤ºä¾‹å·¥ä½œæµ\n    *   ä»Ž ComfyUI åŠ è½½`Workflows template`ä¸­çš„`Text to Image`å·¥ä½œæµ\n    *   ä½¿ç”¨å¸¦æœ‰`metadata` çš„å›¾ç‰‡ä¸­åŠ è½½å·¥ä½œæµ\n2.  æŒ‡å¯¼ä½ å®Œæˆæ¨¡åž‹\n    *   è‡ªåŠ¨å®‰è£…æ¨¡åž‹\n    *   æ‰‹åŠ¨å®‰è£…æ¨¡åž‹\n    *   ä½¿ç”¨ **ComfyUI Manager** çš„æ¨¡åž‹ç®¡ç†åŠŸèƒ½å®‰è£…æ¨¡åž‹\n3.  è¿›è¡Œä¸€æ¬¡æ–‡æœ¬åˆ°å›¾ç‰‡çš„ç”Ÿæˆ\n\n## å…³äºŽæ–‡ç”Ÿå›¾çš„è¯´æ˜Ž\n\n**æ–‡ç”Ÿå›¾ï¼ˆText to Imageï¼‰**ï¼Œæ˜¯ AI ç»˜å›¾çš„åŸºç¡€ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬æè¿°æ¥ç”Ÿæˆå¯¹åº”çš„å›¾ç‰‡ï¼Œæ˜¯ AI ç»˜å›¾æœ€å¸¸ç”¨çš„åŠŸèƒ½ä¹‹ä¸€ï¼Œä½ å¯ä»¥ç†è§£æˆä½ æŠŠä½ çš„**ç»˜å›¾è¦æ±‚(æ­£å‘æç¤ºè¯ã€è´Ÿå‘æç¤ºè¯)**å‘Šè¯‰ä¸€ä¸ª**ç”»å®¶(ç»˜å›¾æ¨¡åž‹)**ï¼Œç”»å®¶ä¼šæ ¹æ®ä½ çš„è¦æ±‚ï¼Œç”»å‡ºä½ æƒ³è¦çš„å†…å®¹ï¼Œç”±äºŽæœ¬ç¯‡æ•™ç¨‹ä¸»è¦æ˜¯ä¸ºäº†å¼•å¯¼ä½ å¼€å§‹ ComfyUI çš„ä½¿ç”¨ï¼Œå¯¹äºŽæ–‡ç”Ÿå›¾çš„è¯¦ç»†è¯´æ˜Žï¼Œæˆ‘ä»¬å°†åœ¨[æ–‡ç”Ÿå›¾](https://docs.comfy.org/zh-CN/tutorials/basic/image-to-image)ç« èŠ‚è¿›è¡Œè¯¦ç»†è®²è§£\n\n### 1\\. å¯åŠ¨ ComfyUI\n\nè¯·ç¡®å®šä½ å·²ç»æŒ‰ç…§å®‰è£…éƒ¨åˆ†çš„æŒ‡å—å®Œæˆäº† ComfyUI çš„å¯åŠ¨ï¼Œå¹¶å¯ä»¥æˆåŠŸæ‰“å¼€ ComfyUI çš„é¡µé¢ ![ComfyUIç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/comfyui-boot-screen.jpg) å¦‚æžœä½ è¿˜æœªå®‰è£… ComfyUI è¯·æ ¹æ®ä½ çš„è®¾å¤‡æƒ…å†µé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ç‰ˆæœ¬è¿›è¡Œå®‰è£…\n\n### 2\\. åŠ è½½é»˜è®¤æ–‡ç”Ÿå›¾å·¥ä½œæµ\n\næ­£å¸¸æƒ…å†µä¸‹ï¼Œæ‰“å¼€ ComfyUI åŽæ˜¯ä¼šè‡ªåŠ¨åŠ è½½é»˜è®¤çš„æ–‡ç”Ÿå›¾å·¥ä½œæµçš„, ä¸è¿‡ä½ ä»æ—§å¯ä»¥å°è¯•ä»¥ä¸‹ä¸åŒæ–¹å¼åŠ è½½å·¥ä½œæµæ¥ç†Ÿæ‚‰ ComfyUI çš„ä¸€äº›åŸºç¡€æ“ä½œ\n\n ![ComfyUI ç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1.jpg) è¯·å¯¹ç…§å›¾ç‰‡ä¸­åºå·æ‰€å¯¹åº”çš„é¡ºåºè¿›è¡Œæ“ä½œ\n\n1.  ç‚¹å‡» ComfyUI ç•Œé¢å³ä¸‹è§’çš„**Fit View**æŒ‰é’®ï¼Œé˜²æ­¢å·²åŠ è½½å·¥ä½œæµæ˜¯åœ¨è§†å›¾å¤–å¯¼è‡´ä¸å¯è§\n2.  ç‚¹å‡»ä¾§è¾¹æ çš„**æ–‡ä»¶å¤¹å›¾æ ‡ï¼ˆworkflowsï¼‰**\n3.  ç‚¹å‡» å·¥ä½œæµï¼ˆWorkflowsï¼‰é¢æ¿é¡¶éƒ¨çš„**æµè§ˆå·¥ä½œæµç¤ºä¾‹ï¼ˆBrowse example workflowsï¼‰** æŒ‰é’®\n\nä¸‹å›¾ç»§ç»­![åŠ è½½å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg)\n\n4.  é€‰æ‹©é»˜è®¤çš„ç¬¬ä¸€ä¸ªå·¥ä½œæµ **Image Generation** ä»¥åŠ è½½å›¾æ ‡\n\næˆ–è€…ä½ ä¹Ÿå¯ä»¥ä»Ž`workflow`èœå•ä¸­é€‰æ‹©**Browse workflow templates** æµè§ˆå·¥ä½œæµæ¨¡æ¿ ![ComfyUI èœå• - æµè§ˆå·¥ä½œæµæ¨¡æ¿](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg) \n\n### 3\\. å®‰è£…ç»˜å›¾æ¨¡åž‹\n\né€šå¸¸åœ¨ ComfyUI çš„åˆå§‹å®‰è£…ä¸­ï¼Œå¹¶ä¸ä¼šåŒ…å«ä»»ä½•çš„ç»˜å›¾æ¨¡åž‹ï¼Œä½†æ˜¯æ¨¡åž‹æ˜¯æˆ‘ä»¬è¿è¡Œå›¾ç‰‡ç”Ÿæˆå¿…ä¸å¯å°‘çš„éƒ¨åˆ†ã€‚ åœ¨ä½ å®Œæˆç¬¬äºŒæ­¥ï¼Œå·¥ä½œæµçš„åŠ è½½åŽï¼Œå¦‚æžœä½ çš„ç”µè„‘ä¸Šæ²¡æœ‰å®‰è£…[v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) è¿™ä¸ªæ¨¡åž‹æ–‡ä»¶æ—¶ï¼Œä¸€èˆ¬ä¼šå‡ºçŽ°ä¸‹å›¾çš„æç¤º ![æ¨¡åž‹ç¼ºå¤±](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg) ä½ å¯ä»¥ç›´æŽ¥é€‰æ‹©ç‚¹å‡» `Download` æŒ‰é’®ï¼Œè®© ComfyUI è‡ªåŠ¨å®Œæˆå¯¹åº”çš„æ¨¡åž‹çš„ä¸‹è½½ï¼Œä½†ç”±äºŽåœ¨æœ‰äº›åœ°åŒºä¸èƒ½å¤Ÿé¡ºåˆ©è®¿é—®å¯¹åº”æ¨¡åž‹çš„ä¸‹è½½æºï¼Œæ‰€ä»¥åœ¨è¿™ä¸ªæ­¥éª¤ä¸­ï¼Œæˆ‘å°†è¯´æ˜Žå‡ ç§ä¸åŒçš„æ¨¡åž‹å®‰è£…æ–¹æ³•ã€‚ æ— è®ºä½¿ç”¨å“ªç§æ–¹æ³•ï¼Œæ¨¡åž‹éƒ½ä¼šè¢«ä¿å­˜åˆ° `<ä½ çš„ ComfyUI å®‰è£…ä½ç½®>/ComfyUI/models/` æ–‡ä»¶å¤¹ä¸‹ï¼Œä½ å¯ä»¥åœ¨ä½ çš„ç”µè„‘ä¸Šå°è¯•æ‰¾åˆ°è¿™ä¸ªæ–‡ä»¶å¤¹ä½ç½®ï¼Œä½ å¯ä»¥çœ‹åˆ°è®¸å¤šæ–‡ä»¶å¤¹æ¯”å¦‚ `checkpoints`ã€`embeddings`ã€`vae`ã€`lora`ã€`upscale_model` ç­‰ï¼Œè¿™äº›éƒ½æ˜¯ä¸åŒçš„æ¨¡åž‹ä¿å­˜çš„æ–‡ä»¶å¤¹ï¼Œé€šå¸¸ä»¥æ–‡ä»¶å¤¹åç§°åŒºåˆ†ï¼ŒComfyUI åœ¨å¯åŠ¨æ—¶ä¼šæ£€æµ‹è¿™äº›æ–‡ä»¶å¤¹ä¸‹çš„æ¨¡åž‹æ–‡ä»¶ï¼Œä»¥åŠ`extra_model_paths.yaml` æ–‡ä»¶ä¸­é…ç½®çš„æ–‡ä»¶è·¯å¾„ ![ComfyUI æ¨¡åž‹æ–‡ä»¶å¤¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg) è¢«æ£€æµ‹åˆ°çš„ä¸åŒçš„æ–‡ä»¶å¤¹é‡Œçš„æ¨¡åž‹å°†å¯ä»¥åœ¨ ComfyUI çš„ä¸åŒ **æ¨¡åž‹åŠ è½½èŠ‚ç‚¹** é‡Œä½¿ç”¨ï¼Œä¸‹é¢è®©æˆ‘ä»¬å¼€å§‹äº†è§£ä¸åŒæ¨¡åž‹çš„å®‰è£…æ–¹å¼ï¼š\n\nåœ¨ä½ ç‚¹å‡» **Download** æŒ‰é’®åŽï¼ŒComfyUI å°†ä¼šæ‰§è¡Œä¸‹è½½,æ ¹æ®ä½ ä½¿ç”¨çš„ç‰ˆæœ¬ä¸åŒï¼Œå°†ä¼šæ‰§è¡Œä¸åŒçš„è¡Œä¸º\n\næ¡Œé¢ç‰ˆå°†è‡ªåŠ¨å®Œæˆæ¨¡åž‹çš„ä¸‹è½½å¹¶ä¿å­˜åˆ° `<ä½ çš„ ComfyUI å®‰è£…ä½ç½®>/ComfyUI/models/checkpoints` ç›®å½•ä¸‹ ä½ å¯ä»¥ç­‰å¾…å®‰è£…å®Œæˆæˆ–è€…åœ¨ä¾§è¾¹æ çš„æ¨¡åž‹é¢æ¿é‡ŒæŸ¥çœ‹å®‰è£…è¿›åº¦![æ¨¡åž‹ä¸‹è½½è¿›åº¦](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg)å¦‚æžœä¸€åˆ‡é¡ºåˆ©ï¼Œæ¨¡åž‹åº”è¯¥å¯ä»¥è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå¦‚æžœé•¿æ—¶é—´æœªä¸‹è½½æˆåŠŸï¼Œè¯·å°è¯•å…¶å®ƒå®‰è£…æ–¹æ³•\n\n### 4\\. åŠ è½½æ¨¡åž‹ï¼Œå¹¶è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆ\n\nåœ¨å®Œæˆäº†å¯¹åº”çš„ç»˜å›¾æ¨¡åž‹å®‰è£…åŽï¼Œè¯·å‚è€ƒä¸‹å›¾æ­¥éª¤åŠ è½½å¯¹åº”çš„æ¨¡åž‹ï¼Œå¹¶è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡çš„ç”Ÿæˆ ![å›¾ç‰‡ç”Ÿæˆ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg) è¯·å¯¹åº”å›¾ç‰‡åºå·ï¼Œå®Œæˆä¸‹é¢æ“ä½œ\n\n1.  è¯·åœ¨ **Load Checkpoint** èŠ‚ç‚¹ä½¿ç”¨ç®­å¤´æˆ–è€…ç‚¹å‡»æ–‡æœ¬åŒºåŸŸç¡®ä¿ **v1-5-pruned-emaonly-fp16.safetensors** è¢«é€‰ä¸­ï¼Œä¸”å·¦å³åˆ‡æ¢ç®­å¤´ä¸ä¼šå‡ºçŽ° **null** çš„æ–‡æœ¬\n2.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl + enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\nç­‰å¾…å¯¹åº”æµç¨‹æ‰§è¡Œå®ŒæˆåŽï¼Œä½ åº”è¯¥å¯ä»¥åœ¨ç•Œé¢çš„ **ä¿å­˜å›¾åƒ(Save Image)** èŠ‚ç‚¹ä¸­çœ‹åˆ°å¯¹åº”çš„å›¾ç‰‡ç»“æžœï¼Œå¯ä»¥åœ¨ä¸Šé¢å³é”®ä¿å­˜åˆ°æœ¬åœ° ![ComfyUI é¦–æ¬¡å›¾ç‰‡ç”Ÿæˆç»“æžœ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg) å¯¹äºŽæ–‡ç”Ÿå›¾çš„è¯¦ç»†è¯´æ˜Žï¼Œä¸‹é¢çš„æŒ‡å—ä¸­ä¼šæœ‰è¯¦ç»†çš„è¯´æ˜Žå’ŒæŒ‡å¯¼\n\n[\n\n## ComfyUI æ–‡ç”Ÿå›¾å·¥ä½œæµç¤ºä¾‹è¯´æ˜Ž\n\nç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹æ–‡ç”Ÿå›¾å·¥ä½œæµçš„è¯¦ç»†è¯´æ˜Ž\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)\n\n## æ•…éšœæŽ’é™¤\n\n### æ¨¡åž‹åŠ è½½é—®é¢˜\n\nå¦‚æžœ `Load Checkpoint` èŠ‚ç‚¹æ²¡æœ‰ä»»ä½•æ¨¡åž‹å¯ä»¥é€‰æ‹©ï¼Œæˆ–è€…æ˜¾ç¤ºä¸º **null**ï¼Œè¯·å…ˆç¡®è®¤ä½ çš„æ¨¡åž‹å®‰è£…ä½ç½®æ­£ç¡®ï¼Œæˆ–è€…å°è¯• **åˆ·æ–°** æˆ–è€… **é‡å¯ ComfyUI** ä½¿å¾—å¯¹åº”æ–‡ä»¶å¤¹ä¸‹çš„æ¨¡åž‹å¯ä»¥è¢«æ£€æµ‹åˆ°"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/dall-e-3",
  "markdown": "# OpenAI DALLÂ·E 3 èŠ‚ç‚¹ - ComfyUI\n\nOpenAI DALLÂ·E 3 æ˜¯ ComfyUI API èŠ‚ç‚¹ç³»åˆ—ä¸­çš„ä¸€å‘˜ï¼Œå®ƒå…è®¸ç”¨æˆ·é€šè¿‡ OpenAI çš„ **DALLÂ·E 3** æ¨¡åž‹ç”Ÿæˆå›¾åƒã€‚æ­¤èŠ‚ç‚¹æ”¯æŒæ–‡æœ¬åˆ°å›¾åƒçš„ç”ŸæˆåŠŸèƒ½ã€‚ ![OpenAI DALLÂ·E 2 èŠ‚ç‚¹æˆªå›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-dall-e-3.jpg)\n\n## èŠ‚ç‚¹æ¦‚è¿°\n\nDALLÂ·E 3 æ˜¯ OpenAI çš„æœ€æ–°å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºåˆ›å»ºè¯¦ç»†ä¸”é«˜è´¨é‡çš„å›¾åƒã€‚é€šè¿‡ ComfyUI ä¸­çš„è¿™ä¸ªèŠ‚ç‚¹ï¼Œæ‚¨å¯ä»¥ç›´æŽ¥è®¿é—® DALLÂ·E 3 çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæ— éœ€ç¦»å¼€ ComfyUI ç•Œé¢ã€‚ **OpenAI DALLÂ·E 2** èŠ‚ç‚¹é€šè¿‡ OpenAI çš„å›¾åƒç”Ÿæˆ API åŒæ­¥ç”Ÿæˆå›¾åƒã€‚å®ƒæŽ¥æ”¶æ–‡æœ¬æç¤ºå¹¶è¿”å›žç¬¦åˆæè¿°çš„å›¾åƒã€‚\n\n## å‚æ•°è¯¦è§£\n\n### å¿…éœ€å‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | æè¿°  |\n| --- | --- | --- |\n| prompt | æ–‡æœ¬  | ç”¨äºŽç”Ÿæˆå›¾åƒçš„æ–‡æœ¬æç¤ºã€‚æ”¯æŒå¤šè¡Œè¾“å…¥ï¼Œå¯ä»¥è¯¦ç»†æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„å›¾åƒå†…å®¹ã€‚ |\n\n### widget å‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | å¯é€‰å€¼ | é»˜è®¤å€¼ | æè¿°  |\n| --- | --- | --- | --- | --- |\n| seed | æ•´æ•°  | 0-2147483647 | 0   | ç”¨äºŽæŽ§åˆ¶ç”Ÿæˆç»“æžœçš„éšæœºç§å­ |\n| quality | é€‰é¡¹  | standard, hd | standard | å›¾åƒè´¨é‡è®¾ç½®ã€‚â€œhdâ€é€‰é¡¹ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒï¼Œä½†å¯èƒ½éœ€è¦æ›´å¤šè®¡ç®—èµ„æº |\n| style | é€‰é¡¹  | natural, vivid | natural | å›¾åƒé£Žæ ¼ã€‚â€œvividâ€å€¾å‘äºŽç”Ÿæˆè¶…çœŸå®žå’Œæˆå‰§æ€§çš„å›¾åƒï¼Œâ€œnaturalâ€åˆ™äº§ç”Ÿæ›´è‡ªç„¶ã€ä¸é‚£ä¹ˆå¤¸å¼ çš„å›¾åƒ |\n| size | é€‰é¡¹  | 1024x1024, 1024x1792, 1792x1024 | 1024x1024 | ç”Ÿæˆå›¾åƒçš„å°ºå¯¸ã€‚å¯ä»¥é€‰æ‹©æ–¹å½¢æˆ–ä¸åŒæ–¹å‘çš„çŸ©å½¢å›¾åƒ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\nä½ å¯ä»¥ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![ComfyUI openai-dall-e-3å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png) ç”±äºŽå¯¹åº”å·¥ä½œæµéžå¸¸ç®€å•ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æŽ¥åœ¨ ComfyUI ä¸­æ·»åŠ  **OpenAI DALLÂ·E 3** èŠ‚ç‚¹ï¼Œå¹¶è¾“å…¥æ‚¨æƒ³è¦ç”Ÿæˆçš„å›¾åƒæè¿°ï¼Œç„¶åŽè¿è¡Œå·¥ä½œæµå³å¯ ![ComfyUI openai-dall-e-3 å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg)\n\n1.  åœ¨ ComfyUI ä¸­æ·»åŠ  **OpenAI DALLÂ·E 3** èŠ‚ç‚¹\n2.  åœ¨æç¤ºæ–‡æœ¬æ¡†ä¸­è¾“å…¥æ‚¨æƒ³è¦ç”Ÿæˆçš„å›¾åƒæè¿°\n3.  æ ¹æ®éœ€è¦è°ƒæ•´å¯é€‰å‚æ•°ï¼ˆè´¨é‡ã€é£Žæ ¼ã€å°ºå¯¸ç­‰ï¼‰\n4.  è¿è¡Œå·¥ä½œæµç¨‹ç”Ÿæˆå›¾åƒ\n\n## å¸¸è§é—®é¢˜"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/workflow",
  "markdown": "# å·¥ä½œæµ - ComfyUI\n\n## èŠ‚ç‚¹å›¾\n\nComfyUI æ˜¯ä¸€ä¸ªç”¨äºŽæž„å»ºå’Œè¿è¡Œç”Ÿæˆå†…å®¹çš„ _**å·¥ä½œæµ**_ çš„çŽ¯å¢ƒã€‚åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œå·¥ä½œæµè¢«å®šä¹‰ä¸ºä¸€ç»„ç§°ä¸º _**èŠ‚ç‚¹**_ çš„ç¨‹åºå¯¹è±¡ï¼Œå®ƒä»¬ç›¸äº’è¿žæŽ¥ï¼Œå½¢æˆä¸€ä¸ªç½‘ç»œã€‚è¿™ä¸ªç½‘ç»œä¹Ÿè¢«ç§°ä¸º _**å›¾**_ã€‚ ComfyUI å·¥ä½œæµå¯ä»¥ç”Ÿæˆä»»ä½•ç±»åž‹çš„åª’ä½“ï¼šå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘ã€AI æ¨¡åž‹ã€AI ä»£ç†ç­‰ã€‚\n\n## ç¤ºä¾‹å·¥ä½œæµ\n\nè¦å¼€å§‹ï¼Œè¯·å°è¯•ä¸€äº› [å®˜æ–¹å·¥ä½œæµ](https://comfyanonymous.github.io/ComfyUI_examples)ã€‚è¿™äº›å·¥ä½œæµä»…ä½¿ç”¨ ComfyUI å®‰è£…ä¸­åŒ…å«çš„æ ¸å¿ƒèŠ‚ç‚¹ã€‚ä¸€ä¸ªè“¬å‹ƒå‘å±•çš„å¼€å‘è€…ç¤¾åŒºåˆ›å»ºäº†ä¸°å¯Œçš„ [ç”Ÿæ€ç³»ç»Ÿ](https://registry.comfy.org/) çš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œä»¥æ‰©å±• ComfyUI çš„åŠŸèƒ½ã€‚\n\n### ç®€å•ç¤ºä¾‹\n\n![ç®€å•å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/simple_workflow.jpeg)\n\n## å¯è§†åŒ–ç¼–ç¨‹\n\nåƒ ComfyUI è¿™æ ·çš„åŸºäºŽèŠ‚ç‚¹çš„è®¡ç®—æœºç¨‹åºæä¾›äº†ä¸€ç§ä¼ ç»Ÿèœå•å’ŒæŒ‰é’®é©±åŠ¨åº”ç”¨ç¨‹åºæ— æ³•å®žçŽ°çš„å¼ºå¤§çµæ´»æ€§ã€‚ComfyUI èŠ‚ç‚¹å›¾ä¸å—ä¼ ç»Ÿè®¡ç®—æœºåº”ç”¨ç¨‹åºæä¾›çš„å·¥å…·çš„é™åˆ¶ã€‚å®ƒæ˜¯ä¸€ä¸ªé«˜çº§çš„ _**å¯è§†åŒ–ç¼–ç¨‹çŽ¯å¢ƒ**_ï¼Œå…è®¸ç”¨æˆ·è®¾è®¡å¤æ‚çš„ç³»ç»Ÿï¼Œè€Œæ— éœ€ç¼–å†™ç¨‹åºä»£ç æˆ–ç†è§£é«˜çº§æ•°å­¦ã€‚ è®¸å¤šå…¶ä»–è®¡ç®—æœºåº”ç”¨ç¨‹åºä¹Ÿä½¿ç”¨ç›¸åŒçš„èŠ‚ç‚¹å›¾èŒƒå¼ã€‚ç¤ºä¾‹åŒ…æ‹¬åˆæˆåº”ç”¨ç¨‹åº Nukeã€3D ç¨‹åº Maya å’Œ Blenderã€å®žæ—¶å›¾å½¢å¼•æ“Ž Unrealï¼Œä»¥åŠäº¤äº’åª’ä½“åˆ›ä½œç¨‹åº Maxã€‚\n\n### æ›´å¤æ‚çš„ç¤ºä¾‹\n\n![å¤æ‚å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/complex_workflow.jpeg)\n\n## è¿‡ç¨‹æ¡†æž¶\n\nå¦ä¸€ä¸ªç”¨äºŽæè¿°åŸºäºŽèŠ‚ç‚¹çš„åº”ç”¨ç¨‹åºçš„æœ¯è¯­æ˜¯ _**è¿‡ç¨‹æ¡†æž¶**_ã€‚è¿‡ç¨‹æ„å‘³ç€ç”Ÿæˆï¼šæŸç§è¿‡ç¨‹æˆ–ç®—æ³•è¢«ç”¨æ¥ç”Ÿæˆå†…å®¹ï¼Œä¾‹å¦‚ 3D æ¨¡åž‹æˆ–éŸ³ä¹ä½œå“ã€‚ ComfyUI æ˜¯æ‰€æœ‰è¿™äº›ä¸œè¥¿ï¼šä¸€ä¸ªèŠ‚ç‚¹å›¾ã€ä¸€ä¸ªå¯è§†åŒ–ç¼–ç¨‹çŽ¯å¢ƒå’Œä¸€ä¸ªè¿‡ç¨‹æ¡†æž¶ã€‚ä½¿ ComfyUI ä¸åŒï¼ˆå¹¶ä¸”ä»¤äººæƒŠå¹ï¼ï¼‰çš„æ˜¯ï¼Œå®ƒçš„å¼€æ”¾ç»“æž„å…è®¸æˆ‘ä»¬ç”Ÿæˆä»»ä½•ç±»åž‹çš„åª’ä½“èµ„äº§ï¼Œä¾‹å¦‚å›¾ç‰‡ã€ç”µå½±ã€å£°éŸ³ã€3D æ¨¡åž‹ã€AI æ¨¡åž‹ç­‰ã€‚ åœ¨ ComfyUI çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œ_**å·¥ä½œæµ**_ è¿™ä¸ªæœ¯è¯­æ˜¯èŠ‚ç‚¹ç½‘ç»œæˆ–å›¾çš„åŒä¹‰è¯ã€‚å®ƒå¯¹åº”äºŽ 3D æˆ–å¤šåª’ä½“ç¨‹åºä¸­çš„ _**åœºæ™¯å›¾**_ï¼šç‰¹å®šç£ç›˜æ–‡ä»¶ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„ç½‘ç»œã€‚3D ç¨‹åºç§°ä¹‹ä¸º _**åœºæ™¯æ–‡ä»¶**_ã€‚è§†é¢‘ç¼–è¾‘ã€åˆæˆå’Œå¤šåª’ä½“ç¨‹åºé€šå¸¸ç§°ä¹‹ä¸º _**é¡¹ç›®æ–‡ä»¶**_ã€‚\n\n## ä¿å­˜å·¥ä½œæµ\n\nComfyUI å·¥ä½œæµä¼šè‡ªåŠ¨ä¿å­˜åœ¨ä»»ä½•ç”Ÿæˆå›¾åƒçš„å…ƒæ•°æ®ä¸­ï¼Œå…è®¸ç”¨æˆ·æ‰“å¼€å¹¶ä½¿ç”¨ç”Ÿæˆå›¾åƒçš„å›¾å½¢ã€‚å·¥ä½œæµä¹Ÿå¯ä»¥å­˜å‚¨åœ¨éµå¾ª JSON æ•°æ®æ ¼å¼çš„äººç±»å¯è¯»æ–‡æœ¬æ–‡ä»¶ä¸­ã€‚è¿™å¯¹äºŽä¸æ”¯æŒå…ƒæ•°æ®çš„åª’ä½“æ ¼å¼æ˜¯å¿…è¦çš„ã€‚ä»¥ JSON æ–‡ä»¶æ ¼å¼å­˜å‚¨çš„ ComfyUI å·¥ä½œæµéžå¸¸å°ï¼Œä¾¿äºŽç‰ˆæœ¬æŽ§åˆ¶ã€å½’æ¡£å’Œå…±äº«å›¾å½¢ï¼Œè€Œä¸ä¾èµ–äºŽä»»ä½•ç”Ÿæˆçš„åª’ä½“ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/changelog",
  "markdown": "# æ›´æ–°æ—¥å¿— - ComfyUI\n\n**é«˜çº§é‡‡æ ·ä¸Žè®­ç»ƒåŸºç¡€è®¾æ–½æ”¹è¿›**æœ¬ç‰ˆæœ¬ä¸ºAIç ”ç©¶äººå‘˜å’Œå·¥ä½œæµç¨‹åˆ›å»ºè€…å¼•å…¥äº†é‡‡æ ·ç®—æ³•ã€è®­ç»ƒåŠŸèƒ½å’ŒèŠ‚ç‚¹åŠŸèƒ½çš„é‡å¤§å¢žå¼ºï¼š\n\n## æ–°çš„é‡‡æ ·å’Œç”ŸæˆåŠŸèƒ½\n\n*   **SA-Solveré‡‡æ ·å™¨**ï¼šæ–°çš„é‡æž„SA-Solveré‡‡æ ·ç®—æ³•ï¼Œä¸ºå¤æ‚ç”Ÿæˆå·¥ä½œæµæä¾›å¢žå¼ºçš„æ•°å€¼ç¨³å®šæ€§å’Œè´¨é‡\n*   **å®žéªŒæ€§CFGNormèŠ‚ç‚¹**ï¼šé«˜çº§æ— åˆ†ç±»å™¨å¼•å¯¼æ ‡å‡†åŒ–ï¼Œç”¨äºŽæ”¹è¿›ç”Ÿæˆè´¨é‡å’Œé£Žæ ¼ä¸€è‡´æ€§çš„æŽ§åˆ¶\n*   **åµŒå¥—åŒCFGæ”¯æŒ**ï¼šä¸ºDualCFGGuiderèŠ‚ç‚¹æ·»åŠ åµŒå¥—é£Žæ ¼é…ç½®ï¼Œæä¾›æ›´å¤æ‚çš„å¼•å¯¼æŽ§åˆ¶æ¨¡å¼\n*   **SamplingPercentToSigmaèŠ‚ç‚¹**ï¼šç”¨äºŽä»Žé‡‡æ ·ç™¾åˆ†æ¯”ç²¾ç¡®è®¡ç®—sigmaçš„æ–°å®žç”¨èŠ‚ç‚¹ï¼Œæé«˜å·¥ä½œæµç¨‹çµæ´»æ€§\n\n## å¢žå¼ºçš„è®­ç»ƒåŠŸèƒ½\n\n*   **å¤šå›¾åƒ-æè¿°æ•°æ®é›†æ”¯æŒ**ï¼šLoRAè®­ç»ƒèŠ‚ç‚¹çŽ°åœ¨å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªå›¾åƒ-æè¿°æ•°æ®é›†ï¼Œç®€åŒ–è®­ç»ƒå·¥ä½œæµç¨‹\n*   **æ›´å¥½çš„è®­ç»ƒå¾ªçŽ¯å®žçŽ°**ï¼šä¼˜åŒ–çš„è®­ç»ƒç®—æ³•ï¼Œåœ¨æ¨¡åž‹å¾®è°ƒè¿‡ç¨‹ä¸­æ”¹å–„æ”¶æ•›æ€§å’Œç¨³å®šæ€§\n*   **å¢žå¼ºçš„é”™è¯¯æ£€æµ‹**ï¼šä¸ºLoRAæ“ä½œæ·»åŠ æ¨¡åž‹æ£€æµ‹é”™è¯¯æç¤ºï¼Œåœ¨å‡ºçŽ°é—®é¢˜æ—¶æä¾›æ›´æ¸…æ™°çš„åé¦ˆ\n\n## å¹³å°å’Œæ€§èƒ½æ”¹è¿›\n\n*   **å¼‚æ­¥èŠ‚ç‚¹æ”¯æŒ**ï¼šå®Œå…¨æ”¯æŒå¼‚æ­¥èŠ‚ç‚¹å‡½æ•°ï¼Œä¼˜åŒ–æ—©æœŸæ‰§è¡Œï¼Œæ”¹å–„I/Oå¯†é›†åž‹æ“ä½œçš„å·¥ä½œæµç¨‹æ€§èƒ½\n*   **Chromaçµæ´»æ€§**ï¼šåœ¨Chromaä¸­å–æ¶ˆç¡¬ç¼–ç çš„patch\\_sizeå‚æ•°ï¼Œå…è®¸æ›´å¥½åœ°é€‚åº”ä¸åŒçš„æ¨¡åž‹é…ç½®\n*   **LTXV VAEè§£ç å™¨**ï¼šåˆ‡æ¢åˆ°æ”¹è¿›çš„é»˜è®¤å¡«å……æ¨¡å¼ï¼Œæé«˜LTXVæ¨¡åž‹çš„å›¾åƒè´¨é‡\n*   **Safetensorså†…å­˜ç®¡ç†**ï¼šä¸ºmmapé—®é¢˜æ·»åŠ è§£å†³æ–¹æ¡ˆï¼Œæé«˜åŠ è½½å¤§åž‹æ¨¡åž‹æ–‡ä»¶æ—¶çš„å¯é æ€§\n\n## APIå’Œé›†æˆå¢žå¼º\n\n*   **è‡ªå®šä¹‰æç¤ºID**ï¼šAPIçŽ°åœ¨å…è®¸æŒ‡å®šæç¤ºIDï¼Œä»¥ä¾¿æ›´å¥½åœ°è·Ÿè¸ªå’Œç®¡ç†å·¥ä½œæµç¨‹\n*   **Kling APIä¼˜åŒ–**ï¼šå¢žåŠ è½®è¯¢è¶…æ—¶æ—¶é—´ï¼Œé˜²æ­¢è§†é¢‘ç”Ÿæˆå·¥ä½œæµç¨‹ä¸­çš„ç”¨æˆ·è¶…æ—¶\n*   **åŽ†å²ä»¤ç‰Œæ¸…ç†**ï¼šä»ŽåŽ†å²é¡¹ç›®ä¸­åˆ é™¤æ•æ„Ÿä»¤ç‰Œä»¥æé«˜å®‰å…¨æ€§\n*   **Python 3.9å…¼å®¹æ€§**ï¼šä¿®å¤å…¼å®¹æ€§é—®é¢˜ï¼Œç¡®ä¿æ›´å¹¿æ³›çš„å¹³å°æ”¯æŒ\n\n## é”™è¯¯ä¿®å¤å’Œç¨³å®šæ€§\n\n*   **MaskCompositeä¿®å¤**ï¼šè§£å†³ç›®æ ‡è’™ç‰ˆå…·æœ‰2ä¸ªç»´åº¦æ—¶çš„é”™è¯¯ï¼Œæé«˜è’™ç‰ˆå·¥ä½œæµç¨‹å¯é æ€§\n*   **Frescaè¾“å…¥/è¾“å‡º**ï¼šä¿®æ­£Frescaæ¨¡åž‹å·¥ä½œæµç¨‹çš„è¾“å…¥å’Œè¾“å‡ºå¤„ç†\n*   **å¼•ç”¨é”™è¯¯ä¿®å¤**ï¼šè§£å†³GeminièŠ‚ç‚¹å®žçŽ°ä¸­çš„é”™è¯¯å¼•ç”¨é—®é¢˜\n*   **è¡Œç»“æŸæ ‡å‡†åŒ–**ï¼šè‡ªåŠ¨æ£€æµ‹å’Œåˆ é™¤Windowsè¡Œç»“æŸç¬¦ï¼Œç¡®ä¿è·¨å¹³å°ä¸€è‡´æ€§\n\n## å¼€å‘è€…ä½“éªŒ\n\n*   **è­¦å‘Šç³»ç»Ÿ**ï¼šæ·»åŠ torchå¯¼å…¥é”™è¯¯è­¦å‘Šï¼Œä»¥æ•èŽ·å¸¸è§é…ç½®é—®é¢˜\n*   **æ¨¡æ¿æ›´æ–°**ï¼šå¤šä¸ªæ¨¡æ¿ç‰ˆæœ¬æ›´æ–°ï¼ˆ0.1.36ã€0.1.37ã€0.1.39ï¼‰ï¼Œæ”¹è¿›è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘\n*   **æ–‡æ¡£**ï¼šå¢žå¼ºä¾¿æºå¼é…ç½®ä¸­fast\\_fp16\\_accumulationçš„æ–‡æ¡£\n\nè¿™äº›æ”¹è¿›ä½¿ComfyUIåœ¨ç”Ÿäº§å·¥ä½œæµç¨‹ä¸­æ›´åŠ ç¨³å¥ï¼ŒåŒæ—¶å¼•å…¥äº†å¯¹é«˜çº§AIç ”ç©¶å’Œåˆ›æ„åº”ç”¨å¿…ä¸å¯å°‘çš„å¼ºå¤§æ–°é‡‡æ ·æŠ€æœ¯å’Œè®­ç»ƒåŠŸèƒ½ã€‚\n\n**é«˜çº§é‡‡æ ·å’Œæ¨¡åž‹æŽ§åˆ¶å¢žå¼º**æ­¤ç‰ˆæœ¬åœ¨é‡‡æ ·ç®—æ³•å’Œæ¨¡åž‹æŽ§åˆ¶ç³»ç»Ÿæ–¹é¢æä¾›äº†é‡å¤§æ”¹è¿›ï¼Œç‰¹åˆ«æœ‰åˆ©äºŽé«˜çº§AIç ”ç©¶äººå‘˜å’Œå·¥ä½œæµåˆ›å»ºè€…ï¼š\n\n## æ–°é‡‡æ ·åŠŸèƒ½\n\n*   **TCFGèŠ‚ç‚¹**ï¼šå¢žå¼ºçš„åˆ†ç±»å™¨æ— å…³å¼•å¯¼æŽ§åˆ¶ï¼Œä¸ºæ‚¨çš„å·¥ä½œæµæä¾›æ›´ç»†è‡´çš„ç”ŸæˆæŽ§åˆ¶\n*   **ER-SDEé‡‡æ ·å™¨**ï¼šä»ŽVEè¿ç§»åˆ°VPç®—æ³•ï¼Œé…å¤‡æ–°çš„é‡‡æ ·å™¨èŠ‚ç‚¹ï¼Œä¸ºå¤æ‚ç”Ÿæˆä»»åŠ¡æä¾›æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§\n*   **è·³å±‚å¼•å¯¼ï¼ˆSLGï¼‰**ï¼šç”¨äºŽæŽ¨ç†æœŸé—´ç²¾ç¡®å±‚çº§æŽ§åˆ¶çš„æ›¿ä»£å®žçŽ°ï¼Œå®Œç¾Žé€‚ç”¨äºŽé«˜çº§æ¨¡åž‹å¯¼å‘å·¥ä½œæµ\n\n## å¢žå¼ºçš„å¼€å‘å·¥å…·\n\n*   **è‡ªå®šä¹‰èŠ‚ç‚¹ç®¡ç†**ï¼šæ–°çš„`--whitelist-custom-nodes`å‚æ•°ä¸Ž`--disable-all-custom-nodes`é…å¯¹ï¼Œæä¾›ç²¾ç¡®çš„å¼€å‘æŽ§åˆ¶\n*   **æ€§èƒ½ä¼˜åŒ–**ï¼šåŒCFGèŠ‚ç‚¹çŽ°åœ¨åœ¨CFGä¸º1.0æ—¶è‡ªåŠ¨ä¼˜åŒ–ï¼Œå‡å°‘è®¡ç®—å¼€é”€\n*   **GitHub Actionsé›†æˆ**ï¼šè‡ªåŠ¨åŒ–å‘å¸ƒwebhooké€šçŸ¥è®©å¼€å‘è€…åŠæ—¶äº†è§£æ–°æ›´æ–°\n\n## å›¾åƒå¤„ç†æ”¹è¿›\n\n*   **æ–°å˜æ¢èŠ‚ç‚¹**ï¼šæ·»åŠ äº†ImageRotateå’ŒImageFlipèŠ‚ç‚¹ï¼Œå¢žå¼ºå›¾åƒæ“ä½œå·¥ä½œæµ\n*   **ImageColorToMaskä¿®å¤**ï¼šä¿®æ­£äº†æŽ©ç å€¼è¿”å›žï¼Œæä¾›æ›´å‡†ç¡®çš„åŸºäºŽé¢œè‰²çš„æŽ©ç æ“ä½œ\n*   **3Dæ¨¡åž‹æ”¯æŒ**ï¼šä¸Šä¼ 3Dæ¨¡åž‹åˆ°è‡ªå®šä¹‰å­æ–‡ä»¶å¤¹ï¼Œä¸ºå¤æ‚é¡¹ç›®æä¾›æ›´å¥½çš„ç»„ç»‡\n\n## å¼•å¯¼å’Œæ¡ä»¶å¢žå¼º\n\n*   **PerpNegå¼•å¯¼å™¨**ï¼šæ›´æ–°äº†æ”¹è¿›çš„å‰åŽCFGå¤„ç†ä»¥åŠæ€§èƒ½ä¼˜åŒ–\n*   **æ½œåœ¨æ¡ä»¶ä¿®å¤**ï¼šè§£å†³äº†å¤šæ­¥éª¤å·¥ä½œæµä¸­ç´¢å¼• > 0 çš„æ¡ä»¶é—®é¢˜\n*   **åŽ»å™ªæ­¥éª¤**ï¼šä¸ºå¤šä¸ªé‡‡æ ·å™¨æ·»åŠ åŽ»å™ªæ­¥éª¤æ”¯æŒï¼ŒèŽ·å¾—æ›´æ¸…æ´çš„è¾“å‡º\n\n## å¹³å°ç¨³å®šæ€§\n\n*   **PyTorchå…¼å®¹æ€§**ï¼šä¿®å¤äº†PyTorch nightlyæž„å»ºçš„è¿žç»­å†…å­˜é—®é¢˜\n*   **FP8å›žé€€**ï¼šå½“FP8æ“ä½œé‡åˆ°å¼‚å¸¸æ—¶è‡ªåŠ¨å›žé€€åˆ°å¸¸è§„æ“ä½œ\n*   **éŸ³é¢‘å¤„ç†**ï¼šç§»é™¤äº†å·²å¼ƒç”¨çš„torchaudio.saveå‡½æ•°ä¾èµ–å¹¶ä¿®å¤è­¦å‘Š\n\n## æ¨¡åž‹é›†æˆ\n\n*   **MoonvalleyèŠ‚ç‚¹**ï¼šä¸ºMoonvalleyæ¨¡åž‹å·¥ä½œæµæ·»åŠ åŽŸç”Ÿæ”¯æŒ\n*   **è°ƒåº¦å™¨é‡æ–°æŽ’åº**ï¼šç®€å•è°ƒåº¦å™¨çŽ°åœ¨é»˜è®¤ä¼˜å…ˆï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ\n*   **æ¨¡æ¿æ›´æ–°**ï¼šå¤šä¸ªæ¨¡æ¿ç‰ˆæœ¬æ›´æ–°ï¼ˆ0.1.31-0.1.35ï¼‰ï¼Œæ”¹è¿›è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘\n\n## å®‰å…¨æ€§å’Œå®‰å…¨ä¿æŠ¤\n\n*   **å®‰å…¨åŠ è½½**ï¼šåœ¨ä¸å®‰å…¨åŠ è½½æ–‡ä»¶æ—¶æ·»åŠ è­¦å‘Šï¼Œæ–‡æ¡£è¯´æ˜Žæ£€æŸ¥ç‚¹æ–‡ä»¶é»˜è®¤å®‰å…¨åŠ è½½\n*   **æ–‡ä»¶éªŒè¯**ï¼šå¢žå¼ºæ£€æŸ¥ç‚¹åŠ è½½å®‰å…¨æŽªæ–½ï¼Œç¡®ä¿å·¥ä½œæµå®‰å…¨æ‰§è¡Œ\n\nè¿™äº›æ”¹è¿›ä½¿ComfyUIåœ¨ç”Ÿäº§å·¥ä½œæµä¸­æ›´åŠ ç¨³å¥ï¼ŒåŒæ—¶ä¸ºä½¿ç”¨é«˜çº§é‡‡æ ·æŠ€æœ¯å’Œæ¨¡åž‹æŽ§åˆ¶ç³»ç»Ÿçš„AIè‰ºæœ¯å®¶æ‰©å±•äº†åˆ›ä½œå¯èƒ½æ€§ã€‚\n\n**å¢žå¼ºæ¨¡åž‹æ”¯æŒä¸Žå·¥ä½œæµå¯é æ€§**æœ¬æ¬¡å‘å¸ƒåœ¨æ¨¡åž‹å…¼å®¹æ€§å’Œå·¥ä½œæµç¨³å®šæ€§æ–¹é¢å¸¦æ¥äº†é‡å¤§æ”¹è¿›ï¼š\n\n*   **æ‰©å±•æ¨¡åž‹æ–‡æ¡£**ï¼šä¸º Flux Kontext å’Œ Omnigen 2 æ¨¡åž‹æ·»åŠ äº†å…¨é¢çš„æ”¯æŒæ–‡æ¡£ï¼Œè®©åˆ›ä½œè€…æ›´å®¹æ˜“å°†è¿™äº›å¼ºå¤§çš„æ¨¡åž‹é›†æˆåˆ°ä»–ä»¬çš„å·¥ä½œæµä¸­\n*   **VAE ç¼–ç æ”¹è¿›**ï¼šç§»é™¤äº† VAE ç¼–ç è¿‡ç¨‹ä¸­ä¸å¿…è¦çš„éšæœºå™ªå£°æ³¨å…¥ï¼Œä½¿å·¥ä½œæµè¿è¡Œçš„è¾“å‡ºæ›´åŠ ä¸€è‡´å’Œå¯é¢„æµ‹\n*   **å†…å­˜ç®¡ç†ä¿®å¤**ï¼šè§£å†³äº†ä¸“é—¨å½±å“ Kontext æ¨¡åž‹ä½¿ç”¨çš„å…³é”®å†…å­˜ä¼°ç®—é”™è¯¯ï¼Œé˜²æ­¢å†…å­˜ä¸è¶³é”™è¯¯å¹¶æé«˜å·¥ä½œæµç¨³å®šæ€§\n\nè¿™äº›å˜æ›´æå‡äº†é«˜çº§æ¨¡åž‹å·¥ä½œæµçš„å¯é æ€§ï¼ŒåŒæ—¶ä¿æŒäº† ComfyUI ä¸ºä»Žäº‹å‰æ²¿ç”Ÿæˆæ¨¡åž‹å·¥ä½œçš„ AI è‰ºæœ¯å®¶å’Œç ”ç©¶äººå‘˜æä¾›çš„çµæ´»æ€§ã€‚\n\n**ä¸»è¦æ¨¡åž‹æ”¯æŒæ–°å¢ž**\n\n*   **Cosmos Predict2 æ”¯æŒ**ï¼šå…¨é¢å®žçŽ°æ–‡æœ¬åˆ°å›¾åƒï¼ˆ2B å’Œ 14B æ¨¡åž‹ï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆå·¥ä½œæµï¼Œæ‰©å±•è§†é¢‘åˆ›ä½œåŠŸèƒ½\n*   **å¢žå¼ºçš„ Flux å…¼å®¹æ€§**ï¼šChroma Text Encoder çŽ°åœ¨èƒ½ä¸Žå¸¸è§„ Flux æ¨¡åž‹æ— ç¼åä½œï¼Œæå‡æ–‡æœ¬æ¡ä»¶è´¨é‡\n*   **LoRA è®­ç»ƒé›†æˆ**ï¼šä½¿ç”¨æƒé‡é€‚é…å™¨æ–¹æ¡ˆçš„å…¨æ–°åŽŸç”Ÿ LoRA è®­ç»ƒèŠ‚ç‚¹ï¼Œæ”¯æŒåœ¨ ComfyUI å·¥ä½œæµä¸­ç›´æŽ¥è¿›è¡Œæ¨¡åž‹å¾®è°ƒ\n\n**æ€§èƒ½å’Œç¡¬ä»¶ä¼˜åŒ–**\n\n*   **AMD GPU å¢žå¼º**ï¼šåœ¨ GFX1201 å’Œå…¶ä»–å…¼å®¹çš„ AMD GPU ä¸Šå¯ç”¨ FP8 æ“ä½œå’Œ PyTorch æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŠ é€ŸæŽ¨ç†\n*   **Apple Silicon ä¿®å¤**ï¼šè§£å†³äº† Apple è®¾å¤‡ä¸Šé•¿æœŸå­˜åœ¨çš„ FP16 æ³¨æ„åŠ›é—®é¢˜ï¼Œæå‡ Mac ç”¨æˆ·çš„ç¨³å®šæ€§\n*   **Flux æ¨¡åž‹ç¨³å®šæ€§**ï¼šè§£å†³äº†ç‰¹å®š Flux æ¨¡åž‹åœ¨ FP16 ç²¾åº¦ä¸‹ç”Ÿæˆé»‘è‰²å›¾åƒçš„é—®é¢˜\n\n**é«˜çº§é‡‡æ ·æ”¹è¿›**\n\n*   **Rectified Flow (RF) é‡‡æ ·å™¨**ï¼šæ–°å¢žæ”¯æŒ RF çš„ SEEDS å’Œå¤šæ­¥ DPM++ SDE é‡‡æ ·å™¨ï¼Œä¸ºå‰æ²¿æ¨¡åž‹æä¾›æ›´å¤šé‡‡æ ·é€‰é¡¹\n*   **ModelSamplingContinuousEDM**ï¼šæ–°å¢ž cosmos\\_rflow é€‰é¡¹ï¼Œå¢žå¼ºå¯¹ Cosmos æ¨¡åž‹çš„é‡‡æ ·æŽ§åˆ¶\n*   **å†…å­˜ä¼˜åŒ–**ï¼šæ”¹è¿›äº†æ”¯æŒæ— é™åˆ†è¾¨çŽ‡çš„ Cosmos æ¨¡åž‹çš„å†…å­˜ä¼°ç®—\n\n**å¼€å‘è€…å’Œé›†æˆåŠŸèƒ½**\n\n*   **SQLite æ•°æ®åº“æ”¯æŒ**ï¼šå¢žå¼ºè‡ªå®šä¹‰èŠ‚ç‚¹å’Œå·¥ä½œæµå­˜å‚¨çš„æ•°æ®ç®¡ç†åŠŸèƒ½\n*   **PyProject.toml é›†æˆ**ï¼šä»Ž pyproject æ–‡ä»¶è‡ªåŠ¨æ³¨å†Œ web æ–‡ä»¶å¤¹å’Œé…ç½®è®¾ç½®\n*   **å‰ç«¯çµæ´»æ€§**ï¼šæ”¯æŒè¯­ä¹‰åŒ–ç‰ˆæœ¬åŽç¼€å’Œé¢„å‘å¸ƒå‰ç«¯ç‰ˆæœ¬ï¼Œé€‚ç”¨äºŽè‡ªå®šä¹‰éƒ¨ç½²\n*   **åˆ†è¯å™¨å¢žå¼º**ï¼šé€šè¿‡ tokenizer\\_data é…ç½® min\\_length è®¾ç½®ï¼Œä¼˜åŒ–æ–‡æœ¬å¤„ç†\n\n**ä½¿ç”¨ä½“éªŒæ”¹è¿›**\n\n*   **Kontext å®½é«˜æ¯”ä¿®å¤**ï¼šè§£å†³äº†ä»…é™å°ç»„ä»¶çš„é™åˆ¶ï¼ŒçŽ°åœ¨åœ¨æ‰€æœ‰è¿žæŽ¥æ¨¡å¼ä¸‹éƒ½èƒ½æ­£å¸¸å·¥ä½œ\n*   **SaveLora ä¸€è‡´æ€§**ï¼šç»Ÿä¸€æ‰€æœ‰ä¿å­˜èŠ‚ç‚¹çš„æ–‡ä»¶åæ ¼å¼ï¼Œä¼˜åŒ–æ–‡ä»¶ç»„ç»‡\n*   **Python ç‰ˆæœ¬è­¦å‘Š**ï¼šä¸ºè¿‡æ—¶çš„ Python å®‰è£…æ·»åŠ è­¦æŠ¥ï¼Œé˜²æ­¢å…¼å®¹æ€§é—®é¢˜\n*   **WebcamCapture ä¿®å¤**ï¼šä¿®æ­£äº† IS\\_CHANGED ç­¾åï¼Œç¡®ä¿å®žæ—¶è¾“å…¥å·¥ä½œæµçš„å¯é æ€§\n\næ­¤ç‰ˆæœ¬æ˜¾è‘—æ‰©å±•äº† ComfyUI çš„æ¨¡åž‹ç”Ÿæ€ç³»ç»Ÿæ”¯æŒï¼ŒåŒæ—¶æä¾›äº†å…³é”®çš„ç¨³å®šæ€§æ”¹è¿›å’Œè·¨å¹³å°ç¡¬ä»¶å…¼å®¹æ€§å¢žå¼ºã€‚\n\næœ¬æ¬¡å‘å¸ƒä¸º ComfyUI åˆ›ä½œè€…å¸¦æ¥äº†å¼ºå¤§çš„æ–°å·¥ä½œæµå®žç”¨å·¥å…·å’Œæ€§èƒ½ä¼˜åŒ–ï¼š\n\n## æ–°çš„å·¥ä½œæµå·¥å…·\n\n*   **ImageStitch èŠ‚ç‚¹**ï¼šåœ¨å·¥ä½œæµä¸­æ— ç¼æ‹¼æŽ¥å¤šä¸ªå›¾åƒ - éžå¸¸é€‚åˆåˆ›å»ºå¯¹æ¯”ç½‘æ ¼æˆ–å¤åˆè¾“å‡º\n*   **GetImageSize èŠ‚ç‚¹**ï¼šæå–å›¾åƒå°ºå¯¸å¹¶æ”¯æŒæ‰¹å¤„ç†ï¼Œå¯¹äºŽåŠ¨æ€è°ƒæ•´å¤§å°çš„å·¥ä½œæµè‡³å…³é‡è¦\n*   **Regex Replace èŠ‚ç‚¹**ï¼šé«˜çº§æ–‡æœ¬å¤„ç†åŠŸèƒ½ï¼Œé€‚ç”¨äºŽæç¤ºè¯å·¥ç¨‹å’Œå­—ç¬¦ä¸²å¤„ç†å·¥ä½œæµ\n\n## å¢žå¼ºçš„æ¨¡åž‹å…¼å®¹æ€§\n\n*   **æ”¹è¿›çš„å¼ é‡å¤„ç†**ï¼šç®€åŒ–çš„åˆ—è¡¨å¤„ç†ä½¿å¤æ‚çš„å¤šæ¨¡åž‹å·¥ä½œæµæ›´åŠ å¯é \n*   **BFL API ä¼˜åŒ–**ï¼šå®Œå–„äº†å¯¹ Kontext \\[pro\\] å’Œ \\[max\\] æ¨¡åž‹çš„æ”¯æŒï¼Œæä¾›æ›´æ¸…æ™°çš„èŠ‚ç‚¹ç•Œé¢\n*   **æ€§èƒ½æå‡**ï¼šåœ¨è‰²åº¦å¤„ç†ä¸­ä½¿ç”¨èžåˆä¹˜åŠ è¿ç®—ï¼ŒåŠ å¿«ç”Ÿæˆé€Ÿåº¦\n\n## å¼€å‘è€…ä½“éªŒæ”¹è¿›\n\n*   **è‡ªå®šä¹‰èŠ‚ç‚¹æ”¯æŒ**ï¼šæ·»åŠ  pyproject.toml æ”¯æŒï¼Œæ”¹å–„è‡ªå®šä¹‰èŠ‚ç‚¹ä¾èµ–ç®¡ç†\n*   **å¸®åŠ©èœå•é›†æˆ**ï¼šåœ¨èŠ‚ç‚¹åº“ä¾§è¾¹æ ä¸­æ–°å¢žå¸®åŠ©ç³»ç»Ÿï¼ŒåŠ å¿«èŠ‚ç‚¹å‘çŽ°é€Ÿåº¦\n*   **API æ–‡æ¡£**ï¼šå¢žå¼º API èŠ‚ç‚¹æ–‡æ¡£ï¼Œæ”¯æŒå·¥ä½œæµè‡ªåŠ¨åŒ–\n\n## å‰ç«¯å’Œ UI å¢žå¼º\n\n*   **å‰ç«¯æ›´æ–°è‡³ v1.21.7**ï¼šå¤šé¡¹ç¨³å®šæ€§ä¿®å¤å’Œæ€§èƒ½æ”¹è¿›\n*   **è‡ªå®šä¹‰ API åŸºç¡€æ”¯æŒ**ï¼šæ”¹è¿›äº†è‡ªå®šä¹‰éƒ¨ç½²é…ç½®çš„å­è·¯å¾„å¤„ç†\n*   **å®‰å…¨åŠ å›º**ï¼šä¿®å¤ XSS æ¼æ´žï¼Œç¡®ä¿å·¥ä½œæµåˆ†äº«æ›´å®‰å…¨\n\n## é”™è¯¯ä¿®å¤å’Œç¨³å®šæ€§\n\n*   **Pillow å…¼å®¹æ€§**ï¼šæ›´æ–°äº†å·²å¼ƒç”¨çš„ API è°ƒç”¨ï¼Œä¿æŒä¸Žæœ€æ–°å›¾åƒå¤„ç†åº“çš„å…¼å®¹æ€§\n*   **ROCm æ”¯æŒ**ï¼šæ”¹è¿›äº† AMD GPU ç”¨æˆ·çš„ç‰ˆæœ¬æ£€æµ‹\n*   **æ¨¡æ¿æ›´æ–°**ï¼šå¢žå¼ºäº†è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘çš„é¡¹ç›®æ¨¡æ¿\n\nè¿™äº›æ›´æ–°å¼ºåŒ–äº† ComfyUI å¤„ç†å¤æ‚ AI å·¥ä½œæµçš„åŸºç¡€ï¼ŒåŒæ—¶é€šè¿‡æ”¹è¿›çš„æ–‡æ¡£å’Œè¾…åŠ©å·¥å…·è®©å¹³å°å¯¹æ–°ç”¨æˆ·æ›´åŠ å‹å¥½ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/gpt-image-1",
  "markdown": "# OpenAI GPT-Image-1 èŠ‚ç‚¹ - ComfyUI\n\n![OpenAI GPT-Image-1 èŠ‚ç‚¹æˆªå›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-gpt-image-1.jpg) OpenAI GPT-Image-1 æ˜¯ ComfyUI API èŠ‚ç‚¹ç³»åˆ—ä¸­çš„ä¸€å‘˜ï¼Œå®ƒå…è®¸ç”¨æˆ·é€šè¿‡ OpenAI çš„ **GPT-Image-1** æ¨¡åž‹ç”Ÿæˆå›¾åƒã€‚è¿™æ˜¯ä¸Ž ChatGPT 4o å›¾åƒç”Ÿæˆç›¸åŒçš„æ¨¡åž‹ã€‚ è¿™ä¸ªèŠ‚ç‚¹æ”¯æŒ:\n\n*   æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆ\n*   å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼ˆé€šè¿‡è’™ç‰ˆè¿›è¡Œä¿®å¤ç»˜åˆ¶ï¼‰\n\n## èŠ‚ç‚¹æ¦‚è¿°\n\n**OpenAI GPT-Image-1** èŠ‚ç‚¹é€šè¿‡ OpenAI çš„å›¾åƒç”Ÿæˆ API åŒæ­¥ç”Ÿæˆå›¾åƒã€‚å®ƒæŽ¥æ”¶æ–‡æœ¬æç¤ºå¹¶è¿”å›žç¬¦åˆæè¿°çš„å›¾åƒã€‚GPT-Image-1 æ˜¯ç›®å‰ OpenAI æœ€å…ˆè¿›çš„å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œèƒ½å¤Ÿåˆ›å»ºé«˜åº¦è¯¦ç»†å’Œé€¼çœŸçš„å›¾åƒã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…å¡«å‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | è¯´æ˜Ž  |\n| --- | --- | --- |\n| `prompt` | æ–‡æœ¬  | æè¿°æ‚¨æƒ³è¦ç”Ÿæˆçš„å›¾åƒå†…å®¹çš„æ–‡æœ¬æç¤º |\n\n### Widget å‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | é€‰é¡¹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- | --- |\n| `seed` | æ•´æ•°  | 0-2147483647 | 0   | ç”¨äºŽæŽ§åˆ¶ç”Ÿæˆç»“æžœçš„éšæœºç§å­ |\n| `quality` | é€‰é¡¹  | low, medium, high | low | å›¾åƒè´¨é‡è®¾ç½®ï¼Œå½±å“æˆæœ¬å’Œç”Ÿæˆæ—¶é—´ |\n| `background` | é€‰é¡¹  | opaque, transparent | opaque | è¿”å›žçš„å›¾åƒæ˜¯å¦å¸¦æœ‰èƒŒæ™¯ |\n| `size` | é€‰é¡¹  | auto, 1024x1024, 1024x1536, 1536x1024 | auto | ç”Ÿæˆå›¾åƒçš„å°ºå¯¸ |\n| `n` | æ•´æ•°  | 1-8 | 1   | ç”Ÿæˆçš„å›¾åƒæ•°é‡ |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°å | ç±»åž‹  | é€‰é¡¹  | é»˜è®¤å€¼ | è¯´æ˜Ž  |\n| --- | --- | --- | --- | --- |\n| `image` | å›¾åƒ  | ä»»ä½•å›¾åƒè¾“å…¥ | æ—    | å¯é€‰çš„å‚è€ƒå›¾åƒï¼Œç”¨äºŽå›¾åƒç¼–è¾‘ |\n| `mask` | è’™ç‰ˆ  | è’™ç‰ˆè¾“å…¥ | æ—    | å¯é€‰çš„è’™ç‰ˆï¼Œç”¨äºŽå±€éƒ¨é‡ç»˜ï¼ˆç™½è‰²åŒºåŸŸå°†è¢«æ›¿æ¢ï¼‰ |\n\n## ä½¿ç”¨ç¤ºä¾‹\n\n### æ–‡ç”Ÿå›¾åƒï¼ˆText to Imageï¼‰ç¤ºä¾‹\n\nä¸‹é¢çš„å›¾ç‰‡åŒ…å«äº†ä¸€ä¸ªç®€å•çš„æ–‡ç”Ÿå›¾åƒå·¥ä½œæµï¼Œè¯·ä¸‹è½½å¯¹åº”çš„å›¾åƒï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![ComfyUI openai-gpt-image-1å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/text2image.png) å¯¹åº”çš„å·¥ä½œæµéžå¸¸ç®€å•ï¼š ![ComfyUI openai-gpt-image-1 å·¥ä½œæµç¤ºä¾‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg) ä½ åªéœ€è¦åŠ è½½ `OpenAI GPT-Image-1` èŠ‚ç‚¹ï¼Œåœ¨ `prompt` èŠ‚ç‚¹ä¸­è¾“å…¥ä½ æƒ³è¦ç”Ÿæˆçš„å›¾åƒçš„æè¿°ï¼Œè¿žæŽ¥ä¸€ä¸ª `ä¿å­˜å›¾åƒï¼ˆSave Imageï¼‰` èŠ‚ç‚¹ï¼Œç„¶åŽè¿è¡Œå·¥ä½œæµå³å¯ã€‚\n\n### å›¾ç”Ÿå›¾ï¼ˆImage to Imageï¼‰ç¤ºä¾‹\n\nä¸‹é¢çš„å›¾ç‰‡åŒ…å«äº†ä¸€ä¸ªç®€å•çš„å›¾ç”Ÿå›¾å·¥ä½œæµï¼Œè¯·ä¸‹è½½å¯¹åº”çš„å›¾åƒï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![ComfyUI openai-gpt-image-1å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/image2image.png) æˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥ï¼š ![ComfyUI openai-gpt-image-1 å·¥ä½œæµ input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/input.webp) è¿™ä¸ªå·¥ä½œæµä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ `OpenAI GPT-Image-1` èŠ‚ç‚¹ç”Ÿæˆå›¾åƒï¼Œå¹¶ä½¿ç”¨ `åŠ è½½å›¾åƒï¼ˆLoad Imageï¼‰` èŠ‚ç‚¹åŠ è½½è¾“å…¥çš„å›¾åƒï¼Œç„¶åŽè¿žæŽ¥åˆ° `OpenAI GPT-Image-1` èŠ‚ç‚¹çš„ `image` è¾“å…¥ä¸­ã€‚ ![ComfyUI openai-gpt-image-1 å·¥ä½œæµç¤ºä¾‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg)\n\n### å¤šå¼ å›¾ç‰‡è¾“å…¥ç¤ºä¾‹\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡å¹¶æ‹–å…¥ ComfyUI æ¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![å¤šå¼ å›¾ç‰‡è¾“å…¥ç¤ºä¾‹](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/multiple_image_input.png) ä½¿ç”¨ä¸‹é¢çš„å¸½å­ä½œä¸ºé¢å¤–çš„è¾“å…¥å›¾ç‰‡ ![å¸½å­](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/hat.webp) å¯¹åº”å·¥ä½œæµå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š ![å¤šå¼ å›¾ç‰‡è¾“å…¥ç¤ºä¾‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png) ä½¿ç”¨äº†`Batch Images` èŠ‚ç‚¹æ¥å°†å¤šå¼ å›¾åƒåŠ è½½åˆ° `OpenAI GPT-Image-1` èŠ‚ç‚¹ ä¸­\n\n### å±€éƒ¨é‡ç»˜ï¼ˆInpaintingï¼‰å·¥ä½œæµ\n\nGPT-Image-1 ä¹Ÿæ”¯æŒå›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œå…è®¸æ‚¨ä½¿ç”¨è’™ç‰ˆæŒ‡å®šè¦æ›¿æ¢çš„åŒºåŸŸï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„å±€éƒ¨é‡ç»˜å·¥ä½œæµç¤ºä¾‹ï¼š ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµï¼Œæˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨ å›¾ç”Ÿå›¾å·¥ä½œæµéƒ¨åˆ†çš„è¾“å…¥å›¾ç‰‡ã€‚ ![ComfyUI openai-gpt-image-1å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/inpaint.png) å¯¹åº”å·¥ä½œæµå…¥å›¾æ‰€ç¤º ![ComfyUI openai-gpt-image-1 å·¥ä½œæµç¤ºä¾‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg) ä¸Žå›¾ç”Ÿå›¾å·¥ä½œæµç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨`Load Image`ä¸­é€šè¿‡å³é”®èœå•ä½¿ç”¨ è’™ç‰ˆç¼–è¾‘å™¨ï¼ˆMaskEditorï¼‰ å¹¶ç»˜åˆ¶è’™ç‰ˆï¼Œç„¶åŽè¿žæŽ¥åˆ° `OpenAI GPT-Image-1` èŠ‚ç‚¹çš„ `mask` è¾“å…¥ä¸­ï¼Œæ¥å®Œæˆå¯¹åº”å·¥ä½œæµã€‚ **æ³¨æ„äº‹é¡¹**\n\n*   è’™ç‰ˆå’Œå›¾åƒå¿…é¡»å¤§å°ç›¸åŒ\n*   å½“è¾“å…¥å¤§å°ºå¯¸å›¾ç‰‡æ—¶ï¼ŒèŠ‚ç‚¹ä¼šè‡ªåŠ¨å°†å›¾åƒç¼©å°åˆ°åˆé€‚çš„å°ºå¯¸\n\n## å¸¸è§é—®é¢˜"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/dall-e-2",
  "markdown": "# OpenAI DALLÂ·E 2 èŠ‚ç‚¹ - ComfyUI\n\n![OpenAI DALLÂ·E 2 èŠ‚ç‚¹æˆªå›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/api_nodes/openai-dall-e-2.jpg) OpenAI DALLÂ·E 2 æ˜¯ ComfyUI API èŠ‚ç‚¹ç³»åˆ—ä¸­çš„ä¸€å‘˜ï¼Œå®ƒå…è®¸ç”¨æˆ·é€šè¿‡ OpenAI çš„ **DALLÂ·E 2** æ¨¡åž‹ç”Ÿæˆå›¾åƒã€‚ è¿™ä¸ªèŠ‚ç‚¹æ”¯æŒ:\n\n*   æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆ\n*   å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼ˆé€šè¿‡è’™ç‰ˆè¿›è¡Œä¿®å¤ç»˜åˆ¶ï¼‰\n\n## èŠ‚ç‚¹æ¦‚è¿°\n\n**OpenAI DALLÂ·E 2** èŠ‚ç‚¹é€šè¿‡ OpenAI çš„å›¾åƒç”Ÿæˆ API åŒæ­¥ç”Ÿæˆå›¾åƒã€‚å®ƒæŽ¥æ”¶æ–‡æœ¬æç¤ºå¹¶è¿”å›žç¬¦åˆæè¿°çš„å›¾åƒã€‚\n\n## å‚æ•°è¯´æ˜Ž\n\n### å¿…å¡«å‚æ•°\n\n| å‚æ•°å | è¯´æ˜Ž  |\n| --- | --- |\n| `prompt` | æ–‡æœ¬æç¤ºï¼Œæè¿°ä½ æƒ³è¦ç”Ÿæˆçš„å›¾åƒå†…å®¹ |\n\n### Widget å‚æ•°\n\n| å‚æ•°å | è¯´æ˜Ž  | é€‰é¡¹/èŒƒå›´ | é»˜è®¤å€¼ |\n| --- | --- | --- | --- |\n| `seed` | ç”Ÿæˆå›¾åƒçš„ç§å­å€¼ï¼ˆç›®å‰åœ¨åŽç«¯æœªå®žçŽ°ï¼‰ | 0 åˆ° 2^31-1 | 0   |\n| `size` | è¾“å‡ºå›¾åƒçš„å°ºå¯¸ | â€256x256â€, â€œ512x512â€, â€œ1024x1024\" | \"1024x1024â€ |\n| `n` | ç”Ÿæˆçš„å›¾åƒæ•°é‡ | 1 åˆ° 8 | 1   |\n\n### å¯é€‰å‚æ•°\n\n| å‚æ•°å | è¯´æ˜Ž  | é€‰é¡¹/èŒƒå›´ | é»˜è®¤å€¼ |\n| --- | --- | --- | --- |\n| `image` | å¯é€‰çš„å‚è€ƒå›¾åƒï¼Œç”¨äºŽå›¾åƒç¼–è¾‘ | ä»»ä½•å›¾åƒè¾“å…¥ | æ—    |\n| `mask` | å¯é€‰çš„è’™ç‰ˆï¼Œç”¨äºŽå±€éƒ¨é‡ç»˜ | è’™ç‰ˆè¾“å…¥ | æ—    |\n\n## ä½¿ç”¨æ–¹æ³•\n\n## å·¥ä½œæµç¤ºä¾‹\n\nç›®å‰è¯¥API èŠ‚ç‚¹æ”¯æŒä¸¤ç§å·¥ä½œæµï¼Œåˆ†åˆ«æ˜¯ï¼š\n\n*   æ–‡ç”Ÿå›¾åƒï¼ˆText to Imageï¼‰\n*   å±€éƒ¨é‡ç»˜ï¼ˆInpaintingï¼‰\n\n### æ–‡ç”Ÿå›¾åƒï¼ˆText to Imageï¼‰ç¤ºä¾‹\n\nä¸‹é¢çš„å›¾ç‰‡åŒ…å«äº†ä¸€ä¸ªç®€å•çš„æ–‡ç”Ÿå›¾åƒå·¥ä½œæµï¼Œè¯·ä¸‹è½½å¯¹åº”çš„å›¾åƒï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![ComfyUI openai-dall-e-2å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/text2image.png) å¯¹åº”çš„ç¤ºä¾‹éžå¸¸ç®€å• ![ComfyUI openai-dall-e-2 å·¥ä½œæµç¤ºä¾‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg) ä½ åªéœ€è¦åœ¨åŠ è½½ `OpenAI DALLÂ·E 2` èŠ‚ç‚¹åŽï¼Œåœ¨ `prompt` èŠ‚ç‚¹ä¸­è¾“å…¥ä½ æƒ³è¦ç”Ÿæˆçš„å›¾åƒçš„æè¿°ï¼Œå¹¶è¿žæŽ¥ä¸€ä¸ª `ä¿å­˜å›¾åƒï¼ˆSave Imageï¼‰` èŠ‚ç‚¹ï¼Œç„¶åŽè¿è¡Œå·¥ä½œæµå³å¯\n\n### å±€éƒ¨é‡ç»˜ï¼ˆInpaintingï¼‰å·¥ä½œæµ\n\nDALLÂ·E 2 æ”¯æŒå›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œå…è®¸æ‚¨ä½¿ç”¨è’™ç‰ˆæŒ‡å®šè¦æ›¿æ¢çš„åŒºåŸŸï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„å±€éƒ¨é‡ç»˜å·¥ä½œæµç¤ºä¾‹ï¼š\n\n#### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![ComfyUI openai-dall-e-2å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/inpainting.png) æˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥ï¼š ![ComfyUI openai-dall-e-2 å·¥ä½œæµ input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/input.jpg) \n\n#### 2\\. å·¥ä½œæµæ–‡ä»¶ä½¿ç”¨è¯´æ˜Ž\n\n![ComfyUI openai-dall-e-2 å·¥ä½œæµç¤ºä¾‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg) ç”±äºŽæ­¤å·¥ä½œæµè¾ƒä¸ºç®€å•ï¼Œå¦‚æžœä½ æƒ³è¦è‡ªå·±æ‰‹åŠ¨å®žçŽ°å¯¹åº”çš„å·¥ä½œæµï¼Œå¯ä»¥æŒ‰ç…§ä¸‹é¢çš„æ­¥éª¤å®Œæˆå¯¹åº”çš„å·¥ä½œæµ\n\n1.  ä½¿ç”¨`åŠ è½½å›¾åƒï¼ˆLoad Imageï¼‰`èŠ‚ç‚¹åŠ è½½å›¾åƒ\n2.  åœ¨åŠ è½½å›¾åƒèŠ‚ç‚¹ä¸­å³é”®ï¼Œé€‰æ‹© `é®ç½©ç¼–è¾‘å™¨ï¼ˆMaskEditorï¼‰`\n3.  åœ¨é®ç½©ç¼–è¾‘å™¨ä¸­ï¼Œä½¿ç”¨ç”»ç¬”ç»˜åˆ¶ä½ æƒ³è¦é‡ç»˜çš„åŒºåŸŸ\n4.  åœ¨**OpenAI DALLÂ·E 2** èŠ‚ç‚¹ `image` è¾“å…¥ä¸­è¿žæŽ¥åŠ è½½çš„å›¾åƒ\n5.  **OpenAI DALLÂ·E 2** èŠ‚ç‚¹ `mask` è¾“å…¥ä¸­è¿žæŽ¥è’™ç‰ˆ\n6.  ç¼–è¾‘ `prompt` èŠ‚ç‚¹çš„æç¤ºè¯\n7.  è¿è¡Œå·¥ä½œæµ\n\n**æ³¨æ„äº‹é¡¹**\n\n*   å¦‚æžœæ‚¨æƒ³ä½¿ç”¨å›¾åƒç¼–è¾‘åŠŸèƒ½ï¼Œå¿…é¡»åŒæ—¶æä¾›å›¾åƒå’Œè’™ç‰ˆï¼ˆç¼ºä¸€ä¸å¯ï¼‰\n*   è’™ç‰ˆå’Œå›¾åƒå¿…é¡»å¤§å°ç›¸åŒ\n*   å½“è¾“å…¥å¤§å°ºå¯¸å›¾ç‰‡æ—¶ï¼ŒèŠ‚ç‚¹ä¼šè‡ªåŠ¨å°†å›¾åƒç¼©å°åˆ°åˆé€‚çš„å°ºå¯¸\n*   API è¿”å›žçš„ URL æ˜¯çŸ­æœŸæœ‰æ•ˆçš„ï¼Œè¯·ç¡®ä¿åŠæ—¶ä¿å­˜éœ€è¦çš„ç»“æžœ\n*   æ¯æ¬¡ç”Ÿæˆéƒ½ä¼šæ¶ˆè€—ç§¯åˆ†ï¼Œæ ¹æ®å›¾åƒå¤§å°å’Œæ•°é‡æ”¶è´¹\n\n## å¸¸è§é—®é¢˜"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/backend/more_on_inputs",
  "markdown": "# éšè—ä¸Žçµæ´»è¾“å…¥ - ComfyUI\n\n## éšè—è¾“å…¥\n\né™¤äº†åœ¨å®¢æˆ·ç«¯åˆ›å»ºå¯¹åº”è¾“å…¥æˆ–å°éƒ¨ä»¶çš„ `required`ï¼ˆå¿…éœ€ï¼‰å’Œ `optional`ï¼ˆå¯é€‰ï¼‰è¾“å…¥å¤–ï¼Œè¿˜æœ‰ä¸‰ç§ `hidden`ï¼ˆéšè—ï¼‰è¾“å…¥é€‰é¡¹ï¼Œå…è®¸è‡ªå®šä¹‰èŠ‚ç‚¹ä»ŽæœåŠ¡å™¨è¯·æ±‚ç‰¹å®šä¿¡æ¯ã€‚ è¿™äº›é€‰é¡¹é€šè¿‡åœ¨ `INPUT_TYPES` çš„ `dict` ä¸­è¿”å›ž `hidden` å­—æ®µæ¥è®¿é—®ï¼Œå…¶ç­¾åä¸º `dict[str,str]`ï¼Œå¯åŒ…å« `PROMPT`ã€`EXTRA_PNGINFO` æˆ– `UNIQUE_ID` ä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªã€‚\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": {...},\n        \"optional\": {...},\n        \"hidden\": {\n            \"unique_id\": \"UNIQUE_ID\",\n            \"prompt\": \"PROMPT\", \n            \"extra_pnginfo\": \"EXTRA_PNGINFO\",\n        }\n    }\n```\n\n### UNIQUE\\_ID\n\n`UNIQUE_ID` æ˜¯èŠ‚ç‚¹çš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œä¸Žå®¢æˆ·ç«¯èŠ‚ç‚¹çš„ `id` å±žæ€§ç›¸åŒã€‚å®ƒé€šå¸¸ç”¨äºŽå®¢æˆ·ç«¯ä¸ŽæœåŠ¡å™¨çš„é€šä¿¡ï¼ˆå‚è§ [æ¶ˆæ¯](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_messages#%E8%8E%B7%E5%8F%96%E5%BD%93%E5%89%8D%E8%8A%82%E7%82%B9-id-node-id)ï¼‰ã€‚\n\n### PROMPT\n\n`PROMPT` æ˜¯å®¢æˆ·ç«¯å‘é€åˆ°æœåŠ¡å™¨çš„å®Œæ•´æç¤ºï¼ˆpromptï¼‰ã€‚è¯¦è§ [prompt å¯¹è±¡](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_objects_and_hijacking#prompt)ã€‚\n\n`EXTRA_PNGINFO` æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä¼šè¢«å¤åˆ¶åˆ°ä»»ä½•ä¿å­˜çš„ `.png` æ–‡ä»¶çš„å…ƒæ•°æ®ä¸­ã€‚è‡ªå®šä¹‰èŠ‚ç‚¹å¯ä»¥å°†é¢å¤–ä¿¡æ¯å­˜å‚¨åœ¨è¯¥å­—å…¸ä¸­ç”¨äºŽä¿å­˜ï¼ˆæˆ–ä½œä¸ºä¸Žä¸‹æ¸¸èŠ‚ç‚¹é€šä¿¡çš„ä¸€ç§æ–¹å¼ï¼‰ã€‚\n\n### DYNPROMPT\n\n`DYNPROMPT` æ˜¯ `comfy_execution.graph.DynamicPrompt` çš„ä¸€ä¸ªå®žä¾‹ã€‚å®ƒä¸Ž `PROMPT` ä¸åŒï¼Œ`DYNPROMPT` å¯èƒ½ä¼šåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­å›  [èŠ‚ç‚¹æ‰©å±•](https://docs.comfy.org/zh-CN/custom-nodes/backend/expansion) è€Œå‘ç”Ÿå˜åŒ–ã€‚\n\n## çµæ´»è¾“å…¥\n\n### è‡ªå®šä¹‰æ•°æ®ç±»åž‹\n\nå¦‚æžœä½ å¸Œæœ›åœ¨è‡ªå®šä¹‰èŠ‚ç‚¹ä¹‹é—´ä¼ é€’æ•°æ®ï¼Œå®šä¹‰è‡ªå®šä¹‰æ•°æ®ç±»åž‹ä¼šå¾ˆæœ‰å¸®åŠ©ã€‚è¿™å‡ ä¹Žåªéœ€è¦ä¸ºæ•°æ®ç±»åž‹é€‰æ‹©ä¸€ä¸ªå”¯ä¸€çš„å¤§å†™å­—ç¬¦ä¸²åç§°ï¼Œä¾‹å¦‚ `CHEESE`ã€‚ ç„¶åŽä½ å¯ä»¥åœ¨èŠ‚ç‚¹çš„ `INPUT_TYPES` å’Œ `RETURN_TYPES` ä¸­ä½¿ç”¨ `CHEESE`ï¼ŒComfy å®¢æˆ·ç«¯åªå…è®¸ `CHEESE` è¾“å‡ºè¿žæŽ¥åˆ° `CHEESE` è¾“å…¥ã€‚`CHEESE` å¯ä»¥æ˜¯ä»»æ„ Python å¯¹è±¡ã€‚ éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œç”±äºŽ Comfy å®¢æˆ·ç«¯å¹¶ä¸äº†è§£ `CHEESE`ï¼Œä½ éœ€è¦ï¼ˆé™¤éžä¸º `CHEESE` å®šä¹‰äº†è‡ªå®šä¹‰å°éƒ¨ä»¶ï¼Œè¿™å±žäºŽè¿›é˜¶è¯é¢˜ï¼‰å¼ºåˆ¶å®ƒä½œä¸ºè¾“å…¥è€Œä¸æ˜¯å°éƒ¨ä»¶ã€‚è¿™å¯ä»¥é€šè¿‡è¾“å…¥é€‰é¡¹å­—å…¸ä¸­çš„ `forceInput` é€‰é¡¹å®žçŽ°ï¼š\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": { \"my_cheese\": (\"CHEESE\", {\"forceInput\":True}) }\n    }\n```\n\n### é€šé…è¾“å…¥\n\n```\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": { \"anything\": (\"*\",{})},\n    }\n\n@classmethod\ndef VALIDATE_INPUTS(s, input_types):\n    return True\n```\n\nå‰ç«¯å…è®¸ä½¿ç”¨ `*` è¡¨ç¤ºè¯¥è¾“å…¥å¯ä»¥è¿žæŽ¥åˆ°ä»»æ„æ¥æºã€‚ç”±äºŽåŽç«¯å¹¶æœªæ­£å¼æ”¯æŒæ­¤åŠŸèƒ½ï¼Œä½ å¯ä»¥é€šè¿‡åœ¨ `VALIDATE_INPUTS` å‡½æ•°ä¸­æŽ¥å—åä¸º `input_types` çš„å‚æ•°æ¥è·³è¿‡ç±»åž‹æ ¡éªŒã€‚ï¼ˆè¯¦è§ [VALIDATE\\_INPUTS](https://docs.comfy.org/zh-CN/custom-nodes/backend/server_overview#validate-inputs) äº†è§£æ›´å¤šä¿¡æ¯ã€‚ï¼‰ èŠ‚ç‚¹éœ€è¦è‡ªè¡Œå¤„ç†ä¼ å…¥çš„æ•°æ®ã€‚\n\n### åŠ¨æ€åˆ›å»ºçš„è¾“å…¥\n\nå¦‚æžœè¾“å…¥æ˜¯åœ¨å®¢æˆ·ç«¯åŠ¨æ€åˆ›å»ºçš„ï¼Œåˆ™æ— æ³•åœ¨ Python æºç ä¸­å®šä¹‰ã€‚ä¸ºäº†è®¿é—®è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª `optional` å­—å…¸ï¼Œå…è®¸ Comfy ä¼ é€’ä»»æ„åç§°çš„æ•°æ®ã€‚ç”±äºŽ Comfy æœåŠ¡å™¨\n\n```\nclass ContainsAnyDict(dict):\n    def __contains__(self, key):\n        return True\n...\n\n@classmethod\ndef INPUT_TYPES(s):\n    return {\n        \"required\": {},\n        \"optional\": ContainsAnyDict()\n    }\n...\n\ndef main_method(self, **kwargs):\n    # åŠ¨æ€åˆ›å»ºçš„è¾“å…¥æ•°æ®ä¼šåœ¨ kwargs å­—å…¸ä¸­\n\n```\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/properties",
  "markdown": "# å±žæ€§ - ComfyUI\n\n## èŠ‚ç‚¹æ˜¯å±žæ€§çš„å®¹å™¨\n\nèŠ‚ç‚¹é€šå¸¸å…·æœ‰ _**å±žæ€§**_ã€‚ä¹Ÿç§°ä¸º _**å‚æ•°**_ æˆ– _**ç‰¹æ€§**_ï¼ŒèŠ‚ç‚¹å±žæ€§æ˜¯å¯ä»¥æ›´æ”¹çš„å˜é‡ã€‚ä¸€äº›å±žæ€§å¯ä»¥é€šè¿‡ç”¨æˆ·æ‰‹åŠ¨è°ƒæ•´ï¼Œä½¿ç”¨ç§°ä¸º _**å°éƒ¨ä»¶**_ çš„æ•°æ®è¾“å…¥å­—æ®µã€‚å…¶ä»–å±žæ€§å¯ä»¥é€šè¿‡è¿žæŽ¥åˆ°å±žæ€§ _**è¾“å…¥æ’æ§½**_ æˆ–ç«¯å£çš„å…¶ä»–èŠ‚ç‚¹è‡ªåŠ¨é©±åŠ¨ã€‚é€šå¸¸ï¼Œå±žæ€§å¯ä»¥åœ¨å°éƒ¨ä»¶å’Œè¾“å…¥ä¹‹é—´è½¬æ¢ï¼Œä»Žè€Œå…è®¸ç”¨æˆ·æ‰‹åŠ¨æˆ–è‡ªåŠ¨æŽ§åˆ¶å±žæ€§å€¼ã€‚ å±žæ€§å¯ä»¥é‡‡å–å¤šç§å½¢å¼ï¼Œå¹¶åŒ…å«å¤šç§ä¸åŒç±»åž‹çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œ**åŠ è½½æ£€æŸ¥ç‚¹** èŠ‚ç‚¹å…·æœ‰ä¸€ä¸ªå±žæ€§ï¼šç”Ÿæˆæ¨¡åž‹æ£€æŸ¥ç‚¹æ–‡ä»¶çš„æ–‡ä»¶è·¯å¾„ã€‚**KSampler** èŠ‚ç‚¹å…·æœ‰å¤šä¸ªå±žæ€§ï¼Œä¾‹å¦‚é‡‡æ · **æ­¥éª¤**ã€**CFG** æ¯”ä¾‹ã€**é‡‡æ ·å™¨åç§°** ç­‰ç­‰ã€‚ ![èŠ‚ç‚¹å±žæ€§](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_properties.png)\n\n## æ•°æ®ç±»åž‹\n\nä¿¡æ¯å¯ä»¥ä»¥å¤šç§ä¸åŒå½¢å¼å‡ºçŽ°ï¼Œç§°ä¸º _**æ•°æ®ç±»åž‹**_ã€‚ä¾‹å¦‚ï¼Œå­—æ¯æ•°å­—æ–‡æœ¬ç§°ä¸º _**å­—ç¬¦ä¸²**_ï¼Œæ•´æ•°ç§°ä¸º _**æ•´æ•°**_ï¼Œå¸¦å°æ•°ç‚¹çš„æ•°å­—ç§°ä¸º _**æµ®ç‚¹æ•°**_ æˆ– _**æµ®ç‚¹**_ã€‚æ–°çš„æ•°æ®ç±»åž‹æ€»æ˜¯è¢«æ·»åŠ åˆ° ComfyUI ä¸­ã€‚ ComfyUI æ˜¯ç”¨ Python è„šæœ¬è¯­è¨€ç¼–å†™çš„ï¼Œè¯¥è¯­è¨€å¯¹æ•°æ®ç±»åž‹éžå¸¸å®½å®¹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒComfyUI çŽ¯å¢ƒæ˜¯éžå¸¸ _**å¼ºç±»åž‹**_ çš„ã€‚è¿™æ„å‘³ç€ä¸åŒçš„æ•°æ®ç±»åž‹ä¸èƒ½æ··åˆã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä¸èƒ½å°†å›¾åƒè¾“å‡ºè¿žæŽ¥åˆ°æ•´æ•°è¾“å…¥ã€‚è¿™å¯¹ç”¨æˆ·æ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„å¥½å¤„ï¼ŒæŒ‡å¯¼ä»–ä»¬æ­£ç¡®æž„å»ºå·¥ä½œæµç¨‹å¹¶é˜²æ­¢ç¨‹åºé”™è¯¯ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/rodin/model-generation",
  "markdown": "# Rodin API èŠ‚ç‚¹æ¨¡åž‹ç”Ÿæˆ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\nHyper3D Rodin (hyper3d.ai) æ˜¯ä¸€ä¸ªä¸“æ³¨äºŽé€šè¿‡äººå·¥æ™ºèƒ½å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡ã€å¯ç›´æŽ¥ç”¨äºŽç”Ÿäº§çŽ¯å¢ƒçš„3Dæ¨¡åž‹å’Œæè´¨çš„å¹³å°ã€‚ ç›®å‰ ComfyUI å·²åŽŸç”Ÿé›†æˆäº†å¯¹åº” Rodin æ¨¡åž‹ç”Ÿæˆ API ï¼ŒçŽ°åœ¨ä½ å¯ä»¥åœ¨ ComfyUI ä¸­ä¾¿æ·åœ°ä½¿ç”¨ç›¸å…³èŠ‚ç‚¹æ¥è¿›è¡Œæ¨¡åž‹ç”Ÿæˆ ç›®å‰ ComfyUI çš„ API èŠ‚ç‚¹ä¸­å·²ç»æ”¯æŒ Rodin ä»¥ä¸‹æ¨¡åž‹ç”Ÿæˆèƒ½åŠ›ï¼š\n\n*   å•è§†è§’æ¨¡åž‹ç”Ÿæˆ\n*   å¤šè§†è§’æ¨¡åž‹ç”Ÿæˆ\n*   å¤šç§ä¸åŒç²¾åº¦çš„æ¨¡åž‹ç”Ÿæˆ\n\n## å•è§†è§’æ¨¡åž‹ç”Ÿæˆå·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/rodin_image_to_model.json)\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/doll.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Rodin Image to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„è¾“å…¥å›¾ç‰‡\n2.  (å¯é€‰)åœ¨ `Rodin 3D Generate - Regular Generate` è°ƒæ•´å¯¹åº”å‚æ•°\n    *   polygon\\_count: å¯ä»¥è®¾ç½®ä¸åŒçš„é¢æ•°, è¶Šå¤§æ¨¡åž‹è¶Šå¹³æ»‘è¶Šç²¾ç»†\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œæ¨¡åž‹çš„ç”Ÿæˆï¼Œå·¥ä½œæµå®ŒæˆåŽå¯¹åº”çš„æ¨¡åž‹ä¼šè‡ªåŠ¨ä¿å­˜è‡³ `ComfyUI/output/Rodin` ç›®å½•ä¸‹\n4.  åœ¨ `Preview 3D` èŠ‚ç‚¹ä¸­ç‚¹å‡»å±•å¼€èœå•\n5.  é€‰æ‹©`Export` å¯ä»¥ç›´æŽ¥å°†å¯¹åº”æ¨¡åž‹å¯¼å‡º\n\n## å¤šè§†è§’æ¨¡åž‹ç”Ÿæˆå·¥ä½œæµ\n\nå¯¹åº”çš„ `Rodin 3D Generate - Regular Generate` æœ€å¤šå…è®¸5å¼ å›¾åƒè¾“å…¥\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä½ å¯ä»¥å°†å•è§†è§’éƒ¨åˆ†çš„å·¥ä½œæµä¿®æ”¹ä¸ºå¤šè§†è§’å·¥ä½œæµï¼Œæˆ–è€…ç›´æŽ¥ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµæ–‡ä»¶ ä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/api_rodin_multiview_to_model.json)\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/front.jpg) ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/back.jpg) ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/left.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Rodin Image to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„è¾“å…¥å›¾ç‰‡\n2.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œæ¨¡åž‹çš„ç”Ÿæˆï¼Œå·¥ä½œæµå®ŒæˆåŽå¯¹åº”çš„æ¨¡åž‹ä¼šè‡ªåŠ¨ä¿å­˜è‡³ `ComfyUI/output/Rodin` ç›®å½•ä¸‹\n3.  åœ¨ `Preview 3D` èŠ‚ç‚¹ä¸­ç‚¹å‡»å±•å¼€èœå•\n4.  é€‰æ‹©`Export` å¯ä»¥ç›´æŽ¥å°†å¯¹åº”æ¨¡åž‹å¯¼å‡º\n\n## å…¶å®ƒç›¸å…³èŠ‚ç‚¹\n\nç›®å‰åœ¨ ComfyUI ä¸­ï¼Œ Rodin æä¾›äº†ä¸åŒç±»åž‹çš„æ¨¡åž‹ç”ŸæˆèŠ‚ç‚¹ï¼Œç”±äºŽå¯¹åº”è¾“å…¥æ¡ä»¶ä¸Žæœ¬æ–‡ä»‹ç»çš„å·¥ä½œæµç›¸åŒï¼Œä½ å¯ä»¥æŒ‰éœ€å¯ç”¨ï¼Œå¦å¤–åœ¨å¯¹åº”æ¨¡æ¿ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å¯¹åº”çš„èŠ‚ç‚¹ï¼Œä½ ä¹Ÿå¯ä»¥æŒ‰éœ€ä¿®æ”¹å¯¹åº”èŠ‚ç‚¹æ¨¡å¼æ¥å¯ç”¨ ![Rodin å…¶å®ƒç›¸å…³èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/rodin/other_nodes.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/runway/video-generation",
  "markdown": "# Runway API èŠ‚ç‚¹ è§†é¢‘ç”Ÿæˆ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\nRunway æ˜¯ä¸€å®¶ä¸“æ³¨äºŽç”Ÿæˆå¼ AI çš„ç§‘æŠ€å…¬å¸ï¼Œæä¾›å¼ºå¤§çš„è§†é¢‘ç”ŸæˆåŠŸèƒ½ã€‚ç›®å‰ ComfyUI å·²é›†æˆ Runway APIï¼Œä½ å¯ä»¥ç›´æŽ¥åœ¨ ComfyUI ä¸­ä½¿ç”¨ç›¸å…³èŠ‚ç‚¹è¿›è¡Œè§†é¢‘ç”Ÿæˆã€‚ ç›®å‰ ComfyUI ä¸­åŽŸç”Ÿé›†æˆäº† Runway çš„ä»¥ä¸‹è§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼š\n\n*   Runway Gen3a turbo\n*   Runway Gen4 turbo\n*   Runway First Last Frame to video\n\n## Gen3a turbo å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„è§†é¢‘çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/runway_image_to_video_gen3a_turbo.json)\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![ComfyUI Runway gen3a turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/steampunk.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Runway gen3a turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„è¾“å…¥å›¾ç‰‡\n2.  åœ¨ `Runway Gen3a turbo` èŠ‚ç‚¹ä¸­è®¾ç½® `prompt` æè¿°è§†é¢‘å†…å®¹ï¼Œä¿®æ”¹ `duration` å‚æ•°æ¥è®¾ç½®è§†é¢‘æ—¶é•¿, ä¿®æ”¹ `ratio` å‚æ•°æ¥è®¾ç½®è§†é¢‘å®½é«˜æ¯”\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘çš„ç”Ÿæˆã€‚\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Video` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„è§†é¢‘ï¼ˆå³é”®èœå•å¯ä»¥ä¿å­˜ï¼‰ï¼Œå¯¹åº”çš„è§†é¢‘ä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n## Gen4 turbo å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„è§†é¢‘çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/runway_gen4_turo_image_to_video.json)\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/godfather.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Runway gen4 turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„è¾“å…¥å›¾ç‰‡\n2.  åœ¨ `Runway Gen4 turbo` èŠ‚ç‚¹ä¸­è®¾ç½® `prompt` æè¿°è§†é¢‘å†…å®¹ï¼Œä¿®æ”¹ `duration` å‚æ•°æ¥è®¾ç½®è§†é¢‘æ—¶é•¿, ä¿®æ”¹ `ratio` å‚æ•°æ¥è®¾ç½®è§†é¢‘å®½é«˜æ¯”\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘çš„ç”Ÿæˆã€‚\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Video` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„è§†é¢‘ï¼ˆå³é”®èœå•å¯ä»¥ä¿å­˜ï¼‰ï¼Œå¯¹åº”çš„è§†é¢‘ä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n## é¦–å°¾å¸§è§†é¢‘ç”Ÿæˆå·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„è§†é¢‘çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/runway_first_last_frame.json)\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/first.jpg) ![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/last.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Runway gen4 turbo image to video Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½èµ·å§‹å¸§\n2.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½ç»“æŸå¸§\n3.  åœ¨ `Runway First-Last-Frame to Video` èŠ‚ç‚¹ä¸­è®¾ç½® `prompt` æè¿°è§†é¢‘å†…å®¹ï¼Œä¿®æ”¹ `duration` å‚æ•°æ¥è®¾ç½®è§†é¢‘æ—¶é•¿, ä¿®æ”¹ `ratio` å‚æ•°æ¥è®¾ç½®è§†é¢‘å®½é«˜æ¯”\n4.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘çš„ç”Ÿæˆã€‚\n5.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Video` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„è§†é¢‘ï¼ˆå³é”®èœå•å¯ä»¥ä¿å­˜ï¼‰ï¼Œå¯¹åº”çš„è§†é¢‘ä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/models",
  "markdown": "# æ¨¡åž‹ - ComfyUI\n\n## æ¨¡åž‹æ˜¯å¿…ä¸å¯å°‘çš„\n\næ¨¡åž‹æ˜¯åª’ä½“ç”Ÿæˆå·¥ä½œæµç¨‹çš„æ ¸å¿ƒç»„ä»¶ã€‚é€šè¿‡ç»„åˆå’Œæ··åˆï¼Œå®ƒä»¬èƒ½å¤Ÿå®žçŽ°å¤šæ ·åŒ–çš„åˆ›æ„æ•ˆæžœã€‚ _**æ¨¡åž‹**_ ä¸€è¯æœ‰å¤šç§å«ä¹‰ã€‚åœ¨è¿™é‡Œï¼Œå®ƒæŒ‡çš„æ˜¯ä¸€ç§æ•°æ®æ–‡ä»¶ï¼ŒåŒ…å«èŠ‚ç‚¹å›¾æ‰§è¡Œä»»åŠ¡æ‰€éœ€çš„ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ç§æ•°æ®ç»“æž„ï¼Œç”¨äºŽè¡¨ç¤ºæŸç§åŠŸèƒ½ã€‚ä½œä¸ºåŠ¨è¯ï¼Œå»ºæ¨¡æ„å‘³ç€å¯¹æŸç‰©è¿›è¡Œè¡¨ç¤ºæˆ–æä¾›ç¤ºä¾‹ã€‚ åœ¨ComfyUIä¸­ï¼Œæ¨¡åž‹æ•°æ®æ–‡ä»¶çš„å…¸åž‹ç¤ºä¾‹æ˜¯AI _**æ‰©æ•£æ¨¡åž‹**_ã€‚å®ƒæ˜¯ä¸€ç»„åºžå¤§çš„æ•°æ®é›†ï¼Œè¡¨ç¤ºæ–‡æœ¬ä¸Žå›¾åƒä¹‹é—´çš„å¤æ‚å…³ç³»ï¼Œä»Žè€Œå®žçŽ°æ–‡å­—ä¸Žå›¾ç‰‡çš„ç›¸äº’è½¬æ¢ã€‚å…¶ä»–ç”¨äºŽå›¾åƒç”Ÿæˆçš„å¸¸è§æ¨¡åž‹ç¤ºä¾‹åŒ…æ‹¬å¤šæ¨¡æ€è§†è§‰å’Œè¯­è¨€æ¨¡åž‹ï¼Œå¦‚CLIPï¼Œä»¥åŠå›¾åƒæ”¾å¤§æ¨¡åž‹ï¼Œå¦‚RealESRGANã€‚\n\n## æ¨¡åž‹æ–‡ä»¶\n\næ¨¡åž‹æ–‡ä»¶æ˜¯ç”Ÿæˆåª’ä½“åˆ¶ä½œçš„å¿…éœ€å“ã€‚æ²¡æœ‰æ¨¡åž‹æ–‡ä»¶ï¼Œå·¥ä½œæµç¨‹å°†æ— æ³•è¿›è¡Œã€‚ComfyUIå®‰è£…åŒ…ä¸­ä¸åŒ…å«æ¨¡åž‹æ–‡ä»¶ï¼Œä½†å®ƒé€šå¸¸å¯ä»¥è‡ªåŠ¨ä¸‹è½½å¹¶å®‰è£…ç¼ºå¤±çš„æ¨¡åž‹æ–‡ä»¶ã€‚è®¸å¤šæ¨¡åž‹å¯ä»¥é€šè¿‡**ComfyUIç®¡ç†å™¨**çª—å£ä¸‹è½½å’Œå®‰è£…ã€‚æ¨¡åž‹è¿˜å¯ä»¥åœ¨ä»¥ä¸‹ç½‘ç«™èŽ·å–ï¼š[huggingface.co](https://huggingface.co/)ã€[civitai.green](https://civitai.green/)å’Œ[github.com](https://github.com/)ã€‚\n\n### åœ¨ComfyUIä¸­ä½¿ç”¨æ¨¡åž‹\n\n1.  ä¸‹è½½å¹¶å°†å…¶æ”¾ç½®åœ¨ComfyUIç¨‹åºç›®å½•ä¸­\n    1.  åœ¨**models**æ–‡ä»¶å¤¹ä¸­ï¼Œæ‚¨ä¼šæ‰¾åˆ°å„ç§ç±»åž‹æ¨¡åž‹çš„å­æ–‡ä»¶å¤¹ï¼Œä¾‹å¦‚**checkpoints**\n    2.  **ComfyUIç®¡ç†å™¨**å¸®åŠ©è‡ªåŠ¨åŒ–æœç´¢ã€ä¸‹è½½å’Œå®‰è£…çš„è¿‡ç¨‹\n    3.  å¦‚æžœComfyUIæ­£åœ¨è¿è¡Œï¼Œè¯·é‡å¯å®ƒ\n2.  åœ¨æ‚¨çš„å·¥ä½œæµç¨‹ä¸­ï¼Œåˆ›å»ºé€‚åˆæ¨¡åž‹ç±»åž‹çš„èŠ‚ç‚¹ï¼Œä¾‹å¦‚**Load Checkpoints**ã€**Load LoRA**ã€**Load VAE**\n3.  åœ¨åŠ è½½èŠ‚ç‚¹ä¸­ï¼Œé€‰æ‹©æ‚¨å¸Œæœ›ä½¿ç”¨çš„æ¨¡åž‹\n4.  å°†åŠ è½½èŠ‚ç‚¹è¿žæŽ¥åˆ°å·¥ä½œæµç¨‹ä¸­çš„å…¶ä»–èŠ‚ç‚¹\n\n## æ·»åŠ å¤–éƒ¨æ¨¡åž‹è·¯å¾„\n\nå¦‚æžœä½ æƒ³è¦åœ¨ `ComfyUI/models` ä¹‹å¤–ç®¡ç†ä½ çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¯èƒ½å‡ºäºŽä»¥ä¸‹åŽŸå› :\n\n*   ä½ æœ‰å¤šä¸ª ComfyUI å®žä¾‹ï¼Œä½ æƒ³è¦è®©è¿™äº›å®žä¾‹å…±äº«æ¨¡åž‹æ–‡ä»¶ï¼Œä»Žè€Œå‡å°‘ç£ç›˜å ç”¨\n*   ä½ æœ‰å¤šä¸ªä¸åŒçš„ç±»åž‹çš„ GUI ç¨‹åºï¼Œå¦‚ï¼šWebUI, ä½ æƒ³è¦ä»–ä»¬å…±ç”¨æ¨¡åž‹æ–‡ä»¶\n*   æ¨¡åž‹æ–‡ä»¶æ— æ³•è¢«è¯†åˆ«æˆ–è¯»å–åˆ°\n\næˆ‘ä»¬æä¾›äº†é€šè¿‡ `extra_model_paths.yaml` é…ç½®æ–‡ä»¶æ¥æ·»åŠ é¢å¤–æ¨¡åž‹æœç´¢è·¯å¾„çš„æ–¹æ³•ã€‚\n\n### ä¸åŒ ComfyUI ç‰ˆæœ¬é…ç½®æ–‡ä»¶ä½ç½®\n\nå¯¹äºŽ[ä¾¿æºç‰ˆ](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)å’Œ[æ‰‹åŠ¨å®‰è£…](https://docs.comfy.org/zh-CN/installation/manual_install)çš„ ComfyUIç‰ˆæœ¬ï¼Œä½ å¯ä»¥åœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹æ‰¾åˆ° `extra_model_paths.yaml.example` çš„ç¤ºä¾‹æ–‡ä»¶\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nå¤åˆ¶å¹¶é‡å‘½åä¸º `extra_model_paths.yaml` æ¥ä½¿ç”¨, å¹¶ä¿æŒåœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹, è·¯å¾„åº”è¯¥æ˜¯ `ComfyUI/extra_model_paths.yaml`ä½ ä¹Ÿå¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ‰¾åˆ°é…ç½®ç¤ºä¾‹æ–‡ä»¶\n\n### é…ç½®ç¤ºä¾‹\n\næ¯”å¦‚ï¼Œä½ éœ€è¦é¢å¤–è®© ComfyUI è¯†åˆ«çš„æ¨¡åž‹æ–‡ä»¶ä½äºŽä¸‹é¢çš„æ–‡ä»¶å¤¹:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ loras/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\né‚£ä¹ˆä½ å¯ä»¥è¿›è¡Œå¦‚ä¸‹çš„é…ç½®æ¥è®© ComfyUI è¯†åˆ«åˆ°ä½ è®¾å¤‡ä¸Šçš„æ¨¡åž‹è·¯å¾„\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\næˆ–è€…ä½¿ç”¨\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\næˆ–è€…ä½ ä¹Ÿå¯ä»¥å‚è€ƒé»˜è®¤çš„ [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ¥é…ç½®ï¼Œä¿å­˜ä¹‹åŽï¼Œ éœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆã€‚ ä¸‹é¢æ˜¯å®Œæ•´çš„åŽŸå§‹çš„é…ç½®é…ç½®ç¤ºä¾‹:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### æ·»åŠ é¢å¤–è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\né™¤äº†æ·»åŠ å¤–éƒ¨æ¨¡åž‹ä¹‹å¤–ï¼Œä½ åŒæ ·å¯ä»¥æ·»åŠ ä¸åœ¨ ComfyUI é»˜è®¤è·¯å¾„ä¸‹çš„è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\nä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„é…ç½®ç¤ºä¾‹ï¼ˆMac ç³»ç»Ÿï¼‰ï¼Œè¯·æ ¹æ®ä½ çš„å®žé™…æƒ…å†µè¿›è¡Œä¿®æ”¹ï¼Œå¹¶æ–°å¢žåˆ°å¯¹åº”çš„é…ç½®æ–‡ä»¶ä¸­ï¼Œä¿å­˜åŽéœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆ:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```\n\n### æ–‡ä»¶å¤§å°\n\nç›¸å¯¹äºŽå›¾åƒæ–‡ä»¶ï¼Œæ¨¡åž‹æ–‡ä»¶å¯èƒ½éžå¸¸å¤§ã€‚ä¸€ä¸ªå…¸åž‹çš„æœªåŽ‹ç¼©å›¾åƒå¯èƒ½éœ€è¦å‡ MBçš„ç£ç›˜å­˜å‚¨ã€‚ç”Ÿæˆçš„AIæ¨¡åž‹å¯èƒ½å¤§åˆ°æ•°ä¸‡å€ï¼Œå•ä¸ªæ¨¡åž‹å¯è¾¾æ•°åGBã€‚å®ƒä»¬å ç”¨å¤§é‡ç£ç›˜ç©ºé—´ï¼Œå¹¶ä¸”åœ¨ç½‘ç»œä¸Šä¼ è¾“éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚\n\n## æ¨¡åž‹è®­ç»ƒå’Œä¼˜åŒ–\n\nç”Ÿæˆå¼AIæ¨¡åž‹æ˜¯é€šè¿‡åœ¨éžå¸¸å¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒæœºå™¨å­¦ä¹ ç¨‹åºåˆ›å»ºçš„ï¼Œä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬æè¿°çš„é…å¯¹ã€‚AIæ¨¡åž‹å¹¶ä¸æ˜Žç¡®å­˜å‚¨è®­ç»ƒæ•°æ®ï¼Œè€Œæ˜¯å­˜å‚¨æ•°æ®ä¸­éšå«çš„ç›¸å…³æ€§ã€‚ åƒStability AIå’ŒBlack Forest Labsè¿™æ ·çš„ç»„ç»‡å’Œå…¬å¸å‘å¸ƒâ€œåŸºç¡€â€æ¨¡åž‹ï¼Œè¿™äº›æ¨¡åž‹æºå¸¦å¤§é‡é€šç”¨ä¿¡æ¯ã€‚è¿™äº›æ˜¯é€šç”¨çš„ç”ŸæˆAIæ¨¡åž‹ã€‚é€šå¸¸ï¼ŒåŸºç¡€æ¨¡åž‹éœ€è¦è¿›è¡Œ_**ä¼˜åŒ–**_ï¼Œä»¥èŽ·å¾—é«˜è´¨é‡çš„ç”Ÿæˆè¾“å‡ºã€‚ä¸€ä¸ªä¸“é—¨çš„ç¤¾åŒºè‡´åŠ›äºŽä¼˜åŒ–åŸºç¡€æ¨¡åž‹ã€‚æ–°çš„ä¼˜åŒ–æ¨¡åž‹äº§ç”Ÿæ›´å¥½çš„è¾“å‡ºï¼Œæä¾›æ–°çš„æˆ–ä¸åŒçš„åŠŸèƒ½ï¼Œå¹¶/æˆ–ä½¿ç”¨æ›´å°‘çš„èµ„æºã€‚ä¼˜åŒ–åŽçš„æ¨¡åž‹é€šå¸¸å¯ä»¥åœ¨è®¡ç®—èƒ½åŠ›å’Œ/æˆ–å†…å­˜è¾ƒå°‘çš„ç³»ç»Ÿä¸Šè¿è¡Œã€‚\n\n## è¾…åŠ©æ¨¡åž‹\n\næ¨¡åž‹åŠŸèƒ½å¯ä»¥é€šè¿‡è¾…åŠ©æ¨¡åž‹è¿›è¡Œæ‰©å±•ã€‚ä¾‹å¦‚ï¼Œè‰ºæœ¯æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒçš„å·¥ä½œæµç¨‹ä»¥å®žçŽ°ç‰¹å®šç»“æžœï¼Œå•é æ‰©æ•£æ¨¡åž‹å¯èƒ½ä¼šå¾ˆå›°éš¾æˆ–ä¸å¯èƒ½ã€‚é¢å¤–çš„æ¨¡åž‹å¯ä»¥åœ¨å·¥ä½œæµç¨‹å›¾ä¸­ä¼˜åŒ–æ‰©æ•£æ¨¡åž‹ï¼Œä»¥äº§ç”Ÿæ‰€éœ€çš„ç»“æžœã€‚ç¤ºä¾‹åŒ…æ‹¬**LoRA**ï¼ˆä½Žç§©é€‚åº”ï¼‰ï¼Œä¸€ä¸ªé’ˆå¯¹ç‰¹å®šä¸»é¢˜è®­ç»ƒçš„å°æ¨¡åž‹ï¼›**ControlNet**ï¼Œä¸€ä¸ªä½¿ç”¨å¼•å¯¼å›¾åƒå¸®åŠ©æŽ§åˆ¶æž„å›¾çš„æ¨¡åž‹ï¼›ä»¥åŠ**Inpainting**ï¼Œä¸€ä¸ªå…è®¸æŸäº›æ‰©æ•£æ¨¡åž‹åœ¨çŽ°æœ‰å›¾åƒä¸­ç”Ÿæˆæ–°å†…å®¹çš„æ¨¡åž‹ã€‚ ![è¾…åŠ©æ¨¡åž‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_auxiliary-model.png)"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/dependencies",
  "markdown": "# ä¾èµ–å…³ç³» - ComfyUI\n\n## å·¥ä½œæµæ–‡ä»¶ä¾èµ–äºŽå…¶ä»–æ–‡ä»¶\n\næˆ‘ä»¬ç»å¸¸å¯ä»¥ä»Žç¤¾åŒºé‡ŒèŽ·å–åˆ°å„ç§å„æ ·çš„å·¥ä½œæµæ–‡ä»¶ï¼Œä½†å¾€å¾€åœ¨è¿è¡Œä¹‹åŽå‘çŽ°å·¥ä½œæµæ— æ³•ç›´æŽ¥è¿è¡Œï¼Œè¿™æ˜¯å› ä¸ºä¸€ä¸ªå·¥ä½œæµæ–‡ä»¶é™¤äº†å·¥ä½œæµè‡ªèº«ä¹‹å¤–è¿˜è¦ä¾èµ–äºŽå…¶å®ƒæ–‡ä»¶ï¼Œæ¯”å¦‚åª’ä½“èµ„äº§çš„è¾“å…¥ã€æ¨¡åž‹ã€è‡ªå®šä¹‰èŠ‚ç‚¹ã€ç›¸å…³çš„ Python ä¾èµ–ç­‰ç­‰ã€‚ ComfyUI çš„å·¥ä½œæµåªæœ‰åœ¨æ‰€æœ‰ç›¸å…³ä¾èµ–æ¡ä»¶éƒ½è¢«æ»¡è¶³çš„æƒ…å†µä¸‹æ‰èƒ½æ­£å¸¸è¿è¡Œã€‚ ComfyUI å·¥ä½œæµçš„è¿è¡Œä¾èµ–ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼š\n\n*   èµ„äº§ï¼ˆåª’ä½“æ–‡ä»¶åŒ…æ‹¬éŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒç­‰ç­‰è¾“å…¥ï¼‰\n*   è‡ªå®šä¹‰èŠ‚ç‚¹\n*   Python ä¾èµ–\n*   æ¨¡åž‹ï¼ˆå¦‚ï¼šStable Diffusion æ¨¡åž‹ç­‰ï¼‰\n\n## èµ„äº§\n\nAI æ¨¡åž‹æ˜¯ä¸€ä¸ª _**èµ„äº§**_ çš„ä¾‹å­ã€‚åœ¨åª’ä½“åˆ¶ä½œä¸­ï¼Œèµ„äº§æ˜¯æä¾›è¾“å…¥æ•°æ®çš„æŸç§åª’ä½“æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼Œè§†é¢‘ç¼–è¾‘ç¨‹åºå¤„ç†å­˜å‚¨åœ¨ç£ç›˜ä¸Šçš„ç”µå½±æ–‡ä»¶ã€‚ç¼–è¾‘ç¨‹åºçš„é¡¹ç›®æ–‡ä»¶ä¿å­˜äº†è¿™äº›ç”µå½±æ–‡ä»¶èµ„äº§çš„é“¾æŽ¥ï¼Œå…è®¸éžç ´åæ€§ç¼–è¾‘ï¼Œè€Œä¸æ”¹å˜åŽŸå§‹ç”µå½±æ–‡ä»¶ã€‚ ComfyUI çš„å·¥ä½œæ–¹å¼ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å·¥ä½œæµåªæœ‰åœ¨æ‰¾åˆ°å¹¶åŠ è½½æ‰€æœ‰å¿…éœ€çš„èµ„äº§æ—¶æ‰èƒ½è¿è¡Œã€‚ç”Ÿæˆæ€§ AI æ¨¡åž‹ã€å›¾åƒã€ç”µå½±å’Œå£°éŸ³æ˜¯å·¥ä½œæµå¯èƒ½ä¾èµ–çš„ä¸€äº›èµ„äº§ç¤ºä¾‹ã€‚å› æ­¤ï¼Œè¿™äº›è¢«ç§°ä¸º _**ä¾èµ–èµ„äº§**_ æˆ– _**èµ„äº§ä¾èµ–å…³ç³»**_ã€‚\n\n## è‡ªå®šä¹‰èŠ‚ç‚¹\n\nè‡ªå®šä¹‰èŠ‚ç‚¹æ˜¯ ComfyUI çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼Œ\n\n## Python ä¾èµ–\n\nComfyUI æ˜¯ä¸€ä¸ªåŸºäºŽ Python çš„é¡¹ç›®ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªç‹¬ç«‹çš„ Python è¿è¡ŒçŽ¯å¢ƒï¼Œæ¥è¿è¡Œ ComfyUIï¼Œæ‰€æœ‰çš„ç›¸å…³ä¾èµ–éƒ½ä¼šè¢«å®‰è£…åœ¨åœ¨è¿™ä¸ªç‹¬ç«‹çš„ Python è¿è¡ŒçŽ¯å¢ƒä¸­ã€‚\n\n### ComfyUI çš„ä¾èµ–\n\nä½ å¯ä»¥åœ¨ ComfyUI çš„ [requirements.txt](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) æ–‡ä»¶ä¸­æŸ¥çœ‹ ComfyUI å½“å‰çš„ä¾èµ–\n\n```\ncomfyui-frontend-package==1.14.5\ntorch\ntorchsde\ntorchvision\ntorchaudio\nnumpy>=1.25.0\neinops\ntransformers>=4.28.1\ntokenizers>=0.13.3\nsentencepiece\nsafetensors>=0.4.2\naiohttp>=3.11.8\nyarl>=1.18.0\npyyaml\nPillow\nscipy\ntqdm\npsutil\n\n#non essential dependencies:\nkornia>=0.7.1\nspandrel\nsoundfile\nav\n```\n\néšç€ ComfyUI çš„å‘å±•ï¼Œæˆ‘ä»¬å¯èƒ½ä¹Ÿä¼šè°ƒæ•´ç›¸åº”çš„ä¾èµ–ï¼Œæ¯”å¦‚æ·»åŠ æ–°çš„ä¾èµ–ï¼Œæˆ–è€…åˆ é™¤ä¸€äº›ä¸å†éœ€è¦çš„ä¾èµ–ã€‚ æ‰€ä»¥å¦‚æžœä½ æ˜¯ä½¿ç”¨ Git æ¥æ›´æ–° ComfyUI é‚£ä¹ˆä½ éœ€è¦åœ¨æ‹‰å–æœ€æ–°çš„æ›´æ–°ä¹‹åŽåœ¨å¯¹åº”çš„çŽ¯å¢ƒä¸‹ä½¿ç”¨\n\n```\npip install -r requirements.txt\n```\n\nä»Žè€Œæ¥å®‰è£… ComfyUI æœ€æ–°çš„ä¾èµ–ä»¥ä¿è¯ ComfyUI çš„æ­£å¸¸è¿è¡Œï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡ä¿®æ”¹ç‰¹å®šåŒ…çš„ä¾èµ–ç‰ˆæœ¬æ¥å®žçŽ°éƒ¨åˆ†ä¾èµ–çš„å‡çº§æˆ–è€…é™çº§ å¦å¤– ComfyUI çš„å‰ç«¯ [ComfyUI\\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend) ç›®å‰æ˜¯ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„é¡¹ç›®æ¥è¿›è¡Œç»´æŠ¤ï¼Œæˆ‘ä»¬ä¼šåœ¨å¯¹åº”ç‰ˆæœ¬ç¨³å®šä¹‹åŽæ›´æ–°å¯¹åº”çš„ `comfyui-frontend-package` ä¾èµ–ç‰ˆæœ¬ï¼Œå¦‚æžœä½ éœ€è¦åˆ‡æ¢å¯¹åº”çš„å‰ç«¯ç‰ˆæœ¬ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://pypi.org/project/comfyui-frontend-package/#history)æŸ¥çœ‹å¯¹åº”çš„ç‰ˆæœ¬ä¿¡æ¯ã€‚\n\n### è‡ªå®šä¹‰èŠ‚ç‚¹çš„ä¾èµ–\n\næ„Ÿè°¢ ComfyUI ç¤¾åŒºä¼—å¤šä½œè€…çš„åŠªåŠ›ï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ä¸åŒçš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼ˆCustom Nodesï¼‰æ¥æ‰©å±• ComfyUI çš„åŠŸèƒ½ï¼Œå®žçŽ°ä»¤äººèµžå¹çš„åˆ›æ„ã€‚ é€šå¸¸ï¼Œæ¯ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹éƒ½ä¼šæœ‰ä¸€ä¸ªç‹¬ç«‹çš„ä¾èµ–ï¼Œå¹¶ä¸”æ¯ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹éƒ½ä¼šæœ‰ä¸€ä¸ªç‹¬ç«‹çš„ `requirements.txt` æ–‡ä»¶ã€‚ å¦‚æžœä½ ä½¿ç”¨ [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) æ¥å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œé‚£ä¹ˆé€šå¸¸ ComfyUI Manager ä¼šè‡ªåŠ¨å¸®ä½ å®‰è£…å¯¹åº”çš„ä¾èµ–ã€‚ å½“ç„¶ä¹Ÿæœ‰ä¸€äº›éœ€è¦ä½ æ‰‹åŠ¨è¿›è¡Œä¾èµ–å®‰è£…çš„æƒ…å†µï¼Œç›®å‰æ‰€æœ‰çš„è‡ªå®šä¹‰èŠ‚ç‚¹éƒ½å°†è¢«å®‰è£…è‡³ `ComfyUI/custom_nodes` ç›®å½•ä¸‹ï¼Œ ä½ éœ€è¦åœ¨ä½ çš„ ComfyUI Python çŽ¯å¢ƒä¸­è¿›å…¥åˆ°å¯¹åº”æ’ä»¶çš„ç›®å½•ç„¶åŽæ‰§è¡Œ `pip install -r requirements.txt` æ¥å®‰è£…å¯¹åº”çš„ä¾èµ–ã€‚ å¦‚æžœæ˜¯ [Windows ä¾¿æºç‰ˆ](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)ï¼Œä½ å¯ä»¥åœ¨ä¾¿æºç‰ˆçš„`ComfyUI_windows_portable`ç›®å½•ä¸‹ä½¿ç”¨\n\n```\npython_embeded\\python.exe -m pip install -r ComfyUI\\custom_nodes\\<custom_node_name>\\requirements.txt\n```\n\næ¥å®‰è£…å¯¹åº”èŠ‚ç‚¹çš„ä¾èµ–ã€‚\n\n### ä¾èµ–å†²çª\n\nä¾èµ–å†²çªæ˜¯æˆ‘ä»¬åœ¨ä½¿ç”¨ ComfyUI çš„æ—¶å€™ç»å¸¸ä¼šé‡åˆ°çš„é—®é¢˜ï¼Œä½ å¯èƒ½ä¼šå‘çŽ°åœ¨å®‰è£…æˆ–è€…æ›´æ–°å¥½æŸä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹åŽï¼Œä¹‹å‰å®‰è£…çš„ä¸€äº›è‡ªå®šä¹‰èŠ‚ç‚¹åœ¨ ComfyUI çš„èŠ‚ç‚¹åº“ä¸­æŸ¥æ‰¾ä¸åˆ°äº†ï¼Œæˆ–è€…å‡ºçŽ°äº†é”™è¯¯å¼¹çª—ï¼Œå…¶ä¸­ä¸€ä¸ªå¯èƒ½çš„åŽŸå› å°±æ˜¯ä¾èµ–å†²çªã€‚ ä¾èµ–å†²çªçš„åŽŸå› å¯èƒ½æœ‰å¾ˆå¤šï¼Œæ¯”å¦‚ï¼š\n\n1.  è‡ªå®šä¹‰èŠ‚ç‚¹çš„ç‰ˆæœ¬é”å®š\n\néƒ¨åˆ†æ’ä»¶åœ¨å¼€å‘æ—¶ä¼šå›ºå®šä¾èµ–åº“çš„ç²¾ç¡®ç‰ˆæœ¬ï¼ˆå¦‚`open_clip_torch==2.26.1`ï¼‰ï¼Œè€Œå…¶ä»–æ’ä»¶å¯èƒ½è¦æ±‚æ›´é«˜ç‰ˆæœ¬ï¼ˆå¦‚`open_clip_torch>=2.29.0`ï¼‰ï¼Œå¯¼è‡´ç‰ˆæœ¬æ— æ³•åŒæ—¶æ»¡è¶³ã€‚ **è§£å†³æ–¹æ³•**ï¼šä½ å¯ä»¥è¯•ç€æŠŠå¯¹åº”çš„å›ºå®šç‰ˆæœ¬ä¾èµ–æ”¹ä¸ºèŒƒå›´çº¦æŸæ¯”å¦‚ `open_clip_torch>=2.26.1`ï¼Œç„¶åŽé‡æ–°æ‰§è¡Œä¾èµ–çš„å®‰è£…æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚\n\n2.  çŽ¯å¢ƒæ±¡æŸ“\n\nåœ¨æ‰§è¡Œè‡ªå®šä¹‰èŠ‚ç‚¹ä¾èµ–å®‰è£…çš„è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¼šè¦†ç›–å…¶å®ƒæ’ä»¶å·²ç»å®‰è£…çš„åº“çš„ç‰ˆæœ¬ï¼Œæ¯”å¦‚å¤šä¸ªæ’ä»¶ä¾èµ– `PyTorch` ä½†è¦æ±‚çš„æ˜¯ä¸åŒçš„ CUDA ç‰ˆæœ¬ï¼ŒåŽå®‰è£…çš„æ’ä»¶ä¼šç ´ååŽŸæœ‰çš„çŽ¯å¢ƒã€‚ **è§£å†³æ–¹æ³•**ï¼š\n\n*   ä½ å¯ä»¥è¯•ç€æ‰‹åŠ¨åœ¨ python è™šæ‹ŸçŽ¯å¢ƒä¸­æ‰‹åŠ¨å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„ä¾èµ–ï¼Œæ¥è§£å†³è¿™ç±»é—®é¢˜ã€‚\n*   æˆ–è€…ä¸ºä¸åŒçš„æ’ä»¶åˆ›å»ºä¸åŒçš„ python è™šæ‹ŸçŽ¯å¢ƒï¼Œæ¥è§£å†³è¿™ç±»é—®é¢˜ã€‚\n*   è¯•ç€é€ä¸ªå®‰è£…æ’ä»¶ï¼Œåœ¨å®‰è£…å®Œæ¯ä¸ªæ’ä»¶åŽï¼Œé‡æ–°å¯åŠ¨ ComfyUI æ¥è§‚å¯Ÿæ˜¯å¦ä¼šå‡ºçŽ°ä¾èµ–å†²çªã€‚\n\n3.  è‡ªå®šä¹‰èŠ‚ç‚¹ä¾èµ–ç‰ˆæœ¬ä¸Ž ComfyUI ä¾èµ–ç‰ˆæœ¬ä¸å…¼å®¹\n\nè¿™ç±»ä¾èµ–å†²çªçš„é—®é¢˜å¯èƒ½è¾ƒéš¾è§£å†³ï¼Œä½ å¯èƒ½éœ€è¦é€šè¿‡å‡é™çº§ ComfyUI æˆ–è€…æ›´æ”¹è‡ªå®šä¹‰èŠ‚ç‚¹çš„ä¾èµ–ç‰ˆæœ¬ï¼Œæ¥è§£å†³è¿™ç±»é—®é¢˜ã€‚ **è§£å†³æ–¹æ³•**ï¼šè¿™ç±»ä¾èµ–å†²çªçš„é—®é¢˜å¯èƒ½è¾ƒéš¾è§£å†³ï¼Œä½ å¯èƒ½éœ€è¦é€šè¿‡å‡é™çº§ ComfyUI æˆ–è€…æ›´æ”¹è‡ªå®šä¹‰èŠ‚ç‚¹çš„ä¾èµ–ç‰ˆæœ¬ï¼Œæ¥è§£å†³è¿™ç±»é—®é¢˜ã€‚\n\n## æ¨¡åž‹\n\næ¨¡åž‹æ˜¯ ComfyUI çš„ä¸€ä¸ªé‡è¦èµ„äº§ä¾èµ–ï¼ŒåŸºæœ¬ä¸Šå„ç±»çš„è‡ªå®šä¹‰èŠ‚ç‚¹å’Œå·¥ä½œæµéƒ½å›´ç»•ç€ç‰¹å®šçš„æ¨¡åž‹å±•å¼€ï¼Œæ¯”å¦‚ stable diffusion ç³»åˆ—ã€Flux ç³»åˆ—ã€Ltxv ç­‰ç­‰ã€‚ è¿™äº›æ¨¡åž‹æ˜¯æˆ‘ä»¬ä½¿ç”¨ ComfyUI è¿›è¡Œåˆ›ä½œçš„é‡è¦åŸºç¡€ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨ä½¿ç”¨ ComfyUI çš„æ—¶å€™éœ€è¦ç¡®ä¿æˆ‘ä»¬ä½¿ç”¨çš„æ¨¡åž‹æ˜¯æ­£å¸¸å¯ç”¨çš„ï¼Œé€šå¸¸ï¼Œæˆ‘ä»¬å¯¹åº”çš„æ¨¡åž‹éƒ½åœ¨ `ComfyUI/models/` ç›®å½•çš„å¯¹åº”ç›®å½•è¿›è¡Œä¿å­˜ï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥é€šè¿‡ä¿®æ”¹æ¨¡æ¿åˆ›å»ºä¸€ä¸ª [extra\\_model\\_paths.yaml](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ¥ä½¿å¾—é¢å¤–çš„æ¨¡åž‹è·¯å¾„è¢« ComfyUI è¯†åˆ«ã€‚ è¿™æ ·å°±å¯ä»¥å®žçŽ°å¤šä¸ª ComfyUI å®žä¾‹å…±äº«åŒä¸€ä¸ªæ¨¡åž‹åº“ï¼Œä»Žè€Œå‡å°‘ç£ç›˜çš„å ç”¨ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/custom-nodes",
  "markdown": "# è‡ªå®šä¹‰èŠ‚ç‚¹ - ComfyUI\n\n## å…³äºŽè‡ªå®šä¹‰èŠ‚ç‚¹\n\nå½“ä½ åœ¨å®‰è£…äº† ComfyUI åŽï¼Œä½ å°†ä¼šå‘çŽ° ComfyUI ä¸­å·²ç»åŒ…å«äº†è®¸å¤šèŠ‚ç‚¹ï¼Œè¿™äº›åŽŸç”Ÿçš„èŠ‚ç‚¹ç§°ä¸º **Comfy Core** èŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹éƒ½æ˜¯ç”± ComfyUI å®˜æ–¹ç»´æŠ¤çš„ã€‚ æ­¤å¤–è¿˜æœ‰è®¸å¤šæ¥è‡ª ComfyUI ç¤¾åŒºçš„ä¼—å¤šä½œè€…å¸¦æ¥çš„å„ç§å„æ ·çš„ [**è‡ªå®šä¹‰èŠ‚ç‚¹**](https://registry.comfy.org/)ï¼Œè¿™äº›è‡ªå®šä¹‰èŠ‚ç‚¹ä¸º ComfyUI å¸¦æ¥äº†è¯¸å¤šçš„æ‰©å±•åŠŸèƒ½ï¼Œå¤§å¤§æ‰©å±•äº† ComfyUI çš„åŠŸèƒ½å’Œèƒ½åŠ›è¾¹ç•Œã€‚ åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•è‡ªå®šä¹‰èŠ‚ç‚¹ç›¸å…³çš„ä¸€äº›æ“ä½œï¼ŒåŒ…æ‹¬å®‰è£…ã€æ›´æ–°ã€ç¦ç”¨ã€å¸è½½ã€ä¾èµ–å®‰è£…ç­‰ã€‚ æ¯ä¸ªäººéƒ½å¯ä»¥å¼€å‘è‡ªå·±çš„è‡ªå®šä¹‰çš„æ‰©å±•åŠŸèƒ½åˆ° ComfyUI ä¸­ï¼Œå¹¶åˆ†äº«ç»™å…¶ä»–äººä½¿ç”¨ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://registry.comfy.org/)æ‰¾åˆ°è®¸å¤šæ¥è‡ªç¤¾åŒºçš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œå¦‚æžœä½ æƒ³è¦å¼€å‘è‡ªå·±çš„è‡ªå®šä¹‰èŠ‚ç‚¹è¯·è®¿é—®ä¸‹é¢çš„éƒ¨åˆ†å¼€å§‹ï¼š\n\n[](https://docs.comfy.org/zh-CN/custom-nodes/overview)\n\n## è‡ªå®šä¹‰èŠ‚ç‚¹ç®¡ç†\n\nåœ¨è¿™ä¸ªéƒ¨åˆ†æˆ‘ä»¬å°†è®²è§£ï¼š\n\n*   å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹\n*   å®‰è£…èŠ‚ç‚¹ä¾èµ–\n*   è‡ªå®šä¹‰èŠ‚ç‚¹ç‰ˆæœ¬æŽ§åˆ¶\n*   å¸è½½è‡ªå®šä¹‰èŠ‚ç‚¹\n*   ä¸´æ—¶ç¦ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹\n*   å¤„ç†è‡ªå®šä¹‰èŠ‚ç‚¹ä¾èµ–å†²çª\n\n### 1\\. å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹\n\nç›®å‰ ComfyUI æ”¯æŒé€šè¿‡å¤šç§æ–¹å¼å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬ï¼š\n\n*   \\[é€šè¿‡ ComfyUI Manager å®‰è£…ï¼ˆæŽ¨èï¼‰\\](#é€šè¿‡ ComfyUI Manager å®‰è£…)\n*   é€šè¿‡ Git å®‰è£…\n*   æ‰‹åŠ¨å®‰è£…\n\næˆ‘ä»¬æŽ¨èé€šè¿‡ **ComfyUI Manager** æ¥å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ ComfyUI è‡ªå®šä¹‰èŠ‚ç‚¹ç”Ÿæ€ä¸­å…·æœ‰éžå¸¸é‡è¦æ„ä¹‰çš„ä¸€ä¸ªå·¥å…·ï¼Œå®ƒä½¿å¾—è‡ªå®šä¹‰èŠ‚ç‚¹ç®¡ç†ï¼ˆå¦‚æœç´¢ã€å®‰è£…ã€æ›´æ–°ã€ç¦ç”¨å’Œå¸è½½ï¼‰å˜å¾—ç®€å•ï¼Œä½ åªéœ€è¦åœ¨ ComfyUI Manager ä¸­æœç´¢ä½ æƒ³è¦å®‰è£…çš„èŠ‚ç‚¹ï¼Œç„¶åŽç‚¹å‡»å®‰è£…å³å¯ã€‚ ä½†ç”±äºŽç›®å‰æ‰€æœ‰çš„è‡ªå®šä¹‰èŠ‚ç‚¹éƒ½æ˜¯å­˜å‚¨åœ¨ GitHub ä¸Šï¼Œæ‰€ä»¥åœ¨æœ¬ç¯‡é’ˆå¯¹äºŽæŸäº›æ— æ³•æ­£å¸¸è®¿é—® GitHub çš„çš„åœ°åŒºï¼Œæˆ‘ä»¬åœ¨æœ¬ç¯‡æ’°å†™äº†è¯¦å°½çš„ä¸åŒçš„è‡ªå®šä¹‰èŠ‚ç‚¹çš„å®‰è£…æ–¹å¼ã€‚ å¦å¤–ç”±äºŽæˆ‘ä»¬æŽ¨èä½¿ç”¨ **ComfyUI Manager** è¿›è¡Œå¯¹åº”çš„æ’ä»¶ç®¡ç†ï¼Œæˆ‘ä»¬æŽ¨èä½¿ç”¨è¿™ä¸€å·¥å…·æ¥è¿›è¡Œæ’ä»¶çš„ç®¡ç†ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/Comfy-Org/ComfyUI-Manager)æ‰¾åˆ°å®ƒçš„æºç ã€‚ æ‰€ä»¥åœ¨æœ¬ç¯‡æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬å°†ä¼šä½¿ç”¨å®‰è£… ComfyUI Manager ä½œä¸ºè‡ªå®šä¹‰èŠ‚ç‚¹å®‰è£…ç¤ºä¾‹ï¼Œå¹¶åœ¨æœ¬ç¯‡çš„ç›¸å…³ä»‹ç»éƒ¨åˆ†è¡¥å……å¦‚ä½•ä½¿ç”¨å®ƒæ¥è¿›è¡ŒèŠ‚ç‚¹çš„ç®¡ç†ã€‚\n\n### 2\\. å®‰è£…èŠ‚ç‚¹ä¾èµ–\n\nè‡ªå®šä¹‰èŠ‚ç‚¹éƒ½éœ€è¦è¿›è¡Œç›¸å…³çš„ä¾èµ–çš„å®‰è£…ï¼Œæ¯”å¦‚å¯¹äºŽ ComfyUI-Manager æ¥è¯´ï¼Œä½ å¯ä»¥è®¿é—®[requirements.txt](https://github.com/Comfy-Org/ComfyUI-Manager/blob/main/requirements.txt) æ–‡ä»¶æ¥æŸ¥çœ‹å¯¹åº”çš„ä¾èµ–åŒ…çš„è¦æ±‚, åœ¨ä¹‹å‰çš„æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬ä»…ä»…æ˜¯æŠŠå¯¹åº”çš„è‡ªå®šä¹‰èŠ‚ç‚¹ä»£ç å…‹éš†åˆ°äº†æœ¬åœ°ï¼Œå¹¶æ²¡æœ‰å®‰è£…å¯¹åº”çš„ä¾èµ–ï¼Œæ‰€ä»¥æŽ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å®‰è£…å¯¹åº”çš„ä¾èµ–ã€‚\n\nåœ¨å…³äºŽ[ä¾èµ–å…³ç³»](https://docs.comfy.org/zh-CN/development/core-concepts/dependencies)ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† ComfyUI ä¸­ä¾èµ–å…³ç³»çš„ç›¸å…³å†…å®¹ï¼ŒComfyUI æ˜¯ä¸€ä¸ªåŸºäºŽ **Python** çš„é¡¹ç›®ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªç”¨äºŽè¿è¡Œ ComfyUI çš„ç‹¬ç«‹ **Python** è¿è¡ŒçŽ¯å¢ƒï¼Œæ‰€æœ‰çš„ç›¸å…³ä¾èµ–éƒ½éœ€è¦è¢«å®‰è£…åœ¨åœ¨è¿™ä¸ªç‹¬ç«‹çš„ **Python** è¿è¡ŒçŽ¯å¢ƒä¸­ã€‚ å¦‚æžœä½ ç›´æŽ¥åœ¨ç³»ç»Ÿçº§çš„ç»ˆç«¯è¿è¡Œ `pip install -r requirements.txt`ï¼Œå¯¹åº”çš„ä¾èµ–å¯èƒ½ä¼šè¢«å®‰è£…åˆ°äº†ç³»ç»Ÿçº§çš„ **Python** çŽ¯å¢ƒä¸­ï¼Œä¼šå¯¼è‡´å¯¹åº”çš„è‡ªå®šä¹‰èŠ‚ç‚¹åœ¨ ComfyUI çš„çŽ¯å¢ƒä¸­ä¾èµ–è¿˜æ˜¯ä¸¢å¤±çš„ï¼Œå¯¼è‡´å¯¹åº”è‡ªå®šä¹‰èŠ‚ç‚¹æ— æ³•æ­£å¸¸è¿è¡Œã€‚ æ‰€ä»¥æŽ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä½¿ç”¨ ComfyUI çš„ç‹¬ç«‹ Python è¿è¡ŒçŽ¯å¢ƒæ¥å®Œæˆå¯¹åº”çš„ä¾èµ–å®‰è£…ã€‚ ä¾æ®ä¸åŒçš„ ComfyUI ç‰ˆæœ¬æˆ‘ä»¬å°†ä½¿ç”¨ä¸åŒçš„æ–¹å¼æ¥è¿›è¡Œå¯¹åº”çš„ä¾èµ–å®‰è£…ï¼Œ\n\nå¯¹äºŽ ComfyUI ä¾¿æºç‰ˆï¼ˆPortableï¼‰æ¥è¯´ï¼Œå®ƒä½¿ç”¨çš„æ˜¯ä¸€ä¸ªåµŒå…¥å¼çš„ Python ï¼Œå¯¹åº” Python ä½äºŽ `\\ComfyUI_windows_portable\\python_embeded` ç›®å½•ä¸‹, æˆ‘ä»¬éœ€è¦ä½¿ç”¨å¯¹åº”çš„ Python æ¥å®Œæˆå¯¹åº”çš„ä¾èµ–å®‰è£…ã€‚é¦–å…ˆå…ˆåœ¨ä¾¿æºç‰ˆçš„ç›®å½•ä¸‹å¯åŠ¨ terminal æˆ–è€…å¯åŠ¨ terminal åŽä½¿ç”¨ `cd` å‘½ä»¤è¿›å…¥åˆ° `\\ComfyUI_windows_portable\\` ç›®å½•ä¸‹![å¯åŠ¨ terminal](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/custom_nodes/open_terminal.jpg)ç¡®ä¿å¯¹åº”ç»ˆç«¯çš„ç›®å½•ä¸º `\\ComfyUI_windows_portable\\` ç›®å½•ä¸‹ï¼Œå¦‚ä¸‹å›¾ä¸º `D:\\ComfyUI_windows_portable\\`![ç»ˆç«¯](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/custom_nodes/terminal.jpg)ç„¶åŽä½¿ç”¨ `python_embeded\\python.exe` æ¥å®Œæˆå¯¹åº”çš„ä¾èµ–å®‰è£…ã€‚\n\n```\npython_embeded\\python.exe -m pip install -r ComfyUI\\custom_nodes\\ComfyUI-Manager\\requirements.txt\n```\n\nå½“ç„¶ä½ å¯ä»¥æŠŠå¯¹åº”çš„ ComfyUI-Manager æ›¿æ¢ä¸ºä½ å®žé™…å®‰è£…çš„è‡ªå®šä¹‰èŠ‚ç‚¹åç§°ï¼Œä½†éœ€è¦ç¡®ä¿å¯¹åº”èŠ‚ç‚¹ç›®å½•ä¸‹çš„ç¡®å­˜åœ¨ `requirements.txt` æ–‡ä»¶ã€‚\n\n### è‡ªå®šä¹‰èŠ‚ç‚¹ç‰ˆæœ¬æŽ§åˆ¶\n\nè‡ªå®šä¹‰èŠ‚ç‚¹çš„ç‰ˆæœ¬æŽ§åˆ¶ï¼Œå®žé™…ä¸Šæ˜¯åŸºäºŽ Git çš„ç‰ˆæœ¬æŽ§åˆ¶æ¥è¿›è¡Œçš„ï¼Œä½ å¯ä»¥é€šè¿‡ Git æ¥è¿›è¡Œå¯¹åº”çš„èŠ‚ç‚¹ç‰ˆæœ¬ç®¡ç†ï¼Œä½†å®žé™…ä¸Šåœ¨ ComfyUI Manager ä¸­å·²ç»å¾ˆå¥½åœ°é›†æˆäº†è¿™ä¸€ç‰ˆæœ¬ç®¡ç†åŠŸèƒ½ï¼Œéžå¸¸æ„Ÿè°¢ [@Dr.Lt.Data](https://github.com/ltdrdata) ä¸ºæˆ‘ä»¬å¸¦æ¥å¦‚æ­¤ä¾¿æ·çš„å·¥å…·ã€‚ åœ¨è¿™ä¸ªéƒ¨åˆ†æˆ‘ä»¬ä¾æ—§ä¼šä¸ºä½ è®²è§£è¿™ä¸¤ç§ä¸åŒæ’ä»¶ç‰ˆæœ¬ç®¡ç†çš„æ–¹æ³•ï¼Œä½†å¦‚æžœä½ æ˜¯ä½¿ç”¨ zip åŽ‹ç¼©åŒ…è¿›è¡Œæ‰‹åŠ¨å®‰è£…çš„ï¼Œé‚£ä¹ˆå¯¹åº”çš„ git ç‰ˆæœ¬åŽ†å²ä¿¡æ¯ä¼šä¸¢å¤±ï¼Œä¼šå¯¼è‡´ä½ æ— æ³•è¿›è¡Œå¯¹åº”çš„ç‰ˆæœ¬ç®¡ç†ã€‚\n\n### å¸è½½è‡ªå®šä¹‰èŠ‚ç‚¹\n\nå¾…æ›´æ–°\n\n### ä¸´æ—¶ç¦ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹\n\nå¾…æ›´æ–°\n\n### è‡ªå®šä¹‰èŠ‚ç‚¹ä¾èµ–å†²çª\n\nå¾…æ›´æ–°\n\n## ComfyUI Manager\n\n![ComfyUI ç®¡ç†å™¨ç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png) ç›®å‰åœ¨ [Desktop ç‰ˆæœ¬](https://docs.comfy.org/zh-CN/installation/desktop/windows) ä¸­å·²é»˜è®¤åŒ…å«è¯¥å·¥å…·,è€Œåœ¨[ä¾¿æºï¼ˆPortableï¼‰ç‰ˆ](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)ä¸­ï¼Œä½ éœ€è¦å‚è€ƒæœ¬æ–‡æ¡£ä¸­[å®‰è£…ç®¡ç†å™¨](#%E5%AE%89%E8%A3%85%E8%87%AA%E5%AE%9A%E4%B9%89%E8%8A%82%E7%82%B9)ç« èŠ‚ä¸­çš„è¯´æ˜Žè¿›è¡Œå®‰è£…ã€‚\n\n### å®‰è£…ç®¡ç†å™¨\n\nå¦‚æžœæ‚¨æ­£åœ¨è¿è¡Œ ComfyUI æœåŠ¡å™¨åº”ç”¨ç¨‹åºï¼Œåˆ™éœ€è¦å®‰è£…ç®¡ç†å™¨ã€‚å¦‚æžœ ComfyUI æ­£åœ¨è¿è¡Œï¼Œè¯·åœ¨ç»§ç»­ä¹‹å‰å°†å…¶å…³é—­ã€‚ ç¬¬ä¸€æ­¥æ˜¯å®‰è£… Gitï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºŽè½¯ä»¶ç‰ˆæœ¬æŽ§åˆ¶çš„å‘½ä»¤è¡Œåº”ç”¨ç¨‹åºã€‚Git å°†ä»Ž [github.com](https://github.com/) ä¸‹è½½ ComfyUI ç®¡ç†å™¨ã€‚ä»Ž [git-scm.com](https://git-scm.com/) ä¸‹è½½å¹¶å®‰è£… Gitã€‚ å®‰è£… Git åŽï¼Œå¯¼èˆªåˆ° ComfyUI æœåŠ¡å™¨ç¨‹åºç›®å½•ï¼Œè¿›å…¥æ ‡è®°ä¸º **custom\\_nodes** çš„æ–‡ä»¶å¤¹ã€‚æ‰“å¼€å‘½ä»¤çª—å£æˆ–ç»ˆç«¯ã€‚ç¡®ä¿å‘½ä»¤è¡Œæ˜¾ç¤ºå½“å‰ç›®å½•è·¯å¾„ä¸º **custom\\_nodes**ã€‚è¾“å…¥ä»¥ä¸‹å‘½ä»¤ã€‚è¿™å°†ä¸‹è½½ç®¡ç†å™¨ã€‚ä»ŽæŠ€æœ¯ä¸Šè®²ï¼Œè¿™è¢«ç§°ä¸º _å…‹éš† Git ä»“åº“_ã€‚\n\n### æ£€æµ‹ç¼ºå¤±çš„èŠ‚ç‚¹\n\nåœ¨å®‰è£…ç®¡ç†å™¨åŽï¼Œä½ å¯ä»¥åœ¨ç®¡ç†å™¨ä¸­æ£€æµ‹åˆ°ç¼ºå¤±çš„èŠ‚ç‚¹ã€‚ ![ComfyUI ç®¡ç†å™¨ç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png)\n\n## å¼€å‘ä¸€ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹\n\nå¦‚æžœä½ å…·æœ‰ä¸€å®šçš„å¼€å‘èƒ½åŠ›ï¼Œè¯·ä»Žä¸‹é¢çš„æ–‡æ¡£å¼€å§‹ä»¥äº†è§£å¦‚ä½•å¼€å§‹å¼€å‘ä¸€ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹ã€‚\n\n[](https://docs.comfy.org/zh-CN/custom-nodes/overview)\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/nodes",
  "markdown": "# èŠ‚ç‚¹ - ComfyUI\n\nåœ¨ ComfyUI ä¸­ï¼ŒèŠ‚ç‚¹æ˜¯æˆ‘ä»¬æ‰§è¡Œä»»åŠ¡çš„å•å…ƒï¼Œä»–ä»¬æ˜¯æž„å»ºå¥½çš„ä¸€ä¸ªä¸ªç‹¬ç«‹çš„æ¨¡å—ï¼Œæ— è®ºæ˜¯ **Comfy Core** è¿˜æ˜¯ **è‡ªå®šä¹‰èŠ‚ç‚¹** ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å—ï¼Œæœ‰ç€è‡ªå·±ç‹¬ç‰¹çš„åŠŸèƒ½ï¼ŒèŠ‚ç‚¹ä¹‹é—´é€šè¿‡è¿žçº¿è¿žæŽ¥ï¼Œæˆ‘ä»¬å¯ä»¥åƒæ­ä¹é«˜ç§¯æœ¨ä¸€æ ·æ­å»ºèµ·æ¥å¤æ‚çš„åŠŸèƒ½ã€‚ å¯ä»¥è¯´ï¼Œä¸åŒçš„èŠ‚ç‚¹ç»„åˆæž„å»ºå‡ºäº† ComfyUI çš„æ— é™å¯èƒ½ã€‚ ![Comfy Core K-Sampler èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg) ä¾‹å¦‚åœ¨ K-Sampler èŠ‚ç‚¹ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°å®ƒæœ‰å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºï¼Œä¹ŸåŒæ—¶åŒ…å«å¤šä¸ªå‚æ•°è®¾ç½®ï¼Œè¿™äº›å‚æ•°å†³å®šäº†èŠ‚ç‚¹æ‰§è¡Œçš„é€»è¾‘ï¼Œå®ƒçš„èƒŒåŽæ˜¯å¯¹åº”ç¼–å†™å¥½çš„ Python é€»è¾‘ï¼Œä»Žè€Œå¯ä»¥è®©ä½ ä¸ç”¨åŽ»æŽ¥è§¦ä»£ç å°±å¯ä»¥å®žçŽ°å¯¹åº”çš„åŠŸèƒ½ã€‚\n\n## èŠ‚ç‚¹çš„çš„ä¸åŒçŠ¶æ€\n\n![èŠ‚ç‚¹çŠ¶æ€](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/status.jpg) åœ¨ ComfyUI ä¸­ï¼ŒèŠ‚ç‚¹æœ‰å¤šç§çŠ¶æ€ï¼Œä¸‹é¢æ˜¯ä¸€äº›å¸¸è§çš„èŠ‚ç‚¹çŠ¶æ€ï¼š\n\n1.  **æ­£å¸¸(Normal)çŠ¶æ€**ï¼š æ­£å¸¸çŠ¶æ€\n2.  **è¿è¡Œ(Running)çŠ¶æ€**ï¼š è¿è¡Œä¸­çŠ¶æ€ï¼Œé€šå¸¸åœ¨ä½ å¼€å§‹è¿è¡Œå·¥ä½œæµåŽï¼Œæ­£åœ¨æ‰§è¡Œçš„èŠ‚ç‚¹ä¼šæ˜¾ç¤ºè¿™ä¸ªçŠ¶æ€\n3.  **é”™è¯¯(Error)çŠ¶æ€**ï¼š èŠ‚ç‚¹é”™è¯¯ï¼Œé€šå¸¸åœ¨è¿è¡Œå·¥ä½œæµåŽï¼Œå¦‚æžœå¯¹åº”çš„èŠ‚ç‚¹è¾“å…¥å­˜åœ¨é—®é¢˜ï¼Œå¯¼è‡´äº†é”™è¯¯ä¼šæ˜¾ç¤ºè¿™ä¸ªçŠ¶æ€ï¼Œå¹¶ç”¨çº¢è‰²æ ‡è¯†å¯¹åº”å‡ºé”™çš„è¾“å…¥èŠ‚ç‚¹ï¼Œä½ éœ€è¦è§£å†³å¯¹åº”å‡ºé”™çš„è¾“å…¥æ¥ä¿è¯å·¥ä½œæµæ­£å¸¸è¿è¡Œ\n4.  **ä¸¢å¤±(Missing)çŠ¶æ€**ï¼š è¿™ä¸ªçŠ¶æ€é€šå¸¸åœ¨ä½ å¯¼å…¥äº†ä¸€äº›å·¥ä½œæµåŽä¼šå‡ºçŽ°ï¼Œå­˜åœ¨ä¸¤ç§å¯èƒ½\n    *   ComfyCore åŽŸç”ŸèŠ‚ç‚¹ä¸¢å¤±ï¼š è¿™é€šå¸¸æ˜¯å› ä¸º ComfyUI çš„ç‰ˆæœ¬æ›´æ–°äº†ï¼Œè€Œä½ å½“å‰ä½¿ç”¨çš„ ComfyUI ç‰ˆæœ¬è¾ƒæ—§ï¼Œä½ éœ€è¦æ›´æ–° ComfyUI æ¥è§£å†³è¿™ä¸ªé—®é¢˜\n    *   è‡ªå®šä¹‰èŠ‚ç‚¹ä¸¢å¤±ï¼š å·¥ä½œæµä¸­æ˜¯ç”¨äº†ç¬¬ä¸‰æ–¹ä½œè€…å¼€å‘çš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œè€Œä½ çš„æœ¬åœ°çš„ ComfyUI ç‰ˆæœ¬æ²¡æœ‰å®‰è£…å¯¹åº”çš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œä½ å¯ä»¥ä½¿ç”¨ [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) æ¥æŸ¥æ‰¾ä¸¢å¤±çš„è‡ªå®šä¹‰èŠ‚ç‚¹\n\n## èŠ‚ç‚¹ä¹‹é—´çš„è¿žæŽ¥\n\nåœ¨ ComfyUI ä¸­ï¼ŒèŠ‚ç‚¹é€šè¿‡[è¿žçº¿](https://docs.comfy.org/zh-CN/development/core-concepts/links)è¿žæŽ¥ï¼Œä»Žè€Œè®©ç›¸åŒçš„æ•°æ®ç±»åž‹åœ¨ä¸åŒçš„å¤„ç†å•å…ƒä¹‹é—´è¿›è¡Œæµè½¬å¤„ç†,ä»Žè€ŒèŽ·å¾—æœ€ç»ˆçš„ç»“æžœã€‚ ![ComfyUI èŠ‚ç‚¹è¿žçº¿](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/inpaint.jpg) æ¯ä¸ªèŠ‚ç‚¹éƒ½ä¼šæŽ¥æ”¶ä¸€äº›è¾“å…¥å†…å®¹ï¼Œç„¶åŽç»è¿‡æ¨¡å—å¤„ç†å°†ä»–ä»¬è½¬æ¢ä¸ºå¯¹åº”çš„è¾“å‡ºï¼Œä¸åŒçš„èŠ‚ç‚¹é“¾æŽ¥ä¹‹é—´ï¼Œå¿…é¡»ç¬¦åˆæ•°æ®ç±»åž‹è§„å®šçš„è¦æ±‚ï¼Œåœ¨ ComfyUI ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„é¢œè‰²æ¥åŒºåˆ†èŠ‚ç‚¹çš„æ•°æ®ç±»åž‹,ä¸‹é¢æ˜¯ä¸€äº›åŸºç¡€çš„æ•°æ®ç±»åž‹ã€‚ ![ComfyUI èŠ‚ç‚¹æ•°æ®ç±»åž‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/data_type.jpg)\n\n| æ•°æ®ç±»åž‹ | é¢œè‰²  |\n| --- | --- |\n| æ‰©æ•£æ¨¡åž‹ | è–°è¡£è‰è‰² |\n| CLIP æ¨¡åž‹ | é»„è‰²  |\n| VAE æ¨¡åž‹ | çŽ«ç‘°è‰² |\n| æ¡ä»¶åŒ– | æ©™è‰²  |\n| æ½œåœ¨å›¾åƒ | ç²‰è‰²  |\n| åƒç´ å›¾åƒ | è“è‰²  |\n| è’™ç‰ˆ  | ç»¿è‰²  |\n| æ•°å­— (æ•´æ•°æˆ–æµ®ç‚¹æ•°) | æµ…ç»¿è‰² |\n| ç½‘æ ¼ï¼ˆMeshï¼‰ | äº®ç»¿è‰² |\n\néšç€ ComfyUI çš„è¿­ä»£ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæ‹“å±•æ›´å¤šçš„æ•°æ®ç±»åž‹ï¼Œä»¥ç¬¦åˆæ›´å¤šåœºæ™¯çš„éœ€æ±‚ã€‚\n\n### èŠ‚ç‚¹ä¹‹é—´çš„è¿žæŽ¥å’Œå–æ¶ˆè¿žæŽ¥\n\n![ComfyUI èŠ‚ç‚¹è¿žæŽ¥](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/link_nodes.gif) **è¿žæŽ¥**ï¼š åœ¨ä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºç‚¹ä¸­æ‹–æ‹½åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ç›¸åŒé¢œè‰²çš„è¾“å…¥ä¸­ï¼Œå³å¯è¿žæŽ¥ **å–æ¶ˆè¿žæŽ¥**ï¼š åœ¨è¢«è¾“å…¥çš„ç«¯ç‚¹ï¼Œç‚¹å‡»åŽé¼ æ ‡å·¦é”®æ‹–æ‹½è¾“å…¥ï¼Œå³å¯å–æ¶ˆè¿žæŽ¥ï¼Œæˆ–è€…é€šè¿‡è¿žçº¿çš„ä¸­ç‚¹èœå•æ¥å–æ¶ˆè¿žæŽ¥ã€‚\n\n## èŠ‚ç‚¹çš„å¤–è§‚\n\n![èŠ‚ç‚¹å¤–è§‚](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/node.jpg) æˆ‘ä»¬ä¸ºæä¾›äº†å¤šç§æ ·å¼è®¾ç½®ï¼Œä½ å¯ä»¥æ ¹æ®ä½ çš„éœ€æ±‚æ¥è®¾ç½®èŠ‚ç‚¹çš„å¤–è§‚:\n\n*   ä¿®æ”¹æ ·å¼\n*   åŒå‡»èŠ‚ç‚¹æ ‡é¢˜ä¿®æ”¹èŠ‚ç‚¹åç§°\n*   é€šè¿‡ä¸Šä¸‹æ–‡èœå•å°†èŠ‚ç‚¹è¾“å…¥åœ¨ input å’Œ ç»„ä»¶ï¼ˆwidgetï¼‰ä¹‹é—´è¿›è¡Œåˆ‡æ¢\n*   é€šè¿‡èŠ‚ç‚¹å³ä¸‹è§’æ¥ç¼©æ”¾èŠ‚ç‚¹å°ºå¯¸\n\n### èŠ‚ç‚¹æ ‡ç­¾ Badges\n\n![èŠ‚ç‚¹æ ‡ç­¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/badge.jpg) æˆ‘ä»¬æä¾›äº†å¤šä¸ªèŠ‚ç‚¹æ ‡ç­¾ï¼ˆBadgesï¼‰çš„æ˜¾ç¤ºåŠŸèƒ½ï¼Œæ¯”å¦‚ï¼š\n\n*   èŠ‚ç‚¹ID\n*   èŠ‚ç‚¹æ¥æº\n\nç›®å‰ **Comfy Core èŠ‚ç‚¹** é‡‡ç”¨å°ç‹ç‹¸çš„å›¾æ ‡æ¥å±•ç¤ºï¼Œè‡ªå®šä¹‰èŠ‚ç‚¹åˆ™é‡‡ç”¨å…¶åç§°ï¼Œè¿™æ ·ä½ å¯ä»¥å¿«é€Ÿäº†è§£åˆ°å¯¹åº”èŠ‚ç‚¹æ˜¯æ¥è‡ªå“ªä¸ªèŠ‚ç‚¹åŒ…ã€‚ ä½ å¯ä»¥åœ¨èœå•ä¸­è®¾ç½®å¯¹åº”çš„æ˜¾ç¤ºï¼š ![æ ‡ç­¾è®¾ç½®](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/badge_setting.jpg)\n\n## èŠ‚ç‚¹ä¸Šä¸‹æ–‡èœå•\n\nèŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡èœå•ä¸»è¦åˆ†ä¸ºä¸¤ç§\n\n*   é’ˆå¯¹èŠ‚ç‚¹æœ¬èº«çš„ä¸Šä¸‹æ–‡èœå•\n*   é’ˆå¯¹è¾“å…¥ / è¾“å‡ºçš„ä¸Šä¸‹æ–‡èœå•\n\n### èŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡èœå•\n\né€šè¿‡åœ¨èŠ‚ç‚¹ä¸Šç‚¹å‡»é¼ æ ‡å³é”®ï¼Œä½ å¯ä»¥å±•å¼€å¯¹åº”çš„èŠ‚ç‚¹ä¸Šä¸‹æ–‡èœå•ï¼Œä¸‹é¢æ˜¯å¯¹åº”çš„èœå•æˆªå›¾ï¼š ![èŠ‚ç‚¹ä¸Šä¸‹æ–‡èœå•](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/context_menus_1.jpg) åœ¨èŠ‚ç‚¹çš„å³é”®ä¸Šä¸‹æ–‡èœå•ä¸­ä½ å¯ä»¥\n\n*   è°ƒæ•´èŠ‚ç‚¹çš„é¢œè‰²æ ·å¼\n*   ä¿®æ”¹æ ‡é¢˜\n*   å…‹éš†ã€å¤åˆ¶ã€åˆ é™¤èŠ‚ç‚¹\n*   è®¾ç½®èŠ‚ç‚¹çš„æ¨¡å¼\n\nåœ¨è¿™ä¸ªèœå•ä¸­ï¼Œé™¤äº†å¤–è§‚ç›¸å…³çš„è®¾ç½®æ¯”è¾ƒé‡è¦çš„æ˜¯ä¸‹é¢çš„èœå•æ“ä½œ\n\n*   **æ¨¡å¼ï¼ˆModeï¼‰**ï¼š è®¾ç½®èŠ‚ç‚¹çš„æ¨¡å¼ï¼ŒAlwaysã€Neverã€ç»•è¿‡ï¼ˆBypassï¼‰\n*   **åˆ‡æ¢èŠ‚ç‚¹è¾“å…¥çš„æŽ§ä»¶ï¼ˆWidgetï¼‰å’Œ è¾“å…¥æ¨¡å¼**ï¼š åˆ‡æ¢èŠ‚ç‚¹è¾“å…¥çš„æŽ§ä»¶ï¼ˆWidgetï¼‰å’Œ è¾“å…¥æ¨¡å¼\n\n#### æ¨¡å¼ï¼ˆModeï¼‰\n\nå¯¹äºŽæ¨¡å¼ï¼Œä½ å¯èƒ½æ³¨æ„åˆ°ç›®å‰æˆ‘ä»¬æä¾›äº†ï¼šAlwaysã€Neverã€On Eventã€On Trigger å››ç§æ¨¡å¼ï¼Œä½†å®žé™…ä¸Šåªæœ‰ **Always** å’Œ **Never** æ˜¯æœ‰æ•ˆçš„ï¼Œ**On Event** å’Œ **On Trigger** å®žé™…ä¸Šæ˜¯æ— æ•ˆçš„ï¼Œç›®å‰æˆ‘ä»¬å°šæœªå®Œå…¨å®žçŽ°è¿™ä¸ªåŠŸèƒ½ï¼Œå¦å¤–ä½ å¯ä»¥æŠŠ **ç»•è¿‡ï¼ˆBypassï¼‰** ä¹Ÿç†è§£ä¸ºä¸€ç§æ¨¡å¼ï¼Œä¸‹é¢æ˜¯å¯¹äºŽå‡ ç§å¯ç”¨æ¨¡å¼çš„è§£é‡Š\n\n*   **Always**ï¼š èŠ‚ç‚¹é»˜è®¤æ¨¡å¼ï¼Œå½“èŠ‚ç‚¹é¦–æ¬¡è¿è¡Œæˆ–è€…è‡ªä¸Šä¸€æ¬¡æ‰§è¡ŒåŽï¼Œå¯¹åº”è¾“å…¥æœ‰å˜åŒ–å¯¹åº”èŠ‚ç‚¹éƒ½ä¼šæ‰§è¡Œ\n*   **Never**ï¼š èŠ‚ç‚¹åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½ä¸ä¼šæ‰§è¡Œï¼Œå°±åƒèŠ‚ç‚¹è¢«åˆ é™¤äº†ï¼ŒåŽç»­èŠ‚ç‚¹æ— æ³•è¯»å–æŽ¥æ”¶åˆ°ä»»ä½•æ¥è‡ªå®ƒçš„æ•°æ®\n*   **ç»•è¿‡ï¼ˆBypassï¼‰**ï¼š èŠ‚ç‚¹åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½ä¸ä¼šæ‰§è¡Œï¼Œä½†æ˜¯åŽç»­çš„èŠ‚ç‚¹ä»æ—§å¯ä»¥è¯•ç€èŽ·å–åˆ°æœªç»è¿™ä¸ªèŠ‚ç‚¹çš„å¤„ç†çš„æ•°æ®\n\nä¸‹é¢æ˜¯å¯¹äºŽ `Never` å’Œ `Bypass` æ¨¡å¼çš„å¯¹æ¯”ï¼š ![Never å’Œ Bypass æ¨¡å¼](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/never_vs_bypass.jpg) åœ¨è¿™ä¸ªå¯¹æ¯”çš„ä¾‹å­ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°ï¼Œä¸¤ä¸ªå·¥ä½œæµéƒ½æ˜¯åŒæ—¶åº”ç”¨äº†ä¸¤ä¸ª LoRA æ¨¡åž‹ï¼Œå·®å¼‚åœ¨äºŽå…¶ä¸­ä¸€ä¸ª`Load LoRA` èŠ‚ç‚¹è¢«è®¾ç½®ä¸º `Never` æ¨¡å¼è€Œå¦ä¸€ä¸ªè¢«è®¾ç½®ä¸º`Bypass` æ¨¡å¼ã€‚\n\n*   è¢«è®¾ç½®ä¸º `Never` æ¨¡å¼çš„èŠ‚ç‚¹ï¼ŒåŽç»­çš„èŠ‚ç‚¹ç”±äºŽæŽ¥æ”¶ä¸åˆ°ä»»ä½•çš„è¾“å…¥æ•°æ®è€Œå‡ºçŽ°äº†æŠ¥é”™\n*   è¢«è®¾ç½®ä¸º `Bypass` æ¨¡å¼çš„èŠ‚ç‚¹ï¼ŒåŽç»­çš„èŠ‚ç‚¹ä»æ—§å¯ä»¥èŽ·å–åˆ°æœªç»è¿™ä¸ªèŠ‚ç‚¹å¤„ç†çš„æ•°æ®ï¼Œä»Žè€ŒåŠ è½½äº†ç¬¬ä¸€ä¸ª`Load LoRA` èŠ‚ç‚¹çš„è¾“å‡ºæ•°æ®ï¼Œæ‰€ä»¥åŽç»­çš„å·¥ä½œæµä¾æ—§å¯ä»¥æ­£å¸¸è¿è¡Œ\n\n#### åˆ‡æ¢èŠ‚ç‚¹è¾“å…¥çš„æŽ§ä»¶ï¼ˆWidgetï¼‰å’Œ è¾“å…¥æ¨¡å¼\n\nåœ¨æœ‰äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æ¥è‡ªå…¶å®ƒèŠ‚ç‚¹çš„è¾“å‡ºç»“æžœä½œä¸ºè¾“å…¥ï¼Œæ­¤æ—¶æˆ‘ä»¬å°±å¯ä»¥é€šè¿‡åˆ‡æ¢èŠ‚ç‚¹è¾“å…¥çš„æŽ§ä»¶ï¼ˆWidgetï¼‰å’Œ è¾“å…¥æ¨¡å¼æ¥å®žçŽ°ã€‚ ä¸‹é¢æ˜¯ä¸€ä¸ªéžå¸¸ç®€å•çš„ä¾‹å­ï¼š ![åˆ‡æ¢æŽ§ä»¶å’Œè¾“å…¥æ¨¡å¼](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/switch_widget.jpg) é€šè¿‡å°† K-Sampler çš„ Seed ä»Žè¾“å…¥æŽ§ä»¶ï¼ˆWidgetï¼‰åˆ‡æ¢ä¸ºè¾“å…¥æ¨¡å¼ï¼Œä»Žè€Œç»Ÿä¸€å¤šä¸ªèŠ‚ç‚¹çš„ seed ï¼Œå®žçŽ°å¤šä¸ªé‡‡æ ·é—´çš„å˜é‡ç»Ÿä¸€ã€‚ å¯¹æ¯”ç¬¬ä¸€ä¸ªèŠ‚ç‚¹å’ŒåŽç»­çš„ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œä½ å¯ä»¥çœ‹åˆ°åŽä¸¤ä¸ªèŠ‚ç‚¹çš„ seed æ˜¯è¾“å…¥æ¨¡å¼äº†ï¼ŒåŒæ ·ä½ è¿˜å¯ä»¥æŠŠå®ƒå†è½¬æ¢å›žæŽ§ä»¶æ¨¡å¼ï¼š ![è½¬æ¢æŽ§ä»¶å’Œè¾“å…¥æ¨¡å¼](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/convert_input.jpg)\n\n### è¾“å…¥ / è¾“å‡ºçš„ä¸Šä¸‹æ–‡èœå•\n\nè¿™é‡Œä¸Šä¸‹æ–‡èœå•ä¸»è¦å’Œå¯¹åº”è¾“å…¥è¾“å‡ºçš„æ•°æ®ç±»åž‹ç›¸å…³ ![èŠ‚ç‚¹è¾“å…¥è¾“å‡ºä¸Šä¸‹æ–‡èœå•](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/context_menus_2.jpg) åœ¨æ‹–åŠ¨èŠ‚ç‚¹çš„è¾“å…¥ / è¾“å‡ºçš„æ—¶å€™ï¼Œå½“æœ‰è¿žçº¿å‡ºçŽ°ï¼Œä½†ä½ æœªè¿žæŽ¥åˆ°å…¶å®ƒèŠ‚ç‚¹çš„è¾“å…¥æˆ–è¾“å‡ºçš„æ—¶å€™ï¼Œæ­¤æ—¶é‡Šæ”¾é¼ æ ‡åˆ™ä¼šå¼¹å‡ºé’ˆå¯¹è¾“å…¥ / è¾“å‡ºçš„ä¸Šä¸‹æ–‡èœå•ï¼Œç”¨äºŽå¿«é€Ÿæ·»åŠ ç›¸å…³ç±»åž‹çš„èŠ‚ç‚¹ã€‚ ä½ å¯ä»¥åœ¨è®¾ç½®ä¸­è°ƒæ•´å¯¹åº”çš„èŠ‚ç‚¹å»ºè®®çš„æ•°é‡ ![èŠ‚ç‚¹å»ºè®®æ•°é‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/node_suggestions.jpg)\n\n## èŠ‚ç‚¹é€‰æ‹©å·¥å…·ç®±\n\n**èŠ‚ç‚¹é€‰æ‹©å·¥å…·ç®±ï¼ˆSelection tool boxï¼‰** æ˜¯ä¸€ä¸ªä¸ºèŠ‚ç‚¹æä¾›å¿«é€Ÿæ“ä½œçš„ä¸€ä¸ªæµ®å±‚å·¥å…·ï¼Œå½“ä½ é€‰ä¸­ä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶å€™ï¼Œå®ƒä¼šæ‚¬æµ®åœ¨é€‰ä¸­çš„èŠ‚ç‚¹ä¸Šæ–¹ï¼Œé€šè¿‡è¿™ä¸ªèŠ‚ç‚¹ä½ å¯ä»¥ï¼š\n\n*   ä¿®æ”¹èŠ‚ç‚¹çš„é¢œè‰²\n*   å¿«é€Ÿè®¾ç½®èŠ‚ç‚¹ä¸º Bypass æ¨¡å¼(åœ¨è¿è¡Œæ—¶å€™ä¸æ‰§è¡Œ)\n*   å›ºå®šèŠ‚ç‚¹\n*   åˆ é™¤èŠ‚ç‚¹\n\nå½“ç„¶ï¼Œè¿™äº›åŠŸèƒ½åœ¨å¯¹åº”èŠ‚ç‚¹çš„å³é”®èœå•ä¸­ä¹Ÿå¯ä»¥æ‰¾åˆ°ï¼ŒèŠ‚ç‚¹é€‰æ‹©å·¥å…·ç®±åªæ˜¯æä¾›äº†ä¸€ä¸ªå¿«æ·æ“ä½œï¼Œå¦‚æžœä½ æƒ³è¦å…³é—­è¿™ä¸ªåŠŸèƒ½ï¼Œå¯ä»¥åœ¨è®¾ç½®ä¸­å…³é—­ã€‚ ![å…³é—­èŠ‚ç‚¹é€‰æ‹©å·¥å…·ç®±](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/core-concept/node/setting_selection_toolbox.jpg)\n\n## èŠ‚ç‚¹ç»„\n\nåœ¨ ComfyUI ä¸­ï¼Œä½ å¯ä»¥å°†ä¸€ä¸ªå·¥å…·æµçš„éƒ¨åˆ†ï¼ŒåŒæ—¶é€‰ç”¨ï¼Œå†ä½¿ç”¨å³é”®èœå•å°†å®ƒä»¬åˆå¹¶æˆä¸€ä¸ªèŠ‚ç‚¹ç»„ï¼Œä½¿å¾—å¯¹åº”çš„éƒ¨åˆ†å¯ä»¥æˆä¸ºä¸€ä¸ªå¯å¤ç”¨çš„æ¨¡å—ï¼Œä»Žè€Œåœ¨ä½ çš„ ComfyUI ä¸­è¿›è¡Œé‡å¤è°ƒç”¨\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/audio/ace-step/ace-step-v1",
  "markdown": "# ComfyUI ACE-Step åŽŸç”Ÿç¤ºä¾‹ - ComfyUI\n\nACE-Stepæ˜¯ç”±ä¸­å›½å›¢é˜Ÿé˜¶è·ƒæ˜Ÿè¾°ï¼ˆStepFunï¼‰ä¸ŽACE Studioè”åˆå¼€å‘çš„â€‹â€‹å¼€æºéŸ³ä¹ç”ŸæˆåŸºç¡€å¤§æ¨¡åž‹â€‹â€‹ï¼Œæ—¨åœ¨ä¸ºéŸ³ä¹åˆ›ä½œè€…æä¾›é«˜æ•ˆã€çµæ´»ä¸”é«˜è´¨é‡çš„éŸ³ä¹ç”Ÿæˆä¸Žç¼–è¾‘å·¥å…·ã€‚ è¯¥æ¨¡åž‹é‡‡ç”¨[Apache-2.0](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-license)è®¸å¯è¯å‘å¸ƒï¼Œå¯å…è´¹å•†ç”¨ã€‚ ACE-Step ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„éŸ³ä¹ç”ŸæˆåŸºåº§ï¼Œæä¾›äº†ä¸°å¯Œçš„æ‰©å±•èƒ½åŠ›ã€‚é€šè¿‡ LoRAã€ControlNet ç­‰å¾®è°ƒæŠ€æœ¯ï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®å®žé™…éœ€æ±‚å¯¹æ¨¡åž‹è¿›è¡Œå®šåˆ¶åŒ–è®­ç»ƒã€‚ æ— è®ºæ˜¯éŸ³é¢‘ç¼–è¾‘ã€æ­Œå£°åˆæˆã€ä¼´å¥åˆ¶ä½œã€å£°éŸ³å…‹éš†è¿˜æ˜¯é£Žæ ¼è½¬æ¢ç­‰åº”ç”¨åœºæ™¯ï¼ŒACE-Step éƒ½èƒ½æä¾›ç¨³å®šå¯é çš„æŠ€æœ¯æ”¯æŒã€‚ è¿™ç§çµæ´»çš„æž¶æž„è®¾è®¡å¤§å¤§ç®€åŒ–äº†éŸ³ä¹ AI åº”ç”¨çš„å¼€å‘æµç¨‹ï¼Œè®©æ›´å¤šåˆ›ä½œè€…èƒ½å¤Ÿå¿«é€Ÿå°† AI æŠ€æœ¯åº”ç”¨åˆ°éŸ³ä¹åˆ›ä½œä¸­ã€‚ ç›®å‰ ACE-Step å·²ç»å‘å¸ƒç›¸å…³çš„è®­ç»ƒä»£ç ï¼ŒåŒ…æ‹¬ LoRA æ¨¡åž‹è®­ç»ƒç­‰ï¼Œå¯¹åº” ControlNet çš„è®­ç»ƒä»£ç ä¹Ÿå°†åœ¨æœªæ¥é™†ç»­å‘å¸ƒï¼Œä½ å¯ä»¥è®¿é—®ä»–ä»¬çš„[Github](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-roadmap) æ¥äº†è§£æ›´å¤šè¯¦æƒ…ã€‚\n\n### 1\\. å·¥ä½œæµåŠç›¸å…³æ¨¡åž‹ä¸‹è½½\n\nç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®ä¸‹è½½å¯¹åº”çš„å·¥ä½œæµæ–‡ä»¶ï¼Œæ‹–å…¥ ComfyUI ä¸­å³å¯åŠ è½½å¯¹åº”çš„å·¥ä½œæµä¿¡æ¯ï¼Œå¯¹åº”å·¥ä½œæµå·²åŒ…å«æ¨¡åž‹ä¸‹è½½ä¿¡æ¯ã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_t2m.json)\n\nä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨ä¸‹è½½[ace\\_step\\_v1\\_3.5b.safetensors](https://huggingface.co/Comfy-Org/ACE-Step_ComfyUI_repackaged/blob/main/all_in_one/ace_step_v1_3.5b.safetensors) åŽä¿å­˜åˆ° `ComfyUI/models/checkpoints` æ–‡ä»¶å¤¹ä¸‹\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg)\n\n1.  ç¡®ä¿ `Load Checkpoints` èŠ‚ç‚¹åŠ è½½äº† `ace_step_v1_3.5b.safetensors` æ¨¡åž‹\n2.  ï¼ˆå¯é€‰ï¼‰åœ¨ `EmptyAceStepLatentAudio` èŠ‚ç‚¹ä¸Šä½ å¯ä»¥è®¾ç½®ç”ŸæˆéŸ³ä¹çš„æ—¶é•¿\n3.  ï¼ˆå¯é€‰ï¼‰åœ¨ `LatentOperationTonemapReinhard` èŠ‚ç‚¹ï¼Œä½ å¯ä»¥è°ƒæ•´ `multiplier` æ¥è°ƒæ•´äººå£°çš„éŸ³é‡å¤§å°ï¼ˆæ•°å­—è¶Šå¤§ï¼Œäººå£°éŸ³é‡è¶Šæ˜Žæ˜¾ï¼‰\n4.  ï¼ˆå¯é€‰ï¼‰åœ¨ `TextEncodeAceStepAudio` çš„ `tags` è¾“å…¥å¯¹åº”çš„éŸ³ä¹é£Žæ ¼ç­‰ç­‰\n5.  ï¼ˆå¯é€‰ï¼‰åœ¨ `TextEncodeAceStepAudio` çš„ `lyrics` ä¸­è¾“å…¥å¯¹åº”çš„æ­Œè¯ï¼Œå¦‚æžœä½ ä¸çŸ¥é“è¯¥è¾“å…¥å“ªäº›æ­Œè¯\n6.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡ŒéŸ³é¢‘çš„ç”Ÿæˆã€‚\n7.  å·¥ä½œæµå®ŒæˆåŽï¼Œä½ å¯åœ¨ `Save Audio` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„éŸ³é¢‘ï¼Œä½ å¯ä»¥ç‚¹å‡»æ’­æ”¾è¯•å¬ï¼Œå¯¹åº”çš„éŸ³é¢‘ä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/audio` ï¼ˆç”±`Save Audio`èŠ‚ç‚¹å†³å®šå­ç›®å½•åç§°ï¼‰ã€‚\n\n## ACE-Step ComfyUI éŸ³é¢‘åˆ°éŸ³é¢‘å·¥ä½œæµ\n\nä½ å¯ä»¥åƒå›¾ç”Ÿå›¾å·¥ä½œæµä¸€æ ·ï¼Œè¾“å…¥ä¸€æ®µéŸ³ä¹ï¼Œä½¿ç”¨ä¸‹é¢çš„å·¥ä½œæµæ¥è¾¾åˆ°é‡æ–°å¯¹éŸ³ä¹é‡‡æ ·ç”Ÿæˆï¼ŒåŒæ ·ï¼Œä½ ä¹Ÿå¯ä»¥é€šè¿‡æŽ§åˆ¶ `Ksampler` çš„ `denoise` æ¥è°ƒæ•´å’ŒåŽŸå§‹éŸ³é¢‘çš„åŒºåˆ«ç¨‹åº¦ã€‚ é€šè¿‡è¿™æ ·çš„æµç¨‹ï¼Œå¯ä»¥å®žçŽ°å¯¹éŸ³ä¹çš„é‡æ–°ç¼–è¾‘ï¼Œæ¥è¾¾åˆ°ä½ æƒ³è¦çš„æ•ˆæžœã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®ä¸‹è½½å¯¹åº”çš„å·¥ä½œæµæ–‡ä»¶ï¼Œæ‹–å…¥ ComfyUI ä¸­å³å¯åŠ è½½å¯¹åº”çš„å·¥ä½œæµä¿¡æ¯\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_m2m_editing.json)\n\nä¸‹è½½ä¸‹é¢çš„éŸ³é¢‘ä½œä¸ºè¾“å…¥éŸ³é¢‘\n\n[\n\nä¸‹è½½ç¤ºä¾‹éŸ³é¢‘æ–‡ä»¶ç”¨äºŽè¾“å…¥\n\n](https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/audio/ace-step/input.mp3)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ACE-Step æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg)\n\n1.  ç¡®ä¿ `Load Checkpoints` èŠ‚ç‚¹åŠ è½½äº† `ace_step_v1_3.5b.safetensors` æ¨¡åž‹\n2.  åœ¨ `LoadAudio` èŠ‚ç‚¹ä¸­ä¸Šä¼ æä¾›çš„éŸ³é¢‘æ–‡ä»¶\n3.  ï¼ˆå¯é€‰ï¼‰åœ¨ `TextEncodeAceStepAudio` çš„ `tags` å’Œ `lyrics` ä¸­è¾“å…¥å¯¹åº”çš„éŸ³ä¹é£Žæ ¼æ­Œè¯ç­‰ï¼Œæä¾›æ­Œè¯å¯¹äºŽéŸ³é¢‘ç¼–è¾‘æ¥è¯´éžå¸¸é‡è¦\n4.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹ `Ksampler` èŠ‚ç‚¹çš„ `denoise` å‚æ•°ï¼Œæ¥è°ƒæ•´é‡‡æ ·è¿‡ç¨‹ä¸­æ·»åŠ çš„å™ªå£°æ¥è°ƒæ•´ä¸ŽåŽŸå§‹éŸ³é¢‘çš„ç›¸ä¼¼ç¨‹åº¦ï¼Œï¼ˆè¶Šå°ä¸ŽåŽŸå§‹éŸ³é¢‘è¶Šç›¸ä¼¼ï¼Œå¦‚æžœè®¾ç½®ä¸º `1.00`åˆ™å¯ä»¥è¿‘ä¼¼è®¤ä¸ºæ²¡æœ‰éŸ³é¢‘è¾“å…¥ï¼‰\n5.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡ŒéŸ³é¢‘çš„ç”Ÿæˆã€‚\n6.  å·¥ä½œæµå®ŒæˆåŽï¼Œä½ å¯åœ¨ `Save Audio` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„éŸ³é¢‘ï¼Œä½ å¯ä»¥ç‚¹å‡»æ’­æ”¾è¯•å¬ï¼Œå¯¹åº”çš„éŸ³é¢‘ä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/audio` ï¼ˆç”±`Save Audio`èŠ‚ç‚¹å†³å®šå­ç›®å½•åç§°ï¼‰ã€‚\n\n### 3\\. å·¥ä½œæµè¡¥å……è¯´æ˜Ž\n\n1.  åœ¨ `TextEncodeAceStepAudio` çš„ `tags` ä¸­ç¤ºä¾‹å·¥ä½œæµä¸­ï¼Œå°†åŽŸæœ¬ç”·å£°çš„ `tags` ä¿®æ”¹ä¸º `female voice` æ¥ç”Ÿæˆå¥³å£°çš„éŸ³é¢‘\n2.  åœ¨ `TextEncodeAceStepAudio` çš„ `lyrics` ä¸­ç¤ºä¾‹å·¥ä½œæµä¸­ï¼Œä¸­å¯¹åŽŸæœ¬çš„æ­Œè¯è¿›è¡Œäº†è°ƒæ•´ä¿®æ”¹ï¼Œå…·ä½“ç¼–è¾‘ä½ å¯ä»¥å‚è€ƒ ACE-Step é¡¹ç›®é¡µé¢ä¸­çš„ç¤ºä¾‹æ¥äº†è§£å¦‚ä½•å®Œæˆä¿®æ”¹\n\n## ACE-Step æç¤ºè¯æŒ‡å—\n\nACE çš„æç¤ºè¯ç›®å‰ä½¿ç”¨çš„æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯ `tags` ä¸€ä¸ªæ˜¯ `lyrics`ã€‚\n\n*   `tags`ï¼š ä¸»è¦ç”¨æ¥æè¿°éŸ³ä¹çš„é£Žæ ¼ã€åœºæ™¯ç­‰, å’Œæˆ‘ä»¬å¹³å¸¸å…¶å®ƒç”Ÿæˆçš„ prompt ç±»ä¼¼ï¼Œä¸»è¦æè¿°éŸ³é¢‘æ•´ä½“çš„é£Žæ ¼å’Œè¦æ±‚ï¼Œä½¿ç”¨è‹±æ–‡é€—å·åˆ†éš”\n*   `lyrics`ï¼š ä¸»è¦ç”¨æ¥æè¿°æ­Œè¯ï¼Œæ”¯æŒæ­Œè¯ç»“æž„æ ‡ç­¾ï¼Œå¦‚ \\[verse\\]ï¼ˆä¸»æ­Œï¼‰ã€\\[chorus\\]ï¼ˆå‰¯æ­Œï¼‰å’Œ \\[bridge\\]ï¼ˆè¿‡æ¸¡æ®µï¼‰æ¥åŒºåˆ†æ­Œè¯çš„ä¸åŒéƒ¨åˆ†ï¼Œä¹Ÿå¯ä»¥åœ¨çº¯éŸ³ä¹æƒ…å†µä¸‹è¾“å…¥ä¹å™¨åç§°\n\nå¯¹åº”çš„ `tags` å’Œ `lyrics` åœ¨ [ACE-Step æ¨¡åž‹ä¸»é¡µ](https://ace-step.github.io/) ä¸­å¯ä»¥æ‰¾åˆ°ä¸°å¯Œçš„ç¤ºä¾‹,ä½ å¯ä»¥å‚è€ƒå¯¹åº”ç¤ºä¾‹æ¥å°è¯•å¯¹åº”çš„æç¤ºè¯ï¼Œæœ¬æ–‡æ¡£çš„æç¤ºè¯æŒ‡å—åŸºäºŽé¡¹ç›®åšäº†ä¸€äº›æ•´ç†ï¼Œä»¥ä¾¿è®©ä½ èƒ½å¤Ÿå¿«é€Ÿå°è¯•ç»„åˆï¼Œæ¥è¾¾åˆ°æœ€æƒ³è¦çš„æ•ˆæžœ\n\n### tagsæ ‡ç­¾(prompt)\n\n#### ä¸»æµéŸ³ä¹é£Žæ ¼\n\nä½¿ç”¨ç®€çŸ­æ ‡ç­¾ç»„åˆï¼Œæ¥ç”Ÿæˆç‰¹å®šé£Žæ ¼çš„éŸ³ä¹\n\n*   electronicï¼ˆç”µå­éŸ³ä¹ï¼‰\n*   rockï¼ˆæ‘‡æ»šï¼‰\n*   popï¼ˆæµè¡Œï¼‰\n*   funkï¼ˆæ”¾å…‹ï¼‰\n*   soulï¼ˆçµé­‚ä¹ï¼‰\n*   cyberpunkï¼ˆèµ›åšæœ‹å…‹ï¼‰\n*   Acid jazzï¼ˆé…¸çˆµå£«ï¼‰\n*   electroï¼ˆç”µå­ï¼‰\n*   emï¼ˆç”µå­éŸ³ä¹ï¼‰\n*   soft electric drumsï¼ˆè½¯ç”µé¼“ï¼‰\n*   melodicï¼ˆæ—‹å¾‹ï¼‰\n\n#### åœºæ™¯ç±»åž‹\n\nç»“åˆå…·ä½“ä½¿ç”¨åœºæ™¯å’Œæ°›å›´ï¼Œç”Ÿæˆç¬¦åˆå¯¹åº”æ°›å›´çš„éŸ³ä¹\n\n*   background music for partiesï¼ˆæ´¾å¯¹èƒŒæ™¯éŸ³ä¹ï¼‰\n*   radio broadcastsï¼ˆç”µå°å¹¿æ’­éŸ³ä¹ï¼‰\n*   workout playlistsï¼ˆå¥èº«æ’­æ”¾åˆ—è¡¨éŸ³ä¹ï¼‰\n\n#### ä¹å™¨å…ƒç´ \n\n*   saxophone,\n*   azzï¼ˆè¨å…‹æ–¯é£Žã€çˆµå£«ï¼‰\n*   piano, violinï¼ˆé’¢ç´ã€å°æç´ï¼‰\n\n#### äººå£°ç±»åž‹\n\n*   female voiceï¼ˆå¥³å£°ï¼‰\n*   male voiceï¼ˆç”·å£°ï¼‰\n*   clean vocalsï¼ˆçº¯å‡€äººå£°ï¼‰\n\n#### ä¸“ä¸šç”¨è¯­\n\nä½¿ç”¨éŸ³ä¹ä¸­å¸¸ç”¨çš„ä¸€äº›ä¸“ä¸šçš„ç”¨è¯ï¼Œæ¥ç²¾å‡†æŽ§åˆ¶éŸ³ä¹æ•ˆæžœ\n\n*   110 bpmï¼ˆæ¯åˆ†é’ŸèŠ‚æ‹æ•°ä¸º110ï¼‰\n*   fast tempoï¼ˆå¿«èŠ‚å¥ï¼‰\n*   slow tempoï¼ˆæ…¢èŠ‚å¥ï¼‰\n*   loopsï¼ˆå¾ªçŽ¯ç‰‡æ®µï¼‰\n*   fillsï¼ˆå¡«å……éŸ³ï¼‰\n*   acoustic guitarï¼ˆæœ¨å‰ä»–ï¼‰\n*   electric bassï¼ˆç”µè´æ–¯ï¼‰\n\n### æ­Œè¯ï¼ˆlyricsï¼‰\n\n#### æ­Œè¯ç»“æž„æ ‡ç­¾\n\n*   \\[intro\\] (å‰å¥)\n*   \\[verse\\] (ä¸»æ­Œ)\n*   \\[pre-chorus\\] (å¯¼æ­Œ)\n*   \\[chorus\\] (å‰¯æ­Œ/åˆå”±)\n*   \\[bridge\\] (è¿‡æ¸¡æ®µ/æ¡¥æ®µ)\n*   \\[outro\\] (å°¾å£°)\n*   \\[hook\\] (é’©å­/ä¸»é¢˜æ—‹å¾‹)\n*   \\[refrain\\] (é‡å¤æ®µè½)\n*   \\[interlude\\] (é—´å¥)\n*   \\[breakdown\\] (åˆ†è§£æ®µ)\n*   \\[ad-lib\\] (å³å…´æ®µè½)\n\n#### å¤šè¯­è¨€æ”¯æŒ\n\n*   ACE-Step V1 æ˜¯æ”¯æŒå¤šè¯­è¨€çš„ï¼Œå®žé™…ä½¿ç”¨çš„æ—¶å€™ ACE-Step ä¼šèŽ·å–åˆ°å¯¹åº”çš„ä¸åŒè¯­è¨€è½¬æ¢åŽçš„è‹±æ–‡å­—æ¯ï¼Œç„¶åŽè¿›è¡ŒéŸ³ä¹ç”Ÿæˆã€‚\n*   åœ¨ ComfyUI ä¸­æˆ‘ä»¬å¹¶æ²¡æœ‰å®Œå…¨å®žçŽ°å…¨éƒ¨å¤šè¯­è¨€åˆ°è‹±æ–‡å­—æ¯çš„è½¬æ¢ï¼Œç›®å‰ä»…å®žçŽ°äº†[æ—¥è¯­å¹³å‡åå’Œç‰‡å‡åå­—ç¬¦](https://github.com/comfyanonymous/ComfyUI/commit/5d3cc85e13833aeb6ef9242cdae243083e30c6fc) æ‰€ä»¥å¦‚æžœä½ éœ€è¦ä½¿ç”¨å¤šè¯­è¨€æ¥è¿›è¡Œç›¸å…³çš„éŸ³ä¹ç”Ÿæˆï¼Œä½ éœ€è¦é¦–å…ˆå°†å¯¹åº”çš„è¯­è¨€è½¬æ¢æˆè‹±æ–‡å­—æ¯ï¼Œç„¶åŽåœ¨å¯¹åº” `lyrics` å¼€å¤´è¾“å…¥å¯¹åº”è¯­è¨€ä»£ç çš„ç¼©å†™ï¼Œæ¯”å¦‚ä¸­æ–‡`[zh]` éŸ©è¯­ `[ko]` ç­‰\n\næ¯”å¦‚ï¼š\n\n```\n[verse]\n\n[zh]wo3zou3guo4shen1ye4de5jie1dao4\n[zh]leng3feng1chui1luan4si1nian4de5piao4liang4wai4tao4\n[zh]ni3de5wei1xiao4xiang4xing1guang1hen3xuan4yao4\n[zh]zhao4liang4le5wo3gu1du2de5mei3fen1mei3miao3\n\n[chorus]\n\n[verse]â€‹\n[ko]hamkke si-kkeuleo-un sesang-ui sodong-eul pihaeâ€‹\n[ko]honja ogsang-eseo dalbich-ui eolyeompus-ileul balabodaâ€‹\n[ko]niga salang-eun lideum-i ganghan eum-ag gatdago malhaess-eoâ€‹\n[ko]han ta han tamada ma-eum-ui ondoga eolmana heojeonhanji ijge hae\n\n[bridge]\n[es]cantar mi anhelo por ti sin ocultar\n[es]como poesÃ­a y pintura, lleno de anhelo indescifrable\n[es]tu sombra es tan terca como el viento, inborrable\n[es]persiguiÃ©ndote en vuelo, brilla como cruzar una mar de nubes\n\n[chorus]\n[fr]que tu sois le vent qui souffle sur ma main\n[fr]un contact chaud comme la douce pluie printaniÃ¨re\n[fr]que tu sois le vent qui s'entoure de mon corps\n[fr]un amour profond qui ne s'Ã©loignera jamais\n```\n\nç›®å‰ ACE-Step æ”¯æŒäº† 19 ç§è¯­è¨€ï¼Œä½†ä¸‹é¢åç§è¯­è¨€çš„æ”¯æŒä¼šæ›´å¥½ä¸€äº›ï¼š\n\n*   English\n*   Chinese: \\[zh\\]\n*   Russian: \\[ru\\]\n*   Spanish: \\[es\\]\n*   Japanese: \\[ja\\]\n*   German: \\[de\\]\n*   French: \\[fr\\]\n*   Portuguese: \\[pt\\]\n*   Italian: \\[it\\]\n*   Korean: \\[ko\\]\n\n## ACE-Step ç›¸å…³èµ„æº\n\n*   [é¡¹ç›®ä¸»é¡µ](https://ace-step.github.io/)\n*   [Hugging Face æ¨¡åž‹](https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B)\n*   [GitHub ä»“åº“](https://github.com/ace-step/ACE-Step)\n*   [è®­ç»ƒè„šæœ¬](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-train)"
},
{
  "url": "https://docs.comfy.org/zh-CN/development/core-concepts/links",
  "markdown": "# è¿žçº¿ - ComfyUI\n\n## è¿žçº¿è¿žæŽ¥èŠ‚ç‚¹\n\nåœ¨ ComfyUI çš„æœ¯è¯­ä¸­ï¼ŒèŠ‚ç‚¹ä¹‹é—´çš„çº¿æ¡æˆ–æ›²çº¿ç§°ä¸º _**è¿žçº¿**_ã€‚è¿™äº›è¿žçº¿å¯ä»¥ä»¥å¤šç§æ–¹å¼æ˜¾ç¤ºï¼Œä¾‹å¦‚æ›²çº¿ã€ç›´è§’çº¿ã€ç›´çº¿æˆ–ç›´æŽ¥å®Œå…¨éšè—ã€‚ ![è¿žçº¿æ ·å¼](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/link_styles.jpg) ä½ å¯ä»¥åœ¨ **è®¾ç½®èœå•** â€”> **ç”»é¢ï¼ˆLite Graphï¼‰** â€”> **ç”»é¢ï¼ˆGrapï¼‰** â€”> **è¿žçº¿æ¸²æŸ“æ¨¡å¼(Link Render Mode)** è¿›è¡Œè¿žçº¿æ ·å¼çš„ä¿®æ”¹ã€‚ ![Canvas Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/link/render_mode.jpg) ä¹Ÿå¯ä»¥åœ¨ **Canvas Menu** ä¸­ä¸´æ—¶éšè—è¿žçº¿ã€‚ ![Canvas Menu](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/canvas_menu.jpg) æ ¹æ®æƒ…å†µï¼Œå¯èƒ½éœ€è¦æŸ¥çœ‹æ‰€æœ‰é“¾æŽ¥ã€‚ç‰¹åˆ«æ˜¯åœ¨å­¦ä¹ ã€åˆ†äº«æˆ–ä»…ä»…ç†è§£å·¥ä½œæµæ—¶ï¼Œè¿žçº¿ä¹‹é—´çš„å¯è§æ€§å¯ä»¥è®©å…¶å®ƒç”¨æˆ·ç†è§£ä¸åŒèŠ‚ç‚¹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚ä½†å¯¹äºŽé‚£äº›ä¸æ‰“ç®—è¢«æ›´æ”¹çš„æ‰“åŒ…å·¥ä½œæµï¼Œä½ ä¹Ÿå¯ä»¥éšè—è¿žçº¿ä»Žè€ŒèŽ·å¾—ä¸€ä¸ªå¹²å‡€ç®€æ´çš„å¸ƒå±€ã€‚\n\n### é‡æ–°è·¯ç”±èŠ‚ç‚¹\n\né€šå¸¸ï¼Œå½“æˆ‘ä»¬èŠ‚ç‚¹è¿‡å¤šæ—¶ï¼Œå¯¹åº”çš„è¿žæŽ¥çº¿éš¾å…è¢«é®æŒ¡æˆ–è€…å‡ºçŽ°äº¤å‰ï¼Œè¿™æ—¶ç†è§£å·¥ä½œæµä½œç”¨å°±å˜å¾—ååˆ†å›°éš¾ï¼Œå¦‚æžœä½ æƒ³è¦å¯¹åº”è¿žçº¿ä¿æŒæ¸…æ™°ï¼Œåˆ™å¯ä»¥é‡‡ç”¨ä¸€ä¸ª **é‡æ–°è·¯ç”±ï¼ˆrerouteï¼‰** èŠ‚ç‚¹æ¥æ‰‹åŠ¨è°ƒæ•´è¿žçº¿ã€‚ ![ComfyUI é‡æ–°è·¯ç”±èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/reroute.jpg) åŒæ—¶æˆ‘ä»¬ä¹Ÿåœ¨åŠªåŠ›è¿­ä»£ï¼Œæˆ‘ä»¬ä¹Ÿå·²ç»å®Œå–„ litegraph åŽŸç”Ÿçš„é‡è·¯ç”±åŠŸèƒ½ï¼Œæˆ‘ä»¬æ›´å»ºè®®åœ¨æœªæ¥ä½¿ç”¨è¿™ä¸ªåŠŸèƒ½è¿›è¡Œè¿žçº¿çš„é‡æ–°ç»„ç»‡ã€‚ ![ComfyUI é‡æ–°è·¯ç”±èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/link/native_reroute.jpg)\n\n## é¢œè‰²ç¼–ç \n\nèŠ‚ç‚¹å±žæ€§çš„æ•°æ®ç±»åž‹é€šè¿‡è¾“å…¥/è¾“å‡ºç«¯å£å’Œé“¾æŽ¥è¿žæŽ¥çº¿çš„é¢œè‰²ç¼–ç æ¥è¡¨ç¤ºã€‚æˆ‘ä»¬æ€»æ˜¯å¯ä»¥é€šè¿‡é¢œè‰²åˆ¤æ–­å“ªäº›è¾“å…¥å’Œè¾“å‡ºå¯ä»¥ç›¸äº’è¿žæŽ¥ã€‚ç«¯å£åªèƒ½è¿žæŽ¥åˆ°ç›¸åŒé¢œè‰²çš„å…¶ä»–ç«¯å£æ¥ä¿è¯å¯¹åº”æ•°æ®ç±»åž‹çš„åŒ¹é…ã€‚ ç›®å‰å¸¸è§æ•°æ®ç±»åž‹ï¼š ![ComfyUI èŠ‚ç‚¹æ•°æ®ç±»åž‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/node/data_type.jpg)\n\n| æ•°æ®ç±»åž‹ | é¢œè‰²  |\n| --- | --- |\n| æ‰©æ•£æ¨¡åž‹ | è–°è¡£è‰è‰² |\n| CLIP æ¨¡åž‹ | é»„è‰²  |\n| VAE æ¨¡åž‹ | çŽ«ç‘°è‰² |\n| æ¡ä»¶åŒ– | æ©™è‰²  |\n| æ½œåœ¨å›¾åƒ | ç²‰è‰²  |\n| åƒç´ å›¾åƒ | è“è‰²  |\n| è’™ç‰ˆ  | ç»¿è‰²  |\n| æ•°å­—ï¼ˆæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼‰ | æµ…ç»¿è‰² |\n| ç½‘æ ¼ï¼ˆMeshï¼‰ | äº®ç»¿è‰² |\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/lora",
  "markdown": "# ComfyUI LoRA ä½¿ç”¨ç¤ºä¾‹ - ComfyUI\n\n**LoRA æ¨¡åž‹â€‹ï¼ˆLow-Rank Adaptationï¼‰** æ˜¯ä¸€ç§ç”¨äºŽå¾®è°ƒå¤§åž‹ç”Ÿæˆæ¨¡åž‹ï¼ˆå¦‚ Stable Diffusionï¼‰çš„é«˜æ•ˆæŠ€æœ¯ã€‚ å®ƒé€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡åž‹çš„åŸºç¡€ä¸Šå¼•å…¥å¯è®­ç»ƒçš„ä½Žç§©çŸ©é˜µï¼Œä»…è°ƒæ•´éƒ¨åˆ†å‚æ•°ï¼Œè€Œéžé‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡åž‹ï¼Œä»Žè€Œä»¥è¾ƒä½Žçš„è®¡ç®—æˆæœ¬å®žçŽ°ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–ï¼Œç›¸å¯¹äºŽç±»ä¼¼ SD1.5 è¿™æ ·çš„å¤§æ¨¡åž‹ï¼ŒLoRA æ¨¡åž‹æ›´å°ï¼Œæ›´å®¹æ˜“è®­ç»ƒã€‚ ![LoRA æ¨¡åž‹ä¸ŽåŸºç¡€æ¨¡åž‹å¯¹æ¯”](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/compare.png) ä¸Šé¢çš„å›¾ç‰‡å¯¹æ¯”äº†åŒæ ·å‚æ•°ä¸‹ [dreamshaper\\_8](https://civitai.com/models/4384?modelVersionId=128713) ç›´æŽ¥ç”Ÿæˆå’Œä½¿ç”¨ [blindbox\\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA æ¨¡åž‹ç”Ÿæˆçš„å›¾ç‰‡å¯¹æ¯”ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é€šè¿‡ä½¿ç”¨ LoRA æ¨¡åž‹ï¼Œå¯ä»¥åœ¨ä¸è°ƒæ•´åŸºç¡€æ¨¡åž‹çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆæ›´ç¬¦åˆæˆ‘ä»¬éœ€æ±‚çš„å›¾ç‰‡ã€‚ æˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LoRA çš„ç¤ºä¾‹ã€‚æ‰€æœ‰ LoRA å˜ä½“ï¼šLycoris, loha, lokr, locon, ç­‰â€¦ éƒ½æ˜¯ä»¥è¿™ç§æ–¹å¼ä½¿ç”¨ã€‚ åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å®Œæˆä»¥ä¸‹å†…å®¹æ¥å­¦ä¹ [ComfyUI](https://github.com/comfyanonymous/ComfyUI) ä¸­åŠ è½½å¹¶ä½¿ç”¨ LoRA æ¨¡åž‹ï¼Œå°†æ¶‰åŠä»¥ä¸‹å†…å®¹ï¼š\n\n1.  å®‰è£… LoRA æ¨¡åž‹\n2.  ä½¿ç”¨ LoRA æ¨¡åž‹ç”Ÿæˆå›¾ç‰‡\n3.  `Load LoRA` èŠ‚ç‚¹çš„ç®€å•ä»‹ç»\n\n## ç›¸å…³æ¨¡åž‹å®‰è£…\n\nè¯·ä¸‹è½½ [dreamshaper\\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) å¹¶ä¿å­˜è‡³ `ComfyUI/models/checkpoints` ç›®å½• è¯·ä¸‹è½½ [blindbox\\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) å¹¶ä¿å­˜è‡³ `ComfyUI/models/loras` ç›®å½•\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - LoRA](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/lora.png) \n\n## æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\nè¯·å‚ç…§ä¸‹å›¾æ­¥éª¤ï¼Œæ¥ç¡®ä¿å¯¹åº”çš„å·¥ä½œæµå¯ä»¥æ­£å¸¸è¿è¡Œ ![ComfyUI å·¥ä½œæµ - LoRA æµç¨‹å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/flow_diagram.png)\n\n1.  ç¡®ä¿`Load Checkpoint` åŠ è½½äº† `dreamshaper_8.safetensors`\n2.  ç¡®ä¿`Load LoRA` åŠ è½½äº† `blindbox_V1Mix.safetensors`\n3.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n\n## Load LoRA èŠ‚ç‚¹ä»‹ç»\n\n![Load LoRA èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_lora.jpg) ä½äºŽ`ComfyUI\\models\\loras` çš„æ¨¡åž‹ä¼šè¢« ComfyUI æ£€æµ‹åˆ°ï¼Œå¹¶åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸­åŠ è½½\n\n### è¾“å…¥ç±»åž‹\n\n| å‚æ•°åç§° | ä½œç”¨  |\n| --- | --- |\n| `model` | è¿žæŽ¥åŸºç¡€æ¨¡åž‹ |\n| `clip` | è¿žæŽ¥ CLIP æ¨¡åž‹ |\n| `lora_name` | é€‰æ‹©è¦åŠ è½½ä½¿ç”¨çš„ LoRA æ¨¡åž‹ |\n| `strength_model` | å½±å“ LoRA å¯¹ æ¨¡åž‹æƒé‡ï¼ˆmodelï¼‰çš„å½±å“ç¨‹åº¦ï¼Œæ•°å€¼è¶Šå¤§ LoRA é£Žæ ¼è¶Šå¼º |\n| `strength_clip` | å½±å“ LoRA å¯¹ CLIP è¯åµŒå…¥ï¼ˆclipï¼‰çš„å½±å“ç¨‹åº¦ |\n\n### è¾“å‡ºç±»åž‹\n\n| å‚æ•°åç§° | ä½œç”¨  |\n| --- | --- |\n| `model` | è¾“å‡ºåº”ç”¨äº† LoRA è°ƒæ•´çš„æ¨¡åž‹ |\n| `clip` | è¾“å‡ºåº”ç”¨äº† LoRA è°ƒæ•´çš„ CLIP æ¨¡åž‹ |\n\nè¯¥èŠ‚ç‚¹æ”¯æŒé“¾å¼è¿žæŽ¥ï¼Œå¯ä»¥å°†å¤šä¸ª`Load LoRA` èŠ‚ç‚¹ä¸²è”æ¥åº”ç”¨å¤šä¸ª LoRA æ¨¡åž‹ï¼Œå…·ä½“è¯·å‚è€ƒ[ComfyUI åº”ç”¨å¤šä¸ª LoRA ç¤ºä¾‹](https://docs.comfy.org/zh-CN/tutorials/basic/multiple-loras) ![LoRA èŠ‚ç‚¹é“¾å¼è¿žæŽ¥](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)\n\n## å¼€å§‹ä½ çš„å°è¯•\n\n1.  è¯•ç€ä¿®æ”¹æç¤ºè¯ï¼Œæˆ–è€…è°ƒæ•´ `Load LoRA` èŠ‚ç‚¹çš„ä¸åŒå‚æ•°ï¼Œæ¯”å¦‚ `strength_model` ï¼Œæ¥è§‚å¯Ÿç”Ÿæˆå›¾ç‰‡çš„å˜åŒ–ï¼Œç†Ÿæ‚‰å¯¹åº”èŠ‚ç‚¹ã€‚\n2.  è®¿é—® [CivitAI](https://civitai.com/models) ç½‘ç«™ï¼Œä¸‹è½½å…¶å®ƒé£Žæ ¼çš„ LoRA æ¨¡åž‹ï¼Œå°è¯•ä½¿ç”¨ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/inpaint",
  "markdown": "# ComfyUI å±€éƒ¨é‡ç»˜å·¥ä½œæµ - ComfyUI\n\næœ¬ç¯‡å°†å¼•å¯¼äº†è§£ AI ç»˜å›¾ä¸­ï¼Œå±€éƒ¨é‡ç»˜çš„æ¦‚å¿µï¼Œå¹¶åœ¨ ComfyUI ä¸­å®Œæˆå±€éƒ¨é‡ç»˜å·¥ä½œæµç”Ÿæˆï¼Œæˆ‘ä»¬å°†æŽ¥è§¦ä»¥ä¸‹å†…å®¹ï¼š\n\n*   ä½¿ç”¨å±€éƒ¨é‡ç»˜å·¥ä½œæµå®Œæˆç”»é¢çš„ä¿®æ”¹\n*   äº†è§£å¹¶ä½¿ç”¨ ComfyUI ä¸­é®ç½©ç¼–è¾‘å™¨\n*   äº†è§£ç›¸å…³èŠ‚ç‚¹ VAE Encoder (for Inpainting)\n\n## å…³äºŽå±€éƒ¨é‡ç»˜\n\nåœ¨ AI å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¸¸ä¼šé‡åˆ°ç”Ÿæˆçš„ç”»é¢æ•´ä½“è¾ƒä¸ºæ»¡æ„ï¼Œä½†æ˜¯ç”»é¢ä¸­å­˜åœ¨ä¸€äº›ä¸å¸Œæœ›å‡ºçŽ°æˆ–è€…é”™è¯¯çš„å…ƒç´ ï¼Œä½†æ˜¯é‡æ–°ç”Ÿæˆå¯èƒ½ä¼šç”Ÿæˆå¦å¤–ä¸€å¼ å®Œå…¨ä¸åŒçš„å›¾ç‰‡ï¼Œæ‰€ä»¥è¿™æ—¶å€™åˆ©ç”¨å±€éƒ¨é‡ç»˜æ¥ä¿®å¤è¿™éƒ¨åˆ†çš„å…ƒç´ å°±éžå¸¸æœ‰å¿…è¦äº†ã€‚ è¿™å°±åƒè®© **ç”»å®¶(AI ç»˜å›¾æ¨¡åž‹)** ç”»äº†ä¸€å¹…ç”»ï¼Œä½†æ˜¯æ€»æ˜¯ä¼šæœ‰ç¨å¾®æœ‰ **å±€éƒ¨åŒºåŸŸéœ€è¦è°ƒæ•´**ï¼Œæˆ‘ä»¬éœ€è¦å‘ç”»å®¶è¯´æ˜Ž**éœ€è¦è°ƒæ•´çš„åŒºåŸŸ(é®ç½©)**ï¼Œç„¶åŽè®©ç”»å®¶ä¼šæ ¹æ®æˆ‘ä»¬çš„è¦æ±‚è¿›è¡Œ **é‡æ–°ç»˜åˆ¶(é‡ç»˜)**ã€‚ å±€éƒ¨é‡ç»˜çš„åœºæ™¯åŒ…æ‹¬ï¼š\n\n*   **ç‘•ç–µä¿®å¤ï¼š** æ¶ˆé™¤ç…§ç‰‡ä¸­å¤šä½™ç‰©ä½“ã€é”™è¯¯çš„AIç”Ÿæˆçš„ç”»é¢çš„è‚¢ä½“ç­‰\n*   **ç»†èŠ‚ä¼˜åŒ–ï¼š** ç²¾å‡†è°ƒæ•´å±€éƒ¨å…ƒç´ ï¼ˆå¦‚ä¿®æ”¹æœè£…çº¹ç†ã€è°ƒæ•´é¢éƒ¨è¡¨æƒ…ï¼‰\n*   ç­‰å…¶å®ƒåœºæ™¯\n\n### æ¨¡åž‹åŠç›¸å…³ç´ æå‡†å¤‡\n\n#### 1\\. æ¨¡åž‹å®‰è£…\n\nä¸‹è½½ä¸‹é¢çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¹¶ä¿å­˜åˆ°`ComfyUI/models/checkpoints`ç›®å½•ä¸‹\n\n*   [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)\n\n#### 2\\. å±€éƒ¨é‡ç»˜ç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ä½¿ç”¨è¿™ä¸ªå›¾ç‰‡ä½œä¸ºè¾“å…¥ä½¿ç”¨ ![ComfyUIå±€éƒ¨é‡ç»˜è¾“å…¥å›¾ç‰‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/input.png)\n\n#### 3\\. å±€éƒ¨é‡ç»˜å·¥ä½œæµ\n\nä¸‹é¢è¿™å¼ å›¾çš„ metadata åŒ…å«çš„å¯¹åº”çš„jsonå·¥ä½œæµï¼Œè¯·å°†å…¶ä¸‹è½½åŽ **æ‹–å…¥** ComfyUI ç•Œé¢æˆ–è€…ä½¿ç”¨èœå• **å·¥ä½œæµ(Workflow)** â€”> **æ‰“å¼€å·¥ä½œæµ(Open,å¿«æ·é”® `Ctrl + O`)** æ¥åŠ è½½è¿™ä¸ªå±€éƒ¨é‡ç»˜å·¥ä½œæµ ![ComfyUIå±€éƒ¨é‡ç»˜å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)\n\n### ComfyUI å±€éƒ¨é‡ç»˜å·¥ä½œæµç¤ºä¾‹è®²è§£\n\n![ComfyUI å±€éƒ¨é‡ç»˜å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_workflow.png) è¯·å‚ç…§å›¾ç‰‡åºå·å¯¹ç…§ä¸‹é¢çš„æç¤ºå®Œä¸‹æ“ä½œï¼š\n\n1.  è¯·ç¡®ä¿å·²ç»åŠ è½½äº†ä½ æ‰€ä¸‹è½½ä½¿ç”¨çš„æ¨¡åž‹\n2.  è¯·åœ¨åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½å±€éƒ¨é‡ç»˜çš„ç´ æ\n3.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\n![ComfyUIå±€éƒ¨é‡ç»˜å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png) æ­¤å¤–æˆ‘ä»¬åœ¨è¿™é‡Œå¯ä»¥å¯¹æ¯”ä¸€ä¸‹ï¼Œä¸‹å›¾æ˜¯ä½¿ç”¨[v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) æ¨¡åž‹æ¥è¿›è¡Œ inpainting çš„ç»“æžœã€‚ ![ComfyUI å±€éƒ¨é‡ç»˜å·¥ä½œæµ - SD1.5](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png) ä½ ä¼šå‘çŽ° [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) æ¨¡åž‹ç”Ÿæˆçš„ç»“æžœå±€éƒ¨é‡ç»˜çš„æ•ˆæžœæ›´å¥½è¿‡æ¸¡æ›´è‡ªç„¶ã€‚ è¿™å› ä¸ºè¿™ä¸ªæ¨¡åž‹æ˜¯ä¸“ä¸º inpainting è®¾è®¡çš„æ¨¡åž‹ï¼Œå®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°æŽ§åˆ¶ç”ŸæˆåŒºåŸŸï¼Œä»Žè€ŒèŽ·å¾—æ›´å¥½çš„å±€éƒ¨é‡ç»˜æ•ˆæžœã€‚ è®°å¾—æˆ‘ä»¬ä¸€ç›´ç”¨çš„æ¯”å–»å—ï¼Ÿä¸åŒçš„æ¨¡åž‹å°±åƒèƒ½åŠ›ä¸åŒçš„ç”»å®¶ä¸€æ ·ï¼Œä½†æ¯ä¸ªç”»å®¶éƒ½æœ‰è‡ªå·±èƒ½åŠ›çš„ä¸Šé™ï¼Œé€‰æ‹©åˆé€‚çš„æ¨¡åž‹å¯ä»¥è®©ä½ çš„ç”Ÿæˆæ•ˆæžœæ›´å¥½ã€‚ ä½ å¯ä»¥è¿›è¡Œä¸‹é¢çš„å°è¯•æ¥è®©ç”»é¢è¾¾åˆ°ä½ æƒ³è¦çš„æ•ˆæžœ:\n\n1.  ä¿®æ”¹æ­£å‘ ã€è´Ÿå‘æç¤ºè¯ï¼Œä½¿ç”¨æ›´å…·ä½“çš„æè¿°\n2.  å°è¯•å¤šæ¬¡è¿è¡Œï¼Œè®© `KSampler` ä½¿ç”¨ä¸åŒçš„ç§å­ï¼Œä»Žè€Œå¸¦æ¥ä¸åŒçš„ç”Ÿæˆæ•ˆæžœ\n3.  åœ¨äº†è§£æœ¬ç¯‡é®ç½©ç¼–è¾‘å™¨ä½¿ç”¨çš„éƒ¨åˆ†åŽï¼Œå¯¹äºŽç”Ÿæˆçš„ç»“æžœå†æ¬¡è¿›è¡Œé‡ç»˜ä»¥èŽ·å¾—æ»¡æ„çš„ç»“æžœã€‚\n\næŽ¥ä¸‹æ¥æˆ‘ä»¬å°†ç®€å•äº†è§£å¦‚ä½•ä½¿ç”¨ **é®ç½©ç¼–è¾‘å™¨(Mask Editor)** ï¼Œå› ä¸ºä¹‹å‰æä¾›çš„è¾“å…¥å›¾ç‰‡ä¸­æ˜¯å·²ç»åŒ…å«äº†`alpha`é€æ˜Žé€šé“ï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸Œæœ›åœ¨ç»˜å›¾è¿‡ç¨‹ä¸­è¿›è¡Œç¼–è¾‘çš„åŒºåŸŸï¼‰ï¼Œæ‰€ä»¥å¹¶ä¸éœ€è¦ä½ æ‰‹åŠ¨ç»˜åˆ¶ï¼Œä½†åœ¨æ—¥å¸¸ä½¿ç”¨ä¸­æˆ‘ä»¬ä¼šæ›´ç»å¸¸ä½¿ç”¨ **é®ç½©ç¼–è¾‘å™¨(Mask Editor)** æ¥ç»˜åˆ¶ è’™ç‰ˆ(Mask)\n\n### ä½¿ç”¨é®ç½©ç¼–è¾‘å™¨(Mask Editor) ç»˜åˆ¶è’™ç‰ˆ\n\né¦–å…ˆåœ¨ä¸Šä¸€æ­¥å·¥ä½œæµä¸­çš„`Save Image` èŠ‚ç‚¹ä¸Šå³é”®ï¼Œä½ å¯ä»¥åœ¨å³é”®èœå•ä¸­çœ‹åˆ°`å¤åˆ¶(Clipspace)` é€‰é¡¹ï¼Œç‚¹å‡»åŽä¼šå¤åˆ¶å½“å‰å›¾ç‰‡åˆ°å‰ªè´´æ¿ ![ComfyUI å±€éƒ¨é‡ç»˜ - å¤åˆ¶å›¾ç‰‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png) ç„¶åŽåœ¨ **åŠ è½½å›¾åƒ(Load Image)** èŠ‚ç‚¹ä¸Šå³é”®ï¼Œä½ å¯ä»¥åœ¨å³é”®èœå•ä¸­çœ‹åˆ°`Paste(Clipspace)` é€‰é¡¹ï¼Œç‚¹å‡»åŽä¼šä»Žå‰ªè´´æ¿ä¸­ç²˜è´´å›¾ç‰‡ ![ComfyUI å±€éƒ¨é‡ç»˜ - ç²˜è´´å›¾ç‰‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png) ç„¶åŽåœ¨ **åŠ è½½å›¾åƒ(Load Image)** èŠ‚ç‚¹ä¸Šå³é”®ï¼Œä½ å¯ä»¥åœ¨å³é”®èœå•ä¸­çœ‹åˆ°`åœ¨é®ç½©ç¼–è¾‘å™¨ä¸­æ‰“å¼€(Open in MaskEditor)` é€‰é¡¹ï¼Œç‚¹å‡»åŽä¼šæ‰“å¼€é®ç½©ç¼–è¾‘å™¨ ![æ‰“å¼€é®ç½©ç¼–è¾‘å™¨](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg) ![é®ç½©ç¼–è¾‘å™¨](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint-maskeditor.gif)\n\n1.  ä½ å¯ä»¥å³ä¾§ç¼–è¾‘ç›¸å…³å‚æ•°ï¼Œæ¯”å¦‚è°ƒæ•´ç”»ç¬”å¤§å°ã€é€æ˜Žåº¦ç­‰ç­‰\n2.  ç»˜åˆ¶é”™è¯¯åŒºåŸŸå¯ä»¥ä½¿ç”¨æ©¡çš®æª«æ¥æ“¦é™¤\n3.  ç»˜åˆ¶å®ŒæˆåŽç‚¹å‡» `Save` æŒ‰é’®ä¿å­˜è’™ç‰ˆ\n\nè¿™æ ·ç»˜åˆ¶å®Œæˆçš„å†…å®¹å°±ä¼šä½œä¸º é®ç½©(Mask) è¾“å…¥åˆ° VAE Encoder (for Inpainting) èŠ‚ç‚¹ä¸­ä¸€èµ·è¿›è¡Œç¼–ç  ç„¶åŽè¯•ç€è°ƒæ•´æç¤ºè¯ï¼Œå†æ¬¡è¿›è¡Œç”Ÿæˆï¼Œç›´åˆ°ä½ å¯ä»¥å®Œæˆæ»¡æ„çš„ç”Ÿæˆç»“æžœã€‚\n\n## å±€éƒ¨é‡ç»˜åˆ¶ç›¸å…³èŠ‚ç‚¹\n\né€šè¿‡[æ–‡ç”Ÿå›¾](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)ã€[å›¾ç”Ÿå›¾](https://docs.comfy.org/zh-CN/tutorials/basic/image-to-image) å’Œæœ¬ç¯‡çš„å·¥ä½œæµå¯¹æ¯”ï¼Œæˆ‘æƒ³ä½ åº”è¯¥å¯ä»¥çœ‹åˆ°è¿™å‡ ä¸ªå·¥ä½œæµä¸»è¦çš„å·®å¼‚éƒ½åœ¨äºŽ VAE éƒ¨åˆ†è¿™éƒ¨åˆ†çš„æ¡ä»¶è¾“å…¥, åœ¨è¿™ä¸ªå·¥ä½œæµä¸­æˆ‘ä»¬ä½¿ç”¨åˆ°çš„æ˜¯ **VAE å†…éƒ¨ç¼–ç å™¨** èŠ‚ç‚¹ï¼Œè¿™ä¸ªèŠ‚ç‚¹æ˜¯ä¸“é—¨ç”¨äºŽå±€éƒ¨é‡ç»˜çš„èŠ‚ç‚¹ï¼Œå®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°æŽ§åˆ¶ç”ŸæˆåŒºåŸŸï¼Œä»Žè€ŒèŽ·å¾—æ›´å¥½çš„ç”Ÿæˆæ•ˆæžœã€‚ ![VAE Encoder (for Inpainting) èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg) **è¾“å…¥ç±»åž‹**\n\n| å‚æ•°åç§° | ä½œç”¨  |\n| --- | --- |\n| `pixels` | éœ€è¦ç¼–ç åˆ°æ½œç©ºé—´çš„è¾“å…¥å›¾åƒã€‚ |\n| `vae` | ç”¨äºŽå°†å›¾ç‰‡ä»Žåƒç´ ç©ºé—´ç¼–ç åˆ°æ½œåœ¨ç©ºé—´çš„ VAE æ¨¡åž‹ã€‚ |\n| `mask` | å›¾ç‰‡é®ç½©ï¼Œç”¨æ¥å…·ä½“æŒ‡æ˜Žå“ªä¸ªåŒºåŸŸéœ€è¦è¿›è¡Œä¿®æ”¹ã€‚ |\n| `grow_mask_by` | åœ¨åŽŸæœ‰çš„é®ç½©åŸºç¡€ä¸Šï¼Œå‘å¤–æ‰©å±•çš„åƒç´ å€¼ï¼Œä¿è¯åœ¨é®ç½©åŒºåŸŸå¤–å›´æœ‰ä¸€å®šçš„è¿‡åº¦åŒºåŸŸï¼Œé¿å…é‡ç»˜åŒºåŸŸä¸ŽåŽŸå›¾å­˜åœ¨ç”Ÿç¡¬çš„è¿‡æ¸¡ã€‚ |\n\n**è¾“å‡ºç±»åž‹**\n\n| å‚æ•°åç§° | ä½œç”¨  |\n| --- | --- |\n| `latent` | ç»è¿‡ VAE ç¼–ç åŽçš„æ½œç©ºé—´å›¾åƒã€‚ |"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/overview",
  "markdown": "# ComfyUI ç•Œé¢æ¦‚è§ˆ - ComfyUI\n\nå¯è§†åŒ–ç•Œé¢æ˜¯ç›®å‰ç»å¤§æ•°ç”¨æˆ·ä½¿ç”¨ ComfyUI æ¥è°ƒç”¨ [ComfyUI Server](https://docs.comfy.org/zh-CN/development/comfyui-server/comms_overview) è¿›è¡Œç›¸åº”åª’ä½“èµ„æºç”Ÿæˆçš„æ–¹å¼ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªå¯ä¾›ç”¨æˆ·æ“ä½œå’Œç»„ç»‡å·¥ä½œæµçš„å¯è§†åŒ–ç•Œé¢ï¼Œç”¨äºŽç»„ç»‡å’Œè°ƒè¯•å·¥ä½œæµï¼Œå¹¶ç”Ÿæˆä»¤äººæƒŠå¹çš„ä½œå“ã€‚ åœ¨æœ¬ç¯‡æˆ‘ä»¬å°†ç²—ç•¥ä»‹ç» ComfyUI çš„ç•Œé¢ä»¥åŠå„ä¸ªéƒ¨åˆ†çš„åŠŸèƒ½ï¼Œåœ¨åŽç»­çš„ç« èŠ‚ä¸­æˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»å„ä¸ªéƒ¨åˆ†çš„åŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•ã€‚ é€šå¸¸ï¼Œå½“ä½ å½“ä½ å¯åŠ¨ ComfyUI åŽä½ å¯ä»¥çœ‹åˆ°ä¸‹é¢è¿™æ ·çš„ä¸€ä¸ªç•Œé¢ï¼š ![ComfyUI åŸºç¡€ç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui_new_interface.jpg) å¦‚æžœä½ æ˜¯è¾ƒä¸ºæ—©æœŸçš„ç”¨æˆ·ï¼Œä½ åº”è¯¥è¿˜è§è¿‡ä¹‹å‰çš„è¿™æ ·çš„èœå•ç•Œé¢ï¼š ![ComfyUI æ—§ç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/comfyui_old_interface.jpg) è¿™ä¸¤ä¸ªèœå•ç•Œé¢å¯ä»¥é€šè¿‡è®¾ç½®è¿›è¡Œåˆ‡æ¢ï¼Œä½†éšç€ ComfyUI çš„åŠŸèƒ½æ—¥ç›Šå¼ºå¤§å’Œå¤æ‚ï¼Œæˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨æ–°ç‰ˆçš„èœå•ç•Œé¢æ¥èŽ·å¾—æ›´å¥½çš„ä½¿ç”¨ä½“éªŒã€‚ ç›®å‰ ComfyUIå‰ç«¯æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é¡¹ç›®ï¼Œä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ ComfyUI ä¾èµ–è¿›è¡Œè¿›è¡Œå‘å¸ƒå’Œæ›´æ–°ç»´æŠ¤ï¼Œå¦‚æžœä½ æƒ³è¦å‚ä¸Žè´¡çŒ®ï¼Œå¯ä»¥ fork è¿™ä¸ª[ä»“åº“](https://github.com/Comfy-Org/ComfyUI_frontend)ï¼Œå¹¶è¿›è¡Œ pull requestã€‚\n\n## æœ¬åœ°åŒ–æ”¯æŒ\n\nç›®å‰ ComfyUI æ”¯æŒï¼šåŒ…æ‹¬è‹±æ–‡ã€ä¸­æ–‡ã€ä¿„ç½—æ–¯è¯­ã€æ³•è¯­ã€æ—¥æ–‡ã€éŸ©æ–‡ã€‚ å¦‚æžœä½ éœ€è¦åˆ‡æ¢ç•Œé¢è¯­è¨€åˆ°ä½ éœ€è¦çš„è¯­è¨€å¯ä»¥ç‚¹å‡» **è®¾ç½®é½¿è½®å›¾æ ‡** ç„¶åŽåœ¨ `Comfy` â€”> `Locale` ä¸­é€‰æ‹©ä½ éœ€è¦çš„è¯­è¨€ã€‚ ![ComfyUI æœ¬åœ°åŒ–æ”¯æŒ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/locale.jpg)\n\n## æ–°ç‰ˆèœå•ç•Œé¢\n\n### ç•Œé¢åˆ†åŒºï¼ˆWorkspaceï¼‰\n\nä¸‹é¢æ˜¯ä¸»è¦çš„ ComfyUI çš„ç•Œé¢åˆ†åŒºä»¥åŠå„éƒ¨åˆ†çš„ç®€è¦ä»‹ç»ã€‚ ![ComfyUI å·¥ä½œåŒº](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/comfyui-new-interface-main.png) ç›®å‰ ComfyUI çš„ç•Œé¢é™¤å¼€ä¸»è¦çš„å·¥ä½œæµç•Œé¢ï¼Œä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š\n\n1.  èœå•æ ï¼šæä¾›å·¥ä½œæµã€ç¼–è¾‘ã€å¸®åŠ©èœå•ï¼Œå·¥ä½œæµæ‰§è¡Œã€ComfyUI Managerå…¥å£ç­‰ç­‰\n2.  ä¾§è¾¹æ é¢æ¿åˆ‡æ¢æŒ‰é’®ï¼šç”¨äºŽåˆ‡æ¢å·¥ä½œæµåŽ†å²é˜Ÿåˆ—ã€èŠ‚ç‚¹åº“ã€æ¨¡åž‹åº“ã€æœ¬åœ°ç”¨æˆ·å·¥ä½œæµæµè§ˆç­‰\n3.  åˆ‡æ¢ä¸»é¢˜æŒ‰é’®ï¼š åœ¨ ComfyUI é»˜è®¤çš„æš—è‰²ä¸»é¢˜å’Œäº®è‰²ä¸»é¢˜ä¹‹é—´è¿›è¡Œå¿«é€Ÿåˆ‡æ¢\n4.  è®¾ç½®ï¼šç‚¹å‡»åŽå¯æ‰“å¼€è®¾ç½®æŒ‰é’®\n5.  ç”»å¸ƒèœå•ï¼š æä¾›äº†ComfyUI ç”»å¸ƒçš„è§†å›¾æ”¾å¤§ã€ç¼©å°ã€è‡ªé€‚åº”æ“ä½œç­‰\n\n### èœå•æ åŠŸèƒ½\n\n![ComfyUI å·¥ä½œåŒº](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/comfyui-new-interface-menu-bar.png) ä¸Šå›¾æ˜¯é¡¶éƒ¨èœå•æ çš„å¯¹åº”åŠŸèƒ½ï¼ŒåŒ…å«çš„å¸¸è§çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬ä¼šåœ¨å…·ä½“çš„åŠŸèƒ½ä½¿ç”¨éƒ¨åˆ†å†è¯¦ç»†ä»‹ç»å¯¹åº”çš„åŠŸèƒ½\n\n### ä¾§è¾¹æ é¢æ¿æŒ‰é’®\n\n![ComfyUI ä¾§è¾¹æ é¢æ¿](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/overview/side-panel.png) åœ¨ç›®å‰çš„ ComfyUI ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å››ä¸ªä¾§è¾¹é¢æ¿åŒ…å«äº†ä»¥ä¸‹åŠŸèƒ½ï¼š\n\n1.  å·¥ä½œæµåŽ†å²é˜Ÿåˆ—(Queue)ï¼š æ‰€æœ‰ ComfyUI æ‰§è¡Œåª’ä½“å†…å®¹ç”Ÿæˆçš„é˜Ÿåˆ—ä¿¡æ¯\n2.  èŠ‚ç‚¹åº“(Node Library)ï¼š æ‰€æœ‰ ComfyUI ä¸­çš„èŠ‚ç‚¹åŒ…æ‹¬`Comfy Core` å’Œä½ å®‰è£…çš„è‡ªå®šä¹‰èŠ‚ç‚¹éƒ½ä¼šå¯ä»¥åœ¨è¿™é‡Œè¿›è¡ŒæŸ¥æ‰¾\n3.  æ¨¡åž‹åº“(Model Library)ï¼š ä½ æœ¬åœ°çš„`ComfyUI/models` ç›®å½•ä¸‹çš„æ¨¡åž‹å¯ä»¥åœ¨è¿™é‡Œè¢«æŸ¥æ‰¾åˆ°\n4.  æœ¬åœ°ç”¨æˆ·å·¥ä½œæµ(Workflows)ï¼š ä½ æœ¬åœ°ä¿å­˜çš„å·¥ä½œæµå¯ä»¥åœ¨è¿™é‡Œè¢«æŸ¥æ‰¾åˆ°\n\n## æ—§ç‰ˆèœå•\n\nç›®å‰ ComfyUI é»˜è®¤å¯ç”¨æ–°ç‰ˆç•Œé¢ï¼Œå¦‚æžœä½ æ›´åå¥½ä½¿ç”¨æ—§ç‰ˆç•Œé¢ï¼Œå¯ä»¥ç‚¹å‡» **è®¾ç½®é½¿è½®å›¾æ ‡** ç„¶åŽåœ¨ `Comfy` â€”> `èœå•(Menu)` å°† `ä½¿ç”¨æ–°èœå•(Use new menu)` è®¾ç½®ä¸º `disabled` å³å¯åˆ‡æ¢åˆ°æ—§ç‰ˆæœ¬çš„èœå•ã€‚\n\næ—§ç‰ˆæœ¬èœå•ç•Œé¢åŠŸèƒ½æ ‡æ³¨è¯´æ˜Žå¦‚ä¸‹ï¼š ![ComfyUI æ—§ç‰ˆèœå•](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/comfyui-old-menu.png)"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/maskeditor",
  "markdown": "# é®ç½©ç¼–è¾‘å™¨ - åœ¨ ComfyUI ä¸­åˆ›å»ºå’Œç¼–è¾‘é®ç½©\n\né®ç½©ç¼–è¾‘å™¨æ˜¯ ComfyUI ä¸­ä¸€ä¸ªéžå¸¸å®žç”¨çš„åŠŸèƒ½ï¼Œå®ƒå¯ä»¥å¸®åŠ©ç”¨æˆ·åœ¨å›¾åƒä¸­åˆ›å»ºå’Œç¼–è¾‘é®ç½©ï¼Œè€Œä¸éœ€è¦åœ¨å…¶å®ƒåº”ç”¨ç¨‹åºä¸­è¿›è¡Œæ“ä½œã€‚ é®ç½©ç¼–è¾‘å™¨ç›®å‰é€šè¿‡ `Load image` èŠ‚ç‚¹æ¥è§¦å‘ï¼Œå½“ä½ ä¸Šä¼ å›¾åƒåŽï¼Œå¯ä»¥åœ¨èŠ‚ç‚¹ä¸Šå³é”®é€šè¿‡èœå• `Open in MaskEditor` æ¥æ‰“å¼€é®ç½©ç¼–è¾‘å™¨ã€‚ ![ComfyUI é®ç½©ç¼–è¾‘å™¨](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/maskeditor/maskeditor.jpg) ![ComfyUI é®ç½©ç¼–è¾‘å™¨](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/maskeditor/maskeditor_ui.jpg) ç„¶åŽä½ å°±å¯ä»¥é€šè¿‡é¼ æ ‡åœ¨å›¾åƒä¸Šç‚¹å‡»æ¥åˆ›å»ºå’Œç¼–è¾‘é®ç½©äº†ã€‚\n\n## æ¼”ç¤ºè§†é¢‘\n\nä½ çš„æµè§ˆå™¨ä¸æ”¯æŒ video æ ‡ç­¾ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/troubleshooting/model-issues",
  "markdown": "# å¦‚ä½•æŽ’æŸ¥å’Œè§£å†³ ComfyUI ä¸­æ¨¡åž‹ç›¸å…³çš„é—®é¢˜ - ComfyUI\n\n## æ¨¡åž‹æž¶æž„ä¸åŒ¹é…\n\n**ç—‡çŠ¶ï¼š** ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºçŽ°å¼ é‡ç»´åº¦é”™è¯¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ VAE è§£ç é˜¶æ®µ **å¸¸è§é”™è¯¯æ¶ˆæ¯ï¼š**\n\n*   `Given groups=1, weight of size [64, 4, 3, 3], expected input[1, 16, 128, 128] to have 4 channels, but got 16 channels instead`\n*   `Given groups=1, weight of size [4, 4, 1, 1], expected input[1, 16, 144, 112] to have 4 channels, but got 16 channels instead`\n*   `Given groups=1, weight of size [320, 4, 3, 3], expected input[2, 16, 192, 128] to have 4 channels, but got 16 channels instead`\n*   `The size of tensor a (49) must match the size of tensor b (16) at non-singleton dimension 1`\n*   `Tensors must have same number of dimensions: got 2 and 3`\n*   `mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)`\n\n**æ ¹æœ¬åŽŸå› ï¼š** å°†æ¥è‡ªä¸åŒæž¶æž„ç³»åˆ—çš„æ¨¡åž‹æ··åˆä½¿ç”¨\n\n### è§£å†³æ–¹æ¡ˆ\n\n1.  **éªŒè¯æ¨¡åž‹ç³»åˆ—å…¼å®¹æ€§ï¼š**\n    *   **Flux æ¨¡åž‹**ä½¿ç”¨ 16 é€šé“æ½œåœ¨ç©ºé—´ï¼Œé…åˆåŒæ–‡æœ¬ç¼–ç å™¨è°ƒèŠ‚ï¼ˆCLIP-L + T5-XXLï¼‰\n    *   **SD1.5 æ¨¡åž‹**ä½¿ç”¨ 4 é€šé“æ½œåœ¨ç©ºé—´ï¼Œé…åˆå•ä¸ª CLIP ViT-L/14 æ–‡æœ¬ç¼–ç å™¨\n    *   **SDXL æ¨¡åž‹**ä½¿ç”¨ 4 é€šé“æ½œåœ¨ç©ºé—´ï¼Œé…åˆåŒæ–‡æœ¬ç¼–ç å™¨ï¼ˆCLIP ViT-L/14 + OpenCLIP ViT-bigG/14ï¼‰\n    *   **SD3 æ¨¡åž‹**ä½¿ç”¨ 16 é€šé“æ½œåœ¨ç©ºé—´ï¼Œé…åˆä¸‰é‡æ–‡æœ¬ç¼–ç å™¨è°ƒèŠ‚ï¼ˆCLIP-L + OpenCLIP bigG + T5-XXLï¼‰\n    *   **ControlNet æ¨¡åž‹**å¿…é¡»ä¸ŽåŸºç¡€æ£€æŸ¥ç‚¹çš„æž¶æž„åŒ¹é…ï¼ˆSD1.5 ControlNet ä»…é€‚ç”¨äºŽ SD1.5 æ£€æŸ¥ç‚¹ï¼ŒSDXL ControlNet ä»…é€‚ç”¨äºŽ SDXL æ£€æŸ¥ç‚¹ï¼Œç­‰ç­‰ï¼‰\n2.  **å¸¸è§ä¸åŒ¹é…åœºæ™¯å’Œä¿®å¤ï¼š** **Flux + é”™è¯¯çš„ VAEï¼š**\n    \n    ```\n    é—®é¢˜ï¼šå°† taesd æˆ– sdxl_vae.safetensors ä¸Ž Flux æ£€æŸ¥ç‚¹ä¸€èµ·ä½¿ç”¨\n    ä¿®å¤ï¼šä½¿ç”¨æ¥è‡ª Hugging Face Flux å‘å¸ƒçš„ ae.safetensorsï¼ˆFlux VAEï¼‰\n    ```\n    \n    **Flux + ä¸æ­£ç¡®çš„ CLIP é…ç½®ï¼š**\n    \n    ```\n    é—®é¢˜ï¼šåœ¨ DualClipLoader çš„ä¸¤ä¸ª CLIP æ’æ§½ä¸­éƒ½ä½¿ç”¨ t5xxl_fp8_e4m3fn.safetensors\n    ä¿®å¤ï¼šåœ¨ä¸€ä¸ªæ’æ§½ä¸­ä½¿ç”¨ t5xxl_fp8_e4m3fn.safetensorsï¼Œåœ¨å¦ä¸€ä¸ªæ’æ§½ä¸­ä½¿ç”¨ clip_l.safetensors\n    ```\n    \n    **ControlNet æž¶æž„ä¸åŒ¹é…ï¼š**\n    \n    ```\n    é—®é¢˜ï¼šSD1.5 ControlNet ä¸Ž SDXL æ£€æŸ¥ç‚¹ï¼ˆæˆ–åä¹‹ï¼‰\n    é”™è¯¯ï¼š\"mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)\"\n    ä¿®å¤ï¼šä½¿ç”¨ä¸ºæ‚¨çš„æ£€æŸ¥ç‚¹æž¶æž„è®¾è®¡çš„ ControlNet æ¨¡åž‹\n         - SD1.5 æ£€æŸ¥ç‚¹éœ€è¦ SD1.5 ControlNet\n         - SDXL æ£€æŸ¥ç‚¹éœ€è¦ SDXL ControlNet\n    ```\n    \n3.  **å¿«é€Ÿè¯Šæ–­ï¼š**\n    \n    ```\n    # æ£€æŸ¥é”™è¯¯æ˜¯å¦å‘ç”Ÿåœ¨ VAE è§£ç é˜¶æ®µ\n    # å¯»æ‰¾ \"expected input[X, Y, Z] to have N channels, but got M channels\"\n    # Y å€¼è¡¨ç¤ºé€šé“æ•°ï¼š4 = SD æ¨¡åž‹ï¼Œ16 = Flux æ¨¡åž‹\n    ```\n    \n4.  **é¢„é˜²ç­–ç•¥ï¼š**\n    *   å°†æ‰€æœ‰å·¥ä½œæµæ¨¡åž‹ä¿æŒåœ¨åŒä¸€æž¶æž„ç³»åˆ—å†…\n    *   ä»ŽåŒä¸€æ¥æº/å‘å¸ƒä¸‹è½½å®Œæ•´çš„æ¨¡åž‹åŒ…\n    *   ä½¿ç”¨ ComfyUI ç®¡ç†å™¨çš„æ¨¡åž‹å…¼å®¹æ€§æŒ‡ç¤ºå™¨\n    *   åœ¨è‡ªå®šä¹‰ä¹‹å‰ä½¿ç”¨é»˜è®¤ç¤ºä¾‹æµ‹è¯•å·¥ä½œæµ\n\n## ç¼ºå°‘æ¨¡åž‹é”™è¯¯\n\n**é”™è¯¯æ¶ˆæ¯ï¼š**\n\n```\nPrompt execution failed\nPrompt outputs failed validation:\nCheckpointLoaderSimple:\n- Value not in list: ckpt_name: 'model-name.safetensors' not in []\n```\n\n### è§£å†³æ–¹æ¡ˆ\n\n1.  **ä¸‹è½½æ‰€éœ€æ¨¡åž‹ï¼š**\n    *   ä½¿ç”¨ ComfyUI ç®¡ç†å™¨è‡ªåŠ¨ä¸‹è½½æ¨¡åž‹\n    *   éªŒè¯æ¨¡åž‹åœ¨æ­£ç¡®çš„å­æ–‡ä»¶å¤¹ä¸­\n2.  **æ£€æŸ¥æ¨¡åž‹è·¯å¾„ï¼š**\n    *   **æ£€æŸ¥ç‚¹**ï¼š`models/checkpoints/`\n    *   **VAE**ï¼š`models/vae/`\n    *   **LoRA**ï¼š`models/loras/`\n    *   **ControlNet**ï¼š`models/controlnet/`\n    *   **åµŒå…¥**ï¼š`models/embeddings/`\n3.  **åœ¨ UI ä¹‹é—´å…±äº«æ¨¡åž‹æˆ–ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„ï¼š**\n    *   å‚è§ [ComfyUI æ¨¡åž‹å…±äº«æˆ–è‡ªå®šä¹‰æ¨¡åž‹æ–‡ä»¶å¤¹å­˜å‚¨ä½ç½®é…ç½®](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows#2-comfyui-%E6%A8%A1%E5%9E%8B%E5%85%B1%E4%BA%AB%E6%88%96%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E5%A4%B9%E5%AD%98%E5%82%A8%E4%BD%8D%E7%BD%AE%E9%85%8D%E7%BD%AE) èŽ·å–è¯¦ç»†è¯´æ˜Ž\n    *   ç¼–è¾‘ `extra_model_paths.yaml` æ–‡ä»¶æ·»åŠ è‡ªå®šä¹‰æ¨¡åž‹ç›®å½•\n\n### æ¨¡åž‹æœç´¢è·¯å¾„é…ç½®\n\nå¦‚æžœæ‚¨çš„æ¨¡åž‹åœ¨è‡ªå®šä¹‰ä½ç½®ï¼Œè¯·å‚è§è¯¦ç»†çš„ [ComfyUI æ¨¡åž‹å…±äº«æˆ–è‡ªå®šä¹‰æ¨¡åž‹æ–‡ä»¶å¤¹å­˜å‚¨ä½ç½®é…ç½®](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows#2-comfyui-%E6%A8%A1%E5%9E%8B%E5%85%B1%E4%BA%AB%E6%88%96%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E5%A4%B9%E5%AD%98%E5%82%A8%E4%BD%8D%E7%BD%AE%E9%85%8D%E7%BD%AE) æŒ‡å—æ¥é…ç½® ComfyUI æ‰¾åˆ°å®ƒä»¬ã€‚\n\n## æ¨¡åž‹åŠ è½½é”™è¯¯\n\n**é”™è¯¯æ¶ˆæ¯ï¼š** â€œError while deserializing headerâ€\n\n### è§£å†³æ–¹æ¡ˆ\n\n1.  **é‡æ–°ä¸‹è½½æ¨¡åž‹** - ä¸‹è½½è¿‡ç¨‹ä¸­æ–‡ä»¶å¯èƒ½å·²æŸå\n2.  **æ£€æŸ¥å¯ç”¨ç£ç›˜ç©ºé—´** - ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç©ºé—´ç”¨äºŽæ¨¡åž‹åŠ è½½ï¼ˆæ¨¡åž‹å¯èƒ½ 2-15GB+ï¼‰\n3.  **æ£€æŸ¥æ–‡ä»¶æƒé™** - ç¡®ä¿ ComfyUI å¯ä»¥è¯»å–æ¨¡åž‹æ–‡ä»¶\n4.  **ä½¿ç”¨ä¸åŒæ¨¡åž‹æµ‹è¯•** - éªŒè¯é—®é¢˜æ˜¯æ¨¡åž‹ç‰¹å®šçš„è¿˜æ˜¯ç³»ç»ŸèŒƒå›´çš„\n\n## æ¨¡åž‹æ€§èƒ½é—®é¢˜\n\n### æ¨¡åž‹åŠ è½½ç¼“æ…¢\n\n**ç—‡çŠ¶ï¼š** åˆ‡æ¢æ¨¡åž‹æˆ–å¼€å§‹ç”Ÿæˆæ—¶é•¿æ—¶é—´å»¶è¿Ÿ **è§£å†³æ–¹æ¡ˆï¼š**\n\n1.  **å°†æ¨¡åž‹ä¿æŒåœ¨æ˜¾å­˜ä¸­ï¼š**\n    \n    ```\n    python main.py --highvram\n    ```\n    \n2.  **ä½¿ç”¨æ›´å¿«çš„å­˜å‚¨ï¼š**\n    *   å¦‚æžœä½¿ç”¨ HDDï¼Œå°†æ¨¡åž‹ç§»è‡³ SSD\n    *   ä½¿ç”¨ NVMe SSD èŽ·å¾—æœ€ä½³æ€§èƒ½\n3.  **è°ƒæ•´ç¼“å­˜è®¾ç½®ï¼š**\n    \n    ```\n    python main.py --cache-classic       # ä½¿ç”¨æ—§å¼ï¼ˆç§¯æžï¼‰ç¼“å­˜\n    ```\n    \n\n### å¤§åž‹æ¨¡åž‹çš„å†…å­˜é—®é¢˜\n\n**â€œRuntimeError: CUDA out of memoryâ€ï¼š**\n\n```\n# æ¸è¿›å¼å†…å­˜å‡å°‘\npython main.py --lowvram          # é¦–å…ˆå°è¯•\npython main.py --novram           # å¦‚æžœ lowvram ä¸å¤Ÿ\npython main.py --cpu              # æœ€åŽæ‰‹æ®µ\n```\n\n**æ¨¡åž‹ç‰¹å®šçš„å†…å­˜ä¼˜åŒ–ï¼š**\n\n```\n# å¼ºåˆ¶æ›´ä½Žç²¾åº¦\npython main.py --force-fp16\n\n# å‡å°‘æ³¨æ„åŠ›å†…å­˜ä½¿ç”¨\npython main.py --use-pytorch-cross-attention\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/troubleshooting/overview",
  "markdown": "# å¦‚ä½•æŽ’æŸ¥å’Œè§£å†³ ComfyUI ä¸­å‡ºçŽ°çš„é”™è¯¯ - ComfyUI\n\n[\n\n## è‡ªå®šä¹‰èŠ‚ç‚¹æ•…éšœæŽ’é™¤æŒ‡å—\n\næŸ¥çœ‹å¦‚ä½•æŽ’æŸ¥è‡ªå®šä¹‰èŠ‚ç‚¹å¯¼è‡´çš„é—®é¢˜ã€‚\n\n\n\n](https://docs.comfy.org/zh-CN/troubleshooting/custom-node-issues)\n\n## å¸¸è§é—®é¢˜ä¸Žå¿«é€Ÿä¿®å¤\n\nåœ¨æ·±å…¥è¯¦ç»†æ•…éšœæŽ’é™¤ä¹‹å‰ï¼Œè¯·å°è¯•è¿™äº›å¸¸è§è§£å†³æ–¹æ¡ˆï¼š\n\n### ComfyUI æ— æ³•å¯åŠ¨\n\n**ç—‡çŠ¶ï¼š** åº”ç”¨ç¨‹åºåœ¨å¯åŠ¨æ—¶å´©æºƒã€é»‘å±æˆ–æ— æ³•åŠ è½½ **å¿«é€Ÿä¿®å¤ï¼š**\n\n1.  **æ£€æŸ¥ç³»ç»Ÿè¦æ±‚** - ç¡®ä¿æ‚¨çš„ç³»ç»Ÿç¬¦åˆ[æœ€ä½Žè¦æ±‚](https://docs.comfy.org/zh-CN/installation/system_requirements)\n2.  **æ›´æ–° GPU é©±åŠ¨ç¨‹åº** - ä»Ž NVIDIA/AMD/Intel ä¸‹è½½æœ€æ–°é©±åŠ¨ç¨‹åº\n\n### ç”Ÿæˆå¤±è´¥æˆ–äº§ç”Ÿé”™è¯¯\n\n**ç—‡çŠ¶ï¼š** â€œPrompt execution failedâ€ï¼ˆæç¤ºæ‰§è¡Œå¤±è´¥ï¼‰å¯¹è¯æ¡†ï¼Œå¸¦æœ‰â€Show reportâ€ï¼ˆæ˜¾ç¤ºæŠ¥å‘Šï¼‰æŒ‰é’®ï¼Œå·¥ä½œæµåœæ­¢æ‰§è¡Œ **å¿«é€Ÿä¿®å¤ï¼š**\n\n1.  **ç‚¹å‡»â€Show reportâ€** - é˜…è¯»è¯¦ç»†é”™è¯¯æ¶ˆæ¯ä»¥è¯†åˆ«å…·ä½“é—®é¢˜\n2.  **æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå®šä¹‰èŠ‚ç‚¹é—®é¢˜** - [éµå¾ªæˆ‘ä»¬çš„è‡ªå®šä¹‰èŠ‚ç‚¹æ•…éšœæŽ’é™¤æŒ‡å—](https://docs.comfy.org/zh-CN/troubleshooting/custom-node-issues)\n3.  **éªŒè¯æ¨¡åž‹æ–‡ä»¶** - æŸ¥çœ‹[æ¨¡åž‹æ–‡æ¡£](https://docs.comfy.org/zh-CN/development/core-concepts/models)äº†è§£æ¨¡åž‹è®¾ç½®\n4.  **æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ** - å…³é—­å…¶ä»–ä½¿ç”¨ GPU å†…å­˜çš„åº”ç”¨ç¨‹åº\n\n### æ€§èƒ½ç¼“æ…¢\n\n**ç—‡çŠ¶ï¼š** ç”Ÿæˆæ—¶é—´éžå¸¸æ…¢ã€ç³»ç»Ÿå†»ç»“ã€å†…å­˜ä¸è¶³é”™è¯¯ **å¿«é€Ÿä¿®å¤ï¼š**\n\n1.  **é™ä½Žåˆ†è¾¨çŽ‡/æ‰¹æ¬¡å¤§å°** - å‡å°‘å›¾åƒå¤§å°æˆ–å›¾åƒæ•°é‡\n2.  **ä½¿ç”¨å†…å­˜ä¼˜åŒ–æ ‡å¿—** - è¯·å‚è§ä¸‹æ–¹æ€§èƒ½ä¼˜åŒ–éƒ¨åˆ†\n3.  **å…³é—­ä¸å¿…è¦çš„åº”ç”¨ç¨‹åº** - é‡Šæ”¾ RAM å’Œæ˜¾å­˜\n4.  **æ£€æŸ¥ CPU/GPU ä½¿ç”¨çŽ‡** - ä½¿ç”¨ä»»åŠ¡ç®¡ç†å™¨è¯†åˆ«ç“¶é¢ˆ\n\n**æ€§èƒ½ä¼˜åŒ–å‘½ä»¤ï¼š** å¯¹äºŽä½Žæ˜¾å­˜ç³»ç»Ÿï¼š\n\n```\n# ä½Žæ˜¾å­˜æ¨¡å¼ï¼ˆå°†æ¨¡åž‹åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼‰\npython main.py --lowvram\n\n# å½“ --lowvram ä¸å¤Ÿç”¨æ—¶çš„æ›´ä½Žæ˜¾å­˜æ¨¡å¼\npython main.py --novram\n\n# CPU æ¨¡å¼ï¼ˆéžå¸¸æ…¢ä½†é€‚ç”¨äºŽä»»ä½•ç¡¬ä»¶ï¼‰\npython main.py --cpu\n```\n\næé«˜æ€§èƒ½ï¼š\n\n```\n# ç¦ç”¨é¢„è§ˆï¼ˆèŠ‚çœæ˜¾å­˜å’Œå¤„ç†ï¼‰\npython main.py --preview-method none\n\n# å°†æ¨¡åž‹ä¿æŒåœ¨æ˜¾å­˜ä¸­ï¼ˆæ›´å¿«ä½†ä½¿ç”¨æ›´å¤šæ˜¾å­˜ï¼‰\npython main.py --highvram\n\n# å¼ºåˆ¶ FP16 ç²¾åº¦ï¼ˆæ›´å¿«ï¼Œä½¿ç”¨æ›´å°‘æ˜¾å­˜ï¼‰\npython main.py --force-fp16\n\n# ä½¿ç”¨ä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶\npython main.py --use-pytorch-cross-attention\npython main.py --use-flash-attention\n\n# å¼‚æ­¥æƒé‡å¸è½½\npython main.py --async-offload\n```\n\nå†…å­˜ç®¡ç†ï¼š\n\n```\n# ä¸ºæ“ä½œç³»ç»Ÿä¿ç•™ç‰¹å®šæ˜¾å­˜é‡ï¼ˆä»¥ GB ä¸ºå•ä½ï¼‰\npython main.py --reserve-vram 2\n\n# ç¦ç”¨æ™ºèƒ½å†…å­˜ç®¡ç†\npython main.py --disable-smart-memory\n\n# ä½¿ç”¨ä¸åŒçš„ç¼“å­˜ç­–ç•¥\npython main.py --cache-none  # æ›´å°‘çš„å†…å­˜ä½¿ç”¨\npython main.py --cache-lru 10  # ç¼“å­˜ 10 ä¸ªç»“æžœ\n```\n\n## å®‰è£…è¿‡ç¨‹ä¸­å‡ºçŽ°çš„é—®é¢˜\n\n### æ¡Œé¢åº”ç”¨é—®é¢˜\n\næœ‰å…³å…¨é¢çš„æ¡Œé¢å®‰è£…æ•…éšœæŽ’é™¤ï¼Œè¯·å‚è§[æ¡Œé¢å®‰è£…æŒ‡å—](https://docs.comfy.org/zh-CN/installation/desktop/windows)ã€‚\n\n*   **æ— æ³•å®‰è£…**ï¼šä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œå®‰è£…ç¨‹åº\n*   **ç¼ºå°‘ä¾èµ–é¡¹**ï¼šå®‰è£… [Visual C++ å¯å†å‘è¡Œç»„ä»¶](https://aka.ms/vs/17/release/vc_redist.x64.exe)\n*   **å¯åŠ¨æ—¶å´©æºƒ**ï¼šæ£€æŸ¥ Windows äº‹ä»¶æŸ¥çœ‹å™¨ä»¥èŽ·å–é”™è¯¯è¯¦ç»†ä¿¡æ¯\n\n### æ‰‹åŠ¨å®‰è£…é—®é¢˜\n\n**Python ç‰ˆæœ¬å†²çªï¼š**\n\n```\n# æ£€æŸ¥ Python ç‰ˆæœ¬ï¼ˆéœ€è¦ 3.9+ï¼ŒæŽ¨è 3.12ï¼‰\npython --version\n\n# ä½¿ç”¨è™šæ‹ŸçŽ¯å¢ƒï¼ˆæŽ¨èï¼‰\npython -m venv comfyui_env\nsource comfyui_env/bin/activate  # Linux/Mac\ncomfyui_env\\Scripts\\activate     # Windows\n```\n\n**åŒ…å®‰è£…å¤±è´¥ï¼š**\n\n```\n# é¦–å…ˆæ›´æ–° pip\npython -m pip install --upgrade pip\n\n# å®‰è£…ä¾èµ–é¡¹\npip install -r requirements.txt\n\n# å¯¹äºŽ NVIDIA GPUï¼ˆCUDA 12.8ï¼‰\npip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128\n\n# å¯¹äºŽ AMD GPUï¼ˆä»…é™ Linux - ROCm 6.3ï¼‰\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3\n```\n\n### Linux ç‰¹å®šé—®é¢˜\n\n**LD\\_LIBRARY\\_PATH é”™è¯¯ï¼š** å¸¸è§é”™è¯¯ï¼š\n\n*   â€œlibcuda.so.1: cannot open shared object fileâ€\n*   â€œlibnccl.so: cannot open shared object fileâ€\n*   â€œImportError: libnvinfer.so.X: cannot open shared object fileâ€\n\n**è§£å†³æ–¹æ¡ˆï¼š**\n\n1.  **çŽ°ä»£ PyTorch å®‰è£…ï¼ˆæœ€å¸¸è§ï¼‰ï¼š**\n\n```\n# å¯¹äºŽå¸¦æœ‰ NVIDIA åŒ…çš„è™šæ‹ŸçŽ¯å¢ƒ\nexport LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# å¯¹äºŽ conda çŽ¯å¢ƒ\nexport LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# æˆ–è‡ªåŠ¨æŸ¥æ‰¾æ‚¨çš„ Python site-packages\nPYTHON_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# æ‚¨å¯èƒ½è¿˜éœ€è¦å…¶ä»– NVIDIA åº“\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cuda_runtime/lib:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cublas/lib:$LD_LIBRARY_PATH\n```\n\n2.  **æŸ¥æ‰¾ä½ æ‹¥æœ‰çš„åº“ï¼š**\n\n```\n# æ£€æŸ¥å·²å®‰è£…çš„ NVIDIA åŒ…\npython -c \"import site; import os; nvidia_path=os.path.join(site.getsitepackages()[0], 'nvidia'); print('NVIDIA libs:', [d for d in os.listdir(nvidia_path) if os.path.isdir(os.path.join(nvidia_path, d))] if os.path.exists(nvidia_path) else 'Not found')\"\n\n# æŸ¥æ‰¾ PyTorch éœ€è¦çš„ç¼ºå¤±åº“\npython -c \"import torch; print(torch.__file__)\"\nldd $(python -c \"import torch; print(torch.__file__.replace('__init__.py', 'lib/libtorch_cuda.so'))\")\n```\n\n3.  **ä¸ºä½ çš„çŽ¯å¢ƒæ°¸ä¹…è®¾ç½®ï¼š**\n\n```\n# å¯¹äºŽè™šæ‹ŸçŽ¯å¢ƒï¼Œæ·»åŠ åˆ°æ¿€æ´»è„šæœ¬\necho 'export LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> $VIRTUAL_ENV/bin/activate\n\n# å¯¹äºŽ conda çŽ¯å¢ƒ\nconda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH\n\n# å¯¹äºŽå…¨å±€ bashrcï¼ˆæ ¹æ®éœ€è¦è°ƒæ•´ Python ç‰ˆæœ¬ï¼‰\necho 'export LD_LIBRARY_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> ~/.bashrc\n```\n\n4.  **æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ ldconfigï¼š**\n\n```\n# æ£€æŸ¥å½“å‰åº“ç¼“å­˜\nldconfig -p | grep cuda\nldconfig -p | grep nccl\n\n# å¦‚æžœç¼ºå¤±ï¼Œæ·»åŠ åº“è·¯å¾„ï¼ˆéœ€è¦ root æƒé™ï¼‰\nsudo echo \"/usr/local/cuda/lib64\" > /etc/ld.so.conf.d/cuda.conf\nsudo ldconfig\n```\n\n5.  **è°ƒè¯•åº“åŠ è½½ï¼š**\n\n```\n# è¯¦ç»†åº“åŠ è½½ä»¥æŸ¥çœ‹ç¼ºå¤±çš„å†…å®¹\nLD_DEBUG=libs python main.py 2>&1 | grep \"looking for\"\n\n# æ£€æŸ¥ PyTorch CUDA å¯ç”¨æ€§\npython -c \"import torch; print('CUDA available:', torch.cuda.is_available()); print('CUDA version:', torch.version.cuda)\"\n```\n\n## æ¨¡åž‹ç›¸å…³é—®é¢˜\n\næœ‰å…³ç»¼åˆæ¨¡åž‹æ•…éšœæŽ’é™¤ï¼ŒåŒ…æ‹¬æž¶æž„ä¸åŒ¹é…ã€ç¼ºå°‘æ¨¡åž‹å’ŒåŠ è½½é”™è¯¯ï¼Œè¯·å‚è§ä¸“é—¨çš„[æ¨¡åž‹é—®é¢˜](https://docs.comfy.org/zh-CN/troubleshooting/model-issues)é¡µé¢ã€‚\n\n## ç½‘ç»œå’Œ API é—®é¢˜\n\n### API èŠ‚ç‚¹ä¸å·¥ä½œ\n\n**ç—‡çŠ¶ï¼š** API è°ƒç”¨å¤±è´¥ã€è¶…æ—¶é”™è¯¯ã€é…é¢è¶…å‡º **è§£å†³æ–¹æ¡ˆï¼š**\n\n1.  **æ£€æŸ¥ API å¯†é’¥æœ‰æ•ˆæ€§** - åœ¨[ç”¨æˆ·è®¾ç½®](https://docs.comfy.org/zh-CN/interface/user)ä¸­éªŒè¯å¯†é’¥\n2.  **æ£€æŸ¥è´¦æˆ·ç§¯åˆ†** - ç¡®ä¿æœ‰è¶³å¤Ÿçš„ [API ç§¯åˆ†](https://docs.comfy.org/zh-CN/interface/credits)\n3.  **éªŒè¯äº’è”ç½‘è¿žæŽ¥** - ä½¿ç”¨å…¶ä»–åœ¨çº¿æœåŠ¡è¿›è¡Œæµ‹è¯•\n4.  **æ£€æŸ¥æœåŠ¡çŠ¶æ€** - æä¾›å•†å¯èƒ½æ­£åœ¨ç»åŽ†åœæœº\n\n### è¿žæŽ¥é—®é¢˜\n\n**ç—‡çŠ¶ï¼š** â€œæ— æ³•è¿žæŽ¥åˆ°æœåŠ¡å™¨â€ã€è¶…æ—¶é”™è¯¯ **è§£å†³æ–¹æ¡ˆï¼š**\n\n1.  **æ£€æŸ¥é˜²ç«å¢™è®¾ç½®** - å…è®¸ ComfyUI é€šè¿‡é˜²ç«å¢™\n2.  **å°è¯•ä¸åŒç«¯å£** - é»˜è®¤æ˜¯ 8188ï¼Œå°è¯• 8189 æˆ– 8190\n3.  **ä¸´æ—¶ç¦ç”¨ VPN** - VPN å¯èƒ½é˜»æ­¢è¿žæŽ¥\n4.  **æ£€æŸ¥ä»£ç†è®¾ç½®** - å¦‚æžœä¸éœ€è¦ï¼Œç¦ç”¨ä»£ç†\n\n## ç¡¬ä»¶ç‰¹å®šé—®é¢˜\n\n### NVIDIA GPU é—®é¢˜\n\n**CUDA é”™è¯¯ã€GPU æœªæ£€æµ‹åˆ°ï¼š**\n\n```\n# æ£€æŸ¥ CUDA å®‰è£…\nnvidia-smi\n\n# éªŒè¯ PyTorch CUDA æ”¯æŒ\npython -c \"import torch; print(torch.cuda.is_available())\"\n\n# é‡æ–°å®‰è£…å¸¦ CUDA çš„ PyTorch\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n```\n\n### AMD GPU é—®é¢˜\n\n**ROCm æ”¯æŒã€æ€§èƒ½é—®é¢˜ï¼š**\n\n```\n# å®‰è£… ROCm ç‰ˆæœ¬çš„ PyTorch\npip install torch torchvision --index-url https://download.pytorch.org/whl/rocm5.7\n```\n\n### Apple Silicon (M1/M2/M3) é—®é¢˜\n\n**MPS åŽç«¯é”™è¯¯ï¼š**\n\n```\n# æ£€æŸ¥ MPS å¯ç”¨æ€§\npython -c \"import torch; print(torch.backends.mps.is_available())\"\n\n# å¦‚æžœ MPS å¯¼è‡´é—®é¢˜ï¼Œå¼ºåˆ¶ä½¿ç”¨ CPU\npython main.py --force-fp16 --cpu\n```\n\n## èŽ·å–å¸®åŠ©å’ŒæŠ¥å‘Šé”™è¯¯\n\n### æŠ¥å‘Šé”™è¯¯ä¹‹å‰\n\n1.  **æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥é—®é¢˜ï¼š**\n    *   æœç´¢ [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n    *   æ£€æŸ¥ [ComfyUI è®ºå›](https://forum.comfy.org/)\n    *   æŸ¥çœ‹ [Discord è®¨è®º](https://discord.com/invite/comfyorg)\n2.  **å°è¯•åŸºæœ¬æ•…éšœæŽ’é™¤ï¼š**\n    *   ä½¿ç”¨[é»˜è®¤å·¥ä½œæµ](https://docs.comfy.org/zh-CN/get_started/first_generation)è¿›è¡Œæµ‹è¯•\n    *   ç¦ç”¨æ‰€æœ‰è‡ªå®šä¹‰èŠ‚ç‚¹ï¼ˆå‚è§[è‡ªå®šä¹‰èŠ‚ç‚¹æ•…éšœæŽ’é™¤](https://docs.comfy.org/zh-CN/troubleshooting/custom-node-issues)ï¼‰\n    *   æ£€æŸ¥æŽ§åˆ¶å°/ç»ˆç«¯ä¸­çš„é”™è¯¯æ¶ˆæ¯\n\n### å¦‚ä½•æœ‰æ•ˆæŠ¥å‘Šé”™è¯¯\n\n#### å¯¹äºŽ ComfyUI æ ¸å¿ƒé—®é¢˜\n\n**é—®é¢˜æäº¤ï¼š** [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n\n#### å¯¹äºŽæ¡Œé¢åº”ç”¨é—®é¢˜\n\n**é—®é¢˜æäº¤ï¼š** [æ¡Œé¢ GitHub Issues](https://github.com/Comfy-Org/desktop/issues)\n\n#### å¯¹äºŽå‰ç«¯é—®é¢˜\n\n**é—®é¢˜æäº¤ï¼š** [å‰ç«¯ GitHub Issues](https://github.com/Comfy-Org/ComfyUI_frontend/issues)\n\n#### å¯¹äºŽè‡ªå®šä¹‰èŠ‚ç‚¹é—®é¢˜\n\n**é—®é¢˜æäº¤ï¼š** è¯·åˆ°å¯¹åº”çš„è‡ªå®šä¹‰èŠ‚ç‚¹ä»“åº“ä¸­æäº¤é—®é¢˜\n\n### åœ¨ issue ä¸­ä½ éœ€è¦æä¾›çš„ä¿¡æ¯\n\næŠ¥å‘Šä»»ä½•é—®é¢˜æ—¶ï¼Œè¯·åŒ…æ‹¬ä»¥ä¸‹å†…å®¹ï¼š\n\n## ç¤¾åŒºèµ„æº\n\n*   **å®˜æ–¹è®ºå›ï¼š** [forum.comfy.org](https://forum.comfy.org/)\n*   **Discordï¼š** [ComfyUI Discord æœåŠ¡å™¨](https://discord.com/invite/comfyorg)\n*   **Redditï¼š** [r/comfyui](https://reddit.com/r/comfyui)\n*   **YouTubeï¼š** [ComfyUI æ•™ç¨‹](https://www.youtube.com/@comfyorg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/troubleshooting/custom-node-issues",
  "markdown": "# å¦‚ä½•è§£å†³å’ŒæŽ’æŸ¥ ComfyUI ä¸­è‡ªå®šä¹‰èŠ‚ç‚¹å¯¼è‡´çš„é—®é¢˜ - ComfyUI\n\nå…³äºŽè‡ªå®šä¹‰èŠ‚ç‚¹é—®é¢˜æŽ’æŸ¥ï¼Œæœ¬ç¯‡æ–‡æ¡£çš„æ€»ä½“æ€è·¯å¦‚ä¸‹ï¼š\n\n## å¦‚ä½•ç¦ç”¨æ‰€æœ‰çš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Ÿ\n\nä»Žè®¾ç½®èœå•ä¸­å¯åŠ¨ç¦ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹çš„ ComfyUI æ¡Œé¢ç‰ˆ ![è®¾ç½®èœå•-ç¦ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/troubleshooting/desktop-diable-custom-node.jpg) æˆ–æ‰‹åŠ¨è¿è¡ŒæœåŠ¡å™¨ï¼š\n\n```\ncd path/to/your/comfyui\npython main.py --disable-all-custom-nodes\n```\n\n**ç»“æžœï¼š**\n\n*   âœ… **é—®é¢˜æ¶ˆå¤±**ï¼šè‡ªå®šä¹‰èŠ‚ç‚¹å¯¼è‡´é—®é¢˜ â†’ ç»§ç»­æ­¥éª¤ 2\n*   âŒ **é—®é¢˜ä»ç„¶å­˜åœ¨**ï¼šä¸æ˜¯è‡ªå®šä¹‰èŠ‚ç‚¹é—®é¢˜ â†’ [æŠ¥å‘Šé—®é¢˜](#%E6%8A%A5%E5%91%8A%E9%97%AE%E9%A2%98)\n\n## äºŒåˆ†æ³•\n\nåœ¨æœ¬ç¯‡ä¸­æˆ‘ä»¬å°†ä¼šä»‹ç»ä½¿ç”¨äºŒåˆ†æœç´¢æ¥è¿›è¡Œè‡ªå®šä¹‰èŠ‚ç‚¹é—®é¢˜æŽ’æŸ¥çš„æ€è·¯ï¼Œä¹Ÿå°±æ˜¯ä¸€æ¬¡æŽ’æŸ¥ä¸€åŠçš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œç›´åˆ°å®šä½åˆ°å¯¼è‡´é—®é¢˜çš„è‡ªå®šä¹‰èŠ‚ç‚¹ å…·ä½“æ€è·¯æ¸…å‚è€ƒä¸‹é¢çš„æµç¨‹å›¾ï¼Œå³æ¯æ¬¡å¯ç”¨æ‰€æœ‰æœªå¯ç”¨èŠ‚ç‚¹çš„ä¸€åŠï¼Œçœ‹çœ‹å¯¹åº”çš„é—®é¢˜æ˜¯å¦å‡ºçŽ°ï¼Œç›´åˆ°å®šä½åˆ°å¯¹åº”çš„è‡ªå®šä¹‰èŠ‚ç‚¹æ˜¯å“ªä¸ª\n\n## ä¸¤ç§æŽ’æŸ¥æ–¹æ³•\n\nåœ¨æœ¬ç¯‡æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬å°†æŽ’æŸ¥çš„è‡ªå®šä¹‰èŠ‚ç‚¹åˆ†ä¸ºä¸¤ç±» ![è‡ªå®šä¹‰èŠ‚ç‚¹ç±»åž‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/troubleshooting/custom_node_type.jpg)\n\n*   A:åŒ…å«å‰ç«¯æ‰©å±•çš„è‡ªå®šä¹‰èŠ‚ç‚¹\n*   B: å¸¸è§„èŠ‚ç‚¹\n\né¦–å…ˆè®©æˆ‘ä»¬å…ˆäº†è§£ä¸åŒç±»åž‹çš„è‡ªå®šä¹‰èŠ‚ç‚¹å¯èƒ½å¯¼è‡´çš„é—®é¢˜å’ŒåŽŸå› \n\nå¯¹äºŽè‡ªå®šä¹‰èŠ‚ç‚¹æˆ‘ä»¬åˆç‰¹åˆ«éœ€è¦å¯¹åŒ…å«å‰ç«¯æ‰©å±•çš„è‡ªå®šä¹‰èŠ‚ç‚¹è¿›è¡Œä¼˜å…ˆæŽ’æŸ¥ï¼Œè¿™ç±»èŠ‚ç‚¹å¯¼è‡´çš„é—®é¢˜æ˜¯æœ€å¤šçš„, ä»–ä»¬ä¸»è¦çš„å†²çªæ˜¯ä¸Ž ComfyUI å‰ç«¯ç‰ˆæœ¬æ›´æ–°äº§ç”Ÿçš„å†²çªã€‚å¸¸è§çš„ä¸€äº›é—®é¢˜æœ‰:\n\n*   å·¥ä½œæµæ— æ³•æ‰§è¡Œ\n*   èŠ‚ç‚¹é¢„è§ˆå›¾ä¸¢å¤±\n*   UIå…ƒç´ é”™ä½\n*   æ— æ³•è¿›å…¥ ComfyUI å‰ç«¯\n*   UI å®Œå…¨æŸåæˆ–æ˜¾ç¤ºç©ºç™½å±å¹•\n*   æ— æ³•å’Œ ComfyUI åŽç«¯æ­£å¸¸é€šä¿¡\n*   èŠ‚ç‚¹ä¹‹é—´çš„è¿žçº¿æ— æ³•æ­£å¸¸å·¥ä½œ\n*   ç­‰ç­‰\n\nå¸¸è§è¿™ç±»èŠ‚ç‚¹å¯¼è‡´çš„åŽŸå› æ˜¯å› ä¸ºï¼š\n\n*   æˆ‘ä»¬åœ¨å¯¹å‰ç«¯è¿›è¡Œæ›´æ–°è¿‡ç¨‹ä¸­è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹è°ƒæ•´ï¼Œè¿™äº›è‡ªå®šä¹‰èŠ‚ç‚¹æ²¡æœ‰åŠæ—¶æ›´æ–°\n*   ç”¨æˆ·å¸¸å¸¸åœ¨æ›´æ–°è¿‡ç¨‹ä¸­åªæ˜¯æ›´æ–°äº† ComfyUI å¹¶æ²¡æœ‰å¯¹è‡ªå®šä¹‰èŠ‚ç‚¹è¿›è¡ŒåŒæ­¥å‡çº§ï¼Œè™½ç„¶ä½œè€…è¿›è¡Œäº†æ›´æ–°ï¼Œä½†æ˜¯ç”¨æˆ·å¹¶æ²¡æœ‰åœ¨ä½¿ç”¨æœ€æ–°çš„å…¼å®¹ç‰ˆæœ¬\n*   ä½œè€…åœæ­¢äº†ç»´æŠ¤ï¼Œå¯¼è‡´å¯¹åº”çš„è‡ªå®šä¹‰èŠ‚ç‚¹æ‰©å±•æ— æ³•å’Œå‰ç«¯ç›¸äº’å…¼å®¹\n\n## ä½¿ç”¨äºŒåˆ†æ³•è¿›è¡ŒæŽ’æŸ¥\n\nè¿™ä¸¤ç§ä¸Šé¢ä¸¤ç§ä¸åŒçš„è‡ªå®šä¹‰èŠ‚ç‚¹é—®é¢˜é‡Œï¼Œè‡ªå®šä¹‰èŠ‚ç‚¹å‰ç«¯æ‰©å±•å’Œ ComfyUI çš„å†²çªè¾ƒä¸ºå¸¸è§ï¼Œæˆ‘ä»¬ä¼šä¼˜å…ˆæŽ’æŸ¥è¿™ç±»èŠ‚ç‚¹ï¼ŒåŽç»­çš„æ•´ä½“çš„é—®é¢˜æŽ’æŸ¥æ€è·¯å¦‚ä¸‹\n\n### 1.æŽ’æŸ¥è‡ªå®šä¹‰èŠ‚ç‚¹çš„å‰ç«¯æ‰©å±•\n\nä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œä½ ä¸ç”¨å¤šæ¬¡é‡å¯ ComfyUI ä»…éœ€è¦åœ¨æ¯æ¬¡å¯ç”¨ / ç¦ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹çš„å‰ç«¯æ‰©å±•åŽé‡è½½ ComfyUI å³å¯, è€Œä¸”ä½ çš„æŽ’æŸ¥èŒƒå›´ä¹Ÿåªæ˜¯åœ¨æœ‰å‰ç«¯æ‰©å±•çš„èŠ‚ç‚¹é‡Œï¼Œä¼šå¤§å¤§ç¼©å°èŠ‚ç‚¹æŽ’æŸ¥èŒƒå›´\n\n### 2\\. é€šç”¨çš„è‡ªå®šä¹‰èŠ‚ç‚¹æŽ’æŸ¥æ–¹æ³•\n\n## ä¿®å¤è‡ªå®šä¹‰èŠ‚ç‚¹é—®é¢˜\n\nä¸€æ—¦ä½ è¯†åˆ«å‡ºæœ‰é—®é¢˜çš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼š\n\n### é€‰é¡¹ 1:æ›´æ–°èŠ‚ç‚¹\n\n1.  æ£€æŸ¥ ComfyUI ç®¡ç†å™¨ä¸­æ˜¯å¦æœ‰å¯ç”¨æ›´æ–°\n2.  æ›´æ–°èŠ‚ç‚¹å¹¶å†æ¬¡æµ‹è¯•\n\n### é€‰é¡¹ 2:æ›¿æ¢èŠ‚ç‚¹\n\n1.  å¯»æ‰¾å…·æœ‰ç±»ä¼¼åŠŸèƒ½çš„æ›¿ä»£è‡ªå®šä¹‰èŠ‚ç‚¹\n2.  æŸ¥çœ‹ [ComfyUI æ³¨å†Œè¡¨](https://registry.comfy.org/) å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆ\n\n### é€‰é¡¹ 3:æŠ¥å‘Šé—®é¢˜\n\nè”ç³»è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘è€…ï¼š\n\n1.  æ‰¾åˆ°èŠ‚ç‚¹çš„ GitHub ä»“åº“\n2.  åˆ›å»ºé—®é¢˜å¹¶åŒ…å«ï¼š\n    *   ä½ çš„ ComfyUI ç‰ˆæœ¬\n    *   é”™è¯¯æ¶ˆæ¯/æ—¥å¿—\n    *   é‡çŽ°æ­¥éª¤\n    *   ä½ çš„æ“ä½œç³»ç»Ÿ\n\n### é€‰é¡¹ 4:ç§»é™¤èŠ‚ç‚¹\n\nå¦‚æžœæ²¡æœ‰ä¿®å¤å¯ç”¨ä¸”ä½ ä¸éœ€è¦è¯¥åŠŸèƒ½ï¼š\n\n1.  ä»Ž `custom_nodes/` ä¸­ç§»é™¤æœ‰é—®é¢˜çš„èŠ‚ç‚¹\n2.  é‡å¯ ComfyUI\n\n## æŠ¥å‘Šéžè‡ªå®šä¹‰èŠ‚ç‚¹å¯¼è‡´çš„é—®é¢˜\n\nå¦‚æžœé—®é¢˜ä¸æ˜¯ç”±è‡ªå®šä¹‰èŠ‚ç‚¹å¼•èµ·çš„ï¼Œè¯·å‚è€ƒé€šè¿‡[æ•…éšœæŽ’é™¤æ¦‚è¿°](https://docs.comfy.org/zh-CN/troubleshooting/overview)äº†è§£å…¶ä»–å¸¸è§é—®é¢˜ã€‚\n\n### è‡ªå®šä¹‰èŠ‚ç‚¹ç‰¹å®šé—®é¢˜\n\nè”ç³»è‡ªå®šä¹‰èŠ‚ç‚¹å¼€å‘è€…ï¼š\n\n*   æ‰¾åˆ°èŠ‚ç‚¹çš„ GitHub ä»“åº“\n*   åˆ›å»ºé—®é¢˜å¹¶åŒ…å«ä½ çš„ ComfyUI ç‰ˆæœ¬ã€é”™è¯¯æ¶ˆæ¯ã€é‡çŽ°æ­¥éª¤å’Œæ“ä½œç³»ç»Ÿ\n*   æŸ¥çœ‹èŠ‚ç‚¹æ–‡æ¡£äº†è§£å·²çŸ¥é—®é¢˜\n\n### ComfyUI æ ¸å¿ƒé—®é¢˜\n\n*   **GitHub**ï¼š[ComfyUI Issues](https://github.com/comfyanonymous/ComfyUI/issues)\n*   **è®ºå›**ï¼š[å®˜æ–¹ ComfyUI è®ºå›](https://forum.comfy.org/)\n\n### æ¡Œé¢åº”ç”¨é—®é¢˜\n\n*   **GitHub**ï¼š[ComfyUI æ¡Œé¢é—®é¢˜](https://github.com/Comfy-Org/desktop/issues)\n\n### å‰ç«¯é—®é¢˜\n\n*   **GitHub**ï¼š[ComfyUI å‰ç«¯é—®é¢˜](https://github.com/Comfy-Org/ComfyUI_frontend/issues)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/controlnet",
  "markdown": "# ComfyUI ControlNet ä½¿ç”¨ç¤ºä¾‹ - ComfyUI\n\nåœ¨ AI å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œè¦ç²¾ç¡®æŽ§åˆ¶å›¾åƒç”Ÿæˆå¹¶ä¸æ˜¯ä¸€é”®å®¹æ˜“çš„äº‹æƒ…ï¼Œé€šå¸¸éœ€è¦é€šè¿‡è®¸å¤šæ¬¡çš„å›¾åƒç”Ÿæˆæ‰å¯èƒ½ç”Ÿæˆæ»¡æ„çš„å›¾åƒï¼Œä½†éšç€ **ControlNet** çš„å‡ºçŽ°ï¼Œè¿™ä¸ªé—®é¢˜å¾—åˆ°äº†å¾ˆå¥½çš„è§£å†³ã€‚ ControlNet æ˜¯ä¸€ç§åŸºäºŽæ‰©æ•£æ¨¡åž‹ï¼ˆå¦‚ Stable Diffusionï¼‰çš„æ¡ä»¶æŽ§åˆ¶ç”Ÿæˆæ¨¡åž‹ï¼Œæœ€æ—©ç”±[Lvmin Zhang](https://lllyasviel.github.io/)ä¸Ž Maneesh Agrawala ç­‰äººäºŽ 2023 å¹´æå‡º[Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543) ControlNet æ¨¡åž‹é€šè¿‡å¼•å…¥å¤šæ¨¡æ€è¾“å…¥æ¡ä»¶ï¼ˆå¦‚è¾¹ç¼˜æ£€æµ‹å›¾ã€æ·±åº¦å›¾ã€å§¿åŠ¿å…³é”®ç‚¹ç­‰ï¼‰ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç”Ÿæˆçš„å¯æŽ§æ€§å’Œç»†èŠ‚è¿˜åŽŸèƒ½åŠ›ã€‚ ä½¿å¾—æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥å¼€å§‹æŽ§åˆ¶å›¾åƒçš„é£Žæ ¼ã€ç»†èŠ‚ã€äººç‰©å§¿åŠ¿ã€ç”»é¢ç»“æž„ç­‰ç­‰ï¼Œè¿™äº›é™å®šæ¡ä»¶è®©å›¾åƒç”Ÿæˆå˜å¾—æ›´åŠ å¯æŽ§ï¼Œåœ¨ç»˜å›¾è¿‡ç¨‹ä¸­ä¹Ÿå¯ä»¥åŒæ—¶ä½¿ç”¨å¤šä¸ª ControlNet æ¨¡åž‹ï¼Œä»¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæžœã€‚ åœ¨æ²¡æœ‰ ControlNet ä¹‹å‰ï¼Œæˆ‘ä»¬æ¯æ¬¡åªèƒ½è®©æ¨¡åž‹ç”Ÿæˆå›¾åƒï¼Œç›´åˆ°ç”Ÿæˆæˆ‘ä»¬æ»¡æ„çš„å›¾åƒï¼Œå……æ»¡äº†éšæœºæ€§ã€‚ ![ComfyUI éšæœºç§å­ç”Ÿæˆçš„å›¾ç‰‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/generated_with_random_seed.jpg) ä½†éšç€ ControlNet çš„å‡ºçŽ°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¼•å…¥é¢å¤–çš„æ¡ä»¶ï¼Œæ¥æŽ§åˆ¶å›¾åƒçš„ç”Ÿæˆï¼Œæ¯”å¦‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€å¼ ç®€å•çš„æ¶‚é¸¦ï¼Œæ¥æŽ§åˆ¶å›¾åƒçš„ç”Ÿæˆï¼Œå°±å¯ä»¥ç”Ÿæˆå·®ä¸å¤šç±»ä¼¼çš„å›¾ç‰‡ã€‚ ![ComfyUI æ¶‚é¸¦æŽ§åˆ¶å›¾åƒç”Ÿæˆ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/scribble_example.jpg) åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å®Œæˆåœ¨ [ComfyUI](https://github.com/comfyanonymous/ComfyUI) ä¸­ ControlNet æ¨¡åž‹çš„å®‰è£…ä¸Žä½¿ç”¨, å¹¶å®Œæˆä¸€ä¸ªæ¶‚é¸¦æŽ§åˆ¶å›¾åƒç”Ÿæˆçš„ç¤ºä¾‹ã€‚ ![ComfyUI ControlNet å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)\n\nä¸åŒç±»åž‹çš„ ControlNet æ¨¡åž‹ï¼Œé€šå¸¸éœ€è¦ä½¿ç”¨ä¸åŒç±»åž‹çš„å‚è€ƒå›¾ï¼š ![å‚è€ƒå›¾](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/main/examples/CNAuxBanner.jpg?raw=true)\n\n> å›¾æºï¼š[ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\nç”±äºŽç›®å‰ **Comfy Core** èŠ‚ç‚¹ä¸­ï¼Œä¸åŒ…å«æ‰€æœ‰ç±»åž‹çš„ **é¢„å¤„ç†å™¨** ç±»åž‹ï¼Œä½†åœ¨æœ¬æ–‡æ¡£çš„å®žé™…ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬éƒ½å°†æä¾›å·²ç»ç»è¿‡å¤„ç†åŽçš„å›¾ç‰‡ï¼Œ ä½†åœ¨å®žé™…ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œä½ å¯èƒ½éœ€è¦å€ŸåŠ©ä¸€äº›è‡ªå®šä¹‰èŠ‚ç‚¹æ¥å¯¹å›¾ç‰‡è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥æ»¡è¶³ä¸åŒ ControlNet æ¨¡åž‹çš„éœ€æ±‚ï¼Œä¸‹é¢æ˜¯ä¸€äº›ç›¸å…³çš„æ’ä»¶\n\n*   [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n*   [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n## ComfyUI ControlNet å·¥ä½œæµç¤ºä¾‹è®²è§£\n\n### 1\\. ControlNet å·¥ä½œæµç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä¼šå°†å®ƒä½œä¸ºè¾“å…¥ ![ComfyUI æ¶‚é¸¦å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_input.png)\n\n### 2\\. æ‰‹åŠ¨æ¨¡åž‹å®‰è£…\n\n*   [dreamCreationVirtual3DECommerce\\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n*   [control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ dreamCreationVirtual3DECommerce_v10.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ vae-ft-mse-840000-ema-pruned.safetensors\nâ”‚   â””â”€â”€ controlnet/\nâ”‚       â””â”€â”€ control_v11p_sd15_scribble_fp16.safetensors\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI å·¥ä½œæµ - ControlNet æµç¨‹å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_scribble.png)\n\n1.  ç¡®ä¿`Load Checkpoint`å¯ä»¥åŠ è½½ **dreamCreationVirtual3DECommerce\\_v10.safetensors**\n2.  ç¡®ä¿`Load VAE`å¯ä»¥åŠ è½½ **vae-ft-mse-840000-ema-pruned.safetensors**\n3.  åœ¨`Load Image`ä¸­ç‚¹å‡»`Upload` ä¸Šä¼ ä¹‹å‰æä¾›çš„è¾“å…¥å›¾ç‰‡\n4.  ç¡®ä¿`Load ControlNet`å¯ä»¥åŠ è½½ **control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors**\n5.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n\n## ç›¸å…³èŠ‚ç‚¹è®²è§£\n\n### Load ControlNet èŠ‚ç‚¹è®²è§£\n\n![load controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_controlnet_model.jpg) ä½äºŽ`ComfyUI\\models\\controlnet` çš„æ¨¡åž‹ä¼šè¢« ComfyUI æ£€æµ‹åˆ°ï¼Œå¹¶åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸­è¯†åˆ«å¹¶åŠ è½½\n\n### Apply ControlNet èŠ‚ç‚¹è®²è§£\n\n![apply controlnet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg) è¿™ä¸ªèŠ‚ç‚¹æŽ¥å— `load controlnet` åŠ è½½çš„ ControlNet æ¨¡åž‹ï¼Œå¹¶æ ¹æ®è¾“å…¥çš„å›¾ç‰‡ï¼Œç”Ÿæˆå¯¹åº”çš„æŽ§åˆ¶æ¡ä»¶ã€‚ **è¾“å…¥ç±»åž‹**\n\n| å‚æ•°åç§° | ä½œç”¨  |\n| --- | --- |\n| `positive` | æ­£å‘æ¡ä»¶ |\n| `negative` | è´Ÿå‘æ¡ä»¶ |\n| `control_net` | è¦åº”ç”¨çš„controlNetæ¨¡åž‹ |\n| `image` | ç”¨äºŽ controlNet åº”ç”¨å‚è€ƒçš„é¢„å¤„ç†å™¨å¤„ç†å›¾ç‰‡ |\n| `vae` | Vaeæ¨¡åž‹è¾“å…¥ |\n| `strength` | åº”ç”¨ ControlNet çš„å¼ºåº¦ï¼Œè¶Šå¤§åˆ™ ControlNet å¯¹ç”Ÿæˆå›¾åƒçš„å½±å“è¶Šå¤§ |\n| `start_percent` | ç¡®å®šå¼€å§‹åº”ç”¨controlNetçš„ç™¾åˆ†æ¯”ï¼Œæ¯”å¦‚å–å€¼0.2ï¼Œæ„å‘³ç€ControlNetçš„å¼•å¯¼å°†åœ¨æ‰©æ•£è¿‡ç¨‹å®Œæˆ20%æ—¶å¼€å§‹å½±å“å›¾åƒç”Ÿæˆ |\n| `end_percent` | ç¡®å®šç»“æŸåº”ç”¨controlNetçš„ç™¾åˆ†æ¯”ï¼Œæ¯”å¦‚å–å€¼0.8ï¼Œæ„å‘³ç€ControlNetçš„å¼•å¯¼å°†åœ¨æ‰©æ•£è¿‡ç¨‹å®Œæˆ80%æ—¶åœæ­¢å½±å“å›¾åƒç”Ÿæˆ |\n\n**è¾“å‡ºç±»åž‹**\n\n| å‚æ•°åç§° | ä½œç”¨  |\n| --- | --- |\n| `positive` | åº”ç”¨äº† ControlNet å¤„ç†åŽçš„æ­£å‘æ¡ä»¶æ•°æ® |\n| `negative` | åº”ç”¨äº† ControlNet å¤„ç†åŽçš„è´Ÿå‘æ¡ä»¶æ•°æ® |\n\nä½ å¯ä»¥ä½¿ç”¨é“¾å¼é“¾æŽ¥æ¥åº”ç”¨å¤šä¸ª ControlNet æ¨¡åž‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä½ ä¹Ÿå¯ä»¥å‚è€ƒ [æ··åˆ ControlNet æ¨¡åž‹](https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets) éƒ¨åˆ†çš„æŒ‡å—æ¥äº†è§£æ›´å¤šå…³äºŽæ··åˆ ControlNet æ¨¡åž‹çš„ä½¿ç”¨ ![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg) \n\n## å¼€å§‹ä½ çš„å°è¯•\n\n1.  è¯•ç€åˆ¶ä½œç±»ä¼¼çš„æ¶‚é¸¦å›¾ç‰‡ï¼Œç”šè‡³è‡ªå·±æ‰‹ç»˜ï¼Œå¹¶ä½¿ç”¨ ControlNet æ¨¡åž‹ç”Ÿæˆå›¾åƒï¼Œä½“éªŒ ControlNet å¸¦æ¥çš„ä¹è¶£\n2.  è°ƒæ•´ Apply ControlNet èŠ‚ç‚¹çš„ `Control Strength` å‚æ•°ï¼Œæ¥æŽ§åˆ¶ ControlNet æ¨¡åž‹å¯¹ç”Ÿæˆå›¾åƒçš„å½±å“\n3.  è®¿é—® [ControlNet-v1-1\\_fp16\\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) ä»“åº“ä¸‹è½½å…¶å®ƒç±»åž‹çš„ ControlNet æ¨¡åž‹ï¼Œå¹¶å°è¯•ä½¿ç”¨å®ƒä»¬ç”Ÿæˆå›¾åƒ"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/depth-controlnet",
  "markdown": "# ComfyUI Depth ControlNet ä½¿ç”¨ç¤ºä¾‹ - ComfyUI\n\næ·±åº¦å›¾(Depth Map)æ˜¯ä¸€ç§ç‰¹æ®Šçš„å›¾åƒï¼Œå®ƒé€šè¿‡ç°åº¦å€¼è¡¨ç¤ºåœºæ™¯ä¸­å„ä¸ªç‰©ä½“ä¸Žè§‚å¯Ÿè€…æˆ–ç›¸æœºçš„è·ç¦»ã€‚åœ¨æ·±åº¦å›¾ä¸­ï¼Œç°åº¦å€¼ä¸Žè·ç¦»æˆåæ¯”ï¼šâ€‹è¶Šäº®çš„åŒºåŸŸï¼ˆæŽ¥è¿‘ç™½è‰²ï¼‰è¡¨ç¤ºè·ç¦»è¶Šè¿‘ï¼Œâ€‹è¶Šæš—çš„åŒºåŸŸï¼ˆæŽ¥è¿‘é»‘è‰²ï¼‰è¡¨ç¤ºè·ç¦»è¶Šè¿œã€‚ ![Depth å›¾åƒ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png) Depth ControlNet æ˜¯ä¸“é—¨è®­ç»ƒç”¨äºŽç†è§£å’Œåˆ©ç”¨æ·±åº¦å›¾ä¿¡æ¯çš„ ControlNet æ¨¡åž‹ã€‚å®ƒèƒ½å¤Ÿå¸®åŠ© AI æ­£ç¡®è§£è¯»ç©ºé—´å…³ç³»ï¼Œä½¿ç”Ÿæˆçš„å›¾åƒç¬¦åˆæ·±åº¦å›¾æŒ‡å®šçš„ç©ºé—´ç»“æž„ï¼Œä»Žè€Œå®žçŽ°å¯¹ä¸‰ç»´ç©ºé—´å¸ƒå±€çš„ç²¾ç¡®æŽ§åˆ¶ã€‚\n\n### æ·±åº¦å›¾ç»“åˆ ControlNet åº”ç”¨åœºæ™¯\n\næ·±åº¦å›¾åœ¨å¤šç§åœºæ™¯ä¸­éƒ½æœ‰æ¯”è¾ƒå¤šçš„åº”ç”¨ï¼š\n\n1.  **äººåƒåœºæ™¯**ï¼šæŽ§åˆ¶äººç‰©ä¸ŽèƒŒæ™¯çš„ç©ºé—´å…³ç³»ï¼Œé¿å…é¢éƒ¨ç­‰å…³é”®éƒ¨ä½ç•¸å˜\n2.  **é£Žæ™¯åœºæ™¯**ï¼šæŽ§åˆ¶è¿‘æ™¯ã€ä¸­æ™¯ã€è¿œæ™¯çš„å±‚æ¬¡å…³ç³»\n3.  **å»ºç­‘åœºæ™¯**ï¼šæŽ§åˆ¶å»ºç­‘ç‰©çš„ç©ºé—´ç»“æž„å’Œé€è§†å…³ç³»\n4.  **äº§å“å±•ç¤º**ï¼šæŽ§åˆ¶äº§å“ä¸ŽèƒŒæ™¯çš„åˆ†ç¦»åº¦å’Œç©ºé—´ä½ç½®\n\næœ¬ç¯‡ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ·±åº¦å›¾ç”Ÿæˆå»ºç­‘å¯è§†åŒ–çš„åœºæ™¯ç”Ÿæˆã€‚\n\n## ComfyUI ControlNet å·¥ä½œæµç¤ºä¾‹è®²è§£\n\n### 1\\. ControlNet å·¥ä½œæµç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![Depth å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_controlnet.png)\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä¼šå°†å®ƒä½œä¸ºè¾“å…¥ã€‚ ![Depth å›¾åƒ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)\n\n### 2\\. æ¨¡åž‹å®‰è£…\n\n*   [architecturerealmix\\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [control\\_v11f1p\\_sd15\\_depth\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ architecturerealmix_v11.safetensors\nâ”‚   â””â”€â”€ controlnet/\nâ”‚       â””â”€â”€ control_v11f1p_sd15_depth_fp16.safetensors\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI å·¥ä½œæµ - Depth ControlNet æµç¨‹å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth.jpg)\n\n1.  ç¡®ä¿`Load Checkpoint`å¯ä»¥åŠ è½½ **architecturerealmix\\_v11.safetensors**\n2.  ç¡®ä¿`Load ControlNet`å¯ä»¥åŠ è½½ **control\\_v11f1p\\_sd15\\_depth\\_fp16.safetensors**\n3.  åœ¨`Load Image`ä¸­ç‚¹å‡»`Upload` ä¸Šä¼ ä¹‹å‰æä¾›çš„ Depth å›¾åƒ\n4.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n\n## æ··åˆæ·±åº¦æŽ§åˆ¶ä¸Žå…¶ä»–æŠ€æœ¯\n\næ ¹æ®ä¸åŒåˆ›ä½œéœ€æ±‚ï¼Œå¯ä»¥å°†æ·±åº¦å›¾ ControlNet ä¸Žå…¶å®ƒç±»åž‹çš„ ControlNet æ··åˆä½¿ç”¨æ¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæžœï¼š\n\n1.  **Depth + Lineart**ï¼šä¿æŒç©ºé—´å…³ç³»çš„åŒæ—¶å¼ºåŒ–è½®å»“ï¼Œé€‚ç”¨äºŽå»ºç­‘ã€äº§å“ã€è§’è‰²è®¾è®¡\n2.  **Depth + Pose**ï¼šæŽ§åˆ¶äººç‰©å§¿æ€çš„åŒæ—¶ç»´æŒæ­£ç¡®çš„ç©ºé—´å…³ç³»ï¼Œé€‚ç”¨äºŽäººç‰©åœºæ™¯\n\nå…³äºŽå¤šä¸ª ControlNet æ··åˆä½¿ç”¨ï¼Œå¯ä»¥å‚è€ƒ [æ··åˆ ControlNet](https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets) ç¤ºä¾‹ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_overview",
  "markdown": "# Javascript æ‰©å±• - ComfyUI\n\n## æ‰©å±• Comfy å®¢æˆ·ç«¯\n\nComfy å¯ä»¥é€šè¿‡æ‰©å±•æœºåˆ¶è¿›è¡Œä¿®æ”¹ã€‚è¦æ·»åŠ ä¸€ä¸ªæ‰©å±•ï¼Œä½ éœ€è¦ï¼š\n\n*   ä»Žä½ çš„ Python æ¨¡å—ä¸­å¯¼å‡º `WEB_DIRECTORY`ï¼Œ\n*   å°†ä¸€ä¸ªæˆ–å¤šä¸ª `.js` æ–‡ä»¶æ”¾å…¥è¯¥ç›®å½•ï¼Œ\n*   ä½¿ç”¨ `app.registerExtension` æ³¨å†Œä½ çš„æ‰©å±•ã€‚\n\nä¸‹é¢æ˜¯è¿™ä¸‰ä¸ªæ­¥éª¤ã€‚äº†è§£å¦‚ä½•æ·»åŠ æ‰©å±•åŽï¼Œå¯ä»¥æŸ¥é˜…å¯ç”¨çš„[é’©å­](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_hooks)ä»¥è®©ä½ çš„ä»£ç è¢«è°ƒç”¨ï¼Œ ä¹Ÿå¯ä»¥äº†è§£ä½ å¯èƒ½éœ€è¦çš„å„ç§ [Comfy å¯¹è±¡](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_objects_and_hijacking)ï¼Œ æˆ–è€…ç›´æŽ¥è·³è½¬åˆ°ä¸€äº›[ç¤ºä¾‹ä»£ç ç‰‡æ®µ](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_examples)ã€‚\n\n### å¯¼å‡º `WEB_DIRECTORY`\n\nå¯ä»¥é€šè¿‡åœ¨ä½ çš„è‡ªå®šä¹‰èŠ‚ç‚¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªå­ç›®å½•ï¼ˆé€šå¸¸å« `js`ï¼‰ï¼Œå¹¶å¯¼å‡º `WEB_DIRECTORY` æ¥æ‰©å±• Comfy ç½‘é¡µå®¢æˆ·ç«¯â€”â€”æ‰€ä»¥ä½ çš„ `__init__.py` åº”è¯¥åŒ…å«å¦‚ä¸‹å†…å®¹ï¼š\n\n```\nWEB_DIRECTORY = \"./js\"\n__all__ = [\"NODE_CLASS_MAPPINGS\", \"NODE_DISPLAY_NAME_MAPPINGS\", \"WEB_DIRECTORY\"]\n```\n\n### åŒ…å« `.js` æ–‡ä»¶\n\nåªæœ‰ `.js` æ–‡ä»¶ä¼šè¢«æ·»åŠ åˆ°ç½‘é¡µã€‚å…¶ä»–èµ„æºï¼ˆå¦‚ `.css` æ–‡ä»¶ï¼‰å¯ä»¥é€šè¿‡ `extensions/custom_node_subfolder/the_file.css` è®¿é—®ï¼Œå¹¶å¯é€šè¿‡ä»£ç åŠ¨æ€æ·»åŠ ã€‚\n\n### æ³¨å†Œæ‰©å±•\n\næ‰©å±•çš„åŸºæœ¬ç»“æž„æ˜¯å¯¼å…¥ä¸» Comfy `app` å¯¹è±¡ï¼Œå¹¶è°ƒç”¨ `app.registerExtension`ï¼Œ ä¼ å…¥ä¸€ä¸ªåŒ…å«å”¯ä¸€ `name` å’Œä¸€ä¸ªæˆ–å¤šä¸ªç”± Comfy é’©å­è°ƒç”¨çš„å‡½æ•°çš„å­—å…¸ã€‚ ä¸€ä¸ªå®Œæ•´ã€ç®€å•ä¸”â€œçƒ¦äººâ€çš„æ‰©å±•ç¤ºä¾‹å¦‚ä¸‹ï¼š\n\n```\nimport { app } from \"../../scripts/app.js\";\napp.registerExtension({ \n\tname: \"a.unique.name.for.a.useless.extension\",\n\tasync setup() { \n\t\talert(\"Setup complete!\")\n\t},\n})\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_hooks",
  "markdown": "# Comfy é’©å­ï¼ˆHooksï¼‰ - ComfyUI\n\n## æ‰©å±•é’©å­\n\nåœ¨ Comfy æ‰§è¡Œçš„ä¸åŒé˜¶æ®µï¼Œåº”ç”¨ä¼šè°ƒç”¨ `#invokeExtensionsAsync` æˆ– `#invokeExtensions`ï¼Œå¹¶ä¼ å…¥é’©å­çš„åç§°ã€‚ è¿™äº›æ–¹æ³•ä¼šåœ¨æ‰€æœ‰å·²æ³¨å†Œçš„æ‰©å±•ä¸Šè°ƒç”¨åŒåæ–¹æ³•ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰ï¼Œæ¯”å¦‚ä¸Šé¢ä¾‹å­ä¸­çš„ `setup`ã€‚ Comfy æä¾›äº†å¤šç§é’©å­ï¼Œä¾›è‡ªå®šä¹‰æ‰©å±•ä»£ç ä½¿ç”¨ï¼Œä»¥ä¿®æ”¹å®¢æˆ·ç«¯è¡Œä¸ºã€‚\n\nä¸‹é¢ä»‹ç»äº†ä¸€äº›æœ€é‡è¦çš„é’©å­ã€‚ç”±äºŽ Comfy ä»åœ¨ç§¯æžå¼€å‘ä¸­ï¼Œæ–°çš„é’©å­ä¼šä¸æ—¶åŠ å…¥ï¼Œå› æ­¤å¯ä»¥åœ¨ `app.js` ä¸­æœç´¢ `#invokeExtensions` ä»¥æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨é’©å­ã€‚ å¦è¯·å‚é˜…[é’©å­çš„è°ƒç”¨é¡ºåº](#call-sequences)ã€‚\n\n### å¸¸ç”¨é’©å­\n\nä»Ž `beforeRegisterNodeDef` å¼€å§‹ï¼Œå¤§å¤šæ•°æ‰©å±•éƒ½ä¼šç”¨åˆ°å®ƒï¼Œé€šå¸¸ä¹Ÿæ˜¯å”¯ä¸€éœ€è¦çš„é’©å­ã€‚\n\n#### beforeRegisterNodeDef()\n\nå¯¹æ¯ç§èŠ‚ç‚¹ç±»åž‹ï¼ˆå³ `AddNode` èœå•ä¸­å¯ç”¨çš„èŠ‚ç‚¹åˆ—è¡¨ï¼‰è°ƒç”¨ä¸€æ¬¡ï¼Œç”¨äºŽä¿®æ”¹èŠ‚ç‚¹çš„è¡Œä¸ºã€‚\n\n```\nasync beforeRegisterNodeDef(nodeType, nodeData, app) \n```\n\nä¼ å…¥çš„ `nodeType` å‚æ•°æœ¬è´¨ä¸Šæ˜¯è¯¥ç±»åž‹æ‰€æœ‰èŠ‚ç‚¹çš„æ¨¡æ¿ï¼Œå› æ­¤å¯¹ `nodeType.prototype` çš„ä¿®æ”¹ä¼šåº”ç”¨åˆ°æ‰€æœ‰è¯¥ç±»åž‹èŠ‚ç‚¹ä¸Šã€‚`nodeData` å°è£…äº† Python ä»£ç ä¸­å®šä¹‰çš„èŠ‚ç‚¹ç›¸å…³ä¿¡æ¯ï¼Œå¦‚ç±»åˆ«ã€è¾“å…¥å’Œè¾“å‡ºã€‚`app` æ˜¯ä¸» Comfy app å¯¹è±¡çš„å¼•ç”¨ï¼ˆä½ åº”è¯¥å·²ç»å¯¼å…¥äº†å®ƒï¼ï¼‰\n\nå¸¸è§åšæ³•æ˜¯æ£€æŸ¥ `nodeType.ComfyClass`ï¼Œå®ƒä¿å­˜äº†è¯¥èŠ‚ç‚¹å¯¹åº”çš„ Python ç±»åï¼Œä»¥åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿®æ”¹è¯¥èŠ‚ç‚¹ã€‚é€šå¸¸è¿™æ„å‘³ç€åªä¿®æ”¹ä½ è‡ªå·±æ·»åŠ çš„è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Œä½†æœ‰æ—¶ä¹Ÿå¯èƒ½éœ€è¦ä¿®æ”¹å…¶ä»–èŠ‚ç‚¹ï¼ˆæˆ–å…¶ä»–è‡ªå®šä¹‰èŠ‚ç‚¹ä¹Ÿå¯èƒ½ä¿®æ”¹ä½ çš„èŠ‚ç‚¹ï¼ï¼‰ï¼Œæ­¤æ—¶è¦æ³¨æ„å…¼å®¹æ€§ã€‚\n\nåœ¨ `beforeRegisterNodeDef` ä¸­éžå¸¸å¸¸è§çš„åšæ³•æ˜¯â€åŠ«æŒâ€å·²æœ‰æ–¹æ³•ï¼š\n\n```\nasync beforeRegisterNodeDef(nodeType, nodeData, app) {\n\tif (nodeType.comfyClass==\"MyNodeClass\") { \n\t\tconst onConnectionsChange = nodeType.prototype.onConnectionsChange;\n\t\tnodeType.prototype.onConnectionsChange = function (side,slot,connect,link_info,output) {     \n\t\t\tconst r = onConnectionsChange?.apply(this, arguments);   \n\t\t\tconsole.log(\"Someone changed my connection!\");\n\t\t\treturn r;\n\t\t}\n\t}\n}\n```\n\nè¿™ç§åšæ³•æ˜¯å…ˆä¿å­˜åŽŸåž‹ä¸Šçš„åŽŸæ–¹æ³•ï¼Œç„¶åŽæ›¿æ¢ä¸ºæ–°æ–¹æ³•ã€‚æ–°æ–¹æ³•ä¼šè°ƒç”¨åŽŸæ–¹æ³•ï¼ˆ`?.apply` ä¿è¯å³ä½¿æ²¡æœ‰åŽŸæ–¹æ³•ä¹Ÿä¸ä¼šå‡ºé”™ï¼‰ï¼Œç„¶åŽæ‰§è¡Œé¢å¤–æ“ä½œã€‚æ ¹æ®ä½ çš„ä»£ç é€»è¾‘ï¼Œå¯èƒ½éœ€è¦åœ¨æ–°æ–¹æ³•çš„å…¶ä»–ä½ç½®è°ƒç”¨ `apply`ï¼Œç”šè‡³æœ‰æ¡ä»¶åœ°è°ƒç”¨ã€‚ ä»¥è¿™ç§æ–¹å¼åŠ«æŒæ–¹æ³•æ—¶ï¼Œå»ºè®®æŸ¥çœ‹æ ¸å¿ƒ comfy ä»£ç ï¼ˆæ–­ç‚¹è°ƒè¯•å¾ˆæœ‰ç”¨ï¼‰ï¼Œä»¥ç¡®ä¿æ–¹æ³•ç­¾åä¸€è‡´ã€‚\n\n#### nodeCreated()\n\nå½“æŸä¸ªèŠ‚ç‚¹å®žä¾‹è¢«åˆ›å»ºæ—¶è°ƒç”¨ï¼ˆå°±åœ¨ `nodeType` ä¸Šçš„ `ComfyNode()` æž„é€ å‡½æ•°ç»“æŸæ—¶ï¼‰ã€‚åœ¨è¿™ä¸ªé’©å­é‡Œä½ å¯ä»¥ä¿®æ”¹èŠ‚ç‚¹çš„å…·ä½“å®žä¾‹ã€‚\n\n#### init()\n\nå½“ Comfy ç½‘é¡µè¢«åŠ è½½ï¼ˆæˆ–é‡æ–°åŠ è½½ï¼‰æ—¶è°ƒç”¨ã€‚è°ƒç”¨æ—¶æœºæ˜¯åœ¨å›¾å¯¹è±¡å·²åˆ›å»ºï¼Œä½†è¿˜æœªæ³¨å†Œæˆ–åˆ›å»ºä»»ä½•èŠ‚ç‚¹ä¹‹å‰ã€‚å¯ä»¥ç”¨æ¥åŠ«æŒ app æˆ– graphï¼ˆ`LiteGraph` å¯¹è±¡ï¼‰çš„æ–¹æ³•ï¼Œä»Žè€Œä¿®æ”¹æ ¸å¿ƒ Comfy è¡Œä¸ºã€‚è¯¦è§[Comfy å¯¹è±¡ä¸ŽåŠ«æŒ](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_objects_and_hijacking)ã€‚\n\n#### setup()\n\nåœ¨å¯åŠ¨æµç¨‹ç»“æŸæ—¶è°ƒç”¨ã€‚é€‚åˆæ·»åŠ äº‹ä»¶ç›‘å¬å™¨ï¼ˆæ— è®ºæ˜¯ Comfy äº‹ä»¶è¿˜æ˜¯ DOM äº‹ä»¶ï¼‰ï¼Œæˆ–æ·»åŠ å…¨å±€èœå•ï¼Œç›¸å…³å†…å®¹åœ¨å…¶ä»–åœ°æ–¹æœ‰è¯¦ç»†ä»‹ç»ã€‚\n\n### è°ƒç”¨é¡ºåº\n\nä»¥ä¸‹é¡ºåºæ˜¯é€šè¿‡åœ¨ Comfy `app.js` æ–‡ä»¶ä¸­æ’å…¥æ—¥å¿—ä»£ç èŽ·å¾—çš„ã€‚ä½ ä¹Ÿå¯ä»¥ç”¨ç±»ä¼¼æ–¹æ³•å¸®åŠ©ç†è§£æ‰§è¡Œæµç¨‹ã€‚\n\n```\n/* æˆªè‡³ç›®å‰å¤§çº¦åœ¨ç¬¬ 220 è¡Œï¼š */\n\t#invokeExtensions(method, ...args) {\n\t\tconsole.log(`invokeExtensions      ${method}`) // æ­¤è¡Œä¸ºæ–°å¢ž\n\t\t// ...\n\t}\n/* æˆªè‡³ç›®å‰å¤§çº¦åœ¨ç¬¬ 250 è¡Œï¼š */\n\tasync #invokeExtensionsAsync(method, ...args) {\n\t\tconsole.log(`invokeExtensionsAsync ${method}`) // æ­¤è¡Œä¸ºæ–°å¢ž\n\t\t// ...\n\t}\n```\n\n#### ç½‘é¡µåŠ è½½æ—¶\n\n```\ninvokeExtensionsAsync init\ninvokeExtensionsAsync addCustomNodeDefs\ninvokeExtensionsAsync getCustomWidgets\ninvokeExtensionsAsync beforeRegisterNodeDef    [å¤šæ¬¡é‡å¤]\ninvokeExtensionsAsync registerCustomNodes\ninvokeExtensionsAsync beforeConfigureGraph\ninvokeExtensionsAsync nodeCreated\ninvokeExtensions      loadedGraphNode\ninvokeExtensionsAsync afterConfigureGraph\ninvokeExtensionsAsync setup\n```\n\n#### åŠ è½½å·¥ä½œæµ\n\n```\ninvokeExtensionsAsync beforeConfigureGraph\ninvokeExtensionsAsync beforeRegisterNodeDef   [0ã€1 æˆ–å¤šæ¬¡]\ninvokeExtensionsAsync nodeCreated             [å¤šæ¬¡é‡å¤]\ninvokeExtensions      loadedGraphNode         [å¤šæ¬¡é‡å¤]\ninvokeExtensionsAsync afterConfigureGraph\n```\n\n#### æ·»åŠ æ–°èŠ‚ç‚¹\n\n```\ninvokeExtensionsAsync nodeCreated\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-controlnet",
  "markdown": "# ComfyUI Flux.1 ControlNet ç¤ºä¾‹ - ComfyUI\n\n ![Flux.1 Canny Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-canny-controlnet.png) ![Flux.1 Depth Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-depth-controlnet.png)\n\nFLUX.1 Canny å’Œ Depth æ˜¯ç”± [Black Forest Labs](https://blackforestlabs.ai/) æŽ¨å‡ºçš„ â€‹[FLUX.1 Tools å¥—ä»¶](https://blackforestlabs.ai/flux-1-tools/) ä¸­çš„ä¸¤ä¸ªå¼ºå¤§æ¨¡åž‹ã€‚è¿™å¥—å·¥å…·æ—¨åœ¨ä¸º FLUX.1 æ·»åŠ æŽ§åˆ¶å’Œå¼•å¯¼èƒ½åŠ›ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿä¿®æ”¹å’Œé‡æ–°åˆ›å»ºçœŸå®žæˆ–ç”Ÿæˆçš„å›¾åƒã€‚ **FLUX.1-Depth-dev** å’Œ **FLUX.1-Canny-dev** éƒ½æ˜¯ 12B å‚æ•°çš„ Rectified Flow Transformer æ¨¡åž‹ï¼Œèƒ½å¤ŸåŸºäºŽæ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒï¼ŒåŒæ—¶ä¿æŒä¸Žè¾“å…¥å›¾åƒçš„ä¸€è‡´æ€§ã€‚å…¶ä¸­ Depth ç‰ˆæœ¬é€šè¿‡æ·±åº¦å›¾æå–æŠ€æœ¯æ¥ç»´æŒæºå›¾åƒçš„ç©ºé—´ç»“æž„ï¼Œè€Œ Canny ç‰ˆæœ¬åˆ™åˆ©ç”¨è¾¹ç¼˜æ£€æµ‹æŠ€æœ¯æ¥ä¿æŒæºå›¾åƒçš„ç»“æž„ç‰¹å¾ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ ¹æ®ä¸åŒéœ€æ±‚é€‰æ‹©åˆé€‚çš„æŽ§åˆ¶æ–¹å¼ã€‚ è¿™ä¸¤ä¸ªæ¨¡åž‹éƒ½å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n\n*   é¡¶çº§çš„è¾“å‡ºè´¨é‡å’Œç»†èŠ‚è¡¨çŽ°\n*   å‡ºè‰²çš„æç¤ºéµå¾ªèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒæºå›¾åƒçš„ç»“æž„å¸ƒå±€\n*   ä½¿ç”¨å¼•å¯¼è’¸é¦æŠ€æœ¯è®­ç»ƒï¼Œæé«˜æ•ˆçŽ‡\n*   å¼€æ”¾æƒé‡ä¾›ç¤¾åŒºç ”ç©¶ä½¿ç”¨\n*   æä¾› API æŽ¥å£ï¼ˆpro ç‰ˆï¼‰å’Œå¼€æºæƒé‡ï¼ˆdev ç‰ˆï¼‰\n\næ­¤å¤–ï¼ŒBlack Forest Labs è¿˜æä¾›äº†ä»Žå®Œæ•´æ¨¡åž‹ä¸­æå–çš„ **FLUX.1-Depth-dev-lora** å’Œ **FLUX.1-Canny-dev-lora** é€‚é…å™¨ç‰ˆæœ¬ï¼Œå®ƒä»¬å¯ä»¥åº”ç”¨äºŽ FLUX.1 \\[dev\\] åŸºç¡€æ¨¡åž‹ï¼Œä»¥è¾ƒå°çš„æ–‡ä»¶ä½“ç§¯æä¾›ç±»ä¼¼çš„åŠŸèƒ½ï¼Œç‰¹åˆ«é€‚åˆèµ„æºå—é™çš„çŽ¯å¢ƒã€‚ æœ¬æ–‡å°†ä»¥åˆ†åˆ«ä»¥å®Œæ•´ç‰ˆæœ¬çš„ **FLUX.1-Canny-dev** å’Œ **FLUX.1-Depth-dev-lora** ä¸ºä¾‹ï¼Œå®ŒæˆComfyUI ä¸­ Flux ControlNet çš„å·¥ä½œæµç¤ºä¾‹ã€‚\n\n## FLUX.1-Canny-dev å®Œæ•´ç‰ˆå·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµåŠç›¸å…³ç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev.png) è¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![ComfyUI Flux.1 Canny Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev-input.png)\n\n### 2\\. æ‰‹åŠ¨æ¨¡åž‹ä¸‹è½½\n\nå®Œæ•´æ¨¡åž‹åˆ—è¡¨ï¼š\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) ï¼ˆè¯·ç¡®ä¿ä½ å·²ç»åŒæ„äº†å¯¹åº” repo çš„åè®®ï¼‰\n\næ–‡ä»¶ä¿å­˜ä½ç½®ï¼š\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ t5xxl_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ flux1-canny-dev.safetensors\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Flux.1 Canny Controlnet æ­¥éª¤æµç¨‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg)\n\n1.  ç¡®ä¿åœ¨`Load VAE`ä¸­åŠ è½½äº†`ae.safetensors`\n2.  ç¡®ä¿åœ¨`Load Diffusion Model`åŠ è½½äº†`flux1-canny-dev.safetensors`\n3.  ç¡®ä¿åœ¨`DualCLIPLoader`ä¸­ä¸‹é¢çš„æ¨¡åž‹å·²åŠ è½½ï¼š\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n4.  åœ¨`Load Image`èŠ‚ç‚¹ä¸­ä¸Šä¼ äº†æ–‡æ¡£ä¸­æä¾›çš„è¾“å…¥å›¾ç‰‡\n5.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n### 4\\. å¼€å§‹ä½ çš„å°è¯•\n\nå°è¯•ä½¿ç”¨[FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) æ¨¡åž‹å®Œæˆ Depth ç‰ˆæœ¬çš„å·¥ä½œæµ ä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥ ![ComfyUI å®¤å†…æ·±åº¦å›¾](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png) æˆ–è€…å€ŸåŠ©ä¸‹é¢è‡ªå®šä¹‰èŠ‚ç‚¹ä¸­å®Œæˆå›¾åƒé¢„å¤„ç†:\n\n*   [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n*   [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\n## FLUX.1-Depth-dev-lora å·¥ä½œæµ\n\nLoRA ç‰ˆæœ¬çš„å·¥ä½œæµæ˜¯åœ¨å®Œæ•´ç‰ˆæœ¬çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ äº† LoRA æ¨¡åž‹,ç›¸å¯¹äºŽ[å®Œæ•´ç‰ˆæœ¬çš„ Flux å·¥ä½œæµ](https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-text-to-image),å¢žåŠ äº†å¯¹åº” LoRA æ¨¡åž‹çš„åŠ è½½ä½¿ç”¨èŠ‚ç‚¹ã€‚\n\n### 1\\. å·¥ä½œæµåŠç›¸å…³ç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora.png) è¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![ComfyUI Flux.1 Depth Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora-input.png)\n\n### 2\\. æ‰‹åŠ¨æ¨¡åž‹ä¸‹è½½\n\nå®Œæ•´æ¨¡åž‹åˆ—è¡¨ï¼š\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)\n*   [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)\n\næ–‡ä»¶ä¿å­˜ä½ç½®ï¼š\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ t5xxl_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â”œâ”€â”€ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ flux1-dev.safetensors\nâ”‚   â””â”€â”€ loras/\nâ”‚       â””â”€â”€ flux1-depth-dev-lora.safetensors\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Flux.1 Depth Controlnet æ­¥éª¤æµç¨‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg)\n\n1.  ç¡®ä¿åœ¨`Load Diffusion Model`åŠ è½½äº†`flux1-dev.safetensors`\n2.  ç¡®ä¿åœ¨`LoraLoaderModelOnly`ä¸­åŠ è½½äº†`flux1-depth-dev-lora.safetensors`\n3.  ç¡®ä¿åœ¨`DualCLIPLoader`ä¸­ä¸‹é¢çš„æ¨¡åž‹å·²åŠ è½½ï¼š\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n4.  åœ¨`Load Image`èŠ‚ç‚¹ä¸­ä¸Šä¼ äº†æ–‡æ¡£ä¸­æä¾›çš„è¾“å…¥å›¾ç‰‡\n5.  ç¡®ä¿åœ¨`Load VAE`ä¸­åŠ è½½äº†`ae.safetensors`\n6.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n### 4\\. å¼€å§‹ä½ çš„å°è¯•\n\nå°è¯•ä½¿ç”¨[FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) æ¨¡åž‹å®Œæˆ Canny ç‰ˆæœ¬çš„å·¥ä½œæµ å€ŸåŠ© [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) æˆ–è€… [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) å®Œæˆå›¾åƒé¢„å¤„ç†\n\n## ç¤¾åŒºç‰ˆæœ¬ Flux Controlnets\n\nXLab å’Œ InstantX + Shakker Labs å·²ç»ä¸º Flux å‘å¸ƒäº† Controlnetã€‚ **InstantX:**\n\n*   [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)\n*   [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)\n*   [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)\n\n**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections) å°†è¿™äº›æ–‡ä»¶æ”¾åœ¨ `ComfyUI/models/controlnet` ç›®å½•ä¸‹ã€‚ ä½ å¯ä»¥è®¿é—®[Flux Controlnet ç¤ºä¾‹](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png)æ¥èŽ·å–å¯¹åº”å·¥ä½œæµå›¾ç‰‡ï¼Œå¹¶ä½¿ç”¨[è¿™é‡Œ](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png)çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_objects_and_hijacking",
  "markdown": "# Comfy å¯¹è±¡ - ComfyUI\n\n## LiteGraph\n\nComfy UI æž„å»ºäºŽ [LiteGraph](https://github.com/jagenjo/litegraph.js) ä¹‹ä¸Šã€‚ Comfy çš„è®¸å¤šåŠŸèƒ½éƒ½ç”± LiteGraph æä¾›ï¼Œå› æ­¤å¦‚æžœä½ è¦å¼€å‘æ›´å¤æ‚çš„èŠ‚ç‚¹ï¼Œå»ºè®®å…‹éš†è¯¥ä»“åº“å¹¶æŸ¥é˜…æ–‡æ¡£ï¼Œæ–‡æ¡£ä½äºŽ `doc/index.html`ã€‚\n\n## ComfyApp\n\n`app` å¯¹è±¡ï¼ˆå§‹ç»ˆå¯é€šè¿‡ `import { app } from \"../../scripts/app.js\";` èŽ·å–ï¼‰ä»£è¡¨åœ¨æµè§ˆå™¨ä¸­è¿è¡Œçš„ Comfy åº”ç”¨ï¼ŒåŒ…å«è®¸å¤šæœ‰ç”¨çš„å±žæ€§å’Œå‡½æ•°ï¼Œéƒ¨åˆ†å¦‚ä¸‹æ‰€ç¤ºã€‚\n\n### å±žæ€§\n\n`app` çš„é‡è¦å±žæ€§åŒ…æ‹¬ï¼ˆéžå®Œæ•´åˆ—è¡¨ï¼‰ï¼š\n\n| å±žæ€§  | å†…å®¹  |\n| --- | --- |\n| `canvas` | ä¸€ä¸ª LGraphCanvas å¯¹è±¡ï¼Œä»£è¡¨å½“å‰ç”¨æˆ·ç•Œé¢ã€‚åŒ…å«å¦‚ `node_over` å’Œ `selected_nodes` ç­‰æœ‰ç”¨å±žæ€§ã€‚ |\n| `canvasEl` | DOM `<canvas>` å…ƒç´  |\n| `graph` | æŒ‡å‘å½“å‰å›¾çš„ LGraph å¯¹è±¡çš„å¼•ç”¨ |\n| `runningNodeId` | æ‰§è¡ŒæœŸé—´ï¼Œå½“å‰æ­£åœ¨æ‰§è¡Œçš„èŠ‚ç‚¹ |\n| `ui` | å¯è®¿é—®éƒ¨åˆ† UI å…ƒç´ ï¼Œå¦‚é˜Ÿåˆ—ã€èœå•å’Œå¯¹è¯æ¡† |\n\n`canvas`ï¼ˆç”¨äºŽå›¾å½¢å…ƒç´ ï¼‰å’Œ `graph`ï¼ˆç”¨äºŽé€»è¾‘è¿žæŽ¥ï¼‰å¤§æ¦‚çŽ‡æ˜¯ä½ æœ€å¸¸ç”¨çš„ã€‚\n\n### å‡½æ•°\n\nåŒæ ·ï¼Œå‡½æ•°ä¹Ÿæœ‰å¾ˆå¤šã€‚ä»¥ä¸‹æ˜¯ä¸€äº›é‡è¦çš„ï¼š\n\n| å‡½æ•°  | è¯´æ˜Ž  |\n| --- | --- |\n| graphToPrompt | å°†å›¾è½¬æ¢ä¸ºå¯å‘é€åˆ° Python æœåŠ¡å™¨çš„ prompt |\n| loadGraphData | åŠ è½½ä¸€ä¸ªå›¾ |\n| queuePrompt | å°† prompt æäº¤åˆ°é˜Ÿåˆ— |\n| registerExtension | ä½ å·²ç»è§è¿‡â€”â€”ç”¨äºŽæ·»åŠ æ‰©å±• |\n\n## LGraph\n\n`LGraph` å¯¹è±¡æ˜¯ LiteGraph æ¡†æž¶çš„ä¸€éƒ¨åˆ†ï¼Œä»£è¡¨å½“å‰å›¾çš„é€»è¾‘çŠ¶æ€ï¼ˆèŠ‚ç‚¹å’Œè¿žçº¿ï¼‰ã€‚ å¦‚æžœä½ æƒ³æ“ä½œå›¾ï¼ŒLiteGraph æ–‡æ¡£ï¼ˆå…‹éš† `https://github.com/jagenjo/litegraph.js` åŽåœ¨ `doc/index.html`ï¼‰æè¿°äº†ä½ éœ€è¦çš„å‡½æ•°ã€‚ ä½ å¯ä»¥ç”¨ `graph` èŽ·å–èŠ‚ç‚¹å’Œè¿žçº¿çš„è¯¦ç»†ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š\n\n```\nconst ComfyNode_object_for_my_node = app.graph._nodes_by_id(my_node_id) \nComfyNode_object_for_my_node.inputs.forEach(input => {\n    const link_id = input.link;\n    if (link_id) {\n        const LLink_object = app.graph.links[link_id]\n        const id_of_upstream_node = LLink_object.origin_id\n        // ç­‰ç­‰\n    }\n});\n```\n\n## LLink\n\n`LLink` å¯¹è±¡å¯é€šè¿‡ `graph.links` è®¿é—®ï¼Œä»£è¡¨å›¾ä¸­ä¸€æ¡ä»ŽèŠ‚ç‚¹ `link.origin_id` çš„è¾“å‡ºæ§½ `link.origin_slot` åˆ°èŠ‚ç‚¹ `link.target_id` çš„è¾“å…¥æ§½ `link.target_slot` çš„è¿žçº¿ã€‚å®ƒè¿˜æœ‰ä¸€ä¸ªå­—ç¬¦ä¸²ç±»åž‹ `link.type` å’Œ `link.id`ã€‚ `LLink` æ˜¯åœ¨ `LGraphNode`ï¼ˆ`ComfyNode` æ˜¯å…¶å­ç±»ï¼‰çš„ `connect` æ–¹æ³•ä¸­åˆ›å»ºçš„ã€‚\n\n## ComfyNode\n\n`ComfyNode` æ˜¯ `LGraphNode` çš„å­ç±»ï¼Œå› æ­¤ LiteGraph æ–‡æ¡£å¯¹äºŽé€šç”¨æ“ä½œä¹Ÿå¾ˆæœ‰å¸®åŠ©ã€‚ä¸è¿‡ï¼ŒComfy å¯¹ LiteGraph çš„æ ¸å¿ƒè¡Œä¸ºåšäº†å¤§é‡æ‰©å±•ï¼Œä¹Ÿæ²¡æœ‰ç”¨åˆ° LiteGraph çš„å…¨éƒ¨åŠŸèƒ½ã€‚\n\n`ComfyNode` å¯¹è±¡ä»£è¡¨å½“å‰å·¥ä½œæµä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚å®ƒæœ‰è®¸å¤šé‡è¦å±žæ€§å’Œå¤§é‡å¯ç”¨æˆ–å¯åŠ«æŒçš„å‡½æ•°ï¼Œç”¨äºŽä¿®æ”¹è¡Œä¸ºã€‚ ä¸ºäº†æ›´å…¨é¢åœ°äº†è§£èŠ‚ç‚¹å¯¹è±¡ï¼Œä½ å¯ä»¥åœ¨æ‰©å±•ä¸­æ’å…¥å¦‚ä¸‹ä»£ç ï¼Œå¹¶åœ¨ `console.log` å¤„æ‰“æ–­ç‚¹ã€‚åˆ›å»ºæ–°èŠ‚ç‚¹æ—¶å³å¯ç”¨è°ƒè¯•å™¨æŸ¥çœ‹èŠ‚ç‚¹ã€‚\n\n```\nasync nodeCreated(node) {\n    console.log(\"nodeCreated\")\n}\n```\n\n### å±žæ€§\n\n| å±žæ€§  | å†…å®¹  |\n| --- | --- |\n| `bgcolor` | èŠ‚ç‚¹çš„èƒŒæ™¯è‰²ï¼Œé»˜è®¤ undefined |\n| `comfyClass` | èŠ‚ç‚¹å¯¹åº”çš„ Python ç±» |\n| `flags` | åŒ…å«èŠ‚ç‚¹çŠ¶æ€ç›¸å…³æ ‡å¿—çš„å­—å…¸ã€‚ç‰¹åˆ«æ˜¯ `flags.collapsed` è¡¨ç¤ºèŠ‚ç‚¹æ˜¯å¦æŠ˜å ã€‚ |\n| `graph` | æŒ‡å‘ LGraph å¯¹è±¡çš„å¼•ç”¨ |\n| `id` | å”¯ä¸€ id |\n| `input_type` | è¾“å…¥ç±»åž‹åˆ—è¡¨ï¼ˆå¦‚ â€œSTRINGâ€ã€â€œMODELâ€ã€â€œCLIPâ€ ç­‰ï¼‰ã€‚é€šå¸¸ä¸Ž Python çš„ INPUT\\_TYPES åŒ¹é… |\n| `inputs` | è¾“å…¥åˆ—è¡¨ï¼ˆè§ä¸‹æ–‡ï¼‰ |\n| `mode` | é€šå¸¸ä¸º 0ï¼Œé™éŸ³ä¸º 2ï¼Œæ—è·¯ä¸º 4ã€‚1 å’Œ 3 æœªè¢« Comfy ä½¿ç”¨ |\n| `order` | èŠ‚ç‚¹çš„æ‰§è¡Œé¡ºåºã€‚ç”± `LGraph.computeExecutionOrder()` åœ¨æäº¤ prompt æ—¶è®¾ç½® |\n| `pos` | èŠ‚ç‚¹åœ¨ç”»å¸ƒä¸Šçš„ \\[x,y\\] ä½ç½® |\n| `properties` | åŒ…å« â€œNode name for S&Râ€ çš„å­—å…¸ï¼Œç”± LiteGraph ä½¿ç”¨ |\n| `properties_info` | `properties` ä¸­æ¡ç›®çš„ç±»åž‹å’Œé»˜è®¤å€¼ |\n| `size` | èŠ‚ç‚¹åœ¨ç”»å¸ƒä¸Šçš„å®½é«˜ |\n| `title` | æ˜¾ç¤ºæ ‡é¢˜ |\n| `type` | èŠ‚ç‚¹ç±»çš„å”¯ä¸€åç§°ï¼ˆæ¥è‡ª Pythonï¼‰ |\n| `widgets` | å°éƒ¨ä»¶åˆ—è¡¨ï¼ˆè§ä¸‹æ–‡ï¼‰ |\n| `widgets_values` | å°éƒ¨ä»¶çš„å½“å‰å€¼åˆ—è¡¨ |\n\n### å‡½æ•°\n\nå‡½æ•°éžå¸¸å¤šï¼ˆä¸Šæ¬¡ç»Ÿè®¡æœ‰ 85 ä¸ªï¼‰ã€‚ä»¥ä¸‹æ˜¯éƒ¨åˆ†å¸¸ç”¨å‡½æ•°ã€‚ å¤§å¤šæ•°å‡½æ•°æœªè¢« Comfy ä¿®æ”¹ï¼Œä»ä¸º LiteGraph æ ¸å¿ƒä»£ç ã€‚\n\n#### è¾“å…¥ã€è¾“å‡ºã€å°éƒ¨ä»¶\n\n| å‡½æ•°  | è¯´æ˜Ž  |\n| --- | --- |\n| Inputs / Outputs | å¤§å¤šæ•°æœ‰åŒåçš„è¾“å‡ºæ–¹æ³•ï¼šs/In/Out/ |\n| `addInput` | åˆ›å»ºæ–°è¾“å…¥ï¼Œéœ€æŒ‡å®šåç§°å’Œç±»åž‹ |\n| `addInputs` | `addInput` çš„æ•°ç»„ç‰ˆæœ¬ |\n| `findInputSlot` | é€šè¿‡è¾“å…¥åæŸ¥æ‰¾æ§½ç´¢å¼• |\n| `findInputSlotByType` | æŒ‰ç±»åž‹æŸ¥æ‰¾è¾“å…¥ã€‚å¯é€‰å‚æ•°ä¼˜å…ˆæˆ–ä»…ä½¿ç”¨ç©ºé—²æ§½ |\n| `removeInput` | æŒ‰æ§½ç´¢å¼•ç§»é™¤è¾“å…¥ |\n| `getInputNode` | èŽ·å–è¿žæŽ¥åˆ°è¯¥è¾“å…¥çš„èŠ‚ç‚¹ã€‚è¾“å‡ºç­‰ä»·æ–¹æ³•ä¸º `getOutputNodes`ï¼Œè¿”å›žåˆ—è¡¨ |\n| `getInputLink` | èŽ·å–è¿žæŽ¥åˆ°è¯¥è¾“å…¥çš„ LLinkã€‚æ— è¾“å‡ºç­‰ä»·æ–¹æ³• |\n| Widgets |     |\n| `addWidget` | æ·»åŠ æ ‡å‡† Comfy å°éƒ¨ä»¶ |\n| `addCustomWidget` | æ·»åŠ è‡ªå®šä¹‰å°éƒ¨ä»¶ï¼ˆåœ¨ `getComfyWidgets` é’©å­ä¸­å®šä¹‰ï¼‰ |\n| `addDOMWidget` | æ·»åŠ ç”± DOM å…ƒç´ å®šä¹‰çš„å°éƒ¨ä»¶ |\n| `convertWidgetToInput` | å¦‚æžœ `isConvertableWidget` å…è®¸ï¼Œå°†å°éƒ¨ä»¶è½¬ä¸ºè¾“å…¥ï¼ˆè§ `widgetInputs.js`ï¼‰ |\n\n#### è¿žæŽ¥\n\n| å‡½æ•°  | è¯´æ˜Ž  |\n| --- | --- |\n| `connect` | å°†æœ¬èŠ‚ç‚¹è¾“å‡ºè¿žæŽ¥åˆ°å…¶ä»–èŠ‚ç‚¹è¾“å…¥ |\n| `connectByType` | æŒ‰ç±»åž‹å°†è¾“å‡ºè¿žæŽ¥åˆ°å…¶ä»–èŠ‚ç‚¹â€”â€”è¿žæŽ¥åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„åŒ¹é…æ§½ |\n| `connectByTypeOutput` | æŒ‰ç±»åž‹å°†è¾“å…¥è¿žæŽ¥åˆ°å…¶ä»–èŠ‚ç‚¹è¾“å‡º |\n| `disconnectInput` | ç§»é™¤è¾“å…¥ï¼ˆæŒ‰åç§°æˆ–ç´¢å¼•ï¼‰ä¸Šçš„æ‰€æœ‰è¿žçº¿ |\n| `disconnectOutput` | æ–­å¼€è¾“å‡ºä¸ŽæŒ‡å®šèŠ‚ç‚¹è¾“å…¥çš„è¿žæŽ¥ |\n| `onConnectionChange` | æ¯ä¸ªèŠ‚ç‚¹éƒ½ä¼šè°ƒç”¨ã€‚`side==1` è¡¨ç¤ºæ˜¯æœ¬èŠ‚ç‚¹çš„è¾“å…¥ |\n| `onConnectInput` | åœ¨å»ºç«‹è¿žæŽ¥å‰è°ƒç”¨ã€‚å¦‚æžœè¿”å›ž `false`ï¼Œåˆ™æ‹’ç»è¿žæŽ¥ |\n\n#### æ˜¾ç¤º\n\n| å‡½æ•°  | è¯´æ˜Ž  |\n| --- | --- |\n| `setDirtyCanvas` | æŒ‡å®šå‰æ™¯ï¼ˆèŠ‚ç‚¹ï¼‰å’Œ/æˆ–èƒŒæ™¯ï¼ˆè¿žçº¿å’Œå›¾åƒï¼‰éœ€è¦é‡ç»˜ |\n| `onDrawBackground` | ç”¨ `CanvasRenderingContext2D` å¯¹è±¡ç»˜åˆ¶èƒŒæ™¯ã€‚Comfy ç”¨äºŽæ¸²æŸ“å›¾åƒ |\n| `onDrawForeground` | ç”¨ `CanvasRenderingContext2D` å¯¹è±¡ç»˜åˆ¶èŠ‚ç‚¹ |\n| `getTitle` | è¦æ˜¾ç¤ºçš„æ ‡é¢˜ |\n| `collapse` | åˆ‡æ¢èŠ‚ç‚¹æŠ˜å çŠ¶æ€ |\n\n#### å…¶ä»–\n\n| å‡½æ•°  | è¯´æ˜Ž  |\n| --- | --- |\n| `changeMode` | ç”¨äºŽè®¾ç½®èŠ‚ç‚¹ä¸ºæ—è·¯ï¼ˆ`mode == 4`ï¼‰æˆ–éžæ—è·¯ï¼ˆ`mode == 0`ï¼‰ |\n\n## è¾“å…¥ä¸Žå°éƒ¨ä»¶\n\nè¾“å…¥å’Œå°éƒ¨ä»¶æ˜¯å‘èŠ‚ç‚¹è¾“å…¥æ•°æ®çš„ä¸¤ç§æ–¹å¼ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå°éƒ¨ä»¶å¯ä»¥è½¬ä¸ºè¾“å…¥ï¼Œä½†å¹¶éžæ‰€æœ‰è¾“å…¥éƒ½èƒ½è½¬ä¸ºå°éƒ¨ä»¶ï¼ˆè®¸å¤šæ•°æ®ç±»åž‹æ— æ³•é€šè¿‡ UI å…ƒç´ è¾“å…¥ï¼‰ã€‚ `node.inputs` æ˜¯å½“å‰æ‰€æœ‰è¾“å…¥çš„åˆ—è¡¨ï¼ˆèŠ‚ç‚¹å·¦ä¾§çš„å½©è‰²åœ†ç‚¹ï¼‰ï¼ŒåŒ…å« `.name`ã€`.type` å’Œ `.link`ï¼ˆæŒ‡å‘ `app.graph.links` ä¸­çš„ LLinkï¼‰ã€‚ å¦‚æžœè¾“å…¥æ˜¯å·²è½¬æ¢çš„å°éƒ¨ä»¶ï¼Œè¿˜ä¼šåœ¨ `.widget` ä¸­ä¿å­˜å¯¹è¯¥å°éƒ¨ä»¶ï¼ˆçŽ°å·²å¤±æ•ˆï¼‰çš„å¼•ç”¨ã€‚ `node.widgets` æ˜¯æ‰€æœ‰å°éƒ¨ä»¶çš„åˆ—è¡¨ï¼Œæ— è®ºæ˜¯å¦å·²è½¬ä¸ºè¾“å…¥ã€‚å°éƒ¨ä»¶æœ‰ï¼š\n\n| å±žæ€§/å‡½æ•° | è¯´æ˜Ž  |\n| --- | --- |\n| `callback` | å°éƒ¨ä»¶å€¼å˜åŒ–æ—¶è°ƒç”¨çš„å‡½æ•° |\n| `last_y` | å°éƒ¨ä»¶åœ¨èŠ‚ç‚¹ä¸­çš„åž‚ç›´ä½ç½® |\n| `name` | å°éƒ¨ä»¶åç§°ï¼ˆèŠ‚ç‚¹å†…å”¯ä¸€ï¼‰ |\n| `options` | Python ä»£ç ä¸­æŒ‡å®šçš„é€‰é¡¹ï¼ˆå¦‚é»˜è®¤å€¼ã€æœ€å°å€¼ã€æœ€å¤§å€¼ï¼‰ |\n| `type` | å°éƒ¨ä»¶ç±»åž‹åç§°ï¼ˆè§ä¸‹æ–‡ï¼‰ï¼Œå°å†™ |\n| `value` | å½“å‰å°éƒ¨ä»¶å€¼ã€‚æ­¤å±žæ€§æœ‰ get/set æ–¹æ³• |\n\n### å°éƒ¨ä»¶ç±»åž‹\n\n`app.widgets` æ˜¯å½“å‰å·²æ³¨å†Œå°éƒ¨ä»¶ç±»åž‹çš„å­—å…¸ï¼Œé”®ä¸ºç±»åž‹åçš„å¤§å†™ã€‚ Comfy å†…ç½®å°éƒ¨ä»¶ç±»åž‹åŒ…æ‹¬ç›´è§‚çš„ `BOOLEAN`ã€`INT`ã€`FLOAT`ï¼Œ è¿˜æœ‰ `STRING`ï¼ˆåˆ†å•è¡Œå’Œå¤šè¡Œï¼‰ã€ `COMBO`ï¼ˆä¸‹æ‹‰åˆ—è¡¨é€‰æ‹©ï¼‰ã€`IMAGEUPLOAD`ï¼ˆç”¨äºŽåŠ è½½å›¾ç‰‡èŠ‚ç‚¹ï¼‰ã€‚ å¯é€šè¿‡åœ¨æ‰©å±•ä¸­æä¾› `getCustomWidgets` æ–¹æ³•æ·»åŠ è‡ªå®šä¹‰å°éƒ¨ä»¶ç±»åž‹ã€‚\n\n### å…³è”å°éƒ¨ä»¶\n\nå°éƒ¨ä»¶ä¹Ÿå¯ä»¥å…³è”â€”â€”å¦‚å†…ç½®çš„ `seed` å’Œ `control_after_generate`ã€‚ å…³è”å°éƒ¨ä»¶çš„ `.type = 'base_widget_type:base_widget_name'`ï¼›å¦‚ `control_after_generate` å¯èƒ½æœ‰ç±»åž‹ `int:seed`ã€‚\n\n## Prompt\n\nå½“ä½ åœ¨ Comfy ä¸­ç‚¹å‡»â€Queue Promptâ€æŒ‰é’®æ—¶ï¼Œä¼šè°ƒç”¨ `app.graphToPrompt()` æ–¹æ³•ï¼Œå°†å½“å‰å›¾è½¬æ¢ä¸ºå¯å‘é€åˆ°æœåŠ¡å™¨çš„ promptã€‚ `app.graphToPrompt` è¿”å›žä¸€ä¸ªå¯¹è±¡ï¼ˆä¸‹ç§° `prompt`ï¼‰ï¼ŒåŒ…å« `output` å’Œ `workflow` ä¸¤ä¸ªå±žæ€§ã€‚\n\n### output\n\n`prompt.output` å°†å›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„ `node_id` æ˜ å°„ä¸ºä¸€ä¸ªå¯¹è±¡ï¼ŒåŒ…å«ä¸¤ä¸ªå±žæ€§ï¼š\n\n*   `prompt.output[node_id].class_type`ï¼Œè‡ªå®šä¹‰èŠ‚ç‚¹ç±»çš„å”¯ä¸€åç§°ï¼ˆåœ¨ Python ä»£ç ä¸­å®šä¹‰ï¼‰\n*   `prompt.output[node_id].inputs`ï¼ŒåŒ…å«æ¯ä¸ªè¾“å…¥ï¼ˆæˆ–å°éƒ¨ä»¶ï¼‰çš„å€¼ï¼Œæ˜¯ä¸€ä¸ªä»Žè¾“å…¥ååˆ°ä»¥ä¸‹å†…å®¹çš„æ˜ å°„ï¼š\n    *   å¦‚æžœæ˜¯å°éƒ¨ä»¶ï¼Œåˆ™ä¸ºé€‰ä¸­çš„å€¼\n    *   å¦‚æžœæœ‰è¿žçº¿ï¼Œåˆ™ä¸ºä¸€ä¸ªæ•°ç»„ï¼Œå†…å®¹ä¸ºï¼ˆ`upstream_node_id`, `upstream_node_output_slot`ï¼‰\n    *   å¦‚æžœæ˜¯å·²è½¬ä¸ºè¾“å…¥ä½†æœªè¿žæŽ¥çš„å°éƒ¨ä»¶ï¼Œåˆ™ä¸º undefined\n    *   å…¶ä»–æœªè¿žæŽ¥çš„è¾“å…¥ä¸ä¼šå‡ºçŽ°åœ¨ `.inputs` ä¸­\n\n### workflow\n\n`prompt.workflow` åŒ…å«ä»¥ä¸‹å±žæ€§ï¼š\n\n*   `config` - é¢å¤–é…ç½®é¡¹å­—å…¸ï¼ˆé»˜è®¤ç©ºï¼‰\n*   `extra` - åŒ…å«å·¥ä½œæµé¢å¤–ä¿¡æ¯çš„å­—å…¸ã€‚é»˜è®¤æœ‰ï¼š\n    *   `extra.ds` - æè¿°å½“å‰å›¾è§†å›¾ï¼ˆ`scale` å’Œ `offset`ï¼‰\n*   `groups` - å·¥ä½œæµä¸­çš„æ‰€æœ‰åˆ†ç»„\n*   `last_link_id` - æœ€åŽæ·»åŠ çš„è¿žçº¿ id\n*   `last_node_id` - æœ€åŽæ·»åŠ çš„èŠ‚ç‚¹ id\n*   `links` - å›¾ä¸­æ‰€æœ‰è¿žçº¿çš„åˆ—è¡¨ã€‚æ¯é¡¹ä¸ºäº”ä¸ªæ•´æ•°å’Œä¸€ä¸ªå­—ç¬¦ä¸²çš„æ•°ç»„ï¼š\n    *   (`link_id`, `upstream_node_id`, `upstream_node_output_slot`, `downstream_node_id`, `downstream_node_input_slot`, `data type`)\n*   `nodes` - å›¾ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„åˆ—è¡¨ã€‚æ¯é¡¹ä¸ºèŠ‚ç‚¹éƒ¨åˆ†å±žæ€§çš„æ˜ å°„ï¼Œè§[ä¸Šæ–‡](#comfynode)\n    *   åŒ…å«å±žæ€§ï¼š`flags`ã€`id`ã€`inputs`ã€`mode`ã€`order`ã€`pos`ã€`properties`ã€`size`ã€`type`ã€`widgets_values`\n    *   å¦å¤–ï¼Œé™¤éžèŠ‚ç‚¹æ²¡æœ‰è¾“å‡ºï¼Œè¿˜ä¼šæœ‰ `outputs` å±žæ€§ï¼Œä¸ºè¯¥èŠ‚ç‚¹æ‰€æœ‰è¾“å‡ºçš„åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«ï¼š\n        *   `name` - è¾“å‡ºåç§°\n        *   `type` - è¾“å‡ºæ•°æ®ç±»åž‹\n        *   `links` - ä»Žè¯¥è¾“å‡ºå‡ºå‘çš„æ‰€æœ‰è¿žçº¿çš„ `link_id` åˆ—è¡¨ï¼ˆæ— è¿žæŽ¥æ—¶ä¸ºç©ºæ•°ç»„æˆ– nullï¼‰\n        *   `shape` - ç»˜åˆ¶è¾“å‡ºæ—¶çš„å½¢çŠ¶ï¼ˆé»˜è®¤ 3ï¼Œè¡¨ç¤ºåœ†ç‚¹ï¼‰\n        *   `slot_index` - è¾“å‡ºçš„æ§½ç¼–å·\n*   `version` - LiteGraph ç‰ˆæœ¬å·ï¼ˆå½“å‰ä¸º `0.4`ï¼‰"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_toast",
  "markdown": "# Toast API - ComfyUI\n\nToast API æä¾›äº†ä¸€ç§å‘ç”¨æˆ·æ˜¾ç¤ºéžé˜»å¡žé€šçŸ¥æ¶ˆæ¯çš„æ–¹å¼ã€‚è¿™å¯¹äºŽåœ¨ä¸æ‰“æ–­å·¥ä½œæµçš„æƒ…å†µä¸‹æä¾›åé¦ˆéžå¸¸æœ‰ç”¨ã€‚\n\n## åŸºæœ¬ç”¨æ³•\n\n### ç®€å• Toast\n\n```\n// æ˜¾ç¤ºä¸€ä¸ªç®€å•çš„ä¿¡æ¯æç¤º\napp.extensionManager.toast.add({\n  severity: \"info\",\n  summary: \"ä¿¡æ¯\",\n  detail: \"æ“ä½œå·²æˆåŠŸå®Œæˆ\",\n  life: 3000\n});\n```\n\n### Toast ç±»åž‹\n\n```\n// æˆåŠŸæç¤º\napp.extensionManager.toast.add({\n  severity: \"success\",\n  summary: \"æˆåŠŸ\",\n  detail: \"æ•°æ®ä¿å­˜æˆåŠŸ\",\n  life: 3000\n});\n\n// è­¦å‘Šæç¤º\napp.extensionManager.toast.add({\n  severity: \"warn\",\n  summary: \"è­¦å‘Š\",\n  detail: \"æ­¤æ“ä½œå¯èƒ½å¯¼è‡´é—®é¢˜\",\n  life: 5000\n});\n\n// é”™è¯¯æç¤º\napp.extensionManager.toast.add({\n  severity: \"error\",\n  summary: \"é”™è¯¯\",\n  detail: \"è¯·æ±‚å¤„ç†å¤±è´¥\",\n  life: 5000\n});\n```\n\n### Alert è¾…åŠ©æ–¹æ³•\n\n```\n// å¿«æ·æ–¹å¼åˆ›å»ºè­¦å‘Šæç¤º\napp.extensionManager.toast.addAlert(\"è¿™æ˜¯ä¸€æ¡é‡è¦æ¶ˆæ¯\");\n```\n\n### Toast æ¶ˆæ¯\n\n```\napp.extensionManager.toast.add({\n  severity?: \"success\" | \"info\" | \"warn\" | \"error\" | \"secondary\" | \"contrast\", // æ¶ˆæ¯ä¸¥é‡çº§åˆ«ï¼ˆé»˜è®¤ï¼š\"info\"ï¼‰\n  summary?: string,         // Toast çš„ç®€çŸ­æ ‡é¢˜\n  detail?: any,             // è¯¦ç»†æ¶ˆæ¯å†…å®¹\n  closable?: boolean,       // ç”¨æˆ·æ˜¯å¦å¯ä»¥å…³é—­è¯¥æç¤ºï¼ˆé»˜è®¤ï¼štrueï¼‰\n  life?: number,            // è‡ªåŠ¨å…³é—­å‰çš„æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰\n  group?: string,           // ç”¨äºŽç®¡ç†ç›¸å…³ Toast çš„åˆ†ç»„æ ‡è¯†\n  styleClass?: any,         // æ¶ˆæ¯çš„æ ·å¼ç±»\n  contentStyleClass?: any   // å†…å®¹çš„æ ·å¼ç±»\n});\n```\n\n### Alert è¾…åŠ©æ–¹æ³•\n\n```\napp.extensionManager.toast.addAlert(message: string);\n```\n\n### å…¶ä»–æ–¹æ³•\n\n```\n// ç§»é™¤æŒ‡å®šçš„ toast\napp.extensionManager.toast.remove(toastMessage);\n\n// ç§»é™¤æ‰€æœ‰ toast\napp.extensionManager.toast.removeAll();\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_settings",
  "markdown": "# è®¾ç½® - ComfyUI\n\nä½ å¯ä»¥ä¸º ComfyUI æä¾›ä¸€ä¸ªè®¾ç½®å¯¹è±¡ï¼Œè¿™äº›è®¾ç½®ä¼šæ˜¾ç¤ºåœ¨ç”¨æˆ·æ‰“å¼€ ComfyUI è®¾ç½®é¢æ¿æ—¶ã€‚\n\n## åŸºæœ¬æ“ä½œ\n\n### æ·»åŠ ä¸€ä¸ªè®¾ç½®é¡¹\n\n```\nimport { app } from \"../../scripts/app.js\";\n\napp.registerExtension({\n    name: \"My Extension\",\n    settings: [\n        {\n            id: \"example.boolean\",\n            name: \"ç¤ºä¾‹å¸ƒå°”è®¾ç½®\",\n            type: \"boolean\",\n            defaultValue: false,\n        },\n    ],\n});\n```\n\n`id` å¿…é¡»åœ¨æ‰€æœ‰æ‰©å±•ä¸­å”¯ä¸€ï¼Œå¹¶å°†ç”¨äºŽèŽ·å–è®¾ç½®å€¼ã€‚ å¦‚æžœä½ æ²¡æœ‰[æŒ‡å®šåˆ†ç±»](#categories)ï¼Œé‚£ä¹ˆ `id` ä¼šé€šè¿‡ `.` åˆ†å‰²æ¥å†³å®šå®ƒåœ¨è®¾ç½®é¢æ¿ä¸­çš„æ˜¾ç¤ºä½ç½®ã€‚\n\n*   å¦‚æžœä½ çš„ `id` ä¸åŒ…å« `.`ï¼Œå®ƒä¼šæ˜¾ç¤ºåœ¨â€å…¶ä»–â€åˆ†ç±»ä¸‹ï¼Œå¹¶ä»¥ä½ çš„ `id` ä½œä¸ºåˆ†ç»„æ ‡é¢˜ã€‚\n*   å¦‚æžœä½ çš„ `id` è‡³å°‘åŒ…å«ä¸€ä¸ª `.`ï¼Œæœ€å·¦è¾¹çš„éƒ¨åˆ†ä¼šä½œä¸ºè®¾ç½®åˆ†ç±»ï¼Œç¬¬äºŒéƒ¨åˆ†ä½œä¸ºåˆ†ç»„æ ‡é¢˜ï¼ŒåŽç»­éƒ¨åˆ†ä¼šè¢«å¿½ç•¥ã€‚\n\n### è¯»å–è®¾ç½®é¡¹\n\n```\nimport { app } from \"../../scripts/app.js\";\n\nif (app.extensionManager.setting.get('example.boolean')) {\n    console.log(\"è®¾ç½®å·²å¯ç”¨ã€‚\");\n} else {\n    console.log(\"è®¾ç½®å·²ç¦ç”¨ã€‚\");\n}\n```\n\n### å“åº”è®¾ç½®å˜åŒ–\n\nå½“ç”¨æˆ·åœ¨è®¾ç½®é¢æ¿ä¸­æ›´æ”¹è®¾ç½®æ—¶ï¼Œ`onChange()` äº‹ä»¶å¤„ç†å™¨ä¼šè¢«ç«‹å³è°ƒç”¨ã€‚ æ¯æ¬¡é¡µé¢åŠ è½½ã€æ‰©å±•æ³¨å†Œæ—¶ä¹Ÿä¼šè°ƒç”¨ã€‚\n\n```\n{\n    id: \"example.boolean\",\n    name: \"ç¤ºä¾‹å¸ƒå°”è®¾ç½®\",\n    type: \"boolean\",\n    defaultValue: false,\n    onChange: (newVal, oldVal) => {\n        console.log(`è®¾ç½®ä»Ž ${oldVal} å˜ä¸º ${newVal}`);\n    },\n}\n```\n\n### å†™å…¥è®¾ç½®é¡¹\n\n```\nimport { app } from \"../../scripts/app.js\";\n\ntry {\n    await app.extensionManager.setting.set(\"example.boolean\", true);\n} catch (error) {\n    console.error(`æ›´æ”¹è®¾ç½®æ—¶å‡ºé”™: ${error}`);\n}\n```\n\n### é¢å¤–é…ç½®\n\nè®¾ç½®ç±»åž‹åŸºäºŽ [PrimeVue](https://primevue.org/) ç»„ä»¶ã€‚ åœ¨ `attrs` å­—æ®µä¸­æ·»åŠ  PrimeVue æ–‡æ¡£ä¸­æè¿°çš„å±žæ€§å³å¯ä¸º ComfyUI è®¾ç½®é¡¹é…ç½®æ›´å¤šå‚æ•°ã€‚ ä¾‹å¦‚ï¼Œä¸‹é¢ä¸ºæ•°å­—è¾“å…¥æ¡†æ·»åŠ äº†å¢žå‡æŒ‰é’®ï¼š\n\n```\n{\n    id: \"example.number\",\n    name: \"ç¤ºä¾‹æ•°å­—è®¾ç½®\",\n    type: \"number\",\n    defaultValue: 0,\n    attrs: {\n        showButtons: true,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`è®¾ç½®ä»Ž ${oldVal} å˜ä¸º ${newVal}`);\n    },\n}\n```\n\n## ç±»åž‹\n\n### å¸ƒå°”å€¼ï¼ˆBooleanï¼‰\n\næ˜¾ç¤ºä¸€ä¸ªå¼€å…³ã€‚ åŸºäºŽ [ToggleSwitch PrimeVue ç»„ä»¶](https://primevue.org/toggleswitch/)ã€‚\n\n```\n{\n    id: \"example.boolean\",\n    name: \"ç¤ºä¾‹å¸ƒå°”è®¾ç½®\",\n    type: \"boolean\",\n    defaultValue: false,\n    onChange: (newVal, oldVal) => {\n        console.log(`è®¾ç½®ä»Ž ${oldVal} å˜ä¸º ${newVal}`);\n    },\n}\n```\n\n### æ–‡æœ¬ï¼ˆTextï¼‰\n\nè‡ªç”±æ–‡æœ¬è¾“å…¥ã€‚ åŸºäºŽ [InputText PrimeVue ç»„ä»¶](https://primevue.org/inputtext/)ã€‚\n\n```\n{\n    id: \"example.text\",\n    name: \"ç¤ºä¾‹æ–‡æœ¬è®¾ç½®\",\n    type: \"text\",\n    defaultValue: \"Foo\",\n    onChange: (newVal, oldVal) => {\n        console.log(`è®¾ç½®ä»Ž ${oldVal} å˜ä¸º ${newVal}`);\n    },\n}\n```\n\n### æ•°å­—ï¼ˆNumberï¼‰\n\nç”¨äºŽè¾“å…¥æ•°å­—ã€‚ å¦‚éœ€å…è®¸å°æ•°ä½ï¼Œè¯·å°† `maxFractionDigits` å±žæ€§è®¾ç½®ä¸ºå¤§äºŽ 0 çš„æ•°å­—ã€‚ åŸºäºŽ [InputNumber PrimeVue ç»„ä»¶](https://primevue.org/inputnumber/)ã€‚\n\n```\n{\n    id: \"example.number\",\n    name: \"ç¤ºä¾‹æ•°å­—è®¾ç½®\",\n    type: \"number\",\n    defaultValue: 42,\n    attrs: {\n        showButtons: true,\n        maxFractionDigits: 1,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`è®¾ç½®ä»Ž ${oldVal} å˜ä¸º ${newVal}`);\n    },\n}\n```\n\n### æ»‘å—ï¼ˆSliderï¼‰\n\nå…è®¸ç”¨æˆ·ç›´æŽ¥è¾“å…¥æ•°å­—æˆ–é€šè¿‡æ»‘å—é€‰æ‹©ã€‚ åŸºäºŽ [Slider PrimeVue ç»„ä»¶](https://primevue.org/slider/)ã€‚ä¸æ”¯æŒåŒºé—´ã€‚\n\n```\n{\n    id: \"example.slider\",\n    name: \"ç¤ºä¾‹æ»‘å—è®¾ç½®\",\n    type: \"slider\",\n    attrs: {\n        min: -10,\n        max: 10,\n        step: 0.5,\n    },\n    defaultValue: 0,\n    onChange: (newVal, oldVal) => {\n        console.log(`è®¾ç½®ä»Ž ${oldVal} å˜ä¸º ${newVal}`);\n    },\n}\n```\n\n### ä¸‹æ‹‰é€‰æ‹©ï¼ˆComboï¼‰\n\nå…è®¸ç”¨æˆ·ä»Žä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©ã€‚ ä½ å¯ä»¥ç”¨çº¯å­—ç¬¦ä¸²æˆ–å¸¦ `text` å’Œ `value` å­—æ®µçš„å¯¹è±¡æä¾›é€‰é¡¹ã€‚å¦‚æžœåªæä¾›å­—ç¬¦ä¸²ï¼Œåˆ™ä¼šåŒæ—¶ä½œä¸ºæ˜¾ç¤ºå’Œå®žé™…å€¼ã€‚ é€šè¿‡ `editable: true` å±žæ€§å…è®¸ç”¨æˆ·è¾“å…¥è‡ªå®šä¹‰å†…å®¹ï¼Œé€šè¿‡ `filter: true` å±žæ€§å…è®¸æœç´¢ã€‚ åŸºäºŽ [Select PrimeVue ç»„ä»¶](https://primevue.org/select/)ã€‚ä¸æ”¯æŒåˆ†ç»„ã€‚\n\n```\n{\n    id: \"example.combo\",\n    name: \"ç¤ºä¾‹ä¸‹æ‹‰è®¾ç½®\",\n    type: \"combo\",\n    defaultValue: \"first\",\n    options: [\n        { text: \"æˆ‘çš„ç¬¬ä¸€ä¸ªé€‰é¡¹\", value: \"first\" },\n        \"æˆ‘çš„ç¬¬äºŒä¸ªé€‰é¡¹\",\n    ],\n    attrs: {\n        editable: true,\n        filter: true,\n    },\n    onChange: (newVal, oldVal) => {\n        console.log(`è®¾ç½®ä»Ž ${oldVal} å˜ä¸º ${newVal}`);\n    },\n}\n```\n\n### é¢œè‰²ï¼ˆColorï¼‰\n\nå…è®¸ç”¨æˆ·é€šè¿‡é¢œè‰²é€‰æ‹©å™¨é€‰æ‹©é¢œè‰²æˆ–è¾“å…¥åå…­è¿›åˆ¶é¢œè‰²å€¼ã€‚ æ³¨æ„æ ¼å¼å¿…é¡»ä¸ºå…­ä½åå…­è¿›åˆ¶ï¼Œä¸æ”¯æŒä¸‰ä½ç®€å†™ã€‚ åŸºäºŽ [ColorPicker PrimeVue ç»„ä»¶](https://primevue.org/colorpicker/)ã€‚\n\n```\n{\n    id: \"example.color\",\n    name: \"ç¤ºä¾‹é¢œè‰²è®¾ç½®\",\n    type: \"color\",\n    defaultValue: \"ff0000\",\n    onChange: (newVal, oldVal) => {\n        console.log(`è®¾ç½®ä»Ž ${oldVal} å˜ä¸º ${newVal}`);\n    },\n}\n```\n\n### å›¾ç‰‡ï¼ˆImageï¼‰\n\nå…è®¸ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ã€‚ è®¾ç½®ä¼šä»¥ [data URL](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data) æ ¼å¼ä¿å­˜ã€‚ åŸºäºŽ [FileUpload PrimeVue ç»„ä»¶](https://primevue.org/fileupload/)ã€‚\n\n```\n{\n    id: \"example.image\",\n    name: \"ç¤ºä¾‹å›¾ç‰‡è®¾ç½®\",\n    type: \"image\",\n    onChange: (newVal, oldVal) => {\n        console.log(`è®¾ç½®ä»Ž ${oldVal} å˜ä¸º ${newVal}`);\n    },\n}\n```\n\n### éšè—ï¼ˆHiddenï¼‰\n\néšè—è®¾ç½®ä¸ä¼šæ˜¾ç¤ºåœ¨è®¾ç½®é¢æ¿ï¼Œä½†ä½ å¯ä»¥åœ¨ä»£ç ä¸­è¯»å†™å®ƒä»¬ã€‚\n\n```\n{\n    id: \"example.hidden\",\n    name: \"ç¤ºä¾‹éšè—è®¾ç½®\",\n    type: \"hidden\",\n}\n```\n\n## å…¶ä»–\n\n### åˆ†ç±»ï¼ˆCategoriesï¼‰\n\nä½ å¯ä»¥é€šè¿‡ `category` å­—æ®µå•ç‹¬æŒ‡å®šè®¾ç½®çš„åˆ†ç±»ã€‚ è¿™æ ·å¯ä»¥åœ¨ä¸æ›´æ”¹ `id` çš„æƒ…å†µä¸‹è°ƒæ•´åˆ†ç±»å’Œå‘½åï¼Œä¸ä¼šä¸¢å¤±ç”¨æˆ·å·²è®¾ç½®çš„å€¼ã€‚\n\n```\n{\n    id: \"example.boolean\",\n    name: \"ç¤ºä¾‹å¸ƒå°”è®¾ç½®\",\n    type: \"boolean\",\n    defaultValue: false,\n    category: [\"åˆ†ç±»åç§°\", \"åˆ†ç»„æ ‡é¢˜\", \"è®¾ç½®æ ‡ç­¾\"],\n}\n```\n\n### å·¥å…·æç¤ºï¼ˆTooltipsï¼‰\n\nä½ å¯ä»¥é€šè¿‡ `tooltip` å­—æ®µæ·»åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡å¸®åŠ©ã€‚è¿™ä¼šåœ¨å­—æ®µååŽæ˜¾ç¤ºä¸€ä¸ªå°çš„ â„¹ï¸Ž å›¾æ ‡ï¼Œç”¨æˆ·æ‚¬åœæ—¶ä¼šæ˜¾ç¤ºå¸®åŠ©æ–‡æœ¬ã€‚\n\n```\n{\n    id: \"example.boolean\",\n    name: \"ç¤ºä¾‹å¸ƒå°”è®¾ç½®\",\n    type: \"boolean\",\n    defaultValue: false,\n    tooltip: \"è¿™æ˜¯ä¸€äº›æœ‰ç”¨çš„æç¤ºä¿¡æ¯\",\n}\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/image/cosmos/cosmos-predict2-t2i",
  "markdown": "# Cosmos Predict2 æ–‡ç”Ÿå›¾ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\nCosmos-Predict2 æ˜¯ç”± NVIDIA æŽ¨å‡ºçš„æ–°ä¸€ä»£ç‰©ç†ä¸–ç•ŒåŸºç¡€æ¨¡åž‹ï¼Œä¸“ä¸ºç‰©ç† AI åœºæ™¯ä¸‹çš„é«˜è´¨é‡è§†è§‰ç”Ÿæˆä¸Žé¢„æµ‹ä»»åŠ¡è®¾è®¡ã€‚ è¯¥æ¨¡åž‹å…·å¤‡æžé«˜çš„ç‰©ç†å‡†ç¡®æ€§ã€çŽ¯å¢ƒäº¤äº’æ€§å’Œç»†èŠ‚è¿˜åŽŸèƒ½åŠ›ï¼Œèƒ½å¤ŸçœŸå®žæ¨¡æ‹Ÿå¤æ‚çš„ç‰©ç†çŽ°è±¡ä¸ŽåŠ¨æ€åœºæ™¯ã€‚ Cosmos-Predict2 æ”¯æŒæ–‡æœ¬åˆ°å›¾åƒï¼ˆText2Imageï¼‰å’Œè§†é¢‘åˆ°ä¸–ç•Œï¼ˆVideo2Worldï¼‰ç­‰å¤šç§ç”Ÿæˆæ–¹å¼ï¼Œå¹¿æ³›åº”ç”¨äºŽå·¥ä¸šä»¿çœŸã€è‡ªåŠ¨é©¾é©¶ã€åŸŽå¸‚è§„åˆ’ã€ç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸï¼Œæ˜¯æŽ¨åŠ¨æ™ºèƒ½è§†è§‰ä¸Žç‰©ç†ä¸–ç•Œæ·±åº¦èžåˆçš„é‡è¦åŸºç¡€å·¥å…·ã€‚ GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2) huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959) æœ¬ç¯‡æŒ‡å—å°†å¼•å¯¼ä½ å®Œæˆåœ¨ ComfyUI ä¸­ **æ–‡ç”Ÿå›¾** å·¥ä½œæµç¨‹ã€‚ å¯¹äºŽè§†é¢‘ç”Ÿæˆéƒ¨åˆ†ï¼Œè¯·å‚è€ƒä¸‹é¢çš„éƒ¨åˆ†\n\n[\n\nä½¿ç”¨ Cosmos-Predict2 çš„è¿›è¡Œè§†é¢‘ç”Ÿæˆ\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/video/cosmos/cosmos-predict2-video2world)\n\n## Cosmos Predict2 Video2World å·¥ä½œæµ\n\nå¯¹äºŽ 2B ç‰ˆæœ¬ï¼Œåœ¨å®žé™…è¿è¡Œæ—¶ï¼Œéœ€è¦ 10GB çš„æ˜¾å­˜\n\n### 1.ä¸‹è½½å·¥ä½œæµæ–‡ä»¶\n\n![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/cosmos/predict2/cosmos_predict2_2B_t2i.png)\n\n### 2.æ‰‹åŠ¨æ¨¡åž‹å®‰è£…\n\n**Diffusion model**\n\n*   [cosmos\\_predict2\\_2B\\_t2i.safetensors](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged/resolve/main/cosmos_predict2_2B_t2i.safetensors)\n\nå…¶å®ƒæƒé‡è¯·è®¿é—® [Cosmos\\_Predict2\\_repackaged](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged) è¿›è¡Œä¸‹è½½ **Text encoder** [oldt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/resolve/main/text_encoders/oldt5_xxl_fp8_e4m3fn_scaled.safetensors) **VAE** [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors) æ–‡ä»¶ä¿å­˜ä½ç½®\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€ cosmos_predict2_2B_t2i.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ oldt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚       â””â”€â”€  wan_2.1_vae.safetensors\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµè¿è¡Œ\n\n![å·¥ä½œæµä½¿ç”¨æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/cosmos/cosmos_predict2_2B_t2i_step_guide.jpg) è¯·å‚ç…§å›¾ç‰‡åºå·è¿›è¡Œé€æ­¥ç¡®è®¤ï¼Œæ¥ä¿è¯å¯¹åº”å·¥ä½œæµçš„é¡ºåˆ©è¿è¡Œ\n\n1.  ç¡®ä¿ `Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `cosmos_predict2_2B_t2i.safetensors`\n2.  ç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `oldt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  ç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors`\n4.  åœ¨ `EmptySD3LatentImage` è®¾ç½®å›¾ç‰‡çš„å°ºå¯¸\n5.  åœ¨ `ClipTextEncode` èŠ‚ç‚¹ä¸­ä¿®æ”¹æç¤ºè¯\n6.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œæ–‡ç”Ÿå›¾\n7.  ç”Ÿæˆå®ŒæˆåŽå¯¹åº”çš„å›¾ç‰‡ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `ComfyUI/output/` ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨ `save image` èŠ‚ç‚¹ä¸­é¢„è§ˆæˆ–è€…è°ƒæ•´ä¿å­˜ä½ç½®"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/image/hidream/hidream-e1",
  "markdown": "# ComfyUI åŽŸç”Ÿç‰ˆæœ¬ HiDream-E1, E1.1 å·¥ä½œæµç¤ºä¾‹\n\n![HiDream-E1 æ¼”ç¤º](https://raw.githubusercontent.com/HiDream-ai/HiDream-E1/refs/heads/main/assets/demo.jpg) HiDream-E1 æ˜¯æ™ºè±¡æœªæ¥(HiDream-ai) æ­£å¼å¼€æºçš„äº¤äº’å¼å›¾åƒç¼–è¾‘å¤§æ¨¡åž‹ï¼Œæ˜¯åŸºäºŽ HiDream-I1 æž„å»ºçš„å›¾åƒç¼–è¾‘æ¨¡åž‹ã€‚ å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¥å®žçŽ°å¯¹å›¾åƒçš„ç¼–è¾‘ï¼Œè¯¥æ¨¡åž‹åŸºäºŽ [MIT è®¸å¯è¯](https://github.com/HiDream-ai/HiDream-E1?tab=MIT-1-ov-file) å‘å¸ƒï¼Œæ”¯æŒç”¨äºŽä¸ªäººé¡¹ç›®ã€ç§‘å­¦ç ”ç©¶ä»¥åŠå•†ç”¨ã€‚ é€šè¿‡ä¸Žæ­¤å‰å‘å¸ƒçš„ [hidream-i1](https://docs.comfy.org/zh-CN/tutorials/image/hidream/hidream-i1)çš„å…±åŒç»„åˆï¼Œå®žçŽ°äº† **ä»Žå›¾åƒç”Ÿæˆåˆ°ç¼–è¾‘çš„** åˆ›ä½œèƒ½åŠ›ã€‚\n\n| åç§°  | æ›´æ–°æ—¶é—´ | æŽ¨ç†æ­¥æ•° | åˆ†è¾¨çŽ‡ | HuggingFace ä»“åº“ |\n| --- | --- | --- | --- | --- |\n| HiDream-E1-Full | 2025-4-28 | 28  | 768x768 | ðŸ¤— [HiDream-E1-Full](https://huggingface.co/HiDream-ai/HiDream-E1-Full) |\n| HiDream-E1.1 | 2025-7-16 | 28  | åŠ¨æ€ï¼ˆ1ç™¾ä¸‡åƒç´ ï¼‰ | ðŸ¤— [HiDream-E1.1](https://huggingface.co/HiDream-ai/HiDream-E1-1) |\n\n[HiDream E1 - Github](https://github.com/HiDream-ai/HiDream-E1)\n\næœ¬ç¯‡æŒ‡å—æ¶‰åŠçš„æ‰€æœ‰æ¨¡åž‹ä½ éƒ½å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files)æ‰¾åˆ°, E1, E1.1 é™¤äº† Diffusion model ä¹‹å¤–éƒ½æ˜¯ç”¨ç›¸åŒçš„æ¨¡åž‹ æˆ‘ä»¬åœ¨å¯¹åº”çš„å·¥ä½œæµæ–‡ä»¶ä¸­ä¹Ÿä»¥åŒ…å«äº†å¯¹åº”çš„æ¨¡åž‹ä¿¡æ¯ï¼Œä½ å¯ä»¥é€‰æ‹©æ‰‹åŠ¨ä¸‹è½½æ¨¡åž‹ä¿å­˜ï¼Œæˆ–è€…åœ¨åŠ è½½å·¥ä½œæµåŽæŒ‰å·¥ä½œæµæç¤ºè¿›è¡Œä¸‹è½½ï¼ŒæŽ¨èä½¿ç”¨ E1.1 è¿™ä¸ªæ¨¡åž‹çš„è¿è¡Œå¯¹æ˜¾å­˜å ç”¨è¦æ±‚æžé«˜ï¼Œå…·ä½“æ˜¾å­˜å ç”¨è¯·å‚è€ƒå¯¹åº”éƒ¨åˆ†çš„è¯´æ˜Ž **Diffusion Model** ä½ ä¸ç”¨åŒæ—¶ä¸‹è½½è¿™ä¸¤ä¸ªæ¨¡åž‹ï¼Œç”±äºŽ E1.1 æ˜¯åŸºäºŽ E1 çš„è¿­ä»£ç‰ˆæœ¬ï¼Œåœ¨å®žé™…æµ‹è¯•ä¸­å®ƒçš„è´¨é‡å’Œæ•ˆæžœè¾ƒ E1 éƒ½æœ‰è¾ƒå¤§æå‡\n\n*   [hidream\\_e1\\_1\\_bf16.safetensors(æŽ¨è)](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_1_bf16.safetensors) 34.2GB\n*   [hidream\\_e1\\_full\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_full_bf16.safetensors) 34.2GB\n\n**Text Encoder**ï¼š\n\n*   [clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors) 236.12MB\n*   [clip\\_g\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors) 1.29GB\n*   [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) 4.8GB\n*   [llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors) 8.46GB\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) 319.77MB\n\n> è¿™ä¸ªæ˜¯ Flux çš„ VAE æ¨¡åž‹ï¼Œå¦‚æžœä½ ä¹‹å‰ä½¿ç”¨è¿‡ Flux çš„å·¥ä½œæµï¼Œä½ å¯èƒ½å·²ç»ä¸‹è½½äº†è¿™ä¸ªæ–‡ä»¶ã€‚\n\næ–‡ä»¶ä¿å­˜ä½ç½®\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€â”€ clip_l_hidream.safetensors\nâ”‚   â”‚   â”œâ”€â”€â”€ clip_g_hidream.safetensors\nâ”‚   â”‚   â”œâ”€â”€â”€ t5xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â”‚   â””â”€â”€â”€ llama_3.1_8b_instruct_fp8_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ diffusion_models/\nâ”‚       â”œâ”€â”€ hidream_e1_1_bf16.safetensors\nâ”‚       â””â”€â”€ hidream_e1_full_bf16.safetensors\n```\n\n## HiDream E1.1 ComfyUI åŽŸç”Ÿå·¥ä½œæµç¤ºä¾‹\n\nE1.1 æ˜¯äºŽ 2025å¹´7æœˆ16æ—¥æ›´æ–°è¿­ä»£çš„ç‰ˆæœ¬, è¿™ä¸ªç‰ˆæœ¬æ”¯æŒåŠ¨æ€ä¸€ç™¾ä¸‡åˆ†è¾¨çŽ‡ï¼Œåœ¨å·¥ä½œæµä¸­ä½¿ç”¨äº† `Scale Image to Total Pixels` èŠ‚ç‚¹æ¥å°†è¾“å…¥å›¾ç‰‡åŠ¨æ€è°ƒæ•´ä¸º 1ç™¾ä¸‡åƒç´ \n\n### 1\\. HiDream E1.1 å·¥ä½œæµåŠç›¸å…³ç´ æ\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡å¹¶æ‹–å…¥ ComfyUI å·²åŠ è½½å¯¹åº”å·¥ä½œæµåŠæ¨¡åž‹ ![HiDream E1.1 å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/hidream_e1_1.png) ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥ ![HiDream E1.1 å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/input.webp) \n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆ HiDream-e1 å·¥ä½œæµè¿è¡Œ\n\n![hidream_e1_1_guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/hidream/hidream-e1-1-guide.jpg) æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n1.  ç¡®ä¿`Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `hidream_e1_1_bf16.safetensors` æ¨¡åž‹\n2.  ç¡®ä¿`QuadrupleCLIPLoader` ä¸­å››ä¸ªå¯¹åº”çš„ text encoder è¢«æ­£ç¡®åŠ è½½\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  ç¡®ä¿`Load VAE` èŠ‚ç‚¹ä¸­ä½¿ç”¨çš„æ˜¯ `ae.safetensors` æ–‡ä»¶\n4.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„è¾“å…¥æˆ–ä½ éœ€è¦çš„å›¾ç‰‡\n5.  åœ¨`Empty Text Encoder(Positive)` èŠ‚ç‚¹ä¸­è¾“å…¥ **æƒ³è¦å¯¹å›¾ç‰‡è¿›è¡Œçš„ä¿®æ”¹**\n6.  åœ¨`Empty Text Encoder(Negative)` èŠ‚ç‚¹ä¸­è¾“å…¥ **ä¸æƒ³è¦åœ¨ç”»é¢ä¸­å‡ºçŽ°çš„å†…å®¹**\n7.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\n### 3\\. å·¥ä½œæµè¡¥å……è¯´æ˜Ž\n\n*   ç”±äºŽ HiDream E1.1 æ”¯æŒçš„æ˜¯åŠ¨æ€æ€»åƒç´ ä¸ºä¸€ç™¾ä¸‡åƒç´ è¾“å…¥ï¼Œæ‰€ä»¥å·¥ä½œæµä½¿ç”¨äº† `Scale Image to Total Pixels` æ¥å°†æ‰€æœ‰è¾“å…¥å›¾ç‰‡è¿›è¡Œå¤„ç†è½¬åŒ–ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ¯”ä¾‹å°ºå¯¸ç›¸å¯¹äºŽè¾“å…¥å›¾ç‰‡ä¼šæœ‰æ‰€å˜åŒ–\n*   ä½¿ç”¨ fp16 ç‰ˆæœ¬çš„æ¨¡åž‹ï¼Œåœ¨å®žé™…æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œåœ¨ A100 40GB å’Œ 4090D 24GB æ—¶ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬æ—¶ä¼š Out of memoryï¼Œæ‰€ä»¥å·¥ä½œæµé»˜è®¤è®¾ç½®äº†ä½¿ç”¨ `fp8_e4m3fn_fast` æ¥è¿›è¡ŒæŽ¨ç†\n\n## HiDream E1 ComfyUI åŽŸç”Ÿ å·¥ä½œæµç¤ºä¾‹\n\nE1 æ˜¯äºŽ 2025 å¹´ 4 æœˆ 28 æ—¥å‘å¸ƒçš„ï¼Œè¿™ä¸ªæ¨¡åž‹åªæ”¯æŒ 768\\*768 çš„åˆ†è¾¨çŽ‡\n\n### 1\\. HiDream-e1 å·¥ä½œæµåŠç›¸å…³ç´ æ\n\n#### 1.1 ä¸‹è½½å·¥ä½œæµæ–‡ä»¶\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡å¹¶æ‹–å…¥ ComfyUI ä¸­ï¼Œå·¥ä½œæµå·²åŒ…å«æ¨¡åž‹ä¸‹è½½ä¿¡æ¯ï¼ŒåŠ è½½åŽå°†ä¼šæç¤ºä½ è¿›è¡Œå¯¹åº”çš„æ¨¡åž‹ä¸‹è½½ã€‚ ![ComfyUI åŽŸç”Ÿ HiDream-e1 å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/hidream_e1_full.png)\n\n#### 1.2 ä¸‹è½½è¾“å…¥å›¾ç‰‡\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ç”¨äºŽè¾“å…¥ ![ComfyUI åŽŸç”Ÿ HiDream-e1 å·¥ä½œæµ è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/input.webp)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆ HiDream-e1 å·¥ä½œæµè¿è¡Œ\n\n![hidream_e1_full_step_guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg) æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n1.  ç¡®ä¿`Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `hidream_e1_full_bf16.safetensors` æ¨¡åž‹\n2.  ç¡®ä¿`QuadrupleCLIPLoader` ä¸­å››ä¸ªå¯¹åº”çš„ text encoder è¢«æ­£ç¡®åŠ è½½\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  ç¡®ä¿`Load VAE` èŠ‚ç‚¹ä¸­ä½¿ç”¨çš„æ˜¯ `ae.safetensors` æ–‡ä»¶\n4.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æˆ‘ä»¬ä¹‹å‰ä¸‹è½½çš„è¾“å…¥å›¾ç‰‡\n5.  ï¼ˆé‡è¦ï¼‰åœ¨`Empty Text Encoder(Positive)` èŠ‚ç‚¹ä¸­è¾“å…¥ **æƒ³è¦ä¿®æ”¹çš„ç”»é¢çš„æç¤ºè¯**\n6.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\n### ComfyUI HiDream-e1 å·¥ä½œæµè¡¥å……è¯´æ˜Ž\n\n*   å¯èƒ½éœ€è¦ä¿®æ”¹å¤šæ¬¡æç¤ºè¯æˆ–è€…è¿›è¡Œå¤šæ¬¡çš„ç”Ÿæˆæ‰èƒ½å¾—åˆ°è¾ƒå¥½çš„ç»“æžœ\n*   è¿™ä¸ªæ¨¡åž‹åœ¨æ”¹å˜å›¾ç‰‡é£Žæ ¼ä¸Šæ¯”è¾ƒéš¾ä¿æŒä¸€è‡´æ€§ï¼Œéœ€è¦å°½å¯èƒ½å®Œå–„æç¤ºè¯\n*   ç”±äºŽæ¨¡åž‹æ”¯æŒçš„æ˜¯ 768\\*768 çš„åˆ†è¾¨çŽ‡ï¼Œåœ¨å®žé™…æµ‹è¯•ä¸­è°ƒæ•´è¿‡å…¶å®ƒå°ºå¯¸ï¼Œåœ¨å…¶å®ƒå°ºå¯¸ä¸‹å›¾åƒè¡¨çŽ°èƒ½åŠ›ä¸ä½³ï¼Œç”šè‡³å·®å¼‚è¾ƒå¤§"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_sidebar_tabs",
  "markdown": "# ä¾§è¾¹æ æ ‡ç­¾é¡µ - ComfyUI\n\nä¾§è¾¹æ æ ‡ç­¾é¡µ API å…è®¸æ‰©å±•ä¸º ComfyUI ç•Œé¢çš„ä¾§è¾¹æ æ·»åŠ è‡ªå®šä¹‰æ ‡ç­¾é¡µã€‚è¿™å¯¹äºŽæ·»åŠ éœ€è¦æŒç»­å¯è§æ€§å’Œå¿«é€Ÿè®¿é—®çš„åŠŸèƒ½éžå¸¸æœ‰ç”¨ã€‚\n\n## åŸºæœ¬ç”¨æ³•\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"customSidebar\",\n  icon: \"pi pi-compass\",\n  title: \"è‡ªå®šä¹‰æ ‡ç­¾é¡µ\",\n  tooltip: \"æˆ‘çš„è‡ªå®šä¹‰ä¾§è¾¹æ æ ‡ç­¾é¡µ\",\n  type: \"custom\",\n  render: (el) => {\n    el.innerHTML = '<div>è¿™æ˜¯æˆ‘çš„è‡ªå®šä¹‰ä¾§è¾¹æ å†…å®¹</div>';\n  }\n});\n```\n\n## æ ‡ç­¾é¡µé…ç½®\n\næ¯ä¸ªæ ‡ç­¾é¡µéœ€è¦ä»¥ä¸‹å±žæ€§ï¼š\n\n```\n{\n  id: string,              // æ ‡ç­¾é¡µçš„å”¯ä¸€æ ‡è¯†ç¬¦\n  icon: string,            // æ ‡ç­¾æŒ‰é’®çš„å›¾æ ‡ç±»å\n  title: string,           // æ ‡ç­¾é¡µæ ‡é¢˜æ–‡æœ¬\n  tooltip?: string,        // æ‚¬åœæ—¶çš„æç¤ºæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰\n  type: string,            // æ ‡ç­¾é¡µç±»åž‹ï¼ˆé€šå¸¸ä¸º \"custom\"ï¼‰\n  render: (element) => void // ç”¨äºŽå¡«å……æ ‡ç­¾é¡µå†…å®¹çš„å‡½æ•°\n}\n```\n\n`render` å‡½æ•°ä¼šæŽ¥æ”¶ä¸€ä¸ª DOM å…ƒç´ ï¼Œä½ åº”åœ¨å…¶ä¸­æ’å…¥æ ‡ç­¾é¡µçš„å†…å®¹ã€‚\n\n## å›¾æ ‡é€‰é¡¹\n\nä¾§è¾¹æ æ ‡ç­¾é¡µå›¾æ ‡å¯ä½¿ç”¨å¤šç§å›¾æ ‡é›†ï¼š\n\n*   PrimeVue å›¾æ ‡ï¼š`pi pi-[icon-name]`ï¼ˆå¦‚ `pi pi-home`ï¼‰\n*   Material Design å›¾æ ‡ï¼š`mdi mdi-[icon-name]`ï¼ˆå¦‚ `mdi mdi-robot`ï¼‰\n*   Font Awesome å›¾æ ‡ï¼š`fa-[style] fa-[icon-name]`ï¼ˆå¦‚ `fa-solid fa-star`ï¼‰\n\nä½¿ç”¨è¿™äº›å›¾æ ‡å‰è¯·ç¡®ä¿å·²åŠ è½½ç›¸åº”çš„å›¾æ ‡åº“ã€‚\n\n## æœ‰çŠ¶æ€æ ‡ç­¾é¡µç¤ºä¾‹\n\nä½ å¯ä»¥åˆ›å»ºå¸¦æœ‰çŠ¶æ€çš„æ ‡ç­¾é¡µï¼š\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"statefulTab\",\n  icon: \"pi pi-list\",\n  title: \"ç¬”è®°\",\n  type: \"custom\",\n  render: (el) => {\n    // åˆ›å»ºå…ƒç´ \n    const container = document.createElement('div');\n    container.style.padding = '10px';\n    \n    const notepad = document.createElement('textarea');\n    notepad.style.width = '100%';\n    notepad.style.height = '200px';\n    notepad.style.marginBottom = '10px';\n    \n    // åŠ è½½å·²ä¿å­˜å†…å®¹ï¼ˆå¦‚æœ‰ï¼‰\n    const savedContent = localStorage.getItem('comfyui-notes');\n    if (savedContent) {\n      notepad.value = savedContent;\n    }\n    \n    // è‡ªåŠ¨ä¿å­˜å†…å®¹\n    notepad.addEventListener('input', () => {\n      localStorage.setItem('comfyui-notes', notepad.value);\n    });\n    \n    // ç»„è£… UI\n    container.appendChild(notepad);\n    el.appendChild(container);\n  }\n});\n```\n\n## ä½¿ç”¨ React ç»„ä»¶\n\nä½ å¯ä»¥åœ¨ä¾§è¾¹æ æ ‡ç­¾é¡µä¸­æŒ‚è½½ React ç»„ä»¶ï¼š\n\n```\n// åœ¨ä½ çš„æ‰©å±•ä¸­å¼•å…¥ React ä¾èµ–\nimport React from \"react\";\nimport ReactDOM from \"react-dom/client\";\n\n// æ³¨å†Œå¸¦æœ‰ React å†…å®¹çš„ä¾§è¾¹æ æ ‡ç­¾é¡µ\napp.extensionManager.registerSidebarTab({\n  id: \"reactSidebar\",\n  icon: \"mdi mdi-react\",\n  title: \"React æ ‡ç­¾é¡µ\",\n  type: \"custom\",\n  render: (el) => {\n    const container = document.createElement(\"div\");\n    container.id = \"react-sidebar-container\";\n    el.appendChild(container);\n    \n    // å®šä¹‰ä¸€ä¸ªç®€å•çš„ React ç»„ä»¶\n    function SidebarContent() {\n      const [count, setCount] = React.useState(0);\n      \n      return (\n        <div style={{ padding: \"10px\" }}>\n          <h3>React ä¾§è¾¹æ </h3>\n          <p>è®¡æ•°ï¼š{count}</p>\n          <button onClick={() => setCount(count + 1)}>\n            é€’å¢ž\n          </button>\n        </div>\n      );\n    }\n    \n    // æŒ‚è½½ React ç»„ä»¶\n    ReactDOM.createRoot(container).render(\n      <React.StrictMode>\n        <SidebarContent />\n      </React.StrictMode>\n    );\n  }\n});\n```\n\nå¦‚éœ€æŸ¥çœ‹å°† React åº”ç”¨é›†æˆä¸ºä¾§è¾¹æ æ ‡ç­¾é¡µçš„çœŸå®žæ¡ˆä¾‹ï¼Œè¯·å‚è€ƒ [ComfyUI-Copilot é¡¹ç›®ï¼ˆGitHubï¼‰](https://github.com/AIDC-AI/ComfyUI-Copilot)ã€‚\n\n## åŠ¨æ€å†…å®¹æ›´æ–°\n\nä½ å¯ä»¥æ ¹æ®å›¾å˜åŒ–åŠ¨æ€æ›´æ–°ä¾§è¾¹æ å†…å®¹ï¼š\n\n```\napp.extensionManager.registerSidebarTab({\n  id: \"dynamicSidebar\",\n  icon: \"pi pi-chart-line\",\n  title: \"ç»Ÿè®¡ä¿¡æ¯\",\n  type: \"custom\",\n  render: (el) => {\n    const container = document.createElement('div');\n    container.style.padding = '10px';\n    el.appendChild(container);\n    \n    // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯çš„å‡½æ•°\n    function updateStats() {\n      const stats = {\n        nodes: app.graph._nodes.length,\n        connections: Object.keys(app.graph.links).length\n      };\n      \n      container.innerHTML = `\n        <h3>å·¥ä½œæµç»Ÿè®¡</h3>\n        <ul>\n          <li>èŠ‚ç‚¹æ•°ï¼š${stats.nodes}</li>\n          <li>è¿žæŽ¥æ•°ï¼š${stats.connections}</li>\n        </ul>\n      `;\n    }\n    \n    // åˆå§‹æ›´æ–°\n    updateStats();\n    \n    // ç›‘å¬å›¾å˜åŒ–\n    const api = app.api;\n    api.addEventListener(\"graphChanged\", updateStats);\n    \n    // æ ‡ç­¾é¡µé”€æ¯æ—¶æ¸…ç†ç›‘å¬å™¨\n    return () => {\n      api.removeEventListener(\"graphChanged\", updateStats);\n    };\n  }\n});\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/3d",
  "markdown": "# ComfyUI 3D è®¾ç½® - ComfyUI\n\nè¿™éƒ¨åˆ†çš„è®¾ç½®ä¸»è¦ç”¨äºŽæŽ§åˆ¶ ComfyUI ä¸­ 3D ç›¸å…³ç»„ä»¶çš„åˆå§‹åŒ–è®¾ç½®ï¼ŒåŒ…æ‹¬ç›¸æœºã€å…‰ç…§ã€åœºæ™¯ç­‰ï¼Œåœ¨åˆ›å»ºæ–°çš„3Dç»„ä»¶æ—¶ï¼Œä¼šæ ¹æ®è¿™äº›è®¾ç½®è¿›è¡Œåˆå§‹åŒ–ï¼Œåœ¨åˆ›å»ºåŽï¼Œè¿™äº›è®¾ç½®ä»ç„¶å¯ä»¥å•ç‹¬è°ƒæ•´ã€‚\n\n## ç›¸æœº\n\n### æ‘„åƒæœºç±»åž‹\n\n*   **é€‰é¡¹**:\n    *   `perspective` (é€è§†)\n    *   `orthographic` (æ­£äº¤)\n*   **åŠŸèƒ½**: æŽ§åˆ¶åˆ›å»ºæ–°çš„3Dç»„ä»¶æ—¶ï¼Œé»˜è®¤çš„ç›¸æœºæ˜¯é€è§†è¿˜æ˜¯æ­£äº¤ã€‚è¿™ä¸ªé»˜è®¤è®¾ç½®ä»ç„¶å¯ä»¥åœ¨åˆ›å»ºåŽåœ¨èŠ‚ç‚¹çš„ç”»å¸ƒä¸­å•ç‹¬è°ƒæ•´\n\n![æ‘„åƒæœºç±»åž‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/camera_type.jpg)\n\n## å…‰ç…§\n\nè¿™éƒ¨åˆ†èœå•ç”¨äºŽè®¾ç½® 3D ç›¸å…³ç»„ä»¶çš„å…‰ç…§è®¾ç½®çš„é¢„è®¾, å¯¹åº”çš„è®¾ç½®åœ¨ ComfyUI çš„ 3D è®¾ç½®ä¸­åŒæ ·å¯ä»¥è¿›è¡Œä¿®æ”¹ ![å…‰ç…§](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/light.jpg)\n\n### å…‰ç…§è°ƒæ•´æ­¥é•¿\n\n*   **é»˜è®¤å€¼**: 0.5\n*   **åŠŸèƒ½**: æŽ§åˆ¶åœ¨3Dåœºæ™¯ä¸­è°ƒæ•´å…‰ç…§å¼ºåº¦æ—¶çš„æ­¥é•¿ã€‚è¾ƒå°çš„æ­¥é•¿å€¼å¯ä»¥å®žçŽ°æ›´ç²¾ç»†çš„å…‰ç…§è°ƒæ•´ï¼Œè¾ƒå¤§çš„å€¼åˆ™ä¼šä½¿æ¯æ¬¡è°ƒæ•´çš„å˜åŒ–æ›´åŠ æ˜Žæ˜¾\n\n### å…‰ç…§å¼ºåº¦ä¸‹é™\n\n*   **é»˜è®¤å€¼**: 1\n*   **åŠŸèƒ½**: è®¾ç½®3Dåœºæ™¯å…è®¸çš„æœ€å°å…‰ç…§å¼ºåº¦å€¼ã€‚æ­¤é¡¹å®šä¹‰åœ¨è°ƒæ•´ä»»ä½•3DæŽ§ä»¶ç…§æ˜Žæ—¶å¯è®¾å®šçš„æœ€ä½Žäº®åº¦\n\n### æœ€å¤§å…‰ç…§å¼ºåº¦\n\n*   **é»˜è®¤å€¼**: 10\n*   **åŠŸèƒ½**: è®¾ç½®3Dåœºæ™¯å…è®¸çš„æœ€å¤§å…‰ç…§å¼ºåº¦å€¼ã€‚æ­¤é¡¹å®šä¹‰äº†åœ¨è°ƒæ•´ä»»ä½•3DæŽ§ä»¶ç…§æ˜Žæ—¶å¯è®¾å®šçš„æœ€é«˜äº®åº¦ä¸Šé™\n\n### åˆå§‹å…‰ç…§å¼ºåº¦\n\n*   **é»˜è®¤å€¼**: 3\n*   **åŠŸèƒ½**: è®¾ç½®3Dåœºæ™¯ä¸­ç¯å…‰çš„é»˜è®¤äº®åº¦çº§åˆ«ã€‚è¯¥æ•°å€¼å†³å®šæ–°å»º3DæŽ§ä»¶æ—¶ç¯å…‰ç…§äº®ç‰©ä½“çš„å¼ºåº¦ï¼Œä½†æ¯ä¸ªæŽ§ä»¶åœ¨åˆ›å»ºåŽéƒ½å¯ä»¥å•ç‹¬è°ƒæ•´\n\n## åœºæ™¯\n\nè¿™ä¸ªè®¾ç½®å…è®¸ä½ ä½¿ç”¨è®¾ç½®é»˜è®¤çš„ 3D èŠ‚ç‚¹çš„åå¥½è®¾ç½®\n\n### åˆå§‹èƒŒæ™¯é¢œè‰²\n\n*   **ä½œç”¨**: æŽ§åˆ¶3Dåœºæ™¯çš„é»˜è®¤èƒŒæ™¯é¢œè‰²ã€‚æ­¤è®¾ç½®å†³å®šæ–°å»º3Dç»„ä»¶æ—¶çš„èƒŒæ™¯å¤–è§‚ï¼Œä½†æ¯ä¸ªç»„ä»¶åœ¨åˆ›å»ºåŽéƒ½å¯ä»¥å•ç‹¬è°ƒæ•´\n*   **é»˜è®¤å€¼**: `282828` (æ·±ç°è‰²)\n\nä¿®æ”¹èƒŒæ™¯é¢œè‰²ï¼ŒåŒæ ·å¯ä»¥åœ¨ç”»å¸ƒä¸­è¿›è¡Œè°ƒæ•´ ![åˆå§‹èƒŒæ™¯é¢œè‰²](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/background_color.jpg) \n\n### æ˜¾ç¤ºé¢„è§ˆ\n\n*   **ä½œç”¨**: æŽ§åˆ¶åˆ›å»ºæ–°çš„3Dç»„ä»¶æ—¶æ˜¯å¦é»˜è®¤æ˜¾ç¤ºé¢„è§ˆå±å¹•ã€‚æ­¤é»˜è®¤è®¾ç½®åœ¨åˆ›å»ºåŽä»å¯ä¸ºæ¯ä¸ªç»„ä»¶å•ç‹¬åˆ‡æ¢\n*   **é»˜è®¤å€¼**: true (å¼€å¯)\n\n![é¢„è§ˆ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/hide_preview.jpg)\n\n### æ˜¾ç¤ºç½‘æ ¼\n\n*   **ä½œç”¨**: æŽ§åˆ¶åˆ›å»ºæ–°çš„3Dç»„ä»¶æ—¶æ˜¯å¦é»˜è®¤æ˜¾ç¤ºç½‘æ ¼ã€‚æ­¤é»˜è®¤è®¾ç½®åœ¨åˆ›å»ºåŽä»å¯ä¸ºæ¯ä¸ªç»„ä»¶å•ç‹¬åˆ‡æ¢\n*   **é»˜è®¤å€¼**: true (å¼€å¯)\n\n![éšè—ç½‘æ ¼](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/3d/hide_grid.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/ltxv",
  "markdown": "# LTX-Video - ComfyUI\n\n## å¿«é€Ÿå…¥é—¨\n\n[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) æ˜¯ Lightricks å¼€å‘çš„é«˜æ•ˆè§†é¢‘ç”Ÿæˆæ¨¡åž‹ã€‚ ä½¿ç”¨è¯¥æ¨¡åž‹çš„å…³é”®æ˜¯æä¾›è¯¦ç»†çš„é•¿æè¿°æç¤ºè¯ã€‚ è¯·ä¸‹è½½ [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true) æ–‡ä»¶å¹¶æ”¾å…¥ `ComfyUI/models/checkpoints` ç›®å½•ã€‚ è‹¥å°šæœªä¸‹è½½ [t5xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true) æ–‡ä»¶ï¼Œè¯·å°†å…¶æ”¾å…¥ `ComfyUI/models/text_encoders` ç›®å½•ã€‚\n\nå¦‚æžœåœ¨åŠ è½½ä¸‹é¢çš„å·¥ä½œæµæ–‡ä»¶æ—¶ï¼Œä½ å‘çŽ°å­˜åœ¨èŠ‚ç‚¹ç¼ºå¤±ï¼Œå¯èƒ½æ˜¯å› ä¸ºä»¥ä¸‹æƒ…å†µï¼š\n\n1.  ä½ ä½¿ç”¨çš„ ComfyUI ç‰ˆæœ¬ä¸æ˜¯æœ€æ–°çš„å¼€å‘ï¼ˆnightlyï¼‰ç‰ˆæœ¬ã€‚\n2.  ä½ ä½¿ç”¨çš„ ComfyUI ç‰ˆæœ¬æ˜¯ç¨³å®šï¼ˆreleaseï¼‰ç‰ˆæœ¬æˆ–æ¡Œé¢ç‰ˆï¼ˆdesktopï¼‰ç‰ˆæœ¬ï¼ˆä¸åŒ…å«æœ€æ–°çš„åŠŸèƒ½æ›´æ–°ï¼‰ã€‚\n3.  ä½ ä½¿ç”¨çš„ ComfyUI ç‰ˆæœ¬æ˜¯æœ€æ–°çš„ commit ç‰ˆæœ¬ï¼Œä½†åœ¨å¯åŠ¨è¿‡ç¨‹ä¸­éƒ¨åˆ†èŠ‚ç‚¹å¯¼å…¥å¤±è´¥äº†ã€‚\n\nè¯·å…ˆç¡®ä¿ä½ å·²ç»æˆåŠŸæ›´æ–° ComfyUI åˆ°æœ€æ–°çš„å¼€å‘ï¼ˆnightlyï¼‰ç‰ˆæœ¬, è¯·æŸ¥çœ‹ï¼š[å¦‚ä½•æ›´æ–° ComfyUI](https://docs.comfy.org/zh-CN/installation/update_comfyui) éƒ¨åˆ†äº†è§£å¦‚ä½•æ›´æ–° ComfyUIã€‚\n\n## å¤šå¸§æŽ§åˆ¶\n\né€šè¿‡ç³»åˆ—å›¾åƒæŽ§åˆ¶è§†é¢‘ç”Ÿæˆã€‚å¯ä¸‹è½½è¾“å…¥å›¾åƒï¼š[èµ·å§‹å¸§](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) å’Œ [ç»“æŸå¸§](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png)ã€‚ ![LTX-Video å¤šå¸§æŽ§åˆ¶å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/workflow.webp)\n\n## å›¾ç”Ÿè§†é¢‘\n\né€šè¿‡é¦–å¸§å›¾åƒæŽ§åˆ¶è§†é¢‘ç”Ÿæˆï¼š[ç¤ºä¾‹é¦–å¸§](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png)ã€‚ ![LTX-Video å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/workflow.webp)\n\n## æ–‡ç”Ÿè§†é¢‘\n\n![LTX-Video æ–‡ç”Ÿè§†é¢‘å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/t2v.webp)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/depth-t2i-adapter",
  "markdown": "# ComfyUI Depth T2I Adapter ä½¿ç”¨ç¤ºä¾‹\n\n[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) æ˜¯ç”± â€‹[è…¾è®¯ARCå®žéªŒå®¤](https://github.com/TencentARC) å¼€å‘çš„è½»é‡çº§é€‚é…å™¨ï¼Œç”¨äºŽå¢žå¼ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼ˆå¦‚Stable Diffusionï¼‰çš„ç»“æž„ã€é¢œè‰²å’Œé£Žæ ¼æŽ§åˆ¶èƒ½åŠ›ã€‚ å®ƒé€šè¿‡å¤–éƒ¨æ¡ä»¶ï¼ˆå¦‚è¾¹ç¼˜æ£€æµ‹å›¾ã€æ·±åº¦å›¾ã€è‰å›¾æˆ–é¢œè‰²å‚è€ƒå›¾ï¼‰ä¸Žæ¨¡åž‹å†…éƒ¨ç‰¹å¾å¯¹é½ï¼Œå®žçŽ°é«˜ç²¾åº¦æŽ§åˆ¶ï¼Œæ— éœ€ä¿®æ”¹åŽŸæ¨¡åž‹ç»“æž„ã€‚å…¶å‚æ•°ä»…çº¦77Mï¼ˆä½“ç§¯çº¦300MBï¼‰ï¼ŒæŽ¨ç†é€Ÿåº¦æ¯” [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly) å¿«çº¦3å€ï¼Œæ”¯æŒå¤šæ¡ä»¶ç»„åˆï¼ˆå¦‚è‰å›¾+é¢œè‰²ç½‘æ ¼ï¼‰ã€‚åº”ç”¨åœºæ™¯åŒ…æ‹¬çº¿ç¨¿è½¬å›¾åƒã€è‰²å½©é£Žæ ¼è¿ç§»ã€å¤šå…ƒç´ åœºæ™¯ç”Ÿæˆç­‰ã€‚\n\n### T2I Adapter ä¸Ž ControlNet çš„å¯¹æ¯”\n\nè™½ç„¶åŠŸèƒ½ç›¸ä¼¼ï¼Œä½†ä¸¤è€…åœ¨å®žçŽ°å’Œåº”ç”¨ä¸Šæœ‰æ˜Žæ˜¾åŒºåˆ«ï¼š\n\n1.  **è½»é‡çº§è®¾è®¡**ï¼šT2I Adapter å‚æ•°é‡æ›´å°‘ï¼Œå ç”¨å†…å­˜æ›´å°\n2.  **æŽ¨ç†é€Ÿåº¦**ï¼šT2I Adapter é€šå¸¸æ¯” ControlNet å¿«çº¦3å€\n3.  **æŽ§åˆ¶ç²¾åº¦**ï¼šControlNet åœ¨æŸäº›åœºæ™¯ä¸‹æŽ§åˆ¶æ›´ç²¾ç¡®ï¼Œè€Œ T2I Adapter æ›´é€‚åˆè½»é‡çº§æŽ§åˆ¶\n4.  **å¤šæ¡ä»¶ç»„åˆ**ï¼šT2I Adapter åœ¨å¤šæ¡ä»¶ç»„åˆæ—¶èµ„æºå ç”¨ä¼˜åŠ¿æ›´æ˜Žæ˜¾\n\n### T2I Adapter ä¸»è¦ç±»åž‹\n\nT2I Adapter æä¾›å¤šç§ç±»åž‹ä»¥æŽ§åˆ¶ä¸åŒæ–¹é¢ï¼š\n\n*   **æ·±åº¦ (Depth)**ï¼šæŽ§åˆ¶å›¾åƒçš„ç©ºé—´ç»“æž„å’Œæ·±åº¦å…³ç³»\n*   **çº¿ç¨¿ (Canny/Sketch)**ï¼šæŽ§åˆ¶å›¾åƒçš„è¾¹ç¼˜å’Œçº¿æ¡\n*   **å…³é”®ç‚¹ (Keypose)**ï¼šæŽ§åˆ¶äººç‰©å§¿æ€å’ŒåŠ¨ä½œ\n*   **åˆ†å‰² (Seg)**ï¼šé€šè¿‡è¯­ä¹‰åˆ†å‰²æŽ§åˆ¶åœºæ™¯å¸ƒå±€\n*   **é¢œè‰² (Color)**ï¼šæŽ§åˆ¶å›¾åƒçš„æ•´ä½“é…è‰²æ–¹æ¡ˆ\n\nåœ¨ ComfyUI ä¸­ï¼Œä½¿ç”¨ T2I Adapter ä¸Ž [ControlNet](https://docs.comfy.org/zh-CN/tutorials/controlnet/controlnet) çš„ç•Œé¢å’Œå·¥ä½œæµç›¸ä¼¼ã€‚åœ¨æœ¬ç¯‡ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»¥æ·±åº¦ T2I Adapter æŽ§åˆ¶å®¤å†…åœºæ™¯ä¸ºä¾‹ï¼Œå±•ç¤ºå…¶ä½¿ç”¨æ–¹æ³•ã€‚ ![ComfyUI Depth T2I Adapter å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)\n\n## æ·±åº¦ T2I Adapter åº”ç”¨ä»·å€¼\n\næ·±åº¦å›¾ï¼ˆDepth Mapï¼‰åœ¨å›¾åƒç”Ÿæˆä¸­æœ‰å¤šç§é‡è¦åº”ç”¨ï¼š\n\n1.  **ç©ºé—´å¸ƒå±€æŽ§åˆ¶**ï¼šå‡†ç¡®æè¿°ä¸‰ç»´ç©ºé—´ç»“æž„ï¼Œé€‚ç”¨äºŽå®¤å†…è®¾è®¡ã€å»ºç­‘å¯è§†åŒ–\n2.  **ç‰©ä½“å®šä½**ï¼šæŽ§åˆ¶åœºæ™¯ä¸­ç‰©ä½“çš„ç›¸å¯¹ä½ç½®å’Œå¤§å°ï¼Œé€‚ç”¨äºŽäº§å“å±•ç¤ºã€åœºæ™¯æž„å»º\n3.  **é€è§†å…³ç³»**ï¼šç»´æŒåˆç†çš„é€è§†å’Œæ¯”ä¾‹ï¼Œé€‚ç”¨äºŽé£Žæ™¯ã€åŸŽå¸‚åœºæ™¯ç”Ÿæˆ\n4.  **å…‰å½±å¸ƒå±€**ï¼šåŸºäºŽæ·±åº¦ä¿¡æ¯çš„è‡ªç„¶å…‰å½±åˆ†å¸ƒï¼Œå¢žå¼ºçœŸå®žæ„Ÿ\n\næˆ‘ä»¬å°†ä»¥å®¤å†…è®¾è®¡ä¸ºä¾‹ï¼Œå±•ç¤ºæ·±åº¦ T2I Adapter çš„ä½¿ç”¨æ–¹æ³•ï¼Œä½†è¿™äº›æŠ€å·§ä¹Ÿé€‚ç”¨äºŽå…¶ä»–åº”ç”¨åœºæ™¯ã€‚\n\n## ComfyUI Depth T2I Adapterå·¥ä½œæµç¤ºä¾‹è®²è§£\n\n### 1\\. Depth T2I Adapter å·¥ä½œæµç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - Depth T2I Adapter](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä¼šå°†å®ƒä½œä¸ºè¾“å…¥ ![ComfyUI å®¤å†…æ·±åº¦å›¾](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)\n\n### 2\\. æ¨¡åž‹å®‰è£…\n\n*   [interiordesignsuperm\\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [t2iadapter\\_depth\\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ interiordesignsuperm_v2.safetensors\nâ”‚   â””â”€â”€ controlnet/\nâ”‚       â””â”€â”€ t2iadapter_depth_sd15v2.pth\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI å·¥ä½œæµ - Depth T2I Adapter æµç¨‹å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg)\n\n1.  ç¡®ä¿`Load Checkpoint`å¯ä»¥åŠ è½½ **interiordesignsuperm\\_v2.safetensors**\n2.  ç¡®ä¿`Load ControlNet`å¯ä»¥åŠ è½½ **t2iadapter\\_depth\\_sd15v2.pth**\n3.  åœ¨`Load Image`ä¸­ç‚¹å‡»`Upload` ä¸Šä¼ ä¹‹å‰æä¾›çš„è¾“å…¥å›¾ç‰‡\n4.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n\n## T2I Adapter é€šç”¨ä½¿ç”¨æŠ€å·§\n\n### è¾“å…¥å›¾åƒè´¨é‡ä¼˜åŒ–\n\næ— è®ºåº”ç”¨åœºæ™¯å¦‚ä½•ï¼Œé«˜è´¨é‡çš„è¾“å…¥å›¾åƒéƒ½æ˜¯æˆåŠŸä½¿ç”¨ T2I Adapter çš„å…³é”®ï¼š\n\n1.  **å¯¹æ¯”åº¦é€‚ä¸­**ï¼šæŽ§åˆ¶å›¾åƒï¼ˆå¦‚æ·±åº¦å›¾ã€çº¿ç¨¿ï¼‰åº”æœ‰æ˜Žç¡®çš„å¯¹æ¯”ï¼Œä½†ä¸è¦è¿‡åº¦æžç«¯\n2.  **æ¸…æ™°çš„è¾¹ç•Œ**ï¼šç¡®ä¿ä¸»è¦ç»“æž„å’Œå…ƒç´ è¾¹ç•Œåœ¨æŽ§åˆ¶å›¾åƒä¸­æ¸…æ™°å¯è¾¨\n3.  **å™ªç‚¹æŽ§åˆ¶**ï¼šå°½é‡é¿å…æŽ§åˆ¶å›¾åƒä¸­æœ‰è¿‡å¤šå™ªç‚¹ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å›¾å’Œçº¿ç¨¿\n4.  **åˆç†çš„å¸ƒå±€**ï¼šæŽ§åˆ¶å›¾åƒåº”å½“å…·æœ‰åˆç†çš„ç©ºé—´å¸ƒå±€å’Œå…ƒç´ åˆ†å¸ƒ\n\n## T2I Adapter çš„ä½¿ç”¨ç‰¹ç‚¹\n\nT2I Adapter çš„ä¸€å¤§ä¼˜åŠ¿æ˜¯å¯ä»¥è½»æ¾ç»„åˆå¤šä¸ªæ¡ä»¶ï¼Œå®žçŽ°å¤æ‚çš„æŽ§åˆ¶æ•ˆæžœï¼š\n\n1.  **æ·±åº¦ + è¾¹ç¼˜**ï¼šæŽ§åˆ¶ç©ºé—´å¸ƒå±€çš„åŒæ—¶ä¿æŒç»“æž„è¾¹ç¼˜æ¸…æ™°ï¼Œé€‚ç”¨äºŽå»ºç­‘ã€å®¤å†…è®¾è®¡\n2.  **çº¿ç¨¿ + é¢œè‰²**ï¼šæŽ§åˆ¶å½¢çŠ¶çš„åŒæ—¶æŒ‡å®šé…è‰²æ–¹æ¡ˆï¼Œé€‚ç”¨äºŽè§’è‰²è®¾è®¡ã€æ’ç”»\n3.  **å§¿æ€ + åˆ†å‰²**ï¼šæŽ§åˆ¶äººç‰©åŠ¨ä½œçš„åŒæ—¶å®šä¹‰åœºæ™¯åŒºåŸŸï¼Œé€‚ç”¨äºŽå¤æ‚å™äº‹åœºæ™¯\n\nT2I Adapter ä¹‹é—´çš„æ··åˆï¼Œæˆ–ä¸Žå…¶ä»–æŽ§åˆ¶æ–¹æ³•ï¼ˆå¦‚ControlNetã€åŒºåŸŸæç¤ºè¯ç­‰ï¼‰çš„ç»„åˆï¼Œå¯ä»¥è¿›ä¸€æ­¥æ‰©å±•åˆ›ä½œå¯èƒ½æ€§ã€‚è¦å®žçŽ°æ··åˆï¼Œåªéœ€æŒ‰ç…§ä¸Ž [æ··åˆ ControlNet](https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets) ç›¸åŒçš„æ–¹å¼ï¼Œé€šè¿‡é“¾å¼è¿žæŽ¥å¤šä¸ª `Apply ControlNet` èŠ‚ç‚¹å³å¯ã€‚\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/3d/hunyuan3D-2",
  "markdown": "# ComfyUI Hunyuan3D-2 ç¤ºä¾‹ - ComfyUI\n\n## æ··å…ƒ3D 2.0 ç®€ä»‹\n\n ![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif) ![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif) [æ··å…ƒ3D 2.0](https://github.com/Tencent/Hunyuan3D-2) æ˜¯è…¾è®¯æŽ¨å‡ºçš„å¼€æº 3D èµ„äº§ç”Ÿæˆæ¨¡åž‹ï¼Œå¯ä»¥é€šè¿‡æ–‡æœ¬ã€å›¾åƒæˆ–è‰å›¾ç”Ÿæˆå¸¦æœ‰é«˜åˆ†è¾¨çŽ‡çº¹ç†è´´å›¾çš„é«˜ä¿çœŸ 3D æ¨¡åž‹ã€‚ æ··å…ƒ3D 2.0é‡‡ç”¨ä¸¤é˜¶æ®µç”Ÿæˆï¼Œé¦–å…ˆé‡‡ç”¨ç”Ÿæˆæ— çº¹ç†çš„å‡ ä½•æ¨¡åž‹ï¼Œå†åˆæˆé«˜åˆ†è¾¨çŽ‡çš„çº¹ç†è´´å›¾ï¼Œæœ‰æ•ˆåˆ†ç¦»äº†å½¢çŠ¶å’Œçº¹ç†ç”Ÿæˆçš„å¤æ‚æ€§ï¼Œä¸‹é¢æ˜¯æ··å…ƒ3D 2.0çš„ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶:\n\n1.  **å‡ ä½•ç”Ÿæˆæ¨¡åž‹ï¼ˆHunyuan3D-DiTï¼‰**ï¼šåŸºäºŽæµæ‰©æ•£çš„Transformeræž¶æž„ï¼Œç”Ÿæˆæ— çº¹ç†çš„å‡ ä½•æ¨¡åž‹ï¼Œå¯ç²¾å‡†åŒ¹é…è¾“å…¥æ¡ä»¶ã€‚\n2.  **çº¹ç†ç”Ÿæˆæ¨¡åž‹ï¼ˆHunyuan3D-Paintï¼‰**ï¼šç»“åˆå‡ ä½•æ¡ä»¶å’Œå¤šè§†å›¾æ‰©æ•£æŠ€æœ¯ï¼Œä¸ºæ¨¡åž‹æ·»åŠ é«˜åˆ†è¾¨çŽ‡çº¹ç†ï¼Œæ”¯æŒPBRæè´¨ã€‚\n\n**ä¸»è¦ä¼˜åŠ¿**\n\n*   **é«˜ç²¾åº¦ç”Ÿæˆ**ï¼šå‡ ä½•ç»“æž„é”åˆ©ï¼Œçº¹ç†è‰²å½©ä¸°å¯Œï¼Œæ”¯æŒPBRæè´¨ç”Ÿæˆï¼Œå®žçŽ°æŽ¥è¿‘çœŸå®žçš„å…‰å½±æ•ˆæžœã€‚\n*   **å¤šæ ·åŒ–ä½¿ç”¨æ–¹å¼**ï¼šæä¾›ä»£ç è°ƒç”¨ã€Blenderæ’ä»¶ã€Gradioåº”ç”¨åŠå®˜ç½‘åœ¨çº¿ä½“éªŒï¼Œé€‚åˆä¸åŒç”¨æˆ·éœ€æ±‚ã€‚\n*   **è½»é‡åŒ–ä¸Žå…¼å®¹æ€§**ï¼šHunyuan3D-2miniæ¨¡åž‹ä»…éœ€5GBæ˜¾å­˜ï¼Œæ ‡å‡†ç‰ˆæœ¬å½¢çŠ¶ç”Ÿæˆéœ€6GBæ˜¾å­˜ï¼Œå®Œæ•´æµç¨‹ï¼ˆå½¢çŠ¶+çº¹ç†ï¼‰ä»…éœ€12GBæ˜¾å­˜ã€‚\n\nè¿‘æœŸï¼ˆ2025 å¹´ 3 æœˆ 18 æ—¥ï¼‰ï¼Œæ··å…ƒ3D 2.0 è¿˜æä¾›å¤šè§†è§’å½¢çŠ¶ç”Ÿæˆæ¨¡åž‹ï¼ˆHunyuan3D-2mvï¼‰ï¼Œæ”¯æŒä»Žä¸åŒè§†è§’è¾“å…¥ç”Ÿæˆæ›´ç²¾ç»†çš„å‡ ä½•ç»“æž„ã€‚ åœ¨æœ¬ç¤ºä¾‹ä¸­åŒ…å«ä¸‰ä¸ªå·¥ä½œæµï¼š\n\n*   ä½¿ç”¨ Hunyuan3D-2mv é…åˆå¤šä¸ªè§†å›¾è¾“å…¥ç”Ÿæˆ3Dæ¨¡åž‹\n*   ä½¿ç”¨ Hunyuan3D-2mv-turbo é…åˆå¤šä¸ªè§†å›¾è¾“å…¥ç”Ÿæˆ3Dæ¨¡åž‹\n*   ä½¿ç”¨ Hunyuan3D-2 é…åˆå•ä¸ªè§†å›¾è¾“å…¥ç”Ÿæˆ3Dæ¨¡åž‹\n\nHunyuan3D-2mv å·¥ä½œæµä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¤šè§†è§’çš„å›¾ç‰‡æ¥ç”Ÿæˆ3Dæ¨¡åž‹ï¼Œå¦å¤–å¤šä¸ªè§†è§’çš„å›¾ç‰‡åœ¨è¿™ä¸ªå·¥ä½œæµä¸­å¹¶ä¸æ˜¯å¿…é¡»çš„ï¼Œä½ å¯ä»¥åªè¾“å…¥ `front` è§†è§’çš„å›¾ç‰‡æ¥ç”Ÿæˆ3Dæ¨¡åž‹ã€‚\n\n### 1\\. å·¥ä½œæµ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ, ![Hunyuan3D-2mv workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/hunyuan-3d-multiview-elf.webp) ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼ŒåŒæ—¶æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›å›¾ç‰‡ä½œä¸ºå›¾ç‰‡è¾“å…¥\n\n![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/front.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/left.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/back.png)\n\n### 2\\. æ‰‹åŠ¨å®‰è£…æ¨¡åž‹\n\nä¸‹è½½ä¸‹é¢çš„æ¨¡åž‹ï¼Œå¹¶ä¿å­˜åˆ°å¯¹åº”çš„ ComfyUI æ–‡ä»¶å¤¹\n\n*   hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) ä¸‹è½½åŽå¯é‡å‘½åä¸º `hunyuan3d-dit-v2-mv.safetensors`\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ hunyuan3d-dit-v2-mv.safetensors  // é‡å‘½ååŽçš„æ–‡ä»¶\n```\n\n### 3\\. æŒ‰æ­¥éª¤è¿è¡Œå·¥ä½œæµ\n\n![ComfyUI hunyuan3d_2mv](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg)\n\n1.  ç¡®ä¿ Image Only Checkpoint Loader(img2vid model) åŠ è½½äº†æˆ‘ä»¬ä¸‹è½½å¹¶é‡å‘½åçš„ `hunyuan3d-dit-v2-mv.safetensors` æ¨¡åž‹\n2.  åœ¨ `Load Image` èŠ‚ç‚¹çš„å„ä¸ªè§†è§’ä¸­åŠ è½½äº†å¯¹åº”è§†è§’çš„å›¾ç‰‡\n3.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\nå¦‚æžœä½ éœ€è¦å¢žåŠ æ›´å¤šçš„è§†è§’ï¼Œè¯·ç¡®ä¿ `Hunyuan3Dv2ConditioningMultiView` èŠ‚ç‚¹ä¸­åŠ è½½äº†å…¶å®ƒè§†è§’çš„å›¾ç‰‡ï¼Œå¹¶ç¡®ä¿åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½äº†å¯¹åº”è§†è§’çš„å›¾ç‰‡ã€‚\n\n## ä½¿ç”¨ Hunyuan3D-2mv-turbo å·¥ä½œæµ\n\nHunyuan3D-2mv-turbo å·¥ä½œæµä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Hunyuan3D-2mv-turbo æ¨¡åž‹æ¥ç”Ÿæˆ3Dæ¨¡åž‹ï¼Œè¿™ä¸ªæ¨¡åž‹æ˜¯ Hunyuan3D-2mv çš„åˆ†æ­¥è’¸é¦ï¼ˆStep Distillationï¼‰ç‰ˆæœ¬ï¼Œå¯ä»¥æ›´å¿«åœ°ç”Ÿæˆ3Dæ¨¡åž‹ï¼Œåœ¨è¿™ä¸ªç‰ˆæœ¬çš„å·¥ä½œæµä¸­æˆ‘ä»¬è®¾ç½® `cfg` ä¸º 1.0 å¹¶æ·»åŠ  `flux guidance` èŠ‚ç‚¹æ¥æŽ§åˆ¶ `distilled cfg` çš„ç”Ÿæˆã€‚\n\n### 1\\. å·¥ä½œæµ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ, ![Hunyuan3D-2mv-turbo workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/hunyuan-3d-turbo.webp) æˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºå¤šè§†è§’çš„è¾“å…¥\n\n![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/front.png)![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/right.png)\n\n### 2\\. æ‰‹åŠ¨å®‰è£…æ¨¡åž‹\n\nä¸‹è½½ä¸‹é¢çš„æ¨¡åž‹ï¼Œå¹¶ä¿å­˜åˆ°å¯¹åº”çš„ ComfyUI æ–‡ä»¶å¤¹\n\n*   hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) ä¸‹è½½åŽå¯é‡å‘½åä¸º `hunyuan3d-dit-v2-mv-turbo.safetensors`\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ hunyuan3d-dit-v2-mv-turbo.safetensors  // é‡å‘½ååŽçš„æ–‡ä»¶\n```\n\n### 3\\. æŒ‰æ­¥éª¤è¿è¡Œå·¥ä½œæµ\n\n![ComfyUI hunyuan3d_2mv_turbo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg)\n\n1.  ç¡®ä¿ `Image Only Checkpoint Loader(img2vid model)` èŠ‚ç‚¹åŠ è½½äº†æˆ‘ä»¬é‡å‘½ååŽçš„ `hunyuan3d-dit-v2-mv-turbo.safetensors` æ¨¡åž‹\n2.  åœ¨ `Load Image` èŠ‚ç‚¹çš„å„ä¸ªè§†è§’ä¸­åŠ è½½äº†å¯¹åº”è§†è§’çš„å›¾ç‰‡\n3.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n## ä½¿ç”¨ Hunyuan3D-2 å•è§†å›¾å·¥ä½œæµ\n\nHunyuan3D-2 å·¥ä½œæµä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Hunyuan3D-2 æ¨¡åž‹æ¥ç”Ÿæˆ3Dæ¨¡åž‹ï¼Œè¿™ä¸ªæ¨¡åž‹ä¸æ˜¯ä¸€ä¸ªå¤šè§†è§’çš„æ¨¡åž‹ï¼Œåœ¨è¿™ä¸ªå·¥ä½œæµä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`Hunyuan3Dv2Conditioning` èŠ‚ç‚¹æ›¿æ¢æŽ‰ `Hunyuan3Dv2ConditioningMultiView` èŠ‚ç‚¹ã€‚\n\n### 1\\. å·¥ä½œæµ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![Hunyuan3D-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d-non-multiview-train.webp) åŒæ—¶æˆ‘ä»¬å°†ä½¿ç”¨è¿™å¼ å›¾ç‰‡ä½œä¸ºå›¾ç‰‡è¾“å…¥ ![ComfyUI Hunyuan 3D 2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan_3d_v2_non_multiview_train.png)\n\n### 2\\. æ‰‹åŠ¨å®‰è£…æ¨¡åž‹\n\nä¸‹è½½ä¸‹é¢çš„æ¨¡åž‹ï¼Œå¹¶ä¿å­˜åˆ°å¯¹åº”çš„ ComfyUI æ–‡ä»¶å¤¹\n\n*   hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) ä¸‹è½½åŽå¯é‡å‘½åä¸º `hunyuan3d-dit-v2.safetensors`\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ hunyuan3d-dit-v2.safetensors  // é‡å‘½ååŽçš„æ–‡ä»¶\n```\n\n### 3\\. æŒ‰æ­¥éª¤è¿è¡Œå·¥ä½œæµ\n\n![ComfyUI hunyuan3d_2](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg)\n\n1.  ç¡®ä¿ `Image Only Checkpoint Loader(img2vid model)` èŠ‚ç‚¹åŠ è½½äº†æˆ‘ä»¬é‡å‘½ååŽçš„ `hunyuan3d-dit-v2.safetensors` æ¨¡åž‹\n2.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½äº†å¯¹åº”è§†è§’çš„å›¾ç‰‡\n3.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n## ç¤¾åŒºèµ„æº\n\nä¸‹é¢æ˜¯ Hunyuan3D-2 çš„ç›¸å…³çš„ ComfyUI ç¤¾åŒºèµ„æº\n\n*   [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)\n*   [Kijai/Hunyuan3D-2\\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)\n*   [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)\n\n## æ··å…ƒ3D 2.0 å¼€æºæ¨¡åž‹ç³»åˆ—\n\nç›®å‰æ··å…ƒ3D 2.0 å¼€æºäº†å¤šä¸ªæ¨¡åž‹ï¼Œè¦†ç›–äº†å®Œæ•´çš„3Dç”Ÿæˆæµç¨‹ï¼Œä½ å¯ä»¥è®¿é—® [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) äº†è§£æ›´å¤šã€‚ **Hunyuan3D-2mini ç³»åˆ—**\n\n| æ¨¡åž‹  | æè¿°  | æ—¥æœŸ  | å‚æ•°  | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-mini | Mini å›¾åƒåˆ°å½¢çŠ¶æ¨¡åž‹ | 2025-03-18 | 0.6B | [å‰å¾€](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini) |\n\n**Hunyuan3D-2mv ç³»åˆ—**\n\n| æ¨¡åž‹  | æè¿°  | æ—¥æœŸ  | å‚æ•°  | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-mv-Fast | æŒ‡å¯¼è’¸é¦ç‰ˆæœ¬ï¼Œå¯ä»¥å°† DIT æŽ¨ç†æ—¶é—´å‡åŠ | 2025-03-18 | 1.1B | [å‰å¾€](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast) |\n| Hunyuan3D-DiT-v2-mv | å¤šè§†è§’å›¾åƒåˆ°å½¢çŠ¶æ¨¡åž‹ï¼Œé€‚åˆéœ€è¦ç”¨å¤šä¸ªè§’åº¦ç†è§£åœºæ™¯çš„ 3D åˆ›ä½œ | 2025-03-18 | 1.1B | [å‰å¾€](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv) |\n\n**Hunyuan3D-2 ç³»åˆ—**\n\n| æ¨¡åž‹  | æè¿°  | æ—¥æœŸ  | å‚æ•°  | Huggingface |\n| --- | --- | --- | --- | --- |\n| Hunyuan3D-DiT-v2-0-Fast | æŒ‡å¯¼è’¸é¦æ¨¡åž‹ | 2025-02-03 | 1.1B | [å‰å¾€](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast) |\n| Hunyuan3D-DiT-v2-0 | å›¾åƒåˆ°å½¢çŠ¶æ¨¡åž‹ | 2025-01-21 | 1.1B | [å‰å¾€](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0) |\n| Hunyuan3D-Paint-v2-0 | çº¹ç†ç”Ÿæˆæ¨¡åž‹ | 2025-01-21 | 1.3B | [å‰å¾€](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0) |\n| Hunyuan3D-Delight-v2-0 | å›¾åƒåŽ»å…‰å½±æ¨¡åž‹ | 2025-01-21 | 1.3B | [å‰å¾€](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0) |\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-control",
  "markdown": "# ComfyUI Wan2.1 Fun Control è§†é¢‘ç¤ºä¾‹\n\n**Wan2.1-Fun-Control** æ˜¯é˜¿é‡Œå›¢é˜ŸæŽ¨å‡ºçš„å¼€æºè§†é¢‘ç”Ÿæˆä¸ŽæŽ§åˆ¶é¡¹ç›®ï¼Œé€šè¿‡å¼•å…¥åˆ›æ–°æ€§çš„æŽ§åˆ¶ä»£ç ï¼ˆControl Codesï¼‰æœºåˆ¶ï¼Œç»“åˆæ·±åº¦å­¦ä¹ å’Œå¤šæ¨¡æ€æ¡ä»¶è¾“å…¥ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ä¸”ç¬¦åˆé¢„è®¾æŽ§åˆ¶æ¡ä»¶çš„è§†é¢‘ã€‚è¯¥é¡¹ç›®ä¸“æ³¨äºŽé€šè¿‡å¤šæ¨¡æ€æŽ§åˆ¶æ¡ä»¶å®žçŽ°å¯¹ç”Ÿæˆè§†é¢‘å†…å®¹çš„ç²¾å‡†å¼•å¯¼ã€‚ ç›®å‰ Fun Control æ¨¡åž‹æ”¯æŒå¤šç§æŽ§åˆ¶æ¡ä»¶ï¼ŒåŒ…æ‹¬ **Cannyï¼ˆçº¿ç¨¿ï¼‰**ã€**Depthï¼ˆæ·±åº¦ï¼‰**ã€**OpenPoseï¼ˆäººä½“å§¿åŠ¿ï¼‰**ã€**MLSDï¼ˆå‡ ä½•è¾¹ç¼˜ï¼‰** ç­‰ï¼ŒåŒæ—¶æ”¯æŒä½¿ç”¨ **è½¨è¿¹æŽ§åˆ¶**ã€‚ æ¨¡åž‹è¿˜æ”¯æŒå¤šåˆ†è¾¨çŽ‡è§†é¢‘é¢„æµ‹ï¼Œåˆ†è¾¨çŽ‡å¯é€‰ 512ã€768 å’Œ 1024ï¼Œå¸§çŽ‡ä¸ºæ¯ç§’ 16 å¸§ï¼Œæœ€é•¿å¯ç”Ÿæˆ 81 å¸§ï¼ˆçº¦ 5 ç§’ï¼‰çš„è§†é¢‘ã€‚ æ¨¡åž‹ç‰ˆæœ¬æ–¹é¢ï¼š\n\n*   **1.3B** è½»é‡ç‰ˆï¼šé€‚åˆæœ¬åœ°éƒ¨ç½²å’Œå¿«é€ŸæŽ¨ç†ï¼Œ**å¯¹æ˜¾å­˜è¦æ±‚è¾ƒä½Ž**\n*   **14B** é«˜æ€§èƒ½ç‰ˆï¼šæ¨¡åž‹ä½“ç§¯è¾¾ 32GB+ï¼Œæ•ˆæžœæ›´ä¼˜ä½† **éœ€é«˜æ˜¾å­˜æ”¯æŒ**\n\nä¸‹é¢æ˜¯ç›¸å…³ä»£ç ä»“åº“çš„ç¤ºä¾‹\n\n*   [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)\n*   [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)\n*   ä»£ç ä»“åº“ï¼š[VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)\n\n**ç›®å‰ ComfyUI å·²åŽŸç”Ÿæ”¯æŒäº† Wan2.1 Fun Control æ¨¡åž‹** ï¼Œåœ¨å¼€å§‹æœ¬ç¯‡æ•™ç¨‹å‰ï¼Œè¯·æ›´æ–°ä½ çš„ ComfyUI ä¿è¯ä½ çš„ç‰ˆæœ¬åœ¨[è¿™ä¸ªæäº¤](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82)ç‰ˆæœ¬ä¹‹åŽ åœ¨æœ¬ç¯‡æŒ‡å—ä¸­æˆ‘ä»¬å°†æä¾›ä¸¤ä¸ªå·¥ä½œæµï¼š\n\n*   ä»…ä½¿ç”¨åŽŸç”Ÿçš„ Comfy Core èŠ‚ç‚¹çš„å·¥ä½œæµ\n*   ä½¿ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹çš„å·¥ä½œæµ\n\n## ç›¸å…³æ¨¡åž‹å®‰è£…\n\nè¿™äº›æ¨¡åž‹ä½ ä»…éœ€è¦å®‰è£…ä¸€æ¬¡ï¼Œå¦å¤–åœ¨å¯¹åº”çš„å·¥ä½œæµå›¾ç‰‡ä¸­ä¹ŸåŒ…å«äº†æ¨¡åž‹ä¸‹è½½ä¿¡æ¯ï¼Œä½ å¯ä»¥é€‰æ‹©ä½ å–œæ¬¢çš„æ–¹å¼ä¸‹è½½æ¨¡åž‹ã€‚ ä¸‹é¢çš„æ¨¡åž‹ä½ å¯ä»¥åœ¨ [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) å’Œ [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) æ‰¾åˆ° ç‚¹å‡»å¯¹åº”é“¾æŽ¥è¿›è¡Œä¸‹è½½ï¼Œå¦‚æžœä½ ä¹‹å‰ä½¿ç”¨è¿‡ Wan ç›¸å…³çš„å·¥ä½œæµï¼Œé‚£ä¹ˆä½ ä»…éœ€è¦ä¸‹è½½ **Diffusino models** **Diffusion models** é€‰æ‹© 1.3B æˆ– 14B, 14B çš„æ–‡ä»¶ä½“ç§¯æ›´å¤§ï¼ˆ32GBï¼‰ä½†æ˜¯å¯¹äºŽè¿è¡Œæ˜¾å­˜è¦æ±‚ä¹Ÿè¾ƒé«˜ï¼Œ\n\n*   [wan2.1\\_fun\\_control\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)\n*   [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true)ï¼š å»ºè®®ä¸‹è½½åŽé‡å‘½åä¸º `Wan2.1-Fun-14B-Control.safetensors`\n\n**Text encoders** é€‰æ‹©ä¸‹é¢ä¸¤ä¸ªæ¨¡åž‹ä¸­çš„ä¸€ä¸ªï¼Œfp16 ç²¾åº¦ä½“ç§¯è¾ƒå¤§å¯¹æ€§èƒ½è¦æ±‚é«˜\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\næ–‡ä»¶ä¿å­˜ä½ç½®\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ wan2.1_fun_control_1.3B_bf16.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ wan_2.1_vae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ clip_vision/\nâ”‚       â””â”€â”€  clip_vision_h.safetensors                 \n```\n\n## ComfyUI åŽŸç”Ÿå·¥ä½œæµ\n\nåœ¨æ­¤å·¥ä½œæµä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è½¬æ¢æˆ WebP æ ¼å¼çš„è§†é¢‘ï¼Œè¿™æ˜¯å› ä¸ºç›®å‰`Load Image` èŠ‚ç‚¹è¿˜ä¸æ”¯æŒ mp4 æ ¼å¼çš„è§†é¢‘ï¼Œå¦å¤–æˆ‘ä»¬ä½¿ç”¨ Canny Edge æ¥å¯¹åŽŸå§‹çš„è§†é¢‘è¿›è¡Œå›¾åƒçš„é¢„å¤„ç†, ç”±äºŽç»å¸¸æœ‰ç”¨æˆ·åœ¨å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹è¿‡ç¨‹ä¸­é‡åˆ°å®‰è£…å¤±è´¥å’ŒçŽ¯å¢ƒçš„é—®é¢˜ï¼Œæ‰€ä»¥è¿™ä¸€ç‰ˆæœ¬çš„å·¥ä½œæµå®Œå…¨ä½¿ç”¨åŽŸç”ŸèŠ‚ç‚¹æ¥å®žçŽ°ï¼Œæ¥ä¼˜å…ˆä¿è¯ä½“éªŒã€‚ æ„Ÿè°¢æˆ‘ä»¬å¼ºå¤§çš„ ComfyUI ä½œè€…ä»¬ï¼Œä»–ä»¬å¸¦æ¥äº†åŠŸèƒ½ä¸°å¯Œçš„ç›¸å…³èŠ‚ç‚¹ï¼Œå¦‚æžœä½ éœ€è¦ç›´æŽ¥æŸ¥çœ‹ç›¸å…³ç‰ˆæœ¬ç›´æŽ¥æŸ¥çœ‹[ä½¿ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹çš„å·¥ä½œæµ](#%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E8%8A%82%E7%82%B9%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81)\n\n### 1\\. å·¥ä½œæµç›¸å…³æ–‡ä»¶ä¸‹è½½\n\n#### 1.1 å·¥ä½œæµæ–‡ä»¶\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![Wan2.1 Fun Control åŽŸç”Ÿå·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_native.webp)\n\n#### 1.2 è¾“å…¥å›¾ç‰‡åŠè§†é¢‘ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡åŠè§†é¢‘ï¼Œæˆ‘ä»¬å°†ä½œä¸ºè¾“å…¥ã€‚ ![è¾“å…¥å‚è€ƒå›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_remix.png) ![è¾“å…¥å‚è€ƒè§†é¢‘](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_video.webp)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµ\n\n![Wan2.1 Fun Control å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_native_flow_diagram.png)\n\n1.  ç¡®ä¿ `Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `wan2.1_fun_control_1.3B_bf16.safetensors`\n2.  ç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  ç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors`\n4.  ç¡®ä¿ `Load CLIP Vision` èŠ‚ç‚¹åŠ è½½äº† `clip_vision_h.safetensors`\n5.  åœ¨ `Load Image` èŠ‚ç‚¹ï¼ˆå·²è¢«é‡å‘½åä¸º`Start_image`ï¼‰ ä¸Šä¼ èµ·å§‹å¸§\n6.  åœ¨ç¬¬äºŒä¸ª `Load Image` èŠ‚ç‚¹ä¸Šä¼ ç”¨äºŽæŽ§åˆ¶è§†é¢‘ã€‚æ³¨æ„ï¼š ç›®å‰è¿™ä¸ªèŠ‚ç‚¹è¿˜ä¸æ”¯æŒ mp4 åªèƒ½ä½¿ç”¨ Webp è§†é¢‘\n7.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹ Prompt ä½¿ç”¨ä¸­è‹±æ–‡éƒ½å¯ä»¥\n8.  ï¼ˆå¯é€‰ï¼‰åœ¨ `WanFunControlToVideo` ä¿®æ”¹å¯¹åº”è§†é¢‘çš„å°ºå¯¸ï¼Œä¸è¦ä½¿ç”¨è¿‡å¤§çš„å°ºå¯¸\n9.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n\n### 3\\. ä½¿ç”¨è¯´æ˜Ž\n\n*   ç”±äºŽæˆ‘ä»¬éœ€è¦å’ŒæŽ§åˆ¶è§†é¢‘ä¸€è‡´çš„å¸§æ•°è¾“å…¥åˆ° `WanFunControlToVideo` èŠ‚ç‚¹ï¼Œå¦‚æžœå¯¹åº”çš„å¸§æ•°æ•°å€¼å¤§äºŽå®žé™…çš„æŽ§åˆ¶è§†é¢‘å¸§æ•°ï¼Œå°†ä¼šå¯¼è‡´å¤šä½™çš„å¸§ä¸ç¬¦åˆæŽ§åˆ¶æ¡ä»¶çš„ç”»é¢å‡ºçŽ°ï¼Œè¿™ä¸ªé—®é¢˜æˆ‘ä»¬å°†åœ¨[ä½¿ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹çš„å·¥ä½œæµ](#%E4%BD%BF%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E8%8A%82%E7%82%B9%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81)ä¸­è§£å†³\n*   ä½¿ç”¨ç±»ä¼¼ [ComfyUI-comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) æ¥å®žçŽ°æ›´ä¸°å¯Œçš„æŽ§åˆ¶\n\n## ä½¿ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹çš„å·¥ä½œæµ\n\næˆ‘ä»¬å°†éœ€è¦å®‰è£…ä¸‹é¢ä¸¤ä¸ªè‡ªå®šä¹‰èŠ‚ç‚¹ï¼š\n\n*   [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n*   [ComfyUI-comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n\nä½ å¯ä»¥ä½¿ç”¨ [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) å®‰è£…ç¼ºå¤±èŠ‚ç‚¹çš„åŠŸèƒ½æˆ–è€…å‚ç…§å¯¹åº”è‡ªå®šä¹‰èŠ‚ç‚¹åŒ…çš„å®‰è£…è¯´æ˜Žæ¥å®Œæˆå¯¹åº”èŠ‚ç‚¹çš„å®‰è£…\n\n### 1\\. å·¥ä½œæµç›¸å…³æ–‡ä»¶ä¸‹è½½\n\n#### 1.1 å·¥ä½œæµæ–‡ä»¶\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![å·¥ä½œæµæ–‡ä»¶](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.webp)\n\n#### 1.2 è¾“å…¥å›¾ç‰‡åŠè§†é¢‘ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡åŠè§†é¢‘ï¼Œæˆ‘ä»¬å°†ä¼šç”¨äºŽè¾“å…¥ ![è¾“å…¥å‚è€ƒå›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-robot's_eye.png) \n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµ\n\n![Wan2.1 Fun Control ä½¿ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹çš„å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png)\n\n> æ¨¡åž‹éƒ¨åˆ†åŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œå¦‚æžœä½ å·²ç»ä½“éªŒè¿‡ä»…ä½¿ç”¨åŽŸç”ŸèŠ‚ç‚¹çš„å·¥ä½œæµï¼Œä½ å¯ä»¥ç›´æŽ¥ä¸Šä¼ å¯¹åº”çš„å›¾ç‰‡ç„¶åŽè¿è¡Œå³å¯\n\n1.  ç¡®ä¿ `Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `wan2.1_fun_control_1.3B_bf16.safetensors`\n2.  ç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  ç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors`\n4.  ç¡®ä¿ `Load CLIP Vision` èŠ‚ç‚¹åŠ è½½äº† `clip_vision_h.safetensors`\n5.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸Šä¼ èµ·å§‹å¸§\n6.  åœ¨ `Load Video(Upload)` è‡ªå®šä¹‰èŠ‚ç‚¹ä¸Šä¼  mp4 æ ¼å¼è§†é¢‘ï¼Œè¯·æ³¨æ„å¯¹åº”å·¥ä½œæµæœ‰å¯¹é»˜è®¤çš„ `frame_load_cap`è¿›è¡Œäº†è°ƒæ•´\n7.  `DWPose Estimator` å¤„é’ˆå¯¹å½“å‰å›¾åƒä»…ä½¿ç”¨äº† `detect_face` çš„é€‰é¡¹\n8.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹ Prompt ä½¿ç”¨ä¸­è‹±æ–‡éƒ½å¯ä»¥\n9.  ï¼ˆå¯é€‰ï¼‰åœ¨ `WanFunControlToVideo` ä¿®æ”¹å¯¹åº”è§†é¢‘çš„å°ºå¯¸ï¼Œä¸è¦ä½¿ç”¨è¿‡å¤§çš„å°ºå¯¸\n10.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n\n### 3\\. å·¥ä½œæµè¯´æ˜Ž\n\næ„Ÿè°¢ ComfyUI ç¤¾åŒºä½œè€…å¸¦æ¥çš„è‡ªå®šä¹‰èŠ‚ç‚¹åŒ…\n\n*   åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ä½¿ç”¨äº† `Load Video(Upload)` æ¥å®žçŽ°å¯¹ mp4 è§†é¢‘çš„æ”¯æŒ\n*   `Load Video(Upload)` ä¸­èŽ·å–åˆ°çš„ `video_info` æˆ‘ä»¬å¾—ä»¥å¯¹è¾“å‡ºçš„è§†é¢‘ä¿æŒåŒæ ·çš„ `fps`\n*   ä½ å¯ä»¥æ›¿æ¢ `DWPose Estimator` ä¸º `ComfyUI-comfyui_controlnet_aux` èŠ‚ç‚¹åŒ…ä¸­çš„å…¶å®ƒé¢„å¤„ç†å™¨\n\n## ä½¿ç”¨æŠ€å·§\n\n![Apply Multi Control Videos](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/apply_multi_control_videos.jpg)\n\n*   ä¸€ä¸ªæœ‰ç”¨çš„æŠ€å·§æ˜¯ï¼Œä½ å¯ä»¥ç»“åˆå¤šç§å›¾åƒé¢„å¤„ç†æŠ€æœ¯ï¼Œç„¶åŽä½¿ç”¨ `Image Blend` èŠ‚ç‚¹æ¥å®žçŽ°åŒæ—¶åº”ç”¨å¤šç§æŽ§åˆ¶æ–¹æ³•çš„ç›®çš„ã€‚\n*   ä½ å¯ä»¥ä½¿ç”¨ `ComfyUI-VideoHelperSuite` çš„ `Video Combine` èŠ‚ç‚¹æ¥å®žçŽ°å°†å¯¹åº”è§†é¢‘å­˜å‚¨ä¸º mp4 æ ¼å¼\n*   æˆ‘ä»¬ä½¿ç”¨ `SaveAnimatedWEBP` æ˜¯å› ä¸ºæˆ‘ä»¬ç›®å‰å¹¶ä¸æ”¯æŒåœ¨ **mp4** ä¸­åµŒå…¥å·¥ä½œæµä¿¡æ¯, è€Œä¸”æœ‰äº›è‡ªå®šä¹‰èŠ‚ç‚¹å¯èƒ½æ²¡æœ‰è€ƒè™‘å·¥ä½œæµåµŒå…¥ï¼Œä¸ºäº†åœ¨è§†é¢‘ä¸­ä¿å­˜å·¥ä½œæµï¼Œæ‰€ä»¥æˆ‘ä»¬é€‰æ‹© `SaveAnimatedWEBP` èŠ‚ç‚¹ã€‚\n*   ä¸è¦è®¾ç½®è¿‡å¤§çš„ç”»é¢å°ºå¯¸ï¼Œè¿™å¯èƒ½å¯¼è‡´é‡‡æ ·è¿‡ç¨‹éžå¸¸è€—æ—¶ï¼Œå¯ä»¥è¯•ç€å…ˆç”Ÿæˆå°å°ºå¯¸çš„å›¾ç‰‡ç„¶åŽå†è¿›è¡Œé‡‡æ ·æ”¾å¤§\n*   å‘æŒ¥ä½ çš„æƒ³è±¡åŠ›ï¼Œåœ¨è¿™ä¸ªå·¥ä½œæµåŸºç¡€ä¸ŠåŠ ä¸Šä¸€äº›æ–‡ç”Ÿå›¾æˆ–è€…å…¶å®ƒç±»åž‹çš„å·¥ä½œæµï¼Œå®žçŽ°ç›´æŽ¥ä»Žæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆé£Žæ ¼è½¬æ¢\n*   åœ¨ `WanFunControlToVideo` èŠ‚ç‚¹ä¸­ï¼Œ`control_video` ä¸æ˜¯å¿…é¡»çš„ï¼Œæ‰€ä»¥æœ‰æ—¶å€™ä½ å¯ä»¥ä¸ä½¿ç”¨æŽ§åˆ¶è§†é¢‘ï¼Œå…ˆç”Ÿæˆç‰¹åˆ«å°å°ºå¯¸çš„è§†é¢‘æ¯”å¦‚ 320x320ï¼Œç„¶åŽä½¿ç”¨å†æŠŠå®ƒä»¬ä½œä¸ºæŽ§åˆ¶è§†é¢‘è¾“å…¥æ¥èŽ·å¾—ç¡®å®šçš„ç»“æžœ\n\n## å…¶å®ƒ Wan2.1 Fun Control æˆ–è€…è§†é¢‘ç›¸å…³è‡ªå®šä¹‰èŠ‚ç‚¹\n\n*   [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n*   [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-inp",
  "markdown": "# ComfyUI Wan2.1 Fun InP è§†é¢‘ç¤ºä¾‹\n\nWan-Fun InP æ˜¯é˜¿é‡Œå·´å·´æŽ¨å‡ºçš„å¼€æºè§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼Œå±žäºŽ â€‹â€‹Wan2.1-Funâ€‹â€‹ ç³»åˆ—çš„ä¸€éƒ¨åˆ†ï¼Œä¸“æ³¨äºŽé€šè¿‡å›¾åƒç”Ÿæˆè§†é¢‘å¹¶å®žçŽ°é¦–å°¾å¸§æŽ§åˆ¶ã€‚ **æ ¸å¿ƒåŠŸèƒ½**ï¼š\n\n*   é¦–å°¾å¸§æŽ§åˆ¶ï¼šæ”¯æŒè¾“å…¥é¦–å¸§å’Œå°¾å¸§å›¾åƒï¼Œç”Ÿæˆä¸­é—´è¿‡æ¸¡è§†é¢‘ï¼Œæå‡è§†é¢‘è¿žè´¯æ€§ä¸Žåˆ›æ„è‡ªç”±åº¦ã€‚ç›¸æ¯”æ—©æœŸç¤¾åŒºç‰ˆæœ¬ï¼Œé˜¿é‡Œå®˜æ–¹æ¨¡åž‹çš„ç”Ÿæˆæ•ˆæžœæ›´ç¨³å®šä¸”è´¨é‡æ˜¾è‘—æå‡ã€‚\n*   å¤šåˆ†è¾¨çŽ‡æ”¯æŒï¼šæ”¯æŒç”Ÿæˆ512Ã—512ã€768Ã—768ã€1024Ã—1024ç­‰åˆ†è¾¨çŽ‡çš„è§†é¢‘ï¼Œé€‚é…ä¸åŒåœºæ™¯éœ€æ±‚ã€‚\n\n**æ¨¡åž‹ç‰ˆæœ¬æ–¹é¢**ï¼š\n\n*   1.3B è½»é‡ç‰ˆï¼šé€‚åˆæœ¬åœ°éƒ¨ç½²å’Œå¿«é€ŸæŽ¨ç†ï¼Œå¯¹æ˜¾å­˜è¦æ±‚è¾ƒä½Ž\n*   14B é«˜æ€§èƒ½ç‰ˆï¼šæ¨¡åž‹ä½“ç§¯è¾¾ 32GB+ï¼Œæ•ˆæžœæ›´ä¼˜ä½†éœ€é«˜æ˜¾å­˜æ”¯æŒ\n\nä¸‹é¢æ˜¯ç›¸å…³æ¨¡åž‹æƒé‡å’Œä»£ç ä»“åº“ï¼š\n\n*   [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)\n*   [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)\n*   ä»£ç ä»“åº“ï¼š[VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)\n\n## Wan2.1 Fun Control å·¥ä½œæµ\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![å·¥ä½œæµæ–‡ä»¶](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_inp/wan2.1_fun_inp.webp)\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\n### 2\\. æ‰‹åŠ¨æ¨¡åž‹å®‰è£…\n\nå¦‚æžœå¯¹åº”çš„è‡ªåŠ¨æ¨¡åž‹ä¸‹è½½æ— æ•ˆï¼Œè¯·æ‰‹åŠ¨è¿›è¡Œæ¨¡åž‹ä¸‹è½½ï¼Œå¹¶ä¿å­˜åˆ°å¯¹åº”çš„æ–‡ä»¶å¤¹ ä¸‹é¢çš„æ¨¡åž‹ä½ å¯ä»¥åœ¨ [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) å’Œ [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) æ‰¾åˆ° **Diffusion models** é€‰æ‹© 1.3B æˆ– 14B, 14B çš„æ–‡ä»¶ä½“ç§¯æ›´å¤§ï¼ˆ32GBï¼‰ä½†æ˜¯å¯¹äºŽè¿è¡Œæ˜¾å­˜è¦æ±‚ä¹Ÿè¾ƒé«˜ï¼Œ\n\n*   [wan2.1\\_fun\\_inp\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)\n*   [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true)ï¼š å»ºè®®ä¸‹è½½åŽé‡å‘½åä¸º `Wan2.1-Fun-14B-InP.safetensors`\n\n**Text encoders** é€‰æ‹©ä¸‹é¢ä¸¤ä¸ªæ¨¡åž‹ä¸­çš„ä¸€ä¸ªï¼Œfp16 ç²¾åº¦ä½“ç§¯è¾ƒå¤§å¯¹æ€§èƒ½è¦æ±‚é«˜\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\næ–‡ä»¶ä¿å­˜ä½ç½®\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ wan2.1_fun_inp_1.3B_bf16.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ wan_2.1_vae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ clip_vision/\nâ”‚       â””â”€â”€  clip_vision_h.safetensors                 \n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµ\n\n![ComfyUI Wan2.1 Fun Control è§†é¢‘ç”Ÿæˆå·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_inp_flow_diagram.png)\n\n1.  ç¡®ä¿ `Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `wan2.1_fun_inp_1.3B_bf16.safetensors`\n2.  ç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  ç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors`\n4.  ç¡®ä¿ `Load CLIP Vision` èŠ‚ç‚¹åŠ è½½äº† `clip_vision_h.safetensors`\n5.  åœ¨ `Load Image` èŠ‚ç‚¹ï¼ˆå·²è¢«é‡å‘½åä¸º`Start_image`ï¼‰ ä¸Šä¼ èµ·å§‹å¸§\n6.  åœ¨ç¬¬äºŒä¸ª `Load Image` èŠ‚ç‚¹ä¸Šä¼ ç”¨äºŽæŽ§åˆ¶è§†é¢‘ã€‚æ³¨æ„ï¼š ç›®å‰è¿™ä¸ªèŠ‚ç‚¹è¿˜ä¸æ”¯æŒ mp4 åªèƒ½ä½¿ç”¨ Webp è§†é¢‘\n7.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹ Prompt ä½¿ç”¨ä¸­è‹±æ–‡éƒ½å¯ä»¥\n8.  ï¼ˆå¯é€‰ï¼‰åœ¨ `WanFunInpaintToVideo` ä¿®æ”¹å¯¹åº”è§†é¢‘çš„å°ºå¯¸ï¼Œä¸è¦ä½¿ç”¨è¿‡å¤§çš„å°ºå¯¸\n9.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n\n### 4\\. å·¥ä½œæµè¯´æ˜Ž\n\n*   åœ¨ä½“éªŒ Wan Fun InP æ—¶ï¼Œä½ å¯èƒ½éœ€è¦é¢‘ç¹ä¿®æ”¹æç¤ºè¯ï¼Œä»Žè€Œæ¥ç¡®ä¿å¯¹åº”ç”»é¢çš„è¿‡æ¸¡çš„å‡†ç¡®æ€§\n\n## å…¶å®ƒ Wan2.1 Fun Inp æˆ–è€…è§†é¢‘ç›¸å…³è‡ªå®šä¹‰èŠ‚ç‚¹\n\n*   [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n*   [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n*   [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)"
},
{
  "url": "https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_topbar_menu",
  "markdown": "# é¡¶éƒ¨èœå•æ  - ComfyUI\n\né¡¶éƒ¨èœå•æ  API å…è®¸æ‰©å±•ä¸º ComfyUI çš„é¡¶éƒ¨èœå•æ æ·»åŠ è‡ªå®šä¹‰èœå•é¡¹ã€‚è¿™å¯¹äºŽæä¾›é«˜çº§åŠŸèƒ½æˆ–ä¸å¸¸ç”¨å‘½ä»¤çš„è®¿é—®éžå¸¸æœ‰ç”¨ã€‚\n\n## åŸºæœ¬ç”¨æ³•\n\n```\napp.registerExtension({\n  name: \"MyExtension\",\n  // å®šä¹‰å‘½ä»¤\n  commands: [\n    { \n      id: \"myCommand\", \n      label: \"æˆ‘çš„å‘½ä»¤\", \n      function: () => { alert(\"å‘½ä»¤å·²æ‰§è¡Œï¼\"); } \n    }\n  ],\n  // å°†å‘½ä»¤æ·»åŠ åˆ°èœå•\n  menuCommands: [\n    { \n      path: [\"æ‰©å±•\", \"æˆ‘çš„æ‰©å±•\"], \n      commands: [\"myCommand\"] \n    }\n  ]\n});\n```\n\nå‘½ä»¤å®šä¹‰æ–¹å¼ä¸Ž [å‘½ä»¤ä¸Žå¿«æ·é”®ç»‘å®š API](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_commands_keybindings) ç›¸åŒã€‚è¯¦ç»†å®šä¹‰å‘½ä»¤è¯·å‚è§è¯¥é¡µé¢ã€‚\n\n## å‘½ä»¤é…ç½®\n\næ¯ä¸ªå‘½ä»¤éƒ½éœ€è¦ `id`ã€`label` å’Œ `function`ï¼š\n\n```\n{\n  id: string,              // å‘½ä»¤çš„å”¯ä¸€æ ‡è¯†ç¬¦\n  label: string,           // å‘½ä»¤æ˜¾ç¤ºåç§°\n  function: () => void     // å‘½ä»¤è¢«è§¦å‘æ—¶æ‰§è¡Œçš„å‡½æ•°\n}\n```\n\n## èœå•é…ç½®\n\n`menuCommands` æ•°ç»„å®šä¹‰äº†å‘½ä»¤åœ¨èœå•ç»“æž„ä¸­çš„ä½ç½®ï¼š\n\n```\n{\n  path: string[],          // è¡¨ç¤ºèœå•å±‚çº§çš„æ•°ç»„\n  commands: string[]       // è¦æ·»åŠ åˆ°è¯¥ä½ç½®çš„å‘½ä»¤ ID æ•°ç»„\n}\n```\n\n`path` æ•°ç»„æŒ‡å®šèœå•å±‚çº§ã€‚ä¾‹å¦‚ï¼Œ`[\"æ–‡ä»¶\", \"å¯¼å‡º\"]` ä¼šå°†å‘½ä»¤æ·»åŠ åˆ°â€æ–‡ä»¶â€èœå•ä¸‹çš„â€å¯¼å‡ºâ€å­èœå•ã€‚\n\n## èœå•ç¤ºä¾‹\n\n### æ·»åŠ åˆ°å·²æœ‰èœå•\n\n```\napp.registerExtension({\n  name: \"MenuExamples\",\n  commands: [\n    { \n      id: \"saveAsImage\", \n      label: \"å¦å­˜ä¸ºå›¾ç‰‡\", \n      function: () => { \n        // ä¿å­˜ç”»å¸ƒä¸ºå›¾ç‰‡çš„ä»£ç \n      } \n    },\n    { \n      id: \"exportWorkflow\", \n      label: \"å¯¼å‡ºå·¥ä½œæµ\", \n      function: () => { \n        // å¯¼å‡ºå·¥ä½œæµçš„ä»£ç \n      } \n    }\n  ],\n  menuCommands: [\n    // æ·»åŠ åˆ°æ–‡ä»¶èœå•\n    { \n      path: [\"æ–‡ä»¶\"], \n      commands: [\"saveAsImage\", \"exportWorkflow\"] \n    }\n  ]\n});\n```\n\n### åˆ›å»ºå­èœå•ç»“æž„\n\n```\napp.registerExtension({\n  name: \"SubmenuExample\",\n  commands: [\n    { \n      id: \"option1\", \n      label: \"é€‰é¡¹ 1\", \n      function: () => { console.log(\"é€‰é¡¹ 1\"); } \n    },\n    { \n      id: \"option2\", \n      label: \"é€‰é¡¹ 2\", \n      function: () => { console.log(\"é€‰é¡¹ 2\"); } \n    },\n    { \n      id: \"suboption1\", \n      label: \"å­é€‰é¡¹ 1\", \n      function: () => { console.log(\"å­é€‰é¡¹ 1\"); } \n    }\n  ],\n  menuCommands: [\n    // åˆ›å»ºåµŒå¥—èœå•ç»“æž„\n    { \n      path: [\"æ‰©å±•\", \"æˆ‘çš„å·¥å…·\"], \n      commands: [\"option1\", \"option2\"] \n    },\n    { \n      path: [\"æ‰©å±•\", \"æˆ‘çš„å·¥å…·\", \"é«˜çº§\"], \n      commands: [\"suboption1\"] \n    }\n  ]\n});\n```\n\n### å¤šä¸ªèœå•ä½ç½®\n\nä½ å¯ä»¥å°†åŒä¸€ä¸ªå‘½ä»¤æ·»åŠ åˆ°å¤šä¸ªèœå•ä½ç½®ï¼š\n\n```\napp.registerExtension({\n  name: \"MultiLocationExample\",\n  commands: [\n    { \n      id: \"helpCommand\", \n      label: \"èŽ·å–å¸®åŠ©\", \n      function: () => { window.open(\"https://docs.example.com\", \"_blank\"); } \n    }\n  ],\n  menuCommands: [\n    // æ·»åŠ åˆ°å¸®åŠ©èœå•\n    { \n      path: [\"å¸®åŠ©\"], \n      commands: [\"helpCommand\"] \n    },\n    // ä¹Ÿæ·»åŠ åˆ°æ‰©å±•èœå•\n    { \n      path: [\"æ‰©å±•\"], \n      commands: [\"helpCommand\"] \n    }\n  ]\n});\n```\n\nå‘½ä»¤å¯ä»¥ä¸Žå…¶ä»– ComfyUI APIï¼ˆå¦‚è®¾ç½®ï¼‰é…åˆä½¿ç”¨ã€‚å…³äºŽè®¾ç½® API çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§ [è®¾ç½® API](https://docs.comfy.org/zh-CN/custom-nodes/js/javascript_settings) æ–‡æ¡£ã€‚\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/vace",
  "markdown": "# ComfyUI Wan2.1 VACE è§†é¢‘ç¤ºä¾‹ - ComfyUI\n\nVACE 14B æ˜¯é˜¿é‡Œé€šä¹‰ä¸‡ç›¸å›¢é˜ŸæŽ¨å‡ºçš„å¼€æºè§†é¢‘ç¼–è¾‘ç»Ÿä¸€æ¨¡åž‹ã€‚è¯¥æ¨¡åž‹é€šè¿‡æ•´åˆå¤šä»»åŠ¡èƒ½åŠ›ã€æ”¯æŒé«˜åˆ†è¾¨çŽ‡å¤„ç†åŠçµæ´»çš„å¤šæ¨¡æ€è¾“å…¥æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘åˆ›ä½œçš„æ•ˆçŽ‡ä¸Žè´¨é‡ã€‚ è¯¥æ¨¡åž‹åŸºäºŽ [Apache-2.0](https://github.com/ali-vilab/VACE?tab=Apache-2.0-1-ov-file) åè®®å¼€æºï¼Œå¯ç”¨äºŽä¸ªäººå•†ä¸šç”¨é€”ã€‚ ä»¥ä¸‹æ˜¯å…¶æ ¸å¿ƒç‰¹æ€§ä¸ŽæŠ€æœ¯äº®ç‚¹çš„ç»¼åˆåˆ†æžï¼š\n\n*   å¤šæ¨¡æ€è¾“å…¥:æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€é®ç½©ã€æŽ§åˆ¶ä¿¡å·ç­‰å¤šç§è¾“å…¥å½¢å¼\n*   ç»Ÿä¸€æž¶æž„:å•ä¸€æ¨¡åž‹æ”¯æŒå¤šç§ä»»åŠ¡,å¯è‡ªç”±ç»„åˆåŠŸèƒ½\n*   åŠ¨ä½œè¿ç§»:åŸºäºŽå‚è€ƒè§†é¢‘ç”Ÿæˆè¿žè´¯åŠ¨ä½œ\n*   å±€éƒ¨æ›¿æ¢:é€šè¿‡é®ç½©æ›¿æ¢è§†é¢‘ä¸­çš„ç‰¹å®šåŒºåŸŸ\n*   è§†é¢‘æ‰©å±•:è¡¥å…¨åŠ¨ä½œæˆ–æ‰©å±•èƒŒæ™¯\n*   èƒŒæ™¯æ›¿æ¢:ä¿ç•™ä¸»ä½“æ›´æ¢çŽ¯å¢ƒèƒŒæ™¯\n\nç›®å‰ VACE å‘å¸ƒäº† 1.3B å’Œ 14B ä¸¤ä¸ªç‰ˆæœ¬ï¼Œ14B ç‰ˆæœ¬ç›¸æ¯” 1.3B ç‰ˆæœ¬ï¼Œæ”¯æŒ 720P åˆ†è¾¨çŽ‡è¾“å‡º,ç”»é¢ç»†èŠ‚å’Œç¨³å®šæ€§æ›´å¥½ã€‚\n\n| æ¨¡åž‹  | 480P | 720P |\n| --- | --- | --- |\n| [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B) | âœ…   | âŒ   |\n| [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B) | âœ…   | âœ…   |\n\nç›¸å…³æ¨¡åž‹æƒé‡å’Œä»£ç ä»“åº“ï¼š\n\n*   [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B)\n*   [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B)\n*   [Github](https://github.com/ali-vilab/VACE)\n*   [VACE é¡¹ç›®ä¸»é¡µ](https://ali-vilab.github.io/VACE-Page/)\n\n## æ¨¡åž‹ä¸‹è½½åŠåœ¨å·¥ä½œæµä¸­çš„åŠ è½½\n\nç”±äºŽæœ¬ç¯‡æ–‡æ¡£ä¸­è®¾è®¡çš„å‡ ä¸ªå·¥ä½œæµéƒ½ä½¿ç”¨åŒä¸€å¥—å·¥ä½œæµæ¨¡æ¿ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å…ˆå®Œæˆæ¨¡åž‹ä¸‹è½½åŠåŠ è½½çš„ä¿¡æ¯ä»‹ç»ï¼Œç„¶åŽé€šè¿‡ Bypass ä¸åŒçš„èŠ‚ç‚¹æ¥å¯ç”¨/ ç¦ç”¨ä¸åŒçš„è¾“å…¥æ¥å®žçŽ°ä¸åŒçš„å·¥ä½œæµã€‚ åœ¨å…·ä½“ç¤ºä¾‹ä¸­å¯¹åº”çš„å·¥ä½œæµä¿¡æ¯ä¸­å·²ç»åµŒå…¥äº†æ¨¡åž‹ä¸‹è½½ä¿¡æ¯ï¼Œæ‰€ä»¥ä½ ä¹Ÿå¯ä»¥åœ¨ä¸‹è½½å…·ä½“ç¤ºä¾‹çš„å·¥ä½œæµæ—¶æ¥å®Œæˆæ¨¡åž‹ä¸‹è½½ã€‚\n\n### æ¨¡åž‹ä¸‹è½½\n\n**diffusion\\_models** [wan2.1\\_vace\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_14B_fp16.safetensors) [wan2.1\\_vace\\_1.3B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_1.3B_fp16.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\nä»Ž**Text encoders** é€‰æ‹©ä¸€ä¸ªç‰ˆæœ¬è¿›è¡Œä¸‹è½½\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\næ–‡ä»¶ä¿å­˜ä½ç½®\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€ wan2.1_vace_14B_fp16.safetensors # æˆ– wan2.1_vace_1.3B_fp16.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors # æˆ– umt5_xxl_fp16.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚       â””â”€â”€  wan_2.1_vae.safetensors\n```\n\n### æ¨¡åž‹åŠ è½½\n\nç”±äºŽåœ¨æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬æ‰€ä½¿ç”¨çš„æ¨¡åž‹æ˜¯ä¸€è‡´çš„ï¼Œå·¥ä½œæµä¹Ÿç›¸åŒï¼Œåªæ˜¯ Bypass äº†éƒ¨åˆ†çš„èŠ‚ç‚¹æ¥å¯ç”¨/ ç¦ç”¨ä¸åŒçš„è¾“å…¥ï¼Œè¯·å‚è€ƒä¸‹é¢çš„å›¾ç‰‡ç¡®ä¿åœ¨å¯¹åº”ä¸åŒçš„å·¥ä½œæµä¸­ï¼Œå¯¹åº”çš„æ¨¡åž‹éƒ½å·²æ­£ç¡®åŠ è½½ ![Wan2.1 VACE æ¨¡åž‹åŠ è½½](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-model-loading.jpg)\n\n1.  ç¡®ä¿ `Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `wan2.1_vace_14B_fp16.safetensors`\n2.  ç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors` æˆ–è€… `umt5_xxl_fp16.safetensors`\n3.  ç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors`\n\n### å¦‚ä½•å–æ¶ˆèŠ‚ç‚¹çš„ Bypass çŠ¶æ€\n\nå½“ä¸€ä¸ªèŠ‚ç‚¹è¢«è®¾ç½®ä¸º Bypass çŠ¶æ€æ—¶ï¼Œé€šè¿‡è¯¥èŠ‚ç‚¹çš„æ•°æ®å°†ä¸å—èŠ‚ç‚¹çš„å½±å“ï¼Œç›´æŽ¥è¾“å‡ºï¼Œä¸‹é¢æ˜¯å¦‚ä½•å–æ¶ˆèŠ‚ç‚¹çš„ Bypass çŠ¶æ€çš„ä¸‰ç§æ–¹æ³• æˆ‘ä»¬é€šå¸¸åœ¨ä¸éœ€è¦ä¸€äº›èŠ‚ç‚¹æ—¶è®¾ç½®èŠ‚ç‚¹çš„ Bypass çŠ¶æ€ï¼Œè€Œä¸ç”¨å°†å®ƒä»¬ä»ŽèŠ‚ç‚¹ä¸­åˆ é™¤æ”¹å˜å·¥ä½œæµã€‚ ![å–æ¶ˆ Bypass](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/nodes/cancel-bypass.jpg)\n\n1.  é€‰ä¸­èŠ‚ç‚¹åŽï¼Œåœ¨é€‰æ‹©å·¥å…·ç®±ä¸­ç‚¹å‡»æ ‡è¯†éƒ¨åˆ†çš„ç®­å¤´ï¼Œå³å¯å¿«é€Ÿåˆ‡æ¢èŠ‚ç‚¹çš„ Bypass çŠ¶æ€\n2.  é€‰ä¸­èŠ‚ç‚¹åŽï¼Œé¼ æ ‡å³é”®ç‚¹å‡»èŠ‚ç‚¹ï¼Œé€‰æ‹© `æ¨¡å¼(Mode)` -> `æ€»æ˜¯(Always)` åˆ‡æ¢åˆ° Always æ¨¡å¼\n3.  é€‰ä¸­èŠ‚ç‚¹åŽï¼Œé¼ æ ‡å³é”®ç‚¹å‡»èŠ‚ç‚¹ï¼Œé€‰æ‹© `ç»•è¿‡(Bypass)` é€‰é¡¹ï¼Œåˆ‡æ¢ Bypass çŠ¶æ€\n\n## VACE æ–‡ç”Ÿè§†é¢‘å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµä¸‹è½½\n\nä¸‹è½½ä¸‹é¢è§†é¢‘ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­ï¼Œä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![å›¾åƒ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg) è¯·å‚ç…§å›¾ç‰‡åºå·è¿›è¡Œé€æ­¥ç¡®è®¤ï¼Œæ¥ä¿è¯å¯¹åº”å·¥ä½œæµçš„é¡ºåˆ©è¿è¡Œ\n\n1.  åœ¨ `CLIP Text Encode (Positive Prompt)` èŠ‚ç‚¹ä¸­è¾“å…¥æ­£å‘æç¤ºè¯\n2.  åœ¨ `CLIP Text Encode (Negative Prompt)` èŠ‚ç‚¹ä¸­è¾“å…¥è´Ÿå‘æç¤ºè¯\n3.  åœ¨ `WanVaceToVideo` è®¾ç½®å¯¹åº”å›¾åƒçš„å°ºå¯¸ï¼ˆé¦–æ¬¡è¿è¡Œå»ºè®®è®¾ç½® 640\\*640 çš„åˆ†è¾¨çŽ‡ï¼‰ï¼Œå¸§æ•°ï¼ˆè§†é¢‘çš„æ—¶é•¿ï¼‰\n4.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n5.  ç”Ÿæˆå®ŒæˆåŽå¯¹åº”çš„è§†é¢‘ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `ComfyUI/output/video` ç›®å½•ä¸‹ï¼ˆå­æ–‡ä»¶å¤¹ä½ç½®å–å†³äºŽ `save video` èŠ‚ç‚¹è®¾ç½®ï¼‰\n\n## VACE å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ\n\nä½ å¯ä»¥ç»§ç»­ä½¿ç”¨ä¸Šé¢çš„å·¥ä½œæµæ–‡ä»¶ï¼Œåªéœ€è¦å°† **Load reference image** çš„ `Load image` èŠ‚ç‚¹çš„ Bypass å–æ¶ˆï¼Œå¹¶è¾“å…¥å¯¹åº”çš„å›¾ç‰‡ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å›¾ç‰‡ï¼Œåœ¨è¿™ä¸ªæ–‡ä»¶é‡Œï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†å¯¹åº”çš„å‚æ•°è®¾ç½®ã€‚\n\n### 1\\. å·¥ä½œæµä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„è§†é¢‘ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­ï¼Œä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ\n\nè¯·ä¸‹è½½ä¸‹é¢å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![vace-i2v-input](https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/video/wan/vace/i2v/input.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![å·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg) è¯·å‚ç…§å›¾ç‰‡åºå·è¿›è¡Œé€æ­¥ç¡®è®¤ï¼Œæ¥ä¿è¯å¯¹åº”å·¥ä½œæµçš„é¡ºåˆ©è¿è¡Œ\n\n1.  åœ¨ `Load image` èŠ‚ç‚¹ä¸­è¾“å…¥å¯¹åº”çš„å›¾ç‰‡\n2.  ä½ å¯ä»¥åƒæ–‡ç”Ÿå›¾å·¥ä½œæµä¸€æ ·å®Œæˆæ¥è¿›è¡Œæç¤ºè¯çš„ä¿®æ”¹å’Œç¼–è¾‘\n3.  åœ¨ `WanVaceToVideo` è®¾ç½®å¯¹åº”å›¾åƒçš„å°ºå¯¸ï¼ˆé¦–æ¬¡è¿è¡Œå»ºè®®è®¾ç½® 640\\*640 çš„åˆ†è¾¨çŽ‡ï¼‰ï¼Œå¸§æ•°ï¼ˆè§†é¢‘çš„æ—¶é•¿ï¼‰\n4.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n5.  ç”Ÿæˆå®ŒæˆåŽå¯¹åº”çš„è§†é¢‘ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `ComfyUI/output/video` ç›®å½•ä¸‹ï¼ˆå­æ–‡ä»¶å¤¹ä½ç½®å–å†³äºŽ `save video` èŠ‚ç‚¹è®¾ç½®ï¼‰\n\n### 3\\. å·¥ä½œæµè¡¥å……è¯´æ˜Ž\n\nVACE è¿˜æ”¯æŒåœ¨ä¸€å¼ å›¾åƒä¸­è¾“å…¥å¤šä¸ªå‚è€ƒå›¾åƒï¼Œæ¥ç”Ÿæˆå¯¹åº”çš„è§†é¢‘ï¼Œä½ å¯ä»¥åœ¨ VACE çš„é¡¹ç›®é¡µä¸­çœ‹åˆ°ç›¸å…³çš„[ç¤ºä¾‹](https://ali-vilab.github.io/VACE-Page/)\n\n## VACE è§†é¢‘åˆ°è§†é¢‘å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„è§†é¢‘å¹¶æ‹–å…¥ ComfyUI ä¸­ï¼Œä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ\n\næˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„ç´ æä½œä¸ºè¾“å…¥:\n\n1.  ç”¨äºŽå‚è€ƒå›¾åƒçš„è¾“å…¥å›¾ç‰‡ ![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/vace/v2v/input.jpg) \n2.  ä¸‹é¢çš„è§†é¢‘å·²ç»ç»è¿‡é¢„å¤„ç†ï¼Œæˆ‘ä»¬å°†ç”¨äºŽæŽ§åˆ¶è§†é¢‘çš„ç”Ÿæˆ\n\n3.  ä¸‹é¢çš„è§†é¢‘æ˜¯åŽŸå§‹è§†é¢‘ï¼Œä½ å¯ä»¥ä¸‹è½½ä¸‹é¢çš„ç´ ææ¥ä½¿ç”¨ç±»ä¼¼ [comfyui\\_controlnet\\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) è¿™æ ·çš„é¢„å¤„ç†èŠ‚ç‚¹æ¥å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![å·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg) è¯·å‚ç…§å›¾ç‰‡åºå·è¿›è¡Œé€æ­¥ç¡®è®¤ï¼Œæ¥ä¿è¯å¯¹åº”å·¥ä½œæµçš„é¡ºåˆ©è¿è¡Œ\n\n1.  åœ¨ `Load reference image` ä¸­çš„ `Load Image` èŠ‚ç‚¹è¾“å…¥å‚è€ƒå›¾ç‰‡\n2.  åœ¨ `Load control video` ä¸­çš„ `Load Video` èŠ‚ç‚¹è¾“å…¥æŽ§åˆ¶è§†é¢‘ï¼Œç”±äºŽæä¾›çš„è§†é¢‘æ˜¯ç»è¿‡é¢„å¤„ç†çš„ï¼Œæ‰€ä»¥ä½ ä¸éœ€è¦è¿›è¡Œé¢å¤–çš„å¤„ç†\n3.  å¦‚æžœä½ éœ€è¦è‡ªå·±é’ˆå¯¹åŽŸå§‹è§†é¢‘è¿›è¡Œé¢„å¤„ç†ï¼Œå¯ä»¥ä¿®æ”¹ `Image preprocessing` åˆ†ç»„ï¼Œæˆ–è€…ä½¿ç”¨ `comfyui_controlnet_aux` èŠ‚ç‚¹æ¥å®Œæˆå¯¹åº”çš„èŠ‚ç‚¹é¢„å¤„ç†\n4.  ä¿®æ”¹æç¤ºè¯\n5.  åœ¨ `WanVaceToVideo` è®¾ç½®å¯¹åº”å›¾åƒçš„å°ºå¯¸ï¼ˆé¦–æ¬¡è¿è¡Œå»ºè®®è®¾ç½® 640\\*640 çš„åˆ†è¾¨çŽ‡ï¼‰ï¼Œå¸§æ•°ï¼ˆè§†é¢‘çš„æ—¶é•¿ï¼‰\n6.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n7.  ç”Ÿæˆå®ŒæˆåŽå¯¹åº”çš„è§†é¢‘ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `ComfyUI/output/video` ç›®å½•ä¸‹ï¼ˆå­æ–‡ä»¶å¤¹ä½ç½®å–å†³äºŽ `save video` èŠ‚ç‚¹è®¾ç½®ï¼‰\n\n## VACE è§†é¢‘æ‰©å±•å·¥ä½œæµ\n\n\\[å¾…æ›´æ–°\\]\n\n## VACE é¦–å°¾å¸§è§†é¢‘ç”Ÿæˆ\n\n\\[å¾…æ›´æ–°\\] è¦ä¿è¯é¦–å°¾å¸§ç”Ÿæ•ˆï¼Œéœ€è¦æ»¡è¶³ï¼š\n\n*   å¯¹åº”è§†é¢‘ `length` è®¾ç½®éœ€è¦æ»¡è¶³ `length-1` åŽèƒ½å¤Ÿè¢« `4` æ•´é™¤\n*   å¯¹åº”çš„ `Batch_size` è®¾ç½®éœ€è¦æ»¡è¶³ `Batch_size = length - 2`\n\n## ç›¸å…³èŠ‚ç‚¹æ–‡æ¡£\n\nè¯·æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£ç›¸å…³çš„èŠ‚ç‚¹\n\n[\n\n## WanVaceToVideo èŠ‚ç‚¹æ–‡æ¡£\n\nWanVaceToVideo èŠ‚ç‚¹æ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/conditioning/video-models/wan-vace-to-video)[\n\n## TrimVideoLatent èŠ‚ç‚¹æ–‡æ¡£\n\nComfyUI TrimVideoLatent èŠ‚ç‚¹æ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/latent/video/trim-video-latent)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image",
  "markdown": "# ComfyUI æ–‡ç”Ÿå›¾å·¥ä½œæµ - ComfyUI\n\næœ¬ç¯‡ç›®çš„ä¸»è¦å¸¦ä½ åˆæ­¥äº†è§£ ComfyUI çš„æ–‡ç”Ÿå›¾çš„å·¥ä½œæµï¼Œå¹¶åˆæ­¥äº†è§£ä¸€äº› ComfyUI ç›¸å…³èŠ‚ç‚¹çš„åŠŸèƒ½å’Œä½¿ç”¨ã€‚ åœ¨æœ¬ç¯‡æ–‡æ¡£ä¸­æˆ‘ä»¬å°†å®Œæˆä»¥ä¸‹å†…å®¹ï¼š\n\n*   å®Œæˆä¸€æ¬¡æ–‡ç”Ÿå›¾å·¥ä½œæµ\n*   ç®€å•äº†è§£æ‰©æ•£æ¨¡åž‹åŽŸç†\n*   äº†è§£å·¥ä½œæµä¸­çš„èŠ‚ç‚¹çš„åŠŸèƒ½å’Œä½œç”¨\n*   åˆæ­¥äº†è§£ SD1.5 æ¨¡åž‹\n\næˆ‘ä»¬å°†ä¼šå…ˆè¿›è¡Œæ–‡ç”Ÿå›¾å·¥ä½œæµçš„è¿è¡Œï¼Œç„¶åŽè¿›è¡Œç›¸å…³å†…å®¹çš„è®²è§£ï¼Œè¯·æŒ‰ä½ çš„éœ€è¦é€‰æ‹©å¯¹åº”éƒ¨åˆ†å¼€å§‹ã€‚\n\n## å…³äºŽæ–‡ç”Ÿå›¾\n\n**æ–‡ç”Ÿå›¾(Text to Image)** ï¼Œæ˜¯ AI ç»˜å›¾ä¸­çš„åŸºç¡€æµç¨‹ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬æè¿°æ¥ç”Ÿæˆå¯¹åº”çš„å›¾ç‰‡ï¼Œå®ƒçš„æ ¸å¿ƒæ˜¯ **æ‰©æ•£æ¨¡åž‹**ã€‚ åœ¨æ–‡ç”Ÿå›¾è¿‡ç¨‹ä¸­æˆ‘ä»¬éœ€è¦ä»¥ä¸‹æ¡ä»¶ï¼š\n\n*   **ç”»å®¶ï¼š** ç»˜å›¾æ¨¡åž‹\n*   **ç”»å¸ƒï¼š** æ½œåœ¨ç©ºé—´\n*   \\*\\*å¯¹ç”»é¢çš„è¦æ±‚ï¼ˆæç¤ºè¯ï¼‰ï¼š\\*\\*æç¤ºè¯ï¼ŒåŒ…æ‹¬æ­£å‘æç¤ºè¯ï¼ˆå¸Œæœ›åœ¨ç”»é¢ä¸­å‡ºçŽ°çš„å…ƒç´ ï¼‰å’Œè´Ÿå‘æç¤ºè¯ï¼ˆä¸å¸Œæœ›åœ¨ç”»é¢ä¸­å‡ºçŽ°çš„å…ƒç´ ï¼‰\n\nè¿™ä¸ªæ–‡æœ¬åˆ°å›¾ç‰‡å›¾ç‰‡ç”Ÿæˆè¿‡ç¨‹ï¼Œå¯ä»¥ç®€å•ç†è§£æˆä½ æŠŠä½ çš„**ç»˜å›¾è¦æ±‚(æ­£å‘æç¤ºè¯ã€è´Ÿå‘æç¤ºè¯)**å‘Šè¯‰ä¸€ä¸ª**ç”»å®¶(ç»˜å›¾æ¨¡åž‹)**ï¼Œç”»å®¶ä¼šæ ¹æ®ä½ çš„è¦æ±‚ï¼Œç”»å‡ºä½ æƒ³è¦çš„å†…å®¹ã€‚\n\n### 1\\. å¼€å§‹å¼€å§‹å‰çš„å‡†å¤‡\n\nè¯·ç¡®ä¿ä½ å·²ç»åœ¨ `ComfyUI/models/checkpoints` æ–‡ä»¶å¤¹è‡³å°‘æœ‰ä¸€ä¸ª SD1.5 çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¦‚æžœä½ è¿˜ä¸äº†è§£å¦‚ä½•å®‰è£…æ¨¡åž‹ï¼Œè¯·å‚[å¼€å§‹ ComfyUI çš„ AI ç»˜å›¾ä¹‹æ—…](https://docs.comfy.org/zh-CN/get_started/first_generation#3-%E5%AE%89%E8%A3%85%E7%BB%98%E5%9B%BE%E6%A8%A1%E5%9E%8B)ç« èŠ‚ä¸­å…³äºŽæ¨¡åž‹å®‰è£…çš„éƒ¨åˆ†è¯´æ˜Žã€‚ ä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„è¿™äº›æ¨¡åž‹ï¼š\n\n*   [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)\n*   [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)\n*   [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)\n\n### 2\\. åŠ è½½æ–‡ç”Ÿå›¾å·¥ä½œæµ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶å°†å›¾ç‰‡æ‹–å…¥ ComfyUI çš„ç•Œé¢ä¸­ï¼Œæˆ–è€…ä½¿ç”¨èœå• **å·¥ä½œæµï¼ˆWorkflowsï¼‰** -> **æ‰“å¼€ï¼ˆOpenï¼‰** æ‰“å¼€è¿™ä¸ªå›¾ç‰‡ä»¥åŠ è½½å¯¹åº”çš„ workflow ![ComfyUI-æ–‡ç”Ÿå›¾å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/text-to-image-workflow.png) ä¹Ÿå¯ä»¥ä»Žèœå• **å·¥ä½œæµï¼ˆWorkflowsï¼‰** -> **æµè§ˆå·¥ä½œæµç¤ºä¾‹ï¼ˆBrowse example workflowsï¼‰** ä¸­é€‰æ‹© **Text to Image** å·¥ä½œæµ\n\n### 3\\. åŠ è½½æ¨¡åž‹ï¼Œå¹¶è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆ\n\nåœ¨å®Œæˆäº†å¯¹åº”çš„ç»˜å›¾æ¨¡åž‹å®‰è£…åŽï¼Œè¯·å‚è€ƒä¸‹å›¾æ­¥éª¤åŠ è½½å¯¹åº”çš„æ¨¡åž‹ï¼Œå¹¶è¿›è¡Œç¬¬ä¸€æ¬¡å›¾ç‰‡çš„ç”Ÿæˆ ![å›¾ç‰‡ç”Ÿæˆ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg) è¯·å¯¹åº”å›¾ç‰‡åºå·ï¼Œå®Œæˆä¸‹é¢æ“ä½œ\n\n1.  è¯·åœ¨**Load Checkpoint** èŠ‚ç‚¹ä½¿ç”¨ç®­å¤´æˆ–è€…ç‚¹å‡»æ–‡æœ¬åŒºåŸŸç¡®ä¿ **v1-5-pruned-emaonly-fp16.safetensors** è¢«é€‰ä¸­ï¼Œä¸”å·¦å³åˆ‡æ¢ç®­å¤´ä¸ä¼šå‡ºçŽ°**null** çš„æ–‡æœ¬\n2.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\nç­‰å¾…å¯¹åº”æµç¨‹æ‰§è¡Œå®ŒæˆåŽï¼Œä½ åº”è¯¥å¯ä»¥åœ¨ç•Œé¢çš„\\*\\*ä¿å­˜å›¾åƒï¼ˆSave Imageï¼‰\\*\\*èŠ‚ç‚¹ä¸­çœ‹åˆ°å¯¹åº”çš„å›¾ç‰‡ç»“æžœï¼Œå¯ä»¥åœ¨ä¸Šé¢å³é”®ä¿å­˜åˆ°æœ¬åœ° ![ComfyUI é¦–æ¬¡å›¾ç‰‡ç”Ÿæˆç»“æžœ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)\n\n### 4\\. å¼€å§‹ä½ çš„å°è¯•\n\nä½ å¯ä»¥å°è¯•ä¿®æ”¹**CLIP Text Encoder**å¤„çš„æ–‡æœ¬ ![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg) å…¶ä¸­è¿žæŽ¥åˆ° KSampler èŠ‚ç‚¹çš„`Positive`ä¸ºæ­£å‘æç¤ºè¯ï¼Œè¿žæŽ¥åˆ° KSampler èŠ‚ç‚¹çš„`Negative`ä¸ºè´Ÿå‘æç¤ºè¯ ä¸‹é¢æ˜¯é’ˆå¯¹ SD1.5 æ¨¡åž‹çš„ä¸€äº›ç®€å•æç¤ºè¯åŽŸåˆ™\n\n*   å°½é‡ä½¿ç”¨è‹±æ–‡\n*   æç¤ºè¯ä¹‹é—´ä½¿ç”¨è‹±æ–‡é€—å· `,` éš”å¼€\n*   å°½é‡ä½¿ç”¨çŸ­è¯­è€Œä¸æ˜¯é•¿å¥å­\n*   ä½¿ç”¨æ›´å…·ä½“çš„æè¿°\n*   å¯ä»¥ä½¿ç”¨ç±»ä¼¼ `(golden hour:1.2)` è¿™æ ·çš„è¡¨è¾¾æ¥æå‡ç‰¹å®šå…³é”®è¯çš„æƒé‡ï¼Œè¿™æ ·å®ƒåœ¨ç”»é¢ä¸­å‡ºçŽ°çš„æ¦‚çŽ‡ä¼šæ›´é«˜ï¼Œ`1.2` ä¸ºæƒé‡ï¼Œ`golden hour` ä¸ºå…³é”®è¯\n*   å¯ä»¥ä½¿ç”¨ç±»ä¼¼ `masterpiece, best quality, 4k` ç­‰å…³é”®è¯æ¥æå‡ç”Ÿæˆè´¨é‡\n\nä¸‹é¢æ˜¯å‡ ç»„ä¸åŒçš„ prompt ç¤ºä¾‹ï¼Œä½ å¯ä»¥å°è¯•ä½¿ç”¨è¿™äº› prompt æ¥æŸ¥çœ‹ç”Ÿæˆçš„æ•ˆæžœï¼Œæˆ–è€…ä½¿ç”¨ä½ è‡ªå·±çš„ prompt æ¥å°è¯•ç”Ÿæˆ **1\\. äºŒæ¬¡å…ƒåŠ¨æ¼«é£Žæ ¼** æ­£å‘æç¤ºè¯ï¼š\n\n```\nanime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details\n\nmasterpiece, best quality, 4k\n```\n\nè´Ÿå‘æç¤ºè¯ï¼š\n\n```\nlow quality, blurry, deformed hands, extra fingers\n```\n\n**2\\. å†™å®žé£Žæ ¼** æ­£å‘æç¤ºè¯ï¼š\n\n```\n(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), \nfull body, soft cinematic lighting, (golden hour:1.2), \n(fujifilm XT4:1.1), shallow depth of field, \n(skin texture details:1.3), (film grain:1.1), \ngentle wind flow, warm color grading, (perfect facial symmetry:1.3)\n```\n\nè´Ÿå‘æç¤ºè¯ï¼š\n\n```\n(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)\n```\n\n**3\\. ç‰¹å®šè‰ºæœ¯å®¶é£Žæ ¼** æ­£å‘æç¤ºè¯ï¼š\n\n```\nfantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style\n```\n\nè´Ÿå‘æç¤ºè¯ï¼š\n\n```\nblurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting\n```\n\n## æ–‡ç”Ÿå›¾å·¥ä½œåŽŸç†\n\næ•´ä¸ªæ–‡ç”Ÿå›¾çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥ç†è§£æˆæ˜¯**æ‰©æ•£æ¨¡åž‹çš„åæ‰©æ•£è¿‡ç¨‹**ï¼Œæˆ‘ä»¬ä¸‹è½½çš„ [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) æ˜¯ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„å¯ä»¥ **ä»Žçº¯é«˜æ–¯å™ªå£°ç”Ÿæˆç›®æ ‡å›¾ç‰‡çš„æ¨¡åž‹**ï¼Œæˆ‘ä»¬åªéœ€è¦è¾“å…¥æˆ‘ä»¬çš„æç¤ºè¯ï¼Œå®ƒå°±å¯ä»¥é€šéšæœºçš„å™ªå£°é™å™ªç”Ÿæˆç›®æ ‡å›¾ç‰‡ã€‚\n\næˆ‘ä»¬å¯èƒ½éœ€è¦äº†è§£ä¸‹ä¸¤ä¸ªæ¦‚å¿µï¼Œ\n\n1.  **æ½œåœ¨ç©ºé—´ï¼š** æ½œåœ¨ç©ºé—´ï¼ˆLatent Spaceï¼‰æ˜¯æ‰©æ•£æ¨¡åž‹ä¸­çš„ä¸€ç§æŠ½è±¡æ•°æ®è¡¨ç¤ºæ–¹å¼ï¼Œé€šè¿‡æŠŠå›¾ç‰‡ä»Žåƒç´ ç©ºé—´è½¬æ¢ä¸ºæ½œåœ¨ç©ºé—´ï¼Œå¯ä»¥å‡å°‘å›¾ç‰‡çš„å­˜å‚¨ç©ºé—´ï¼Œå¹¶ä¸”å¯ä»¥æ›´å®¹æ˜“çš„è¿›è¡Œæ‰©æ•£æ¨¡åž‹çš„è®­ç»ƒå’Œå‡å°‘é™å™ªçš„å¤æ‚åº¦ï¼Œå°±åƒå»ºç­‘å¸ˆè®¾è®¡å»ºç­‘æ—¶ä½¿ç”¨è“å›¾ï¼ˆæ½œåœ¨ç©ºé—´ï¼‰æ¥è¿›è¡Œè®¾è®¡ï¼Œè€Œä¸æ˜¯ç›´æŽ¥åœ¨å»ºç­‘ä¸Šè¿›è¡Œè®¾è®¡ï¼ˆåƒç´ ç©ºé—´ï¼‰ï¼Œè¿™ç§æ–¹å¼å¯ä»¥ä¿æŒç»“æž„ç‰¹å¾çš„åŒæ—¶ï¼Œåˆå¤§å¹…åº¦é™ä½Žä¿®æ”¹æˆæœ¬\n2.  **åƒç´ ç©ºé—´ï¼š** åƒç´ ç©ºé—´ï¼ˆPixel Spaceï¼‰æ˜¯å›¾ç‰‡çš„å­˜å‚¨ç©ºé—´ï¼Œå°±æ˜¯æˆ‘ä»¬æœ€ç»ˆçœ‹åˆ°çš„å›¾ç‰‡ï¼Œç”¨äºŽå­˜å‚¨å›¾ç‰‡çš„åƒç´ å€¼ã€‚\n\nå¦‚æžœä½ æƒ³è¦äº†è§£æ›´å¤šæ‰©æ•£æ¨¡åž‹ç›¸å…³å†…å®¹ï¼Œå¯ä»¥é˜…è¯»ä¸‹é¢çš„æ–‡ç« ï¼š\n\n*   [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)\n*   [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)\n*   [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)\n\n## ComfyUI æ–‡ç”Ÿå›¾å·¥ä½œæµèŠ‚ç‚¹è®²è§£\n\n![ComfyUI æ–‡ç”Ÿå›¾å·¥ä½œæµè®²è§£](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/text-image-workflow.jpg)\n\n### A. åŠ è½½æ¨¡åž‹ï¼ˆLoad Checkpointï¼‰èŠ‚ç‚¹\n\n![åŠ è½½æ¨¡åž‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_checkpoint.jpg) è¿™ä¸ªèŠ‚ç‚¹é€šå¸¸ç”¨äºŽåŠ è½½ç»˜å›¾æ¨¡åž‹, é€šå¸¸ `checkpoint` ä¸­ä¼šåŒ…å« `MODELï¼ˆUNetï¼‰`ã€`CLIP` å’Œ `VAE` ä¸‰ä¸ªç»„ä»¶\n\n*   `MODELï¼ˆUNetï¼‰`ï¼šä¸ºå¯¹åº”æ¨¡åž‹çš„ UNet æ¨¡åž‹, è´Ÿè´£æ‰©æ•£è¿‡ç¨‹ä¸­çš„å™ªå£°é¢„æµ‹å’Œå›¾åƒç”Ÿæˆ,é©±åŠ¨æ‰©æ•£è¿‡ç¨‹\n*   `CLIP`ï¼šè¿™ä¸ªæ˜¯æ–‡æœ¬ç¼–ç å™¨,å› ä¸ºæ¨¡åž‹å¹¶ä¸èƒ½ç›´æŽ¥ç†è§£æˆ‘ä»¬çš„æ–‡æœ¬æç¤ºè¯ï¼ˆpromptï¼‰,æ‰€ä»¥éœ€è¦å°†æˆ‘ä»¬çš„æ–‡æœ¬æç¤ºè¯ï¼ˆpromptï¼‰ç¼–ç ä¸ºå‘é‡,è½¬æ¢ä¸ºæ¨¡åž‹å¯ä»¥ç†è§£çš„è¯­ä¹‰å‘é‡\n*   `VAE`ï¼šè¿™ä¸ªæ˜¯å˜åˆ†è‡ªç¼–ç å™¨,æˆ‘ä»¬çš„æ‰©æ•£æ¨¡åž‹å¤„ç†çš„æ˜¯æ½œåœ¨ç©ºé—´,è€Œæˆ‘ä»¬çš„å›¾ç‰‡æ˜¯åƒç´ ç©ºé—´,æ‰€ä»¥éœ€è¦å°†å›¾ç‰‡è½¬æ¢ä¸ºæ½œåœ¨ç©ºé—´,ç„¶åŽè¿›è¡Œæ‰©æ•£,æœ€åŽå°†æ½œåœ¨ç©ºé—´è½¬æ¢ä¸ºå›¾ç‰‡\n\n### B. ç©ºLatentå›¾åƒï¼ˆEmpty Latent Imageï¼‰èŠ‚ç‚¹\n\n![ç©ºLatentå›¾åƒ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/empty_latent_image.jpg) å®šä¹‰ä¸€ä¸ªæ½œåœ¨ç©ºé—´ï¼ˆLatent Spaceï¼‰,å®ƒè¾“å‡ºåˆ° KSampler èŠ‚ç‚¹ï¼Œç©ºLatentå›¾åƒèŠ‚ç‚¹æž„å»ºçš„æ˜¯ä¸€ä¸ª **çº¯å™ªå£°çš„æ½œåœ¨ç©ºé—´** å®ƒçš„å…·ä½“çš„ä½œç”¨ä½ å¯ä»¥ç†è§£ä¸ºå®šä¹‰ç”»å¸ƒå°ºå¯¸çš„å¤§å°ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬æœ€ç»ˆç”Ÿæˆå›¾ç‰‡çš„å°ºå¯¸\n\n### C. CLIPæ–‡æœ¬ç¼–ç å™¨ï¼ˆCLIP Text Encoderï¼‰èŠ‚ç‚¹\n\n![CLIPæ–‡æœ¬ç¼–ç å™¨](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg) ç”¨äºŽç¼–ç æç¤ºè¯ï¼Œä¹Ÿå°±æ˜¯è¾“å…¥ä½ å¯¹ç”»é¢çš„è¦æ±‚\n\n*   è¿žæŽ¥åˆ° KSampler èŠ‚ç‚¹çš„ `Positive` æ¡ä»¶è¾“å…¥çš„ä¸ºæ­£å‘æç¤ºè¯ï¼ˆå¸Œæœ›åœ¨ç”»é¢ä¸­å‡ºçŽ°çš„å…ƒç´ ï¼‰\n*   è¿žæŽ¥åˆ° KSampler èŠ‚ç‚¹çš„ `Negative` æ¡ä»¶è¾“å…¥çš„ä¸ºè´Ÿå‘æç¤ºè¯ï¼ˆä¸å¸Œæœ›åœ¨ç”»é¢ä¸­å‡ºçŽ°çš„å…ƒç´ ï¼‰\n\nå¯¹åº”çš„æç¤ºè¯è¢«æ¥è‡ª `Load Checkpoint` èŠ‚ç‚¹çš„ `CLIP` ç»„ä»¶ç¼–ç ä¸ºè¯­ä¹‰å‘é‡ï¼Œç„¶åŽä½œä¸ºæ¡ä»¶è¾“å‡ºåˆ° KSampler èŠ‚ç‚¹\n\n### D. K é‡‡æ ·å™¨ï¼ˆKSamplerï¼‰èŠ‚ç‚¹\n\n![K é‡‡æ ·å™¨](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg) **K é‡‡æ ·å™¨** æ˜¯æ•´ä¸ªå·¥ä½œæµçš„æ ¸å¿ƒï¼Œæ•´ä¸ªå™ªå£°é™å™ªçš„è¿‡ç¨‹éƒ½åœ¨è¿™ä¸ªèŠ‚ç‚¹ä¸­å®Œæˆï¼Œå¹¶æœ€åŽè¾“å‡ºä¸€ä¸ªæ½œç©ºé—´å›¾åƒ\n\nKSampler èŠ‚ç‚¹çš„å‚æ•°è¯´æ˜Žå¦‚ä¸‹\n\n| å‚æ•°åç§° | æè¿°  | ä½œç”¨  |\n| --- | --- | --- |\n| **model** | åŽ»å™ªä½¿ç”¨çš„æ‰©æ•£æ¨¡åž‹ | å†³å®šç”Ÿæˆå›¾åƒçš„é£Žæ ¼ä¸Žè´¨é‡ |\n| **positive** | æ­£å‘æç¤ºè¯æ¡ä»¶ç¼–ç  | å¼•å¯¼ç”ŸæˆåŒ…å«æŒ‡å®šå…ƒç´ çš„å†…å®¹ |\n| **negative** | è´Ÿå‘æç¤ºè¯æ¡ä»¶ç¼–ç  | æŠ‘åˆ¶ç”Ÿæˆä¸æœŸæœ›çš„å†…å®¹ |\n| **latent\\_image** | å¾…åŽ»å™ªçš„æ½œåœ¨ç©ºé—´å›¾åƒ | ä½œä¸ºå™ªå£°åˆå§‹åŒ–çš„è¾“å…¥è½½ä½“ |\n| **seed** | å™ªå£°ç”Ÿæˆçš„éšæœºç§å­ | æŽ§åˆ¶ç”Ÿæˆç»“æžœçš„éšæœºæ€§ |\n| **control\\_after\\_generate** | ç§å­ç”ŸæˆåŽæŽ§åˆ¶æ¨¡å¼ | å†³å®šå¤šæ‰¹æ¬¡ç”Ÿæˆæ—¶ç§å­çš„å˜åŒ–è§„å¾‹ |\n| **steps** | åŽ»å™ªè¿­ä»£æ­¥æ•° | æ­¥æ•°è¶Šå¤šç»†èŠ‚è¶Šç²¾ç»†ä½†è€—æ—¶å¢žåŠ  |\n| **cfg** | åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ç³»æ•° | æŽ§åˆ¶æç¤ºè¯çº¦æŸå¼ºåº¦ï¼ˆè¿‡é«˜å¯¼è‡´è¿‡æ‹Ÿåˆï¼‰ |\n| **sampler\\_name** | é‡‡æ ·ç®—æ³•åç§° | å†³å®šåŽ»å™ªè·¯å¾„çš„æ•°å­¦æ–¹æ³• |\n| **scheduler** | è°ƒåº¦å™¨ç±»åž‹ | æŽ§åˆ¶å™ªå£°è¡°å‡é€ŸçŽ‡ä¸Žæ­¥é•¿åˆ†é… |\n| **denoise** | é™å™ªå¼ºåº¦ç³»æ•° | æŽ§åˆ¶æ·»åŠ åˆ°æ½œåœ¨ç©ºé—´çš„å™ªå£°å¼ºåº¦ï¼Œ0.0ä¿ç•™åŽŸå§‹è¾“å…¥ç‰¹å¾ï¼Œ1.0ä¸ºå®Œå…¨çš„å™ªå£° |\n\nåœ¨ KSampler èŠ‚ç‚¹ä¸­ï¼Œæ½œåœ¨ç©ºé—´ä½¿ç”¨ `seed` ä½œä¸ºåˆå§‹åŒ–å‚æ•°æž„å»ºéšæœºçš„å™ªå£°,è¯­ä¹‰å‘é‡ `Positive` å’Œ `Negative` ä¼šä½œä¸ºæ¡ä»¶è¾“å…¥åˆ°æ‰©æ•£æ¨¡åž‹ä¸­ ç„¶åŽæ ¹æ® `steps` å‚æ•°æŒ‡å®šçš„åŽ»å™ªæ­¥æ•°ï¼Œè¿›è¡ŒåŽ»å™ªï¼Œæ¯æ¬¡åŽ»å™ªä¼šæ ¹æ® `denoise` å‚æ•°æŒ‡å®šçš„é™å™ªå¼ºåº¦ç³»æ•°ï¼Œå¯¹æ½œåœ¨ç©ºé—´è¿›è¡Œé™å™ªï¼Œå¹¶ç”Ÿæˆæ–°çš„æ½œåœ¨ç©ºé—´å›¾åƒ\n\n### E. VAE è§£ç ï¼ˆVAE Decodeï¼‰èŠ‚ç‚¹\n\n![VAE è§£ç ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/latent/vae_decode.jpg) å°† **K é‡‡æ ·å™¨(KSampler)** è¾“å‡ºçš„æ½œåœ¨ç©ºé—´å›¾åƒè½¬æ¢ä¸ºåƒç´ ç©ºé—´å›¾åƒ\n\n### F. ä¿å­˜å›¾åƒï¼ˆSave Imageï¼‰èŠ‚ç‚¹\n\n![ä¿å­˜å›¾åƒ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/save_image.jpg) é¢„è§ˆå¹¶ä¿å­˜ä»Žæ½œç©ºé—´è§£ç çš„å›¾åƒï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°`ComfyUI/output`æ–‡ä»¶å¤¹ä¸‹\n\n## SD1.5 æ¨¡åž‹ç®€ä»‹\n\n**SD1.5(Stable Diffusion 1.5)** æ˜¯ä¸€ä¸ªç”±[Stability AI](https://stability.ai/)å¼€å‘çš„AIç»˜å›¾æ¨¡åž‹ï¼ŒStable Diffusionç³»åˆ—çš„åŸºç¡€ç‰ˆæœ¬ï¼ŒåŸºäºŽ **512Ã—512** åˆ†è¾¨çŽ‡å›¾ç‰‡è®­ç»ƒï¼Œæ‰€ä»¥å…¶å¯¹ **512Ã—512** åˆ†è¾¨çŽ‡å›¾ç‰‡ç”Ÿæˆæ”¯æŒè¾ƒå¥½ï¼Œä½“ç§¯çº¦ä¸º4GBï¼Œå¯ä»¥åœ¨\\*\\*æ¶ˆè´¹çº§æ˜¾å¡ï¼ˆå¦‚6GBæ˜¾å­˜ï¼‰\\*\\*ä¸Šæµç•…è¿è¡Œã€‚ç›®å‰ SD1.5 çš„ç›¸å…³å‘¨è¾¹ç”Ÿæ€éžå¸¸ä¸°å¯Œï¼Œå®ƒæ”¯æŒå¹¿æ³›çš„æ’ä»¶ï¼ˆå¦‚ControlNetã€LoRAï¼‰å’Œä¼˜åŒ–å·¥å…·ã€‚ ä½œä¸ºAIç»˜ç”»é¢†åŸŸçš„é‡Œç¨‹ç¢‘æ¨¡åž‹ï¼ŒSD1.5å‡­å€Ÿå…¶å¼€æºç‰¹æ€§ã€è½»é‡æž¶æž„å’Œä¸°å¯Œç”Ÿæ€ï¼Œè‡³ä»Šä»æ˜¯æœ€ä½³å…¥é—¨é€‰æ‹©ã€‚å°½ç®¡åŽç»­æŽ¨å‡ºäº†SDXL/SD3ç­‰å‡çº§ç‰ˆæœ¬ï¼Œä½†å…¶åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šçš„æ€§ä»·æ¯”ä»æ— å¯æ›¿ä»£ã€‚\n\n### åŸºç¡€ä¿¡æ¯\n\n*   **å‘å¸ƒæ—¶é—´**ï¼š2022å¹´10æœˆ\n*   **æ ¸å¿ƒæž¶æž„**ï¼šåŸºäºŽLatent Diffusion Model (LDM)\n*   **è®­ç»ƒæ•°æ®**ï¼šLAION-Aesthetics v2.5æ•°æ®é›†ï¼ˆçº¦5.9äº¿æ­¥è®­ç»ƒï¼‰\n*   **å¼€æºç‰¹æ€§**ï¼šå®Œå…¨å¼€æºæ¨¡åž‹/ä»£ç /è®­ç»ƒæ•°æ®\n\n### ä¼˜ç¼ºç‚¹\n\næ¨¡åž‹ä¼˜åŠ¿ï¼š\n\n*   è½»é‡åŒ–ï¼šä½“ç§¯å°ï¼Œä»… 4GB å·¦å³ï¼Œåœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šæµç•…è¿è¡Œ\n*   ä½¿ç”¨é—¨æ§›ä½Žï¼šæ”¯æŒå¹¿æ³›çš„æ’ä»¶å’Œä¼˜åŒ–å·¥å…·\n*   ç”Ÿæ€æˆç†Ÿï¼šæ”¯æŒå¹¿æ³›çš„æ’ä»¶å’Œä¼˜åŒ–å·¥å…·\n*   ç”Ÿæˆé€Ÿåº¦å¿«ï¼šåœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šæµç•…è¿è¡Œ\n\næ¨¡åž‹å±€é™ï¼š\n\n*   ç»†èŠ‚å¤„ç†ï¼šæ‰‹éƒ¨/å¤æ‚å…‰å½±æ˜“ç•¸å˜\n*   åˆ†è¾¨çŽ‡é™åˆ¶ï¼šç›´æŽ¥ç”Ÿæˆ1024x1024è´¨é‡ä¸‹é™\n*   æç¤ºè¯ä¾èµ–ï¼šéœ€ç²¾ç¡®è‹±æ–‡æè¿°æŽ§åˆ¶æ•ˆæžœ"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/image-to-image",
  "markdown": "# ComfyUI å›¾ç”Ÿå›¾å·¥ä½œæµ - ComfyUI\n\n## ä»€ä¹ˆæ˜¯å›¾ç”Ÿå›¾\n\nå›¾ç”Ÿå›¾ï¼ˆImage to Imageï¼‰æ˜¯ ComfyUI ä¸­çš„ä¸€ç§å·¥ä½œæµï¼Œå®ƒå…è®¸ç”¨æˆ·å°†ä¸€å¼ å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆä¸€å¼ æ–°çš„å›¾åƒã€‚ å›¾ç”Ÿå›¾å¯ä»¥ä½¿ç”¨åœ¨ä»¥ä¸‹åœºæ™¯ä¸­ï¼š\n\n*   åŽŸå§‹å›¾åƒé£Žæ ¼çš„è½¬æ¢ï¼Œå¦‚æŠŠå†™å®žç…§ç‰‡è½¬ä¸ºè‰ºæœ¯é£Žæ ¼\n*   å°†çº¿ç¨¿å›¾åƒè½¬æ¢ä¸ºå†™å®žå›¾åƒ\n*   å›¾åƒçš„ä¿®å¤\n*   è€ç…§ç‰‡ç€è‰²\n*   â€¦ ç­‰å…¶å®ƒåœºæ™¯\n\nç”¨ä¸€ä¸ªæ¯”å–»æ¥è®²è§£çš„è¯ï¼Œå¤§æ¦‚æ˜¯è¿™æ ·ï¼š ä½ éœ€è¦ç”»å®¶æ ¹æ®ä½ çš„å‚è€ƒå›¾ç‰‡ï¼Œç”»å‡ºç¬¦åˆä½ è¦æ±‚ç‰¹å®šæ•ˆæžœçš„ä½œå“ã€‚ å¦‚æžœä½ ä»”ç»†æ¯”å¯¹æœ¬ç¯‡æ•™ç¨‹å’Œ[æ–‡ç”Ÿå›¾](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)æ•™ç¨‹ï¼Œä½ ä¼šå‘çŽ°å›¾ç”Ÿå›¾çš„æµç¨‹å’Œæ–‡ç”Ÿå›¾çš„æµç¨‹éžå¸¸ç›¸ä¼¼ï¼Œåªæ˜¯å¤šäº†ä¸ªè¾“å…¥çš„å‚è€ƒå›¾ç‰‡ä½œä¸ºè¾“å…¥æ¡ä»¶ï¼Œä¹Ÿå°±æ˜¯åœ¨æ–‡ç”Ÿå›¾ä¸­ï¼Œæˆ‘ä»¬æ˜¯è®©ç”»å®¶ï¼ˆç»˜å›¾æ¨¡åž‹ï¼‰æ ¹æ®æˆ‘ä»¬çš„æç¤ºè¯ç”Ÿæˆè‡ªç”±å‘æŒ¥ï¼Œè€Œåœ¨å›¾ç”Ÿå›¾ä¸­ï¼Œæˆ‘ä»¬æ˜¯è®©ç”»å®¶ï¼ˆç»˜å›¾æ¨¡åž‹ï¼‰æ ¹æ®æˆ‘ä»¬çš„å‚è€ƒå›¾ç‰‡å’Œæç¤ºè¯ç”Ÿæˆå›¾ç‰‡ã€‚\n\n### 1\\. æ¨¡åž‹å®‰è£…\n\nè¯·ç¡®ä¿ä½ å·²ç»åœ¨ `ComfyUI/models/checkpoints` æ–‡ä»¶å¤¹è‡³å°‘æœ‰ä¸€ä¸ª SD1.5 çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¦‚æžœä½ è¿˜ä¸äº†è§£å¦‚ä½•å®‰è£…æ¨¡åž‹ï¼Œè¯·å‚[å¼€å§‹ ComfyUI çš„ AI ç»˜å›¾ä¹‹æ—…](https://docs.comfy.org/zh-CN/get_started/first_generation#3-%E5%AE%89%E8%A3%85%E7%BB%98%E5%9B%BE%E6%A8%A1%E5%9E%8B)ç« èŠ‚ä¸­å…³äºŽæ¨¡åž‹å®‰è£…çš„éƒ¨åˆ†è¯´æ˜Žã€‚ ä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„è¿™äº›æ¨¡åž‹ï¼š\n\n*   [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)\n*   [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)\n*   [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)\n\n### 2\\. å›¾ç”Ÿå›¾å·¥ä½œæµç›¸å…³æ–‡ä»¶\n\nä¿å­˜å¹¶ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡åˆ°æœ¬åœ°ï¼Œç„¶åŽ **æ‹–æ‹½æˆ–ä½¿ç”¨ ComfyUI æ‰“å¼€** å®ƒï¼Œå°±ä¼šåŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![å›¾ç”Ÿå›¾å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image_to_image.png) æˆ–åœ¨ ComfyUI çš„ **workflow template** ä¸­åŠ è½½ **image to image** å·¥ä½œæµ ![ComfyUI å·¥ä½œæµæ¨¡æ¿ - å›¾ç”Ÿå›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/image-to-image-01-template.jpg) ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºä½¿ç”¨ç¤ºä¾‹ï¼Œæˆ‘ä»¬ä¼šåœ¨åŽé¢çš„æ­¥éª¤ä¸­ä½¿ç”¨å®ƒ ![å›¾ç‰‡ç¤ºä¾‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/input.jpeg) \n\n### 3\\. å¼€å§‹å›¾ç”Ÿå›¾å·¥ä½œæµ\n\n![ComfyUI å›¾ç”Ÿå›¾å·¥ä½œæµ - æ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image-to-image-02-guide.jpg) åœ¨åŠ è½½å›¾ç”Ÿå›¾å·¥ä½œæµåŽï¼Œè¯·å¯¹ç…§å›¾ç‰‡ï¼ŒæŒ‰ç…§åºå·å®Œæˆä»¥ä¸‹æ“ä½œï¼Œå®Œæˆç¤ºä¾‹å·¥ä½œæµçš„ç”Ÿæˆ\n\n1.  åœ¨ **Load Checkpoint** èŠ‚ç‚¹ä¸­åŠ è½½å¥½ä½ æœ¬åœ°çš„ç»˜å›¾æ¨¡åž‹\n2.  åœ¨ **Load Image** èŠ‚ç‚¹ç‚¹å‡» `upload` æŒ‰é’®ï¼Œä¸Šä¼ å‡†å¤‡æ­¥éª¤ä¸­æä¾›çš„å›¾ç‰‡\n3.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\n## å¼€å§‹ä½ è‡ªå·±çš„å°è¯•\n\n1.  è¯•ç€ä¿®æ”¹ **KSampler** èŠ‚ç‚¹ä¸­çš„ `denoise` å‚æ•°ï¼Œé€æ¸ä»Ž 1 åˆ° 0 å˜åŒ–ï¼Œè§‚å¯Ÿç”Ÿæˆå›¾ç‰‡çš„å˜åŒ–\n2.  æ›´æ¢ä½ è‡ªå·±çš„æç¤ºè¯å’Œå‚è€ƒå›¾ç‰‡ï¼Œç”Ÿæˆå±žäºŽä½ è‡ªå·±çš„å›¾ç‰‡æ•ˆæžœ\n\n## å›¾ç”Ÿå›¾å·¥ä½œæµæ ¸å¿ƒè¦ç‚¹\n\nå›¾ç”Ÿå›¾å·¥ä½œæµçš„æ ¸å¿ƒåœ¨äºŽåœ¨äºŽ `KSampler` èŠ‚ç‚¹ä¸­çš„ `denoise` å‚æ•°è¦æ˜¯ **å°äºŽ 1** å¦‚æžœä½ è°ƒæ•´è¿‡ `denoise` å‚æ•°ï¼Œè¿›è¡Œç”ŸæˆåŽä¼šå‘çŽ°ï¼š\n\n*   `denoise` è¶Šå°ï¼Œç”Ÿæˆå›¾ç‰‡å’Œå‚è€ƒå›¾ç‰‡çš„å·®å¼‚å°±ä¼šè¶Šå°ï¼Œ\n*   `denoise` è¶Šå¤§ï¼Œç”Ÿæˆå›¾ç‰‡å’Œå‚è€ƒå›¾ç‰‡çš„å·®å¼‚å°±ä¼šè¶Šå¤§ã€‚\n\nå› ä¸º `denoise` å†³å®šäº†å¯¹åº”å›¾ç‰‡è½¬æ¢ä¸ºæ½œç©ºé—´å›¾åƒåŽï¼Œå‘æ½œåœ¨ç©ºé—´å›¾åƒæ·»åŠ çš„å™ªå£°å¼ºåº¦ï¼Œå¦‚æžœ `denoise` ä¸º 1ï¼Œå¯¹åº”æ½œç©ºé—´å›¾åƒå°±ä¼šå˜æˆä¸€ä¸ªå®Œå…¨éšæœºçš„å™ªå£°ï¼Œé‚£è¿™æ ·å°±å’Œ`empty latent image`èŠ‚ç‚¹ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ä¸€æ ·äº†ï¼Œå°±ä¼šä¸¢å¤±å‚è€ƒå›¾ç‰‡çš„æ‰€æœ‰ç‰¹å¾ã€‚ å¯¹åº”åŽŸç†å¯ä»¥å‚è€ƒ[æ–‡ç”Ÿå›¾](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)æ•™ç¨‹ä¸­çš„åŽŸç†è®²è§£ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/overview",
  "markdown": "# API Nodes - ComfyUI\n\nAPI Nodes æ˜¯ ComfyUI æ–°å¢žçš„è°ƒç”¨é—­æºæ¨¡åž‹çš„æ–¹å¼ï¼Œé€šè¿‡ API è°ƒç”¨ï¼Œè¿™å°†ä¸º ComfyUI ç”¨æˆ·æä¾›è®¿é—®å¤–éƒ¨æœ€å…ˆè¿› AI æ¨¡åž‹çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€å¤æ‚çš„ API å¯†é’¥è®¾ç½®ã€‚\n\nAPI Nodes æ˜¯ä¸€ç»„ç‰¹æ®Šçš„èŠ‚ç‚¹ï¼Œå®ƒä»¬èƒ½å¤Ÿè¿žæŽ¥åˆ°å¤–éƒ¨ API æœåŠ¡ï¼Œè®©æ‚¨ç›´æŽ¥åœ¨ ComfyUI å·¥ä½œæµä¸­ä½¿ç”¨é—­æºæˆ–ç¬¬ä¸‰æ–¹æ‰˜ç®¡çš„ AI æ¨¡åž‹ã€‚è¿™äº›èŠ‚ç‚¹è®¾è®¡ç”¨äºŽæ— ç¼é›†æˆå¤–éƒ¨æ¨¡åž‹çš„åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒ ComfyUI æ ¸å¿ƒçš„å¼€æºç‰¹æ€§ã€‚ ç›®å‰æ”¯æŒçš„æ¨¡åž‹åŒ…æ‹¬ï¼š\n\n*   **Black Forest Labs**: Flux 1.1\\[pro\\] Ultra, Flux .1\\[pro\\], Flux .1 Kontext Pro, Flux .1 Kontext Max\n*   **Google**: Veo2, Gemini 2.5 Pro, Gemini 2.5 Flash\n*   **Ideogram**: V3, V2, V1\n*   **Kling**: 2.0, 1.6, 1.5 & Various Effects\n*   **Luma**: Photon, Ray2, Ray1.6\n*   **MiniMax**: Text-to-Video, Image-to-Video\n*   **OpenAI**: o1, o1-pro, o3, gpt-4o, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, DALLÂ·E 2, DALLÂ·E 3, GPT-Image-1\n*   **PixVerse**: V4 & Effects\n*   **Pika**: 2.2\n*   **Recraft**: V3, V2 & Various Tools\n*   **Rodin**: 3D Generation\n*   **Stability AI**: Stable Image Ultra, Stable Diffusion 3.5 Large, Image Upscale\n*   **Tripo**: v1-4, v2.0, v2.5\n\n## ä½¿ç”¨ API Nodes çš„å‰æè¦æ±‚\n\nè¦ä½¿ç”¨ API Nodes èŠ‚ç‚¹ï¼Œéœ€è¦æœ‰ä»¥ä¸‹èŠ‚ç‚¹è¦æ±‚\n\n### 1\\. ComfyUI ç‰ˆæœ¬è¦æ±‚\n\nè¯·æ›´æ–°ä½ çš„ ComfyUI åˆ°æœ€æ–°ç‰ˆæœ¬ï¼Œç”±äºŽæˆ‘ä»¬åŽæœŸå¯èƒ½ä¼šæ–°å¢žæ›´å¤šçš„ API æ”¯æŒ, ç›¸åº”çš„èŠ‚ç‚¹ä¹Ÿä¼šè¿›è¡Œæ›´æ–°, æ‰€ä»¥è¯·ä¿æŒä½ çš„ ComfyUI å¤„äºŽæœ€æ–°ç‰ˆæœ¬ã€‚\n\n### 2\\. è´¦å·åŠè´¦æˆ·ä½™é¢è¦æ±‚\n\néœ€è¦å½“å‰å·²ç»åœ¨ ComfyUI ä¸­ç™»å½•äº† [Comfyè´¦å·](https://docs.comfy.org/zh-CN/interface/user)ï¼Œå¹¶ä¸”è´¦æˆ·[ç§¯åˆ†](https://docs.comfy.org/zh-CN/interface/credits)å¤§äºŽ 0 åœ¨ `è®¾ç½®` -> `ç”¨æˆ·` ä¸­è¿›è¡Œç™»å½•ï¼š ![ComfyUI ç”¨æˆ·ç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/user.jpg) å¹¶åœ¨ `è®¾ç½®` -> `ç§¯åˆ†` ä¸­è´­ä¹°ç§¯åˆ†ï¼š ![ComfyUI ç§¯åˆ†ç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/menu-credits.jpg) è¯·å‚è€ƒå¯¹åº”çš„è´¦å·åŠç§¯åˆ†éƒ¨åˆ†çš„æ–‡æ¡£æ¥ç¡®ä¿è¿™ä¸€è¦æ±‚ï¼š\n\n*   [Comfyè´¦å·](https://docs.comfy.org/zh-CN/interface/user): åœ¨è®¾ç½®èœå•ä¸­æ‰¾åˆ°`ç”¨æˆ·`éƒ¨åˆ†ï¼Œè¿›è¡Œç™»å½•\n*   [ç§¯åˆ†](https://docs.comfy.org/zh-CN/interface/credits): ç™»å½•åŽè®¾ç½®ç•Œé¢ä¼šå‡ºçŽ°ç§¯åˆ†èœå•ï¼Œæ‚¨å¯ä»¥åœ¨`è®¾ç½®` â†’ `ç§¯åˆ†`ä¸­è´­ä¹°ç§¯åˆ†ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„ä»˜è´¹ï¼Œä¸ä¼šæœ‰æ„å¤–çš„è´¹ç”¨\n\n### 3\\. ç½‘ç»œçŽ¯å¢ƒè¦æ±‚\n\n*   æœ¬åœ°ç½‘ç»œä»…å…è®¸ `127.0.0.1` æˆ–è€… `localhost` è®¿é—®å¯ä»¥ç›´æŽ¥ä½¿ç”¨ç™»å½•åŠŸèƒ½\n*   å¦‚æžœæ˜¯å±€åŸŸç½‘æˆ–è€…éžç™½åå•ç½‘ç«™è®¿é—®è¯·ä½¿ç”¨ API Key ç™»å½•ï¼Œè¯·å‚è€ƒ[ä½¿ç”¨ API Key è¿›è¡Œç™»å½•](https://docs.comfy.org/zh-CN/interface/user#%E4%BD%BF%E7%94%A8-api-key-%E8%BF%9B%E8%A1%8C%E7%99%BB%E5%BD%95)\n*   èƒ½å¤Ÿæ­£å¸¸è®¿é—®æˆ‘ä»¬çš„ API æœåŠ¡ï¼ˆåœ¨æŸäº›åœ°åŒºå¯èƒ½éœ€è¦ä½¿ç”¨ä»£ç†æœåŠ¡ï¼‰\n*   è¦æ±‚åœ¨ `https` çŽ¯å¢ƒä¸‹è®¿é—®ï¼Œä¿è¯è¯·æ±‚çš„å®‰å…¨æ€§\n\n### 4\\. ä½¿ç”¨å¯¹åº”èŠ‚ç‚¹\n\n**æ·»åŠ åˆ°å·¥ä½œæµ**ï¼šå°† API èŠ‚ç‚¹æ·»åŠ åˆ°æ‚¨çš„å·¥ä½œæµä¸­ï¼Œå°±åƒä½¿ç”¨å…¶ä»–èŠ‚ç‚¹ä¸€æ · **è¿è¡Œ**ï¼šè®¾ç½®å¥½å‚æ•°åŽè¿è¡Œå·¥ä½œæµ ![API Nodes](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/sidebar.jpg)\n\n## ä½¿ç”¨ API Key åœ¨éžç™½åå•ç½‘ç«™ç™»å½• ComfyUI è´¦æˆ·æ¥ä½¿ç”¨ API Nodes\n\nç›®å‰æˆ‘ä»¬è®¾ç½®æœ‰ç™½åå•æ¥é™åˆ¶å¯ä»¥ç™»å½•ComfyUI è´¦æˆ·çš„ç½‘ç«™ï¼Œå¦‚æžœéœ€è¦åœ¨ä¸€äº›éžç™½åå•ç½‘ç«™ç™»å½• ComfyUI è´¦æˆ·ï¼Œè¯·å‚è€ƒè´¦å·ç®¡ç†ç›¸å…³çš„éƒ¨åˆ†äº†è§£å¦‚ä½•ä½¿ç”¨ API Key æ¥è¿›è¡Œç™»å½•ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä¸éœ€è¦å¯¹åº”çš„ç½‘ç«™åœ¨æˆ‘ä»¬çš„ç™½åå•ä¸­ã€‚\n\n[\n\n## è´¦æˆ·ç®¡ç†\n\næŸ¥çœ‹å¦‚ä½•ä½¿ç”¨ ComfyUI API Key ç™»å½•\n\n\n\n](https://docs.comfy.org/zh-CN/interface/user#%E4%BD%BF%E7%94%A8-api-key-%E8%BF%9B%E8%A1%8C%E7%99%BB%E5%BD%95)\n\n![Select Comfy API Key Login](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user/user-login-api-1.jpg)\n\n## ä½¿ç”¨ ComfyUI API Key é›†æˆæ¥è°ƒç”¨ä»˜è´¹æ¨¡åž‹ API èŠ‚ç‚¹\n\nç›®å‰æˆ‘ä»¬æ”¯æŒé€šè¿‡ ComfyUI API Key é›†æˆæ¥è®¿é—®æˆ‘ä»¬çš„æœåŠ¡æ¥è°ƒç”¨ä»˜è´¹æ¨¡åž‹ API èŠ‚ç‚¹ï¼Œè¯·å‚è€ƒ API Key é›†æˆç« èŠ‚äº†è§£å¦‚ä½•ä½¿ç”¨ API Key é›†æˆæ¥è°ƒç”¨ä»˜è´¹æ¨¡åž‹ API èŠ‚ç‚¹\n\n[\n\n## API Key é›†æˆ\n\nè¯·å‚è€ƒ API Key é›†æˆç« èŠ‚äº†è§£å¦‚ä½•ä½¿ç”¨ API Key é›†æˆæ¥è°ƒç”¨ä»˜è´¹æ¨¡åž‹ API èŠ‚ç‚¹\n\n\n\n](https://docs.comfy.org/zh-CN/development/comfyui-server/api-key-integration)\n\n## API Nodes çš„ä¼˜åŠ¿\n\nAPI Nodes ä¸º ComfyUI ç”¨æˆ·æä¾›äº†å‡ ä¸ªé‡è¦ä¼˜åŠ¿ï¼š\n\n*   **è®¿é—®é—­æºæ¨¡åž‹**ï¼šä½¿ç”¨æœ€å…ˆè¿›çš„ AI æ¨¡åž‹ï¼Œæ— éœ€è‡ªè¡Œéƒ¨ç½²\n*   **æ— ç¼é›†æˆ**ï¼šAPI èŠ‚ç‚¹ä¸Žå…¶ä»– ComfyUI èŠ‚ç‚¹å®Œå…¨å…¼å®¹ï¼Œå¯ä»¥ç»„åˆåˆ›å»ºå¤æ‚å·¥ä½œæµ\n*   **ç®€åŒ–çš„ä½“éªŒ**ï¼šæ— éœ€ç®¡ç† API å¯†é’¥æˆ–å¤„ç†å¤æ‚çš„ API è¯·æ±‚\n*   **å¯æŽ§çš„æˆæœ¬**ï¼šé¢„ä»˜è´¹ç³»ç»Ÿç¡®ä¿æ‚¨å®Œå…¨æŽ§åˆ¶æ”¯å‡ºï¼Œæ²¡æœ‰æ„å¤–è´¹ç”¨\n\n## è®¡è´¹æ–¹å¼\n\n[\n\n## API èŠ‚ç‚¹è®¡è´¹\n\nè¯·å‚è€ƒå®šä»·é¡µé¢äº†è§£å¯¹åº”çš„ API å®šä»·\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/api-nodes/pricing)\n\n![é€‰æ‹© Comfy API Key ç™»å½•](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/user/user-login-api-1.jpg)\n\n## å…³äºŽå¼€æºå’Œé€‰æ‹©åŠ å…¥\n\né‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œ**API Nodes æ˜¯å®Œå…¨å¯é€‰çš„**ã€‚ComfyUI å°†å§‹ç»ˆä¿æŒå®Œå…¨å¼€æºï¼Œå¹¶å¯¹æœ¬åœ°ç”¨æˆ·å…è´¹ã€‚API èŠ‚ç‚¹è®¾è®¡ä¸ºâ€é€‰æ‹©åŠ å…¥â€åŠŸèƒ½ï¼Œä¸ºé‚£äº›æƒ³è¦è®¿é—®å¤–éƒ¨ SOTAï¼ˆæœ€å…ˆè¿›ï¼‰æ¨¡åž‹çš„ç”¨æˆ·æä¾›ä¾¿åˆ©ã€‚\n\n## å¦‚ä½•ä½¿ç”¨ API Nodes\n\nAPI Nodes çš„ä¸€ä¸ªå¼ºå¤§åº”ç”¨æ˜¯å°†å¤–éƒ¨æ¨¡åž‹çš„è¾“å‡ºä¸Žæœ¬åœ°èŠ‚ç‚¹ç»“åˆã€‚ä¾‹å¦‚ï¼š\n\n*   ä½¿ç”¨ [GPT-Image-1](https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/gpt-image-1) ç”ŸæˆåŸºç¡€å›¾åƒï¼Œç„¶åŽé€šè¿‡æœ¬åœ° `WanImageToVideo` èŠ‚ç‚¹è½¬æ¢ä¸ºè§†é¢‘\n*   ç»“åˆå¤–éƒ¨ç”Ÿæˆçš„å›¾åƒä¸Žæœ¬åœ°çš„ä¸Šé‡‡æ ·æˆ–é£Žæ ¼è½¬æ¢èŠ‚ç‚¹\n*   åˆ›å»ºæ··åˆå·¥ä½œæµï¼Œå……åˆ†åˆ©ç”¨é—­æºå’Œå¼€æºæ¨¡åž‹çš„ä¼˜åŠ¿\n\nè¿™ç§çµæ´»æ€§ä½¿ ComfyUI æˆä¸ºçœŸæ­£çš„é€šç”¨ç”Ÿæˆå¼ AI å…¥å£ï¼Œå°†å„ç§ä¸åŒçš„ AI åŠŸèƒ½æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„å·¥ä½œæµä¸­ï¼Œå¸¦æ¥äº†æ›´å¤šå¯èƒ½æ€§\n\n## å¸¸è§é—®é¢˜"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/faq",
  "markdown": "# API Nodes åœºæ™¯é—®é¢˜ - ComfyUI\n\nä¸ºä»€ä¹ˆæˆ‘æ‰¾ä¸åˆ° API èŠ‚ç‚¹ï¼Ÿ\n\nè¯·æ›´æ–°ä½ çš„ ComfyUI åˆ°æœ€æ–°ç‰ˆæœ¬ï¼ˆæœ€æ–°çš„ commit,æˆ–è€…æœ€æ–°çš„[æ¡Œé¢ç‰ˆ](https://www.comfy.org/download)ï¼‰ï¼Œç”±äºŽæˆ‘ä»¬åŽæœŸå¯èƒ½ä¼šæ–°å¢žæ›´å¤šçš„ API æ”¯æŒ, ç›¸åº”çš„èŠ‚ç‚¹ä¹Ÿä¼šè¿›è¡Œæ›´æ–°, æ‰€ä»¥è¯·ä¿æŒä½ çš„ ComfyUI å¤„äºŽæœ€æ–°ç‰ˆæœ¬ã€‚\n\nè¯·æ³¨æ„éœ€è¦åŒºåˆ† nightly ç‰ˆæœ¬å’Œ release ç‰ˆæœ¬ï¼Œæœ‰äº›æƒ…å†µä¸‹ `nightly` ç‰ˆæœ¬(ä¹Ÿå°±æ˜¯æœ€æ–°çš„ä»£ç  commit æäº¤)æ‰ä¼šåŒ…å«æœ€æ–°çš„èŠ‚ç‚¹ï¼Œå› ä¸º release ç‰ˆæœ¬å¯èƒ½ä¸ä¼šåŠæ—¶æ›´æ–°ã€‚ç”±äºŽæˆ‘ä»¬ä»åœ¨å¿«é€Ÿåœ°è¿­ä»£ä¸­ï¼Œæ‰€ä»¥å½“ä½ æ— æ³•æ‰¾åˆ°å¯¹åº”èŠ‚ç‚¹æ—¶è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°çš„ç‰ˆæœ¬ã€‚\n\nä¸ºä»€ä¹ˆæˆ‘æ— æ³•ä½¿ç”¨ / ç™»å½• API Nodes èŠ‚ç‚¹ï¼Ÿ\n\nAPI è®¿é—®éœ€è¦ä½ å½“å‰çš„è¯·æ±‚æ˜¯åŸºäºŽå®‰å…¨çš„ç½‘ç»œçŽ¯å¢ƒï¼Œç›®å‰å¯¹ API è®¿é—®çš„ç½‘ç»œçŽ¯å¢ƒè¦æ±‚å¦‚ä¸‹:\n\n*   æœ¬åœ°ç½‘ç»œä»…å…è®¸ `127.0.0.1` æˆ–è€… `localhost` è®¿é—®, è¿™å¯èƒ½æ„å‘³ç€ï¼Œä½ æ— æ³•åœ¨å±€åŸŸç½‘çŽ¯å¢ƒä¸‹ä½¿ç”¨å¸¦æœ‰`--listen` å‚æ•°å¯åŠ¨çš„ ComfyUI æœåŠ¡ä¸­ä¸­ä½¿ç”¨ API Nodes èŠ‚ç‚¹\n*   èƒ½å¤Ÿæ­£å¸¸è®¿é—®æˆ‘ä»¬çš„ API æœåŠ¡ï¼ˆåœ¨æŸäº›åœ°åŒºå¯èƒ½éœ€è¦ä½¿ç”¨ä»£ç†æœåŠ¡ï¼‰\n*   ä½ çš„è´¦å·æ²¡æœ‰è¶³å¤Ÿçš„[ç§¯åˆ†](https://docs.comfy.org/zh-CN/interface/credits)\n\nä¸ºä»€ä¹ˆç™»å½•äº†è¿˜æ˜¯æ— æ³•ä½¿ç”¨æˆ–åœ¨ä½¿ç”¨æ—¶è¿˜ä¼šç»§ç»­è¦æ±‚æˆ‘ç™»å½•ï¼Ÿ\n\n*   ç›®å‰ä»…æ”¯æŒ `127.0.0.1` æˆ–è€… `localhost` è®¿é—®,\n*   ç¡®ä¿ä½ çš„è´¦æˆ·æœ‰è¶³å¤Ÿä½™é¢\n\nAPI Nodes èŠ‚ç‚¹å¯ä»¥å…è´¹ä½¿ç”¨å—ï¼Ÿ\n\nAPI Nodes èŠ‚ç‚¹ç”±äºŽéœ€è¦é€šè¿‡ API è°ƒç”¨é—­æºæ¨¡åž‹ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨ç§¯åˆ†ï¼Œä¸æ”¯æŒå…è´¹ä½¿ç”¨\n\nè¦å¦‚ä½•è´­ä¹°ç§¯åˆ†ï¼Ÿ\n\nè¯·å‚è€ƒä¸‹é¢çš„æ–‡æ¡£ï¼š\n\n1.  [Comfyè´¦å·](https://docs.comfy.org/zh-CN/interface/user): åœ¨è®¾ç½®èœå•ä¸­æ‰¾åˆ°`ç”¨æˆ·`éƒ¨åˆ†ï¼Œè¿›è¡Œç™»å½•\n2.  [ç§¯åˆ†](https://docs.comfy.org/zh-CN/interface/credits): ç™»å½•åŽè®¾ç½®ç•Œé¢ä¼šå‡ºçŽ°ç§¯åˆ†èœå•ï¼Œæ‚¨å¯ä»¥åœ¨`è®¾ç½®` â†’ `ç§¯åˆ†`ä¸­è´­ä¹°ç§¯åˆ†ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„ä»˜è´¹ï¼Œä¸ä¼šæœ‰æ„å¤–çš„è´¹ç”¨\n3.  é€šè¿‡ Stripe å®Œæˆä»˜æ¬¾\n4.  æŸ¥çœ‹ç§¯åˆ†æ˜¯å¦æ›´æ–°ï¼Œå¦‚æžœæ²¡æœ‰è¯•ç€é‡å¯æˆ–è€…åˆ·æ–°é¡µé¢\n\næœªç”¨å®Œçš„ç§¯åˆ†æ”¯æŒé€€æ¬¾å—ï¼Ÿ\n\nç›®å‰æˆ‘ä»¬ä¸æ”¯æŒå¯¹ç§¯åˆ†è¿›è¡Œé€€æ¬¾ã€‚ å¦‚æžœä½ è§‰å¾—æ˜¯å› ä¸ºæŠ€æœ¯é—®é¢˜å‡ºçŽ°äº†é”™è¯¯è€Œå­˜åœ¨æœªä½¿ç”¨çš„ä½™é¢ï¼Œè¯·[è”ç³»æ”¯æŒ](mailto:support@comfy.org)\n\nç§¯åˆ†å¯ä»¥å‡ºçŽ°è´Ÿæ•°å—ï¼Ÿ\n\nä¸å…è®¸ç§¯åˆ†å‡ºçŽ°è´Ÿæ•°ï¼Œæ‰€ä»¥åœ¨å¯¹åº” API è°ƒç”¨å‰ï¼Œè¯·ç¡®ä¿ä½ æœ‰è¶³å¤Ÿçš„ç§¯åˆ†ã€‚\n\næˆ‘å¯ä»¥åœ¨å“ªé‡ŒæŸ¥çœ‹ä½¿ç”¨é‡å’ŒèŠ±è´¹ï¼Ÿ\n\nè¯·åœ¨ç™»å½•åŽè®¿é—®[ç§¯åˆ†](https://docs.comfy.org/zh-CN/interface/credits) èœå•ï¼ŒæŸ¥çœ‹ç›¸åº”çš„ç§¯åˆ†ã€‚\n\næ”¯æŒä½¿ç”¨è‡ªå·±çš„ API Key å—ï¼Ÿ\n\nç›®å‰ API Nodes èŠ‚ç‚¹ä»åœ¨æµ‹è¯•é˜¶æ®µï¼Œç›®å‰æš‚ä¸æ”¯æŒï¼Œæˆ‘ä»¬å·²ç»æŠŠè¿™ä¸ªåŠŸèƒ½çº³å…¥è€ƒè™‘ä¸­äº†ã€‚ã€‚\n\nç§¯åˆ†ä¼šè¿‡æœŸå—ï¼Ÿ\n\nä¸ä½ çš„ç§¯åˆ†ä¸ä¼šè¿‡æœŸã€‚\n\nç§¯åˆ†å¯ä»¥è½¬è®©æˆ–è€…å…±äº«å—ï¼Ÿ\n\nä¸ï¼Œä½ çš„ç§¯åˆ†ä¸èƒ½è½¬è®©ç»™å…¶ä»–ç”¨æˆ·ï¼Œä¹Ÿåªé™åˆ¶äºŽå½“å‰ç™»å½•è´¦æˆ·ä½¿ç”¨ï¼Œä½†æ˜¯æˆ‘ä»¬å¹¶ä¸é™åˆ¶ç™»å½•è®¾å¤‡çš„æ•°é‡\n\næˆ‘å¯ä»¥åœ¨ä¸åŒè®¾å¤‡é—´ä½¿ç”¨åŒä¸€ä¸ªè´¦æˆ·å—ï¼Ÿ\n\næˆ‘ä»¬ä¸é™åˆ¶ç™»å½•çš„è®¾å¤‡æ•°é‡ï¼Œä½ å¯ä»¥åœ¨ä½ æƒ³è¦çš„ä»»ä½•åœ°æ–¹ä½¿ç”¨ä½ çš„è´¦å·ï¼Ÿ\n\næˆ‘è¯¥å¦‚ä½•è¯·æ±‚åˆ é™¤æˆ‘çš„è´¦æˆ·æˆ–ä¿¡æ¯ï¼Ÿ"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/pricing",
  "markdown": "# è®¡è´¹ - ComfyUI\n\nBFLFlux 1.1 \\[pro\\] Ultra ImageNA$0.06BFLFlux.1 Canny Control ImageNA$0.05BFLFlux.1 Depth Control ImageNA$0.05BFLFlux.1 Expand ImageNA$0.05BFLFlux.1 Fill ImageNA$0.05BFLFlux.1 Kontext \\[max\\] ImageNA$0.08BFLFlux.1 Kontext \\[pro\\] ImageNA$0.04BFLFlux.1 Kontext \\[pro\\] ImageNA$0.05KlingKling Image Generationkling-v1-5, 1, image to image$0.028KlingKling Image Generationkling-v1-5, 1, text to image$0.014KlingKling Image Generationkling-v1, 1, image to image$0.0035KlingKling Image Generationkling-v1, 1, text to image$0.0035KlingKling Image Generationkling-v2, 1, text to image$0.014KlingKling Virtual Try OnNA$0.07KlingKling, Text to Video (Camera Control)NA$0.49KlingKling Dual Character Video, Effectskling-v1-5, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1-5, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1-5, std, 5$0.28KlingKling Dual Character Video, Effectskling-v1-5, std, 10$0.56KlingKling Dual Character Video, Effectskling-v1-6, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1-6, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1-6, std, 5$0.28KlingKling Dual Character Video, Effectskling-v1-6, std, 10$0.56KlingKling Dual Character Video, Effectskling-v1, pro, 5$0.49KlingKling Dual Character Video, Effectskling-v1, pro, 10$0.98KlingKling Dual Character Video, Effectskling-v1, std, 5$0.14KlingKling Dual Character Video, Effectskling-v1, std, 10$0.28KlingKling Image to Videokling-v1-5, pro, 5$0.49KlingKling Image to Videokling-v1-5, pro, 10$0.98KlingKling Image to Videokling-v1-5, std, 5$0.28KlingKling Image to Videokling-v1-5, std, 10$0.56KlingKling Image to Videokling-v1-6, pro, 5$0.49KlingKling Image to Videokling-v1-6, pro, 10$0.98KlingKling Image to Videokling-v1-6, std, 5$0.28KlingKling Image to Videokling-v1-6, std, 10$0.56KlingKling Image to Videokling-v1, pro, 5$0.49KlingKling Image to Videokling-v1, pro, 10$0.98KlingKling Image to Videokling-v1, std, 5$0.14KlingKling Image to Videokling-v1, std, 10$0.28KlingKling Image to Videokling-v2-maser, pro, 5s$1.4KlingKling Image to Videokling-v2-maser, pro, 10s$2.8KlingKling Image to Videokling-v2-maser, std, 5s$1.4KlingKling Image to Videokling-v2-maser, std, 10s$2.8KlingKling Lip Sync Video with, Audio5s$0.07KlingKling Lip Sync Video with, Audio10s$0.14KlingKling Lip Sync Video with Text5s$0.07KlingKling Lip Sync Video with Text10s$0.14KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1$0.49KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1-5$0.49KlingKling Start-End Frame to Videopro mode / 5s duration / kling-v1-6$0.49KlingKling Start-End Frame to Videopro mode / 10s duration / kling-v1-5$0.98KlingKling Start-End Frame to Videopro mode / 10s duration / kling-v1-6$0.98KlingKling Start-End Frame to Videostandard mode / 5s duration / kling-v1$0.14KlingKling Text to Videopro mode / 5s duration / kling-v1$0.49KlingKling Text to Videopro mode / 5s duration / kling-v2-master$1.4KlingKling Text to Videopro mode / 10s duration / kling-v1$0.98KlingKling Text to Videopro mode / 10s duration / kling-v2-master$2.8KlingKling Text to Videostandard mode / 5s duration / kling-v1$0.14KlingKling Text to Videostandard mode / 5s duration / kling-v1-6$0.28KlingKling Text to Videostandard mode / 5s duration / kling-v2-master$1.4KlingKling Text to Videostandard mode / 10s duration / kling-v1$0.28KlingKling Text to Videostandard mode / 10s duration / kling-v1-6$0.56KlingKling Text to Videostandard mode / 10s duration / kling-v2-master$2.8KlingKling Video Effectsdizzydizzy/bloombloom, 5$0.49KlingKling Video Effectsfuzzyfuzzy/squish/expansion, 5$0.28KlingKling Video ExtendNA$0.28LumaLuma, Text to Imagephoto-flash-1$0.0019LumaLuma, Text to Imagephoto-flash-1$0.0019LumaLuma Image to Imagephoton-1$0.0073LumaLuma Image to Imagephoton-1$0.0073LumaLuma Image to Videoray-1-6, 4k, 5s$3.19LumaLuma Image to Videoray-1-6, 4k, 9s$5.73LumaLuma Image to Videoray-1-6, 540p, 5s$0.2LumaLuma Image to Videoray-1-6, 540p, 9s$0.36LumaLuma Image to Videoray-1-6, 720p, 5s$0.35LumaLuma Image to Videoray-1-6, 720p, 9s$0.64LumaLuma Image to Videoray-1-6, 1080p, 5s$0.8LumaLuma Image to Videoray-1-6, 1080p, 9s$1.43LumaLuma Image to Videoray-2, 4k, 5s$6.37LumaLuma Image to Videoray-2, 4k, 9s$11.47LumaLuma Image to Videoray-2, 540p, 5s$0.4LumaLuma Image to Videoray-2, 540p, 9s$0.72LumaLuma Image to Videoray-2, 720p, 5s$0.71LumaLuma Image to Videoray-2, 720p, 9s$1.27LumaLuma Image to Videoray-2, 1080p, 5s$1.59LumaLuma Image to Videoray-2, 1080p, 9s$2.87LumaLuma Image to Videoray-flash-2, 4k, 5s$2.19LumaLuma Image to Videoray-flash-2, 4k, 9s$3.94LumaLuma Image to Videoray-flash-2, 540p, 5s$0.14LumaLuma Image to Videoray-flash-2, 540p, 9s$0.25LumaLuma Image to Videoray-flash-2, 720p, 5s$0.24LumaLuma Image to Videoray-flash-2, 720p, 9s$0.44LumaLuma Image to Videoray-flash-2, 1080p, 5s$0.55LumaLuma Image to Videoray-flash-2, 1080p, 9s$0.99LumaLuma Text-to-videoray-1-6, 4k, 5s$3.19LumaLuma Text-to-videoray-1-6, 4k, 9s$5.73LumaLuma Text-to-videoray-1-6, 540p, 5s$0.2LumaLuma Text-to-videoray-1-6, 540p, 9s$0.36LumaLuma Text-to-videoray-1-6, 720p, 5s$0.35LumaLuma Text-to-videoray-1-6, 720p, 9s$0.64LumaLuma Text-to-videoray-1-6, 1080p, 5s$0.8LumaLuma Text-to-videoray-1-6, 1080p, 9s$1.43LumaLuma Text-to-videoray-2, 4k, 5s$6.37LumaLuma Text-to-videoray-2, 4k, 9s$11.47LumaLuma Text-to-videoray-2, 540p, 5s$0.4LumaLuma Text-to-videoray-2, 540p, 9s$0.72LumaLuma Text-to-videoray-2, 720p, 5s$0.71LumaLuma Text-to-videoray-2, 720p, 9s$1.27LumaLuma Text-to-videoray-2, 1080p, 5s$1.59LumaLuma Text-to-videoray-2, 1080p, 9s$2.87LumaLuma Text-to-videoray-flash-2, 4k, 5s$2.19LumaLuma Text-to-videoray-flash-2, 4k, 9s$3.94LumaLuma Text-to-videoray-flash-2, 540p, 5s$0.14LumaLuma Text-to-videoray-flash-2, 540p, 9s$0.25LumaLuma Text-to-videoray-flash-2, 720p, 5s$0.24LumaLuma Text-to-videoray-flash-2, 720p, 9s$0.44LumaLuma Text-to-videoray-flash-2, 1080p, 5s$0.55LumaLuma Text-to-videoray-flash-2, 1080p, 9s$0.99GoogleGoogle Veo2 Video Generation5$2.5GoogleGoogle Veo2 Video Generation8$4GoogleGoogle Geminigemini-2.5-flash-preview-04-171.25æ¯ç™¾ä¸‡è¾“å…¥tokens+1.25 æ¯ç™¾ä¸‡è¾“å…¥tokens + 10 æ¯ç™¾ä¸‡è¾“å‡º tokens (< 200K tokens)GoogleGoogle Geminigemini-2.5-pro-preview-05-060.16æ¯ç™¾ä¸‡è¾“å…¥tokens+0.16 æ¯ç™¾ä¸‡è¾“å…¥tokens + 0.6 æ¯ç™¾ä¸‡è¾“å‡º tokens + $1æ¯ç™¾ä¸‡è¾“å…¥éŸ³é¢‘ tokens (< 200K tokens)MinimaxMinimax, Text to Video6s clip$0.43MinimaxMinimax Hailuo-02768P 6s$0.28MinimaxMinimax Hailuo-02768P 10s$0.56MinimaxMinimax Hailuo-021080P 6s$0.49MinimaxMinimax Image to Video6s clip$0.43RecraftRecraft, Creative Upscale ImageNA$0.25RecraftRecraft, Crisp Upscale ImageNA$0.004RecraftRecraft, Image Inpainting1$0.04RecraftRecraft, Image to Image1$0.04RecraftRecraft, Remove BackgroundNA$0.01RecraftRecraft, Replace Background1$0.04RecraftRecraft, Text to Image1$0.04RecraftRecraft, Text to Vector1$0.08RecraftRecraft, Vectorize ImageNA$0.01IdeogramIdeogram, V11, false$0.06IdeogramIdeogram, V11, true$0.02IdeogramIdeogram V21, false$0.08IdeogramIdeogram V21, true$0.05IdeogramIdeogram V31, Balanced$0.06IdeogramIdeogram V31, Quality$0.09IdeogramIdeogram V31, Turbo$0.03RunwayRuway, Text to ImageNA$0.08RunwayRunway, First-Last-Frame to Video5s$0.25RunwayRunway, First-Last-Frame to Video10s$0.5RunwayRunway Image to Video (Gen3a, Turbo)5s$0.25RunwayRunway Image to Video (Gen3a, Turbo)10s$0.5RunwayRunway Image to Video (Gen4, Turbo)5s$0.25RunwayRunway Image to Video (Gen4, Turbo)10s$0.5OpenAIGPT-Image-1 - Actualinput image tokens10/1Mtokens+,inputtexttokens10 / 1M tokens +, input text tokens5 / 1M tokens +,output tokens$40 / 1M tokensOpenAIGPT-Image-1 (è¿‘ä¼¼ä»·æ ¼)high, 1024x1024$0.167OpenAIGPT-Image-1 (è¿‘ä¼¼ä»·æ ¼)high, 1024x1536$0.25OpenAIGPT-Image-1 (è¿‘ä¼¼ä»·æ ¼)high, 1536x1024$0.25OpenAIGPT-Image-1 (è¿‘ä¼¼ä»·æ ¼)low, 1024x1024$0.011OpenAIGPT-Image-1 (è¿‘ä¼¼ä»·æ ¼)low, 1024x1536$0.016OpenAIGPT-Image-1 (è¿‘ä¼¼ä»·æ ¼)low, 1536x1024$0.016OpenAIGPT-Image-1 (è¿‘ä¼¼ä»·æ ¼)medium, 1024x1024$0.042OpenAIGPT-Image-1 (è¿‘ä¼¼ä»·æ ¼)medium, 1024x1536$0.063OpenAIGPT-Image-1 (è¿‘ä¼¼ä»·æ ¼)medium, 1536x1024$0.063OpenAIImage Generation (DALLÂ·E 2)size = 512 \\* 512$0.018OpenAIImage Generation (DALLÂ·E 2)size = 1024 \\* 1024$0.02OpenAIImage Generation (DALLÂ·E 2)size 256 \\* 256$0.016OpenAIImage Generation (DALLÂ·E 3 HD)size = 1024 \\* 1024, hd$0.08OpenAIImage Generation (DALLÂ·E 3 HD)size = 1024 \\* 1792, hd$0.12OpenAIImage Generation (DALLÂ·E 3 HD)size = 1792 \\* 1024, hd$0.12OpenAIImage Generation (DALLÂ·E 3 Std)size = 1024 \\* 1024,std$0.04OpenAIImage Generation (DALLÂ·E 3 Std)size = 1024 \\* 1792, std$0.08OpenAIImage Generation (DALLÂ·E 3 Std)size = 1792 \\* 1024, std$0.08PixversePixVerse, Text to Video360p fast 5s$0.9PixversePixVerse, Text to Video360p normal 5s$0.45PixversePixVerse, Text to Video360p normal 8s$0.9PixversePixVerse, Text to Video540p fast 5s$0.9PixversePixVerse, Text to Video540p normal 5s$0.45PixversePixVerse, Text to Video540p normal 8s$0.9PixversePixVerse, Text to Video720p fast 5s$1.2PixversePixVerse, Text to Video720p normal 5s$0.6PixversePixVerse, Text to Video720p normal 8s$1.2PixversePixVerse, Text to Video1080p normal 5s$1.2PixversePixVerse, Transition Video360p fast 5s$0.9PixversePixVerse, Transition Video360p normal 5s$0.45PixversePixVerse, Transition Video360p normal 8s$0.9PixversePixVerse, Transition Video540p fast 5s$0.9PixversePixVerse, Transition Video540p normal 5s$0.45PixversePixVerse, Transition Video540p normal 8s$0.9PixversePixVerse, Transition Video720p fast 5s$1.2PixversePixVerse, Transition Video720p normal 5s$0.6PixversePixVerse, Transition Video720p normal 8s$1.2PixversePixVerse, Transition Video1080p normal 5s$1.2PixversePixVerse,Image to Video360p fast 5s$0.9PixversePixVerse,Image to Video360p normal 5s$0.45PixversePixVerse,Image to Video360p normal 8s$0.9PixversePixVerse,Image to Video540p fast 5s$0.9PixversePixVerse,Image to Video540p normal 5s$0.45PixversePixVerse,Image to Video540p normal 8s$0.9PixversePixVerse,Image to Video720p fast 5s$1.2PixversePixVerse,Image to Video720p normal 5s$0.6PixversePixVerse,Image to Video720p normal 8s$1.2PixversePixVerse,Image to Video1080p normal 5s$1.2PikaPika, Scenes (Video Image Composition)720p, 5s$0.3PikaPika, Scenes (Video Image Composition)720p, 10s$0.4PikaPika, Scenes (Video Image Composition)1080p, 5s$0.5PikaPika, Scenes (Video Image Composition)1080p, 10s$1.5PikaPika, Start and End Frame to Video720p, 5s$0.2PikaPika, Start and End Frame to Video720p, 10s$0.25PikaPika, Start and End Frame to Video1080p, 5s$0.3PikaPika, Start and End Frame to Video1080p, 10s$1PikaPika, Text to Video720p, 5s$0.2PikaPika, Text to Video720p, 10s$0.6PikaPika, Text to Video1080p, 5s$0.45PikaPika, Text to Video1080p, 10s$1PikaPika,Image to Video720p, 5s$0.2PikaPika,Image to Video720p, 10s$0.6PikaPika,Image to Video1080p, 5s$0.45PikaPika,Image to Video1080p, 10s$1PikaPika Swaps, (Video Object Replacement)NA$0.3PikaPikadditios, (Video Object Insertion)NA$0.3PikaPikaffects, (Video Effects)NA$0.45MoonvalleyImage to video - 5sNA$1.5MoonvalleyText to video - 5sNA$1.5MoonvalleyVideo to video - 5sNA$2.25RodinRodin 3D, Generate - Regular GenerateNA$0.4RodinRodin 3D Generate - Detail, GenerateNA$0.4RodinRodin 3D Generate - Sketch, GenerateNA$0.4RodinRodin 3D Generate - Smooth, GenerateNA$0.4TripoTripo:, Text to Modelany style, false, any quality, false$0.15TripoTripo:, Text to Modelany style, false, any quality, true$0.2TripoTripo:, Text to Modelany style, true, detailed, false$0.35TripoTripo:, Text to Modelany style, true, detailed, true$0.4TripoTripo:, Text to Modelany style, true, standard, false$0.25TripoTripo:, Text to Modelany style, true, standard, true$0.3TripoTripo:, Text to Modelnone, false, any, quality, false$0.1TripoTripo:, Text to Modelnone, false, any quality, true$0.15TripoTripo:, Text to Modelnone, true, detailed, false$0.3TripoTripo:, Text to Modelnone, true, detailed, true$0.35TripoTripo:, Text to Modelnone, true, standard, false$0.2TripoTripo:, Text to Modelnone, true, standard, true$0.25TripoTripo:,Image to Model / Multiview to Modelany style, false, any quality, false$0.25TripoTripo:,Image to Model / Multiview to Modelany style, false, any quality, true$0.3TripoTripo:,Image to Model / Multiview to Modelany style, true, detailed, false$0.45TripoTripo:,Image to Model / Multiview to Modelany style, true, detailed, true$0.5TripoTripo:,Image to Model / Multiview to Modelany style, true, standard, false$0.35TripoTripo:,Image to Model / Multiview to Modelany style, true, standard, true$0.4TripoTripo:,Image to Model / Multiview to Modelnone, false, any, quality, false$0.2TripoTripo:,Image to Model / Multiview to Modelnone, false, any quality, true$0.25TripoTripo:,Image to Model / Multiview to Modelnone, true, detailed, false$0.4TripoTripo:,Image to Model / Multiview to Modelnone, true, detailed, true$0.45TripoTripo:,Image to Model / Multiview to Modelnone, true, standard, false$0.3TripoTripo:,Image to Model / Multiview to Modelnone, true, standard, true$0.35TripoTripo: Convert modelNA$0.1TripoTripo: Refine Draft modelNA$0.3TripoTripo: Retarget rigged modelNA$0.1TripoTripo: Rig modelNA$0.25TripoTripo: Texture modeldetailed$0.2TripoTripo: Texture modelstandard$0.1Stability AIStability, AI Stable Image UltraNA$0.08Stability AIStability AI Stable Diffusion, 3.5 Imagesd3.5-large$0.065Stability AIStability AI Stable Diffusion, 3.5 Imagesd3.5-medium$0.035Stability AIStability AI Upscale, ConservativeNA$0.25Stability AIStability AI Upscale CreativeNA$0.25Stability AIStability AI Upscale FastNA$0.01"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/black-forest-labs/flux-1-kontext",
  "markdown": "# ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\nFLUX.1 Kontext æ˜¯ç”± Black Forest Labs å¼€å‘çš„ä¸€æ¬¾ä¸“ä¸šçš„å›¾åƒåˆ°å›¾åƒç¼–è¾‘æ¨¡åž‹ï¼Œä¸“æ³¨äºŽæ™ºèƒ½ç†è§£å›¾åƒä¸Šä¸‹æ–‡å¹¶æ‰§è¡Œç²¾ç¡®ç¼–è¾‘ã€‚ èƒ½å¤Ÿåœ¨æ— éœ€å¤æ‚æè¿°çš„æƒ…å†µä¸‹å®žçŽ°å¤šç§ç¼–è¾‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯¹è±¡ä¿®æ”¹ã€é£Žæ ¼è½¬æ¢ã€èƒŒæ™¯æ›¿æ¢ã€è§’è‰²ä¸€è‡´æ€§ç¼–è¾‘å’Œæ–‡æœ¬ç¼–è¾‘ç­‰ã€‚ Kontext çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºŽå…¶å‡ºè‰²çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å’Œè§’è‰²ä¸€è‡´æ€§ä¿æŒï¼Œå³ä½¿ç»è¿‡å¤šæ¬¡è¿­ä»£ç¼–è¾‘ï¼Œä¹Ÿèƒ½ç¡®ä¿äººç‰©ç‰¹å¾ã€æž„å›¾å¸ƒå±€ç­‰å…³é”®å…ƒç´ ä¿æŒç¨³å®šã€‚ ç›®å‰ï¼ŒComfyUI ä¸­æ”¯æŒäº† Flux.1 Kontext çš„ä¸¤ä¸ªæ¨¡åž‹ï¼š\n\n*   **Kontext Pro** é€‚åˆç¼–è¾‘ã€åˆæˆå’Œæ··éŸ³ã€‚\n*   **Kontext Max** åœ¨æŽ’ç‰ˆã€æç¤ºè¯ç²¾ç¡®åº¦å’Œé€Ÿåº¦æ–¹é¢çªç ´æžé™ã€‚\n\næœ¬ç¯‡æŒ‡å—ï¼Œæˆ‘ä»¬å°†é€šè¿‡å¯¹åº”çš„å·¥ä½œæµæ¥ç®€å•ä»‹ç»å¦‚ä½•ä½¿ç”¨ Flux.1 Kontext çš„ç›¸å…³ API èŠ‚ç‚¹æ¥å®Œæˆå›¾åƒç¼–è¾‘ã€‚\n\næˆ‘ä»¬æœ€è¿‘æ›´æ–°æ”¯æŒäº†å¤šå›¾è¾“å…¥å·¥ä½œæµï¼Œä½¿ç”¨æ–°å¢žçš„ `Image Stitch` èŠ‚ç‚¹ï¼Œå°†å…è®¸ä½ å°†å¤šå¼ å›¾åƒæ‹¼æŽ¥æˆä¸€å¼ å›¾åƒï¼Œå¹¶ä½¿ç”¨ Flux.1 Kontext è¿›è¡Œç¼–è¾‘ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/multiple_image_input.png) ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ç”¨äºŽè¾“å…¥æˆ–è€…ä½¿ç”¨ä½ è‡ªå·±çš„å›¾ç‰‡ï¼š ![ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/girl.jpg) ![ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/dog.jpg) ![ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/sofa.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆå›¾å·¥ä½œæµçš„è¿è¡Œï¼š\n\n1.  åœ¨ `Load image` èŠ‚ç‚¹ä¸­è¯·åˆ†åˆ«ä¸Šä¼ æä¾›çš„å›¾ç‰‡\n2.  åœ¨ `Flux.1 Kontext Pro Image` ä¿®æ”¹å¿…è¦çš„å‚æ•°ï¼š\n    *   `prompt` è¾“å…¥ä½ æƒ³è¦ç¼–è¾‘çš„å›¾åƒçš„æç¤ºè¯\n    *   `aspect_ratio` è®¾ç½®åŽŸå›¾çš„é«˜å®½æ¯”ï¼Œæ¯”ä¾‹å¿…é¡»åœ¨ 1:4 åˆ° 4:1 ä¹‹é—´\n    *   `prompt_upsampling` è®¾ç½®æ˜¯å¦ä½¿ç”¨æç¤ºè¯ä¸Šé‡‡æ ·ï¼Œå¦‚æžœå¼€å¯ï¼Œä¼šè‡ªåŠ¨ä¿®æ”¹æç¤ºè¯ä»¥èŽ·å¾—æ›´ä¸°å¯Œçš„ç»“æžœï¼Œä½†ç»“æžœæ˜¯ä¸å¯é‡å¤çš„\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç¼–è¾‘ã€‚\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç¼–è¾‘åŽçš„å›¾åƒï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n## Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image.png) ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ç”¨äºŽè¾“å…¥æˆ–è€…ä½¿ç”¨ä½ è‡ªå·±çš„å›¾ç‰‡ï¼š ![ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image_input.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆå›¾å·¥ä½œæµçš„è¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½éœ€è¦ç¼–è¾‘çš„å›¾åƒ\n2.  åœ¨ `Flux.1 Kontext Pro Image` ä¿®æ”¹å¿…è¦çš„å‚æ•°\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç¼–è¾‘ã€‚\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç¼–è¾‘åŽçš„å›¾åƒï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n## Flux.1 Kontext Max Image API èŠ‚ç‚¹ å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![ComfyUI Flux.1 Kontext Max Image API èŠ‚ç‚¹ å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image.png) ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ç”¨äºŽè¾“å…¥æˆ–è€…ä½¿ç”¨ä½ è‡ªå·±çš„å›¾ç‰‡è¿›è¡Œæ¼”ç¤ºï¼š ![ComfyUI Flux.1 Kontext Max Image API èŠ‚ç‚¹ å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image_input.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Flux.1 Kontext Max Image API èŠ‚ç‚¹ å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆå›¾å·¥ä½œæµçš„è¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½éœ€è¦ç¼–è¾‘çš„å›¾åƒ\n2.  åœ¨ `Flux.1 Kontext Max Image` ä¿®æ”¹å¿…è¦çš„å‚æ•°\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç¼–è¾‘ã€‚\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç¼–è¾‘åŽçš„å›¾åƒï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n## Flux Kontext æç¤ºè¯æŠ€å·§\n\n### 1\\. åŸºç¡€ä¿®æ”¹\n\n*   ç®€å•ç›´æŽ¥ï¼š`\"Change the car color to red\"`\n*   ä¿æŒé£Žæ ¼ï¼š`\"Change to daytime while maintaining the same style of the painting\"`\n\n### 2\\. é£Žæ ¼è½¬æ¢\n\n**åŽŸåˆ™ï¼š**\n\n*   æ˜Žç¡®å‘½åé£Žæ ¼ï¼š`\"Transform to Bauhaus art style\"`\n*   æè¿°ç‰¹å¾ï¼š`\"Transform to oil painting with visible brushstrokes, thick paint texture\"`\n*   ä¿ç•™æž„å›¾ï¼š`\"Change to Bauhaus style while maintaining the original composition\"`\n\n### 3\\. è§’è‰²ä¸€è‡´æ€§\n\n**æ¡†æž¶ï¼š**\n\n*   å…·ä½“æè¿°ï¼š`\"The woman with short black hair\"`è€Œéž`\"she\"`\n*   ä¿ç•™ç‰¹å¾ï¼š`\"while maintaining the same facial features, hairstyle, and expression\"`\n*   åˆ†æ­¥ä¿®æ”¹ï¼šå…ˆæ”¹èƒŒæ™¯ï¼Œå†æ”¹åŠ¨ä½œ\n\n### 4\\. æ–‡æœ¬ç¼–è¾‘\n\n*   ä½¿ç”¨å¼•å·ï¼š`\"Replace 'joy' with 'BFL'\"`\n*   ä¿æŒæ ¼å¼ï¼š`\"Replace text while maintaining the same font style\"`\n\n## å¸¸è§é—®é¢˜è§£å†³\n\n### è§’è‰²å˜åŒ–è¿‡å¤§\n\nâŒ é”™è¯¯ï¼š`\"Transform the person into a Viking\"` âœ… æ­£ç¡®ï¼š`\"Change the clothes to be a viking warrior while preserving facial features\"`\n\n### æž„å›¾ä½ç½®æ”¹å˜\n\nâŒ é”™è¯¯ï¼š`\"Put him on a beach\"` âœ… æ­£ç¡®ï¼š`\"Change the background to a beach while keeping the person in the exact same position, scale, and pose\"`\n\n### é£Žæ ¼åº”ç”¨ä¸å‡†ç¡®\n\nâŒ é”™è¯¯ï¼š`\"Make it a sketch\"` âœ… æ­£ç¡®ï¼š`\"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\"`\n\n## æ ¸å¿ƒåŽŸåˆ™\n\n1.  **å…·ä½“æ˜Žç¡®** - ä½¿ç”¨ç²¾ç¡®æè¿°ï¼Œé¿å…æ¨¡ç³Šè¯æ±‡\n2.  **åˆ†æ­¥ç¼–è¾‘** - å¤æ‚ä¿®æ”¹åˆ†ä¸ºå¤šä¸ªç®€å•æ­¥éª¤\n3.  **æ˜Žç¡®ä¿ç•™** - è¯´æ˜Žå“ªäº›è¦ä¿æŒä¸å˜\n4.  **åŠ¨è¯é€‰æ‹©** - ç”¨â€changeâ€ã€â€œreplaceâ€è€Œéžâ€transformâ€\n\n## æœ€ä½³å®žè·µæ¨¡æ¿\n\n**å¯¹è±¡ä¿®æ”¹ï¼š** `\"Change [object] to [new state], keep [content to preserve] unchanged\"` **é£Žæ ¼è½¬æ¢ï¼š** `\"Transform to [specific style], while maintaining [composition/character/other] unchanged\"` **èƒŒæ™¯æ›¿æ¢ï¼š** `\"Change the background to [new background], keep the subject in the exact same position and pose\"` **æ–‡æœ¬ç¼–è¾‘ï¼š** `\"Replace '[original text]' with '[new text]', maintain the same font style\"`\n\n> **è®°ä½ï¼š** è¶Šå…·ä½“è¶Šå¥½ï¼ŒKontext æ“…é•¿ç†è§£è¯¦ç»†æŒ‡ä»¤å¹¶ä¿æŒä¸€è‡´æ€§ã€‚"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2FClipSetLastLayer",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/update_comfyui",
  "markdown": "# å¦‚ä½•æ›´æ–° ComfyUI - ComfyUI\n\nå°½ç®¡æˆ‘ä»¬å¯èƒ½å·²ç»åœ¨ä¸åŒç‰ˆæœ¬çš„å„éƒ¨åˆ†ç« èŠ‚ï¼Œå¯¹äºŽ ComfyUI çš„æ›´æ–°è¿‡ç¨‹éƒ½æœ‰æ‰€è¯´æ˜Žï¼Œä½†æ˜¯ä¸ºäº†æ–¹ä¾¿ç”¨æˆ·èƒ½å¤Ÿæ›´æ¸…æ¥šçš„äº†è§£ ComfyUI çš„æ›´æ–°è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä¼šåœ¨è¿™éƒ¨åˆ†å¯¹äºŽ ComfyUI çš„æ›´æ–°è¿›è¡Œè¯¦ç»†çš„è¯´æ˜Žã€‚\n\nComfyUI ä¾¿æºç‰ˆæä¾›äº†ä¾¿æ·çš„æ‰¹å¤„ç†è„šæœ¬æ¥å®Œæˆæ›´æ–°æ“ä½œã€‚\n\n### æ›´æ–°è„šæœ¬ä½ç½®\n\nåœ¨ä¾¿æºç‰ˆå®‰è£…ç›®å½•ä¸‹çš„ `update` æ–‡ä»¶å¤¹ä¸­ï¼Œå¯ä»¥æ‰¾åˆ°ä»¥ä¸‹æ›´æ–°è„šæœ¬ï¼š\n\n```\nComfyUI_windows_portable\nâ””â”€ ðŸ“‚update\n   â”œâ”€â”€ update.py\n   â”œâ”€â”€ update_comfyui.bat                           // æ›´æ–°åˆ°æœ€æ–°å¼€å‘ç‰ˆæœ¬\n   â”œâ”€â”€ update_comfyui_stable.bat                    // æ›´æ–°åˆ°æœ€æ–°ç¨³å®šç‰ˆæœ¬\n   â””â”€â”€ update_comfyui_and_python_dependencies.bat   // æ›´æ–°ä¾èµ–ï¼ˆé—®é¢˜ä¿®å¤æ—¶ä½¿ç”¨ï¼‰\n```\n\n## ComfyUI çš„ä¸åŒç‰ˆæœ¬è¯´æ˜Ž\n\né¦–å…ˆå–å†³äºŽä½ å®‰è£…æ–¹å¼çš„ä¸åŒï¼Œç›®å‰ ComfyUI æœ‰ä»¥ä¸‹çš„å‡ ç§å®‰è£…ç‰ˆæœ¬ï¼Œä¸‹é¢çš„ç›¸å…³é“¾æŽ¥ä¸­å·²ç»åŒ…å«äº†é’ˆå¯¹ä¸åŒç‰ˆæœ¬çš„æ›´æ–°è¯´æ˜Žã€‚\n\n## åœ¨æ›´æ–° ComfyUI æ—¶éƒ½éœ€è¦æ›´æ–°ä»€ä¹ˆå†…å®¹ï¼Ÿ\n\nç›®å‰ ComfyUI çš„æ›´æ–°ä¸»è¦éœ€è¦ç¡®ä¿ä¸¤éƒ¨åˆ†å†…å®¹ï¼š\n\n1.  æ›´æ–° ComfyUI çš„æ ¸å¿ƒä»£ç \n2.  æ›´æ–° ComfyUI çš„æ ¸å¿ƒä¾èµ–ï¼ŒåŒ…æ‹¬å¿…è¦çš„ Python ä¾èµ–å’Œ ComfyUI çš„åŠŸèƒ½ä¾èµ–åŒ…ã€‚\n\n**æ ¸å¿ƒä»£ç **ï¼š æ–°çš„èŠ‚ç‚¹ï¼Œæ–°çš„æ¨¡åž‹æ”¯æŒï¼Œæ–°çš„åŠŸèƒ½ç­‰ã€‚ **æ ¸å¿ƒä¾èµ–**ï¼š ä¸»è¦åŒ…æ‹¬ ComfyUI çš„å‰ç«¯åŠŸèƒ½ï¼Œå·¥ä½œæµæ¨¡æ¿ï¼ŒèŠ‚ç‚¹å¸®åŠ©æ–‡æ¡£ç­‰ã€‚\n\n```\ncomfyui-frontend-package   # ComfyUI å‰ç«¯åŠŸèƒ½\ncomfyui-workflow-templates # ComfyUI å·¥ä½œæµæ¨¡æ¿  \ncomfyui-embedded-docs      # ComfyUI èŠ‚ç‚¹çš„å¸®åŠ©æ–‡æ¡£\n```\n\nç›®å‰è¿™ä¸‰ä¸ªæ ¸å¿ƒä¾èµ–é¡¹ç›®åˆ†åˆ«åœ¨ä¸åŒçš„ä»“åº“ä¸­ç»´æŠ¤ï¼š\n\n*   [ComfyUI\\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend/) - å‰ç«¯ç•Œé¢å’Œäº¤äº’åŠŸèƒ½\n*   [workflow\\_templates](https://github.com/Comfy-Org/workflow_templates) - é¢„ç½®å·¥ä½œæµæ¨¡æ¿\n*   [comfyui-embedded-docs](https://github.com/Comfy-Org/embedded-docs) - èŠ‚ç‚¹å¸®åŠ©æ–‡æ¡£\n\nå¦å¤–å¾ˆæœ‰å¿…è¦è¯´æ˜Žçš„ä¸€ç‚¹æ˜¯ï¼Œå¼€å‘ç‰ˆæœ¬(nightly) å’Œ ç¨³å®šç‰ˆæœ¬(release) çš„åŒºåˆ«ï¼š\n\n*   **å¼€å‘ç‰ˆæœ¬(nightly)**ï¼šæœ€æ–° commit çš„ä»£ç ï¼Œä½ å¯ä»¥ä½“éªŒåˆ°æˆ‘ä»¬æœ€æ–°æä¾›çš„ä¸€äº›åŠŸèƒ½ï¼Œä½†æ˜¯ä¹Ÿæœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ½œåœ¨çš„é—®é¢˜\n*   **ç¨³å®šç‰ˆæœ¬(release)**ï¼šæ˜¯åŸºäºŽç¨³å®šç‰ˆæœ¬æž„å»ºï¼Œé€šå¸¸ä¼šæ»žåŽäºŽå¼€å‘ç‰ˆæœ¬ï¼Œä½†æ˜¯ç¨³å®šæ€§æ›´é«˜ï¼Œæˆ‘ä»¬ä¼šåœ¨ç›¸å…³åŠŸèƒ½å‘å¸ƒç¨³å®šåŽå¯¹ç¨³å®šç‰ˆæœ¬è¿›è¡Œæ”¯æŒ\n\nç›®å‰è¾ƒå¤šç”¨æˆ·æ€»æ˜¯åœ¨æ›´æ–°è¿‡ç¨‹ä¸­å¤„äºŽ release ç‰ˆæœ¬æˆ–è€…æ¡Œé¢ç‰ˆï¼Œä½†æ˜¯å‘çŽ°éœ€è¦çš„åŠŸèƒ½æ˜¯å¼€å‘ç‰ˆæœ¬ä¸­æä¾›çš„å¯¹åº”ç‰ˆæœ¬å¹¶ä¸å­˜åœ¨ï¼Œå¯¹äºŽæ­¤æƒ…å†µè¯·æ£€æŸ¥æœ¬åœ° `ComfyUI/requirements.txt` å’Œ[nightly ç‰ˆæœ¬çš„ä¾èµ–](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt)æ˜¯å¦ä¸€è‡´ï¼Œæ¥ç¡®å®šå½“å‰æ˜¯å¦æ‰€æœ‰ä¾èµ–éƒ½æ˜¯æˆ‘ä»¬æœ€æ–°ç‰ˆæœ¬çš„åŠŸèƒ½æ”¯æŒã€‚\n\n## å¸¸è§æ›´æ–°é—®é¢˜\n\n### æ›´æ–°åŽå‰ç«¯ã€å·¥ä½œæµæ¨¡æ¿ã€èŠ‚ç‚¹å¸®åŠ©æ–‡æ¡£ç­‰ç¼ºå¤±æˆ–æ»žåŽ\n\nç»å¸¸æœ‰ç”¨æˆ·åªæ˜¯ä½¿ç”¨ `git pull` å‘½ä»¤æ¥æ›´æ–° ComfyUI çš„ä»£ç ï¼Œä½†**å¿½ç•¥äº†æ ¸å¿ƒä¾èµ–æ›´æ–°**ï¼Œå¯¼è‡´å‡ºçŽ°ä»¥ä¸‹é—®é¢˜ï¼š\n\n*   å‰ç«¯åŠŸèƒ½ç¼ºå¤±æˆ–æ˜¾ç¤ºå¼‚å¸¸\n*   æ‰¾ä¸åˆ°æ–°å¢žçš„å·¥ä½œæµæ¨¡æ¿\n*   èŠ‚ç‚¹å¸®åŠ©æ–‡æ¡£è¿‡æ—¶æˆ–ç¼ºå¤±\n*   æ–°åŠŸèƒ½æ²¡æœ‰å¯¹åº”çš„å‰ç«¯æ”¯æŒ\n\nè¯·åœ¨ä½¿ç”¨äº† `git pull` å‘½ä»¤åŽï¼Œåœ¨å¯¹åº”çš„ ComfyUI çŽ¯å¢ƒä½¿ç”¨ `pip install -r requirements.txt` å‘½ä»¤æ¥æ›´æ–°ä¾èµ–ã€‚\n\n### å¦‚ä½•æ­£ç¡®æ›´æ–°æ ¸å¿ƒä¾èµ–\n\n**æŽ¨èæ–¹æ³•**ï¼šä½¿ç”¨ `ComfyUI_windows_portable\\update\\update_comfyui.bat` è¿™ä¸ªæ‰¹å¤„ç†è„šæœ¬ï¼Œè¿™ä¸ªè„šæœ¬ä¼šåŒæ—¶æ›´æ–° ComfyUI ä»£ç å’Œæ‰€æœ‰ Python ä¾èµ–åŒ…ã€‚**æ‰‹åŠ¨æ›´æ–°ä¾èµ–**ï¼š å¦‚æžœä½ éœ€è¦æ‰‹åŠ¨æ›´æ–°ä¾èµ–ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š\n\n```\n# åœ¨ä¾¿æºç‰ˆç›®å½•ä¸‹æ‰“å¼€å‘½ä»¤è¡Œ\n.\\python_embeded\\python.exe -m pip install -r ComfyUI\\requirements.txt\n```\n\n### ä¾èµ–æ›´æ–°æ•…éšœæŽ’é™¤\n\nå¦‚æžœä¾èµ–æ›´æ–°å¤±è´¥ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æŽ’æŸ¥ï¼š\n\n### ä¸ºä»€ä¹ˆæˆ‘æ›´æ–°åŽæ‰¾ä¸åˆ°æ–°åŠŸèƒ½ï¼Ÿ\n\nè¿™æ˜¯æœ€å¸¸è§çš„é—®é¢˜ä¹‹ä¸€ï¼š\n\n*   å¦‚æžœä½ ä½¿ç”¨çš„æ˜¯**æ¡Œé¢ç‰ˆ**ï¼Œå› ä¸ºæ¡Œé¢ç‰ˆæ˜¯åŸºäºŽç¨³å®šç‰ˆæœ¬æž„å»ºçš„ï¼Œå®ƒçš„åŠŸèƒ½æ›´æ–°ç›¸å¯¹æ»žåŽ\n*   è¯·ç¡®å®šä½ ä½¿ç”¨çš„æ˜¯**å¼€å‘ç‰ˆæœ¬(nightly)**ï¼Œè€Œä¸æ˜¯**ç¨³å®šç‰ˆæœ¬(release)**\n\nå¦å¤–è¿˜éœ€è¦ç¡®ä¿åœ¨æ›´æ–°è¿‡ç¨‹ä¸­å¯¹åº”çš„ä¾èµ–å·²ç»æˆåŠŸæ›´æ–°ï¼Œå¦‚æžœæ›´æ–°åŽä»ç„¶å­˜åœ¨é—®é¢˜ï¼Œè¯·å‚è€ƒ[ä¾èµ–æ›´æ–°æ•…éšœæŽ’é™¤](#%E4%BE%9D%E8%B5%96%E6%9B%B4%E6%96%B0%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4)ç« èŠ‚æ¥æŽ’æŸ¥é—®é¢˜ã€‚\n\n### å¦‚ä½•åˆ‡æ¢åˆ°å¼€å‘ï¼ˆnightlyï¼‰ç‰ˆæœ¬æˆ–è€…ç¨³å®šï¼ˆreleaseï¼‰ç‰ˆæœ¬ï¼Ÿ\n\nä¸åŒç‰ˆæœ¬çš„åŒºåˆ«\n\n*   **ç‰¹ç‚¹**ï¼šåŒ…å«æœ€æ–°çš„ commit ä»£ç \n*   **ä¼˜åŠ¿**ï¼šå¯ä»¥ç¬¬ä¸€æ—¶é—´ä½“éªŒåˆ°æœ€æ–°åŠŸèƒ½å’Œæ”¹è¿›\n*   **é£Žé™©**ï¼šå¯èƒ½å­˜åœ¨æœªå‘çŽ°çš„ bug æˆ–ä¸ç¨³å®šå› ç´ \n*   **é€‚åˆäººç¾¤**ï¼šå¼€å‘è€…ã€æµ‹è¯•ç”¨æˆ·ã€æƒ³è¦ä½“éªŒæœ€æ–°åŠŸèƒ½çš„ç”¨æˆ·\n\nä½¿ç”¨ `update_comfyui.bat` è€Œä¸æ˜¯ `update_comfyui_stable.bat`ï¼š\n\n```\n# å¼€å‘ç‰ˆæœ¬ï¼ˆæœ€æ–°åŠŸèƒ½ï¼‰\ndouble-click: update_comfyui.bat\n\n# ç¨³å®šç‰ˆæœ¬\ndouble-click: update_comfyui_stable.bat\n```\n\n### æ›´æ–°åŽå‡ºçŽ°é”™è¯¯æ€Žä¹ˆåŠžï¼Ÿ\n\n1.  **æ£€æŸ¥ä¾èµ–**ï¼šè¿è¡Œ `pip install -r requirements.txt` ç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½å·²æ›´æ–°\n2.  **æ£€æŸ¥è‡ªå®šä¹‰èŠ‚ç‚¹**ï¼šæŸäº›è‡ªå®šä¹‰èŠ‚ç‚¹å¯èƒ½ä¸Žæ–°ç‰ˆæœ¬ä¸å…¼å®¹\n3.  **å›žé€€ç‰ˆæœ¬**ï¼šå¦‚æžœé—®é¢˜ä¸¥é‡ï¼Œå¯ä»¥å›žé€€åˆ°ä¹‹å‰çš„ç¨³å®šç‰ˆæœ¬\n\nå¦‚æžœå‡ºçŽ°é—®é¢˜ï¼Œå¯ä»¥å‚è€ƒæˆ‘ä»¬çš„é—®é¢˜æŽ’æŸ¥é¡µé¢æ¥è§£å†³ã€‚\n\n[](https://docs.comfy.org/zh-CN/troubleshooting/overview)\n\n### å¦‚ä½•äº†è§£æœ€æ–°åŠŸèƒ½ï¼Ÿ\n\n*   **GitHub Releases**ï¼šæŸ¥çœ‹ [ComfyUI Releases](https://github.com/comfyanonymous/ComfyUI/releases) äº†è§£ç¨³å®šç‰ˆæœ¬æ›´æ–°\n*   **GitHub Commits**ï¼šæŸ¥çœ‹ [æœ€æ–°æäº¤](https://github.com/comfyanonymous/ComfyUI/commits/master) äº†è§£å¼€å‘è¿›åº¦\n*   **ç¤¾åŒºè®¨è®º**ï¼šå…³æ³¨æˆ‘ä»¬çš„[åšå®¢](https://blog.comfy.org/)å’Œ[æŽ¨ç‰¹](https://x.com/comfyui)æ¥äº†è§£æœ€æ–°åŠ¨æ€"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/luma/luma-image-to-image",
  "markdown": "# Luma Image to Image API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\n[Luma Image to Image](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-image-to-image) èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨Luma AIçš„æŠ€æœ¯æ ¹æ®æ–‡æœ¬æç¤ºè¯ä¿®æ”¹çŽ°æœ‰å›¾åƒï¼ŒåŒæ—¶ä¿ç•™åŽŸå§‹å›¾åƒçš„æŸäº›ç‰¹å¾å’Œç»“æž„ã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å¦‚ä½•ä½¿ç”¨å¯¹åº”èŠ‚ç‚¹æ¥è¿›è¡Œå›¾ç”Ÿå›¾çš„å·¥ä½œæµè®¾ç½®ã€‚\n\nä½ å¯æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”èŠ‚ç‚¹çš„è¯¦ç»†å‚æ•°è®¾ç½®ç­‰\n\n[\n\n## Luma Image to Image èŠ‚ç‚¹æ–‡æ¡£\n\nLuma Image to Image API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/luma/luma-image-to-image)\n\n## Luma Image to Image API èŠ‚ç‚¹å›¾ç”Ÿå›¾å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![Luma å›¾ç”Ÿå›¾å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/luma_i2i.png) è¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ç”¨åšè¾“å…¥å›¾ï¼š ![Luma å›¾ç”Ÿå›¾å·¥ä½œæµè¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/input.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Luma å›¾ç”Ÿå›¾å·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„å·¥ä½œæµè¿è¡Œ\n\n1.  åœ¨ `Load image` èŠ‚ç‚¹ä¸­ç‚¹å‡» **upload** æŒ‰é’®ä¸Šä¼ è¾“å…¥å›¾ç‰‡\n2.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹å·¥ä½œæµçš„æç¤ºè¯\n3.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹`image_weight` æ¥ä¿®æ”¹è¾“å…¥å›¾ç‰‡çš„æƒé‡ï¼ˆè¶Šå°è¶ŠæŽ¥è¿‘åŽŸå›¾ï¼‰\n4.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n5.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨`Save Image`èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒ, å¯¹åº”çš„å›¾ç‰‡ä¹Ÿä¼šè¢«ä¿å­˜è‡³`ComfyUI/output/` ç›®å½•ä¸‹\n\n### 3\\. ä¸åŒ `image_weight` å‚æ•°è¾“å…¥ç»“æžœ\n\n![æƒé‡å¯¹æ¯”](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/luma/i2i_image_weight.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-camera",
  "markdown": "# ComfyUI Wan2.1 Fun Camera å®˜æ–¹åŽŸç”Ÿç¤ºä¾‹\n\n**Wan2.1 Fun Camera** æ˜¯é˜¿é‡Œå›¢é˜ŸæŽ¨å‡ºçš„è§†é¢‘ç”Ÿæˆé¡¹ç›®ï¼Œä¸“æ³¨äºŽé€šè¿‡æ‘„åƒæœºè¿åŠ¨æ¥æŽ§åˆ¶è§†é¢‘ç”Ÿæˆæ•ˆæžœã€‚ **æ¨¡åž‹æƒé‡ä¸‹è½½åœ°å€**ï¼š\n\n*   [14B ç‰ˆæœ¬](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-14B-Control-Camera)\n*   [1.3B ç‰ˆæœ¬](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-1.3B-Control-Camera)\n\n**ä»£ç ä»“åº“**ï¼š[VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun) **ç›®å‰ ComfyUI å·²åŽŸç”Ÿæ”¯æŒäº† Wan2.1 Fun Camera æ¨¡åž‹**ã€‚\n\n## ç›¸å…³æ¨¡åž‹å®‰è£…\n\nè¿™äº›æ¨¡åž‹ä½ ä»…éœ€è¦å®‰è£…ä¸€æ¬¡ï¼Œå¦å¤–åœ¨å¯¹åº”çš„å·¥ä½œæµå›¾ç‰‡ä¸­ä¹ŸåŒ…å«äº†æ¨¡åž‹ä¸‹è½½ä¿¡æ¯ï¼Œä½ å¯ä»¥é€‰æ‹©ä½ å–œæ¬¢çš„æ–¹å¼ä¸‹è½½æ¨¡åž‹ã€‚ ä¸‹é¢çš„æ‰€æœ‰æ¨¡åž‹ä½ å¯ä»¥åœ¨ [Wan\\_2.1\\_ComfyUI\\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) æ‰¾åˆ° **Diffusion Models** é€‰æ‹© 1.3B æˆ– 14Bï¼š\n\n*   [wan2.1\\_fun\\_camera\\_v1.1\\_1.3B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors)\n*   [wan2.1\\_fun\\_camera\\_v1.1\\_14B\\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_14B_bf16.safetensors)\n\nä¸‹é¢çš„æ¨¡åž‹ï¼Œå¦‚æžœä½ ä½¿ç”¨è¿‡ Wan2.1 çš„ç›¸å…³æ¨¡åž‹ï¼Œé‚£ä¹ˆä½ åº”è¯¥å·²ç»æœ‰äº†ä¸‹é¢çš„æ¨¡åž‹ï¼Œå¦‚æžœæ²¡æœ‰ï¼Œè¯·ä¸‹è½½ä¸‹é¢çš„æ¨¡åž‹ï¼š **Text Encoders** é€‰æ‹©å…¶ä¸­ä¸€ä¸ªï¼š\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)\n\næ–‡ä»¶ä¿å­˜ä½ç½®ï¼š\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚ â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚ â”‚   â”œâ”€â”€ wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors # 1.3B ç‰ˆæœ¬\nâ”‚ â”‚   â””â”€â”€ wan2.1_fun_camera_v1.1_14B_bf16.safetensors # 14B ç‰ˆæœ¬\nâ”‚ â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚ â”‚   â””â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚ â”œâ”€â”€ ðŸ“‚ vae/\nâ”‚ â”‚   â””â”€â”€ wan_2.1_vae.safetensors\nâ”‚ â””â”€â”€ ðŸ“‚ clip_vision/\nâ”‚     â””â”€â”€ clip_vision_h.safetensors\n```\n\n## ComfyUI Wan2.1 Fun Camera 1.3B åŽŸç”Ÿå·¥ä½œæµç¤ºä¾‹\n\n### 1\\. å·¥ä½œæµç›¸å…³æ–‡ä»¶ä¸‹è½½\n\n#### 1.1 å·¥ä½œæµæ–‡ä»¶\n\nä¸‹è½½ä¸‹é¢çš„è§†é¢‘ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµï¼š\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B.json)\n\n#### 1.2 è¾“å…¥å›¾ç‰‡ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä½œä¸ºèµ·å§‹å¸§ï¼š ![è¾“å…¥å‚è€ƒå›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B_input.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµ\n\n![Wan2.1 Fun Camera å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg)\n\n1.  ç¡®ä¿åŠ è½½äº†æ­£ç¡®ç‰ˆæœ¬çš„æ¨¡åž‹æ–‡ä»¶ï¼š\n    *   1.3B ç‰ˆæœ¬ï¼š`wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors`\n    *   14B ç‰ˆæœ¬ï¼š`wan2.1_fun_camera_v1.1_14B_bf16.safetensors`\n2.  ç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  ç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors`\n4.  ç¡®ä¿ `Load CLIP Vision` èŠ‚ç‚¹åŠ è½½äº† `clip_vision_h.safetensors`\n5.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸Šä¼ èµ·å§‹å¸§\n6.  ä¿®æ”¹ Promptï¼Œå¦‚æžœä½ ä½¿ç”¨äº†ä½ è‡ªå·±çš„å›¾åƒè¾“å…¥\n7.  åœ¨ `WanCameraEmbedding` èŠ‚ç‚¹è®¾ç½®ç›¸æœºåŠ¨ä½œ\n8.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ‰§è¡Œç”Ÿæˆ\n\n## ComfyUI Wan2.1 Fun Camera 14B å·¥ä½œæµåŠè¾“å…¥å›¾ç‰‡\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B.json)\n\n**è¾“å…¥å›¾ç‰‡** ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B_input.jpg) \n\n## æ€§èƒ½å‚è€ƒ\n\n**1.3B ç‰ˆæœ¬**ï¼š\n\n*   512Ã—512 RTX 4090 ç”Ÿæˆ 81 å¸§çº¦éœ€ 72 ç§’\n\n**14B ç‰ˆæœ¬**ï¼š\n\n*   RTX4090 24GB æ˜¾å­˜åœ¨ç”Ÿæˆ 512Ã—512 åˆ†è¾¨çŽ‡æ—¶å¯èƒ½ä¼šå‡ºçŽ°æ˜¾å­˜ä¸è¶³, åœ¨ A100 ä¸Šè¿è¡Œå°ºå¯¸è¿‡å¤§æ—¶ä¹Ÿå‡ºçŽ°è¿‡æ˜¾å­˜ä¸è¶³çš„æƒ…å†µ\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/hunyuan-video",
  "markdown": "# ComfyUI æ··å…ƒè§†é¢‘ç¤ºä¾‹ - ComfyUI\n\næ··å…ƒè§†é¢‘ï¼ˆHunyuan Videoï¼‰ç³»åˆ—æ˜¯æ˜¯[è…¾è®¯](https://huggingface.co/tencent)ç ”å‘å¹¶å¼€æºçš„ï¼Œè¯¥æ¨¡åž‹ä»¥æ··åˆæž¶æž„ä¸ºæ ¸å¿ƒï¼Œæ”¯æŒ[æ–‡æœ¬ç”Ÿæˆè§†é¢‘](https://github.com/Tencent/HunyuanVideo) å’Œ[å›¾ç”Ÿæˆè§†é¢‘](https://github.com/Tencent/HunyuanVideo-I2V)ï¼Œå‚æ•°è§„æ¨¡è¾¾ 13Bã€‚ æŠ€æœ¯ç‰¹ç‚¹ï¼š\n\n*   **æ ¸å¿ƒæž¶æž„ï¼š** é‡‡ç”¨ç±»ä¼¼Soraçš„DiTï¼ˆDiffusion Transformerï¼‰æž¶æž„ï¼Œæœ‰æ•ˆèžåˆäº†æ–‡æœ¬ã€å›¾åƒå’ŒåŠ¨ä½œä¿¡æ¯ï¼Œæé«˜äº†ç”Ÿæˆè§†é¢‘å¸§ä¹‹é—´çš„ä¸€è‡´æ€§ã€è´¨é‡å’Œå¯¹é½åº¦ï¼Œé€šè¿‡ç»Ÿä¸€çš„å…¨æ³¨æ„åŠ›æœºåˆ¶å®žçŽ°å¤šè§†è§’é•œå¤´åˆ‡æ¢ï¼Œç¡®ä¿ä¸»ä½“ä¸€è‡´æ€§ã€‚\n*   **3D VAEï¼š** å®šä¹‰çš„ 3D VAE å°†è§†é¢‘åŽ‹ç¼©åˆ°ç´§å‡‘çš„æ½œç©ºé—´ï¼ŒåŒæ—¶åŽ‹ç¼©è§†é¢‘ï¼Œä½¿å¾—å›¾ç”Ÿè§†é¢‘çš„ç”Ÿæˆæ›´åŠ é«˜æ•ˆã€‚\n*   **å“è¶Šçš„å›¾åƒ-è§†é¢‘-æ–‡æœ¬å¯¹é½ï¼š** ä½¿ç”¨ MLLM æ–‡æœ¬ç¼–ç å™¨ï¼Œåœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä¸­è¡¨çŽ°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°éµå¾ªæ–‡æœ¬æŒ‡ä»¤ï¼Œæ•æ‰ç»†èŠ‚ï¼Œå¹¶è¿›è¡Œå¤æ‚æŽ¨ç†ã€‚\n\nä½ å¯ä»¥åœ¨[æ··å…ƒè§†é¢‘](https://github.com/Tencent/HunyuanVideo) å’Œ[æ··å…ƒè§†é¢‘-I2V](https://github.com/Tencent/HunyuanVideo-I2V) äº†è§£åˆ°æ›´å¤šå¼€æºä¿¡æ¯ã€‚ æœ¬ç¯‡æŒ‡å—å°†å¼•å¯¼ä½ å®Œæˆåœ¨ ComfyUI ä¸­ **æ–‡ç”Ÿè§†é¢‘** å’Œ **å›¾ç”Ÿè§†é¢‘** çš„è§†é¢‘ç”Ÿæˆã€‚\n\n## å·¥ä½œæµå…±ç”¨æ¨¡åž‹\n\nåœ¨æ–‡ç”Ÿè§†é¢‘å’Œå›¾ç”Ÿè§†é¢‘çš„å·¥ä½œæµä¸­ä¸‹é¢çš„è¿™äº›æ¨¡åž‹æ˜¯å…±æœ‰çš„ï¼Œè¯·å®Œæˆä¸‹è½½å¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•ä¸­\n\n*   [clip\\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)\n*   [llava\\_llama3\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)\n*   [hunyuan\\_video\\_vae\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)\n\nä¿å­˜ä½ç½®ï¼š\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ llava_llama3_fp8_scaled.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ hunyuan_video_vae_bf16.safetensors\n```\n\n## æ··å…ƒæ–‡ç”Ÿè§†é¢‘å·¥ä½œæµ\n\næ··å…ƒæ–‡ç”Ÿè§†é¢‘å¼€æºäºŽ 2024 å¹´ 12 æœˆï¼Œæ”¯æŒé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆ 5 ç§’çš„çŸ­è§†é¢‘ï¼Œæ”¯æŒä¸­è‹±æ–‡è¾“å…¥ã€‚\n\n### 1\\. æ–‡ç”Ÿè§†é¢‘ç›¸å…³å·¥ä½œæµ\n\nè¯·ä¿å­˜ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - æ··å…ƒæ–‡ç”Ÿè§†é¢‘](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp) \n\n### 2\\. æ··å…ƒæ–‡ç”Ÿå›¾æ¨¡åž‹\n\nè¯·ä¸‹è½½ [hunyuan\\_video\\_t2v\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) å¹¶ä¿å­˜è‡³ `ComfyUI/models/diffusion_models` æ–‡ä»¶å¤¹ä¸­ ç¡®ä¿åŒ…æ‹¬å…±ç”¨æ¨¡åž‹æ–‡ä»¶å¤¹æœ‰ä»¥ä¸‹å®Œæ•´çš„æ¨¡åž‹æ–‡ä»¶ï¼š\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors                       // å…±ç”¨æ¨¡åž‹\nâ”‚   â”‚   â””â”€â”€ llava_llama3_fp8_scaled.safetensors      // å…±ç”¨æ¨¡åž‹\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ hunyuan_video_vae_bf16.safetensors       // å…±ç”¨æ¨¡åž‹\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ hunyuan_video_t2v_720p_bf16.safetensors  // T2V æ¨¡åž‹\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI æ··å…ƒè§†é¢‘ T2V å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg)\n\n1.  ç¡®ä¿åœ¨`DualCLIPLoader`ä¸­ä¸‹é¢çš„æ¨¡åž‹å·²åŠ è½½ï¼š\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  ç¡®ä¿åœ¨`Load Diffusion Model`åŠ è½½äº†`hunyuan_video_t2v_720p_bf16.safetensors`\n3.  ç¡®ä¿åœ¨`Load VAE`ä¸­åŠ è½½äº†`hunyuan_video_vae_bf16.safetensors`\n4.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n## æ··å…ƒå›¾ç”Ÿè§†é¢‘å·¥ä½œæµ\n\næ··å…ƒå›¾ç”Ÿè§†é¢‘æ¨¡åž‹å¼€æºäºŽ2025å¹´3æœˆ6æ—¥ï¼ŒåŸºäºŽ HunyuanVideo æ¡†æž¶ï¼Œæ”¯æŒå°†é™æ€å›¾åƒè½¬åŒ–ä¸ºæµç•…çš„é«˜è´¨é‡è§†é¢‘ï¼ŒåŒæ—¶å¼€æ”¾äº† LoRA è®­ç»ƒä»£ç ï¼Œæ”¯æŒå®šåˆ¶ç‰¹æ®Šè§†é¢‘æ•ˆæžœå¦‚ï¼šå¤´å‘ç”Ÿé•¿ã€ç‰©ä½“å˜å½¢ç­‰ç­‰ã€‚ ç›®å‰æ··å…ƒå›¾ç”Ÿè§†é¢‘æ¨¡åž‹åˆ†ä¸ºä¸¤ä¸ªç‰ˆæœ¬ï¼š\n\n*   v1 â€œconcatâ€ : è§†é¢‘çš„è¿åŠ¨æµç•…æ€§è¾ƒå¥½ï¼Œä½†æ¯”è¾ƒå°‘éµå¾ªå›¾åƒå¼•å¯¼\n*   v2 â€œreplaceâ€: åœ¨v1 æ›´æ–°åŽçš„æ¬¡æ—¥æ›´æ–°çš„ç‰ˆæœ¬ï¼Œå›¾åƒçš„å¼•å¯¼æ€§è¾ƒå¥½ï¼Œä½†ç›¸å¯¹äºŽ V1 ç‰ˆæœ¬ä¼¼ä¹Žä¸é‚£ä¹ˆæœ‰æ´»åŠ›\n\nv1 â€œconcatâ€\n\n![HunyuanVideo v1](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp)\n\nv2 â€œreplaceâ€\n\n![HunyuanVideo v2](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp)\n\n### v1 åŠ v2 ç‰ˆæœ¬å…±ç”¨çš„æ¨¡åž‹\n\nè¯·ä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶ä¿å­˜åˆ° `ComfyUI/models/clip_vision` ç›®å½•ä¸­\n\n*   [llava\\_llama3\\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)\n\n### v1 â€œconcatâ€ å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ\n\n#### 1\\. å·¥ä½œæµåŠç›¸å…³ç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - æ··å…ƒå›¾ç”Ÿè§†é¢‘v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp) è¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒä½œä¸ºå›¾ç”Ÿè§†é¢‘çš„èµ·å§‹å¸§ ![èµ·å§‹å¸§](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png) \n\n#### 2\\. v1 ç‰ˆæœ¬æ¨¡åž‹\n\n*   [hunyuan\\_video\\_image\\_to\\_video\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)\n\nç¡®ä¿åŒ…æ‹¬å…±ç”¨æ¨¡åž‹æ–‡ä»¶å¤¹æœ‰ä»¥ä¸‹å®Œæ•´çš„æ¨¡åž‹æ–‡ä»¶ï¼š\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ clip_vision/\nâ”‚   â”‚   â””â”€â”€ llava_llama3_vision.safetensors                     // I2V å…±ç”¨æ¨¡åž‹\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors                                  //  å…±ç”¨æ¨¡åž‹\nâ”‚   â”‚   â””â”€â”€ llava_llama3_fp8_scaled.safetensors                 //  å…±ç”¨æ¨¡åž‹\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ hunyuan_video_vae_bf16.safetensors                  // å…±ç”¨æ¨¡åž‹\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 \"concat\" ç‰ˆæœ¬æ¨¡åž‹\n```\n\n#### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµ\n\n![ComfyUI æ··å…ƒè§†é¢‘I2V v1 å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg)\n\n1.  ç¡®ä¿ `DualCLIPLoader` ä¸­ä¸‹é¢çš„æ¨¡åž‹å·²åŠ è½½ï¼š\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  ç¡®ä¿ `Load CLIP Vision` åŠ è½½äº† `llava_llama3_vision.safetensors`\n3.  è¯·åœ¨ `Load Image Model` åŠ è½½äº† `hunyuan_video_image_to_video_720p_bf16.safetensors`\n4.  ç¡®ä¿ `Load VAE` ä¸­åŠ è½½äº† `hunyuan_video_vae_bf16.safetensors`\n5.  ç¡®ä¿ `Load Diffusion Model` ä¸­åŠ è½½äº† `hunyuan_video_image_to_video_720p_bf16.safetensors`\n6.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n### v2 â€œreplaceâ€ å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ\n\nv2 ç‰ˆæœ¬çš„å·¥ä½œæµä¸Ž v1 ç‰ˆæœ¬çš„å·¥ä½œæµåŸºæœ¬ç›¸åŒï¼Œä½ åªéœ€è¦ä¸‹è½½ä¸€ä¸ª **replace** çš„æ¨¡åž‹ï¼Œç„¶åŽåœ¨ `Load Diffusion Model` ä¸­ä½¿ç”¨å³å¯ã€‚\n\n#### 1\\. å·¥ä½œæµåŠç›¸å…³ç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - æ··å…ƒå›¾ç”Ÿè§†é¢‘v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp) è¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒä½œä¸ºå›¾ç”Ÿè§†é¢‘çš„èµ·å§‹å¸§ ![èµ·å§‹å¸§](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png) \n\n#### 2\\. v2 ç‰ˆæœ¬æ¨¡åž‹\n\n*   [hunyuan\\_video\\_v2\\_replace\\_image\\_to\\_video\\_720p\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)\n\nç¡®ä¿åŒ…æ‹¬å…±ç”¨æ¨¡åž‹æ–‡ä»¶å¤¹æœ‰ä»¥ä¸‹å®Œæ•´çš„æ¨¡åž‹æ–‡ä»¶ï¼š\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ clip_vision/\nâ”‚   â”‚   â””â”€â”€ llava_llama3_vision.safetensors                                // I2V å…±ç”¨æ¨¡åž‹\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors                                             //  å…±ç”¨æ¨¡åž‹\nâ”‚   â”‚   â””â”€â”€ llava_llama3_fp8_scaled.safetensors                            //  å…±ç”¨æ¨¡åž‹\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ hunyuan_video_vae_bf16.safetensors                             //  å…±ç”¨æ¨¡åž‹\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 \"replace\" ç‰ˆæœ¬æ¨¡åž‹\n```\n\n#### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµ\n\n![ComfyUI æ··å…ƒè§†é¢‘I2V v2 å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg)\n\n1.  ç¡®ä¿ `DualCLIPLoader` ä¸­ä¸‹é¢çš„æ¨¡åž‹å·²åŠ è½½ï¼š\n    *   clip\\_name1: clip\\_l.safetensors\n    *   clip\\_name2: llava\\_llama3\\_fp8\\_scaled.safetensors\n2.  ç¡®ä¿ `Load CLIP Vision` åŠ è½½äº† `llava_llama3_vision.safetensors`\n3.  è¯·åœ¨ `Load Image Model` åŠ è½½äº† `hunyuan_video_image_to_video_720p_bf16.safetensors`\n4.  ç¡®ä¿ `Load VAE` ä¸­åŠ è½½äº† `hunyuan_video_vae_bf16.safetensors`\n5.  ç¡®ä¿ `Load Diffusion Model` ä¸­åŠ è½½äº† `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`\n6.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n## å¼€å§‹ä½ çš„å°è¯•\n\nä¸‹é¢æ˜¯æˆ‘ä»¬æä¾›äº†ä¸€äº›ç¤ºä¾‹å›¾ç‰‡å’Œå¯¹åº”çš„æç¤ºè¯ï¼Œä½ å¯ä»¥åŸºäºŽè¿™äº›å†…å®¹ï¼Œè¿›è¡Œä¿®æ”¹ï¼Œåˆ›ä½œå‡ºå±žäºŽä½ è‡ªå·±çš„è§†é¢‘ã€‚ ![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png)\n\n```\nFuturistic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/samurai.png)\n\n```\nSamurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/a_flying_car.png)\n\n```\nflying car fastly moving and flying through the city\n```\n\n* * *\n\n![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png)\n\n```\ncyberpunk car race in night city, dynamic, super fast, fast shot\n```\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/system_requirements",
  "markdown": "# ç³»ç»Ÿè¦æ±‚ - ComfyUI\n\nåœ¨æœ¬ç¯‡æˆ‘ä»¬å°†ä»‹ç»å®‰è£… ComfyUI çš„ç³»ç»Ÿè¦æ±‚, ç”±äºŽ ComfyUI çš„æ›´æ–°é¢‘ç¹ï¼Œæœ¬ç¯‡æ–‡æ¡£æœªå¿…èƒ½å¤ŸåŠæ—¶æ›´æ–°ï¼Œè¯·å‚è€ƒ[ComfyUI](https://github.com/comfyanonymous/ComfyUI)ä¸­çš„ç›¸å…³è¯´æ˜Žã€‚ æ— è®ºæ˜¯å“ªä¸ªç‰ˆæœ¬çš„ ComfyUIï¼Œéƒ½æ˜¯è¿è¡Œåœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Python çŽ¯å¢ƒä¸­ã€‚\n\n### æ“ä½œç³»ç»Ÿè¦æ±‚\n\nç›®å‰æˆ‘ä»¬æ”¯æŒä»¥ä¸‹æ“ä½œç³»ç»Ÿï¼š\n\n*   Windows\n*   Linux\n*   macOSï¼ˆæ”¯æŒ Apple Silicon M1/M2ï¼‰\n\nè¯·å‚è€ƒ[ComfyUI Windows å’Œ Linux æ‰‹åŠ¨å®‰è£…ç« èŠ‚](https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#manual-install-windows-linux)äº†è§£è¯¦ç»†çš„å®‰è£…æ­¥éª¤ã€‚ ä½ å¯ä»¥å‚è€ƒä¸‹é¢çš„ç« èŠ‚æ¥äº†è§£ä¸åŒç³»ç»Ÿå’Œç‰ˆæœ¬ ComfyUI çš„å®‰è£…æ–¹å¼ï¼Œåœ¨ä¸åŒç‰ˆæœ¬çš„å®‰è£…ä¸­æˆ‘ä»¬ç®€å•å¯¹å®‰è£…çš„ç³»ç»Ÿè¦æ±‚è¿›è¡Œäº†è¯´æ˜Žã€‚\n\n### Python ç‰ˆæœ¬\n\n*   æŽ¨è Python 3.12\n*   æ”¯æŒ Python 3.13ï¼ˆéƒ¨åˆ†è‡ªå®šä¹‰èŠ‚ç‚¹å¯èƒ½ä¸å…¼å®¹ï¼‰\n\n### æ”¯æŒçš„ç¡¬ä»¶\n\n*   NVIDIA æ˜¾å¡\n*   AMD æ˜¾å¡\n*   Intel æ˜¾å¡ï¼ˆåŒ…æ‹¬ Arc ç³»åˆ—ï¼Œæ”¯æŒ IPEXï¼‰\n*   Apple Siliconï¼ˆM1/M2ï¼‰\n*   Ascend NPU\n*   Cambricon MLU\n*   CPUï¼ˆå¯ç”¨ â€”cpu å‚æ•°ï¼Œé€Ÿåº¦è¾ƒæ…¢ï¼‰\n\nè¯·å‚è€ƒ[ComfyUI Windows å’Œ Linux æ‰‹åŠ¨å®‰è£…ç« èŠ‚](https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#manual-install-windows-linux)äº†è§£è¯¦ç»†çš„å®‰è£…æ­¥éª¤ã€‚\n\n### ä¾èµ–\n\n*   éœ€å®‰è£… PyTorchï¼ˆä¸åŒç¡¬ä»¶éœ€ä¸åŒç‰ˆæœ¬ï¼Œè¯¦è§ä¸‹æ–¹ï¼‰\n*   éœ€å®‰è£… ComfyUI çš„ requirements.txt ä¸­æ‰€æœ‰ä¾èµ–\n\n[\n\n## Manual Installation\n\nè¯·å‚è€ƒæ‰‹åŠ¨å®‰è£…ç« èŠ‚äº†è§£è¯¦ç»†çš„å®‰è£…æ­¥éª¤ã€‚\n\n\n\n](https://docs.comfy.org/zh-CN/installation/manual_install)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/ideogram/ideogram-v3",
  "markdown": "# ComfyUI Ideogram 3.0 API èŠ‚ç‚¹å®˜æ–¹ç¤ºä¾‹\n\nIdeogram 3.0 æ˜¯ç”± Ideogram å‘å¸ƒçš„ä¸€æ¬¾å¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼Œä»¥å…¶å“è¶Šçš„ç…§ç‰‡çº§é€¼çœŸåº¦ã€ç²¾ç¡®çš„æ–‡æœ¬æ¸²æŸ“å’Œä¸€è‡´çš„é£Žæ ¼æŽ§åˆ¶è€Œè‘—ç§°ã€‚ ç›®å‰[Ideogram V3](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v3) èŠ‚ç‚¹æ”¯æŒä»¥ä¸‹ä¸¤ç§æ¨¡å¼ï¼š\n\n*   æ–‡ç”Ÿå›¾æ¨¡å¼\n*   å›¾åƒç¼–è¾‘æ¨¡å¼ï¼ˆå½“åŒæ—¶æä¾›äº†å›¾åƒå’Œé®ç½©è¾“å…¥æ—¶ï¼‰\n\nä½ å¯æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”èŠ‚ç‚¹çš„è¯¦ç»†å‚æ•°è®¾ç½®ç­‰\n\n*   [Ideogram V3](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v3)\n\n## Ideogram 3.0 API èŠ‚ç‚¹æ–‡ç”Ÿå›¾æ¨¡å¼\n\nå½“ä½ åªä½¿ç”¨ [Ideogram V3](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/ideogram/ideogram-v3) è€Œä¸è¾“å…¥å›¾åƒå’Œè’™ç‰ˆæ—¶ï¼ŒèŠ‚ç‚¹å°†ä½¿ç”¨æ–‡ç”Ÿå›¾æ¨¡å¼ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![Ideogram 3.0 ComfyUI å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/ideogram/v3/ideogram_v3_t2i.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Ideogram 3.0 å·¥ä½œæµæ‰§è¡Œæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„å·¥ä½œæµè¿è¡Œ\n\n1.  åœ¨ `Ideogram V3` èŠ‚ç‚¹çš„ `prompt` ä¸­è¾“å…¥ä½ æƒ³è¦ç”Ÿæˆçš„å›¾åƒçš„æè¿°\n2.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n3.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨`Save Image`èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒ, å¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³`ComfyUI/output/` ç›®å½•ä¸‹\n\n## Ideogram 3.0 API èŠ‚ç‚¹å›¾åƒç¼–è¾‘æ¨¡å¼\n\n\\[å¾…æ›´æ–°\\]\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2Fapi-node%2Fimage%2Fstability-ai%2Fstability-ai-stable-image-ultra",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/manual_install",
  "markdown": "# å¦‚ä½•åœ¨ä¸åŒç³»ç»Ÿä¸Šæ‰‹åŠ¨å®‰è£… ComfyUI - ComfyUI\n\nå¯¹äºŽ ComfyUI çš„å®‰è£…ï¼Œ ä¸»è¦åˆ†ä¸ºå‡ ä¸ªæ­¥éª¤\n\n1.  åˆ›å»ºä¸€ä¸ªè™šæ‹ŸçŽ¯å¢ƒ(é¿å…æ±¡æŸ“ç³»ç»Ÿçº§ Python çŽ¯å¢ƒ)\n2.  å…‹éš† ComfyUI ä»£ç ä»“åº“\n3.  å®‰è£…ä¾èµ–\n4.  å¯åŠ¨ ComfyUI\n\nä½ ä¹Ÿå¯ä»¥å‚è€ƒ [ComfyUI CLI](https://docs.comfy.org/zh-CN/comfy-cli/getting-started) æ¥å®‰è£… ComfyUI, å®ƒæ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œå·¥å…·ï¼Œå¯ä»¥æ–¹ä¾¿åœ°å®‰è£… ComfyUI å¹¶ç®¡ç†å…¶ä¾èµ–ã€‚\n\n## ï¼ˆå¯é€‰ï¼‰åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ\n\n[Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). è¿™å°†å¸®åŠ©æ‚¨å®‰è£… ComfyUI æ‰€éœ€çš„æ­£ç¡®ç‰ˆæœ¬çš„ Python å’Œå…¶ä»–åº“ã€‚ ä½¿ç”¨ Conda åˆ›å»ºä¸€ä¸ªçŽ¯å¢ƒã€‚\n\n```\nconda create -n comfyenv\nconda activate comfyenv\n```\n\n## å…‹éš†ä»£ç ä»“åº“\n\nä½ éœ€è¦ä¿è¯ä½ çš„ç³»ç»Ÿä¸Šå·²ç»å®‰è£…äº† [Git](https://git-scm.com/downloads), é¦–å…ˆä½ éœ€è¦æ‰“å¼€ç»ˆç«¯ï¼ˆå‘½ä»¤è¡Œï¼‰,ç„¶åŽå…‹éš†ä»£ç ä»“åº“ã€‚\n\n```\ngit clone git@github.com:comfyanonymous/ComfyUI.git\n```\n\n## å®‰è£…GPU åŠ ComfyUI ä¾èµ–\n\n## æ·»åŠ å¤–éƒ¨æ¨¡åž‹è·¯å¾„\n\nå¦‚æžœä½ æƒ³è¦åœ¨ `ComfyUI/models` ä¹‹å¤–ç®¡ç†ä½ çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¯èƒ½å‡ºäºŽä»¥ä¸‹åŽŸå› :\n\n*   ä½ æœ‰å¤šä¸ª ComfyUI å®žä¾‹ï¼Œä½ æƒ³è¦è®©è¿™äº›å®žä¾‹å…±äº«æ¨¡åž‹æ–‡ä»¶ï¼Œä»Žè€Œå‡å°‘ç£ç›˜å ç”¨\n*   ä½ æœ‰å¤šä¸ªä¸åŒçš„ç±»åž‹çš„ GUI ç¨‹åºï¼Œå¦‚ï¼šWebUI, ä½ æƒ³è¦ä»–ä»¬å…±ç”¨æ¨¡åž‹æ–‡ä»¶\n*   æ¨¡åž‹æ–‡ä»¶æ— æ³•è¢«è¯†åˆ«æˆ–è¯»å–åˆ°\n\næˆ‘ä»¬æä¾›äº†é€šè¿‡ `extra_model_paths.yaml` é…ç½®æ–‡ä»¶æ¥æ·»åŠ é¢å¤–æ¨¡åž‹æœç´¢è·¯å¾„çš„æ–¹æ³•ã€‚\n\n### ä¸åŒ ComfyUI ç‰ˆæœ¬é…ç½®æ–‡ä»¶ä½ç½®\n\nå¯¹äºŽ[ä¾¿æºç‰ˆ](https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows)å’Œ[æ‰‹åŠ¨å®‰è£…](https://docs.comfy.org/zh-CN/installation/manual_install)çš„ ComfyUIç‰ˆæœ¬ï¼Œä½ å¯ä»¥åœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹æ‰¾åˆ° `extra_model_paths.yaml.example` çš„ç¤ºä¾‹æ–‡ä»¶\n\n```\nComfyUI/extra_model_paths.yaml.example\n```\n\nå¤åˆ¶å¹¶é‡å‘½åä¸º `extra_model_paths.yaml` æ¥ä½¿ç”¨, å¹¶ä¿æŒåœ¨ ComfyUI çš„æ ¹ç›®å½•ä¸‹, è·¯å¾„åº”è¯¥æ˜¯ `ComfyUI/extra_model_paths.yaml`ä½ ä¹Ÿå¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ‰¾åˆ°é…ç½®ç¤ºä¾‹æ–‡ä»¶\n\n### é…ç½®ç¤ºä¾‹\n\næ¯”å¦‚ï¼Œä½ éœ€è¦é¢å¤–è®© ComfyUI è¯†åˆ«çš„æ¨¡åž‹æ–‡ä»¶ä½äºŽä¸‹é¢çš„æ–‡ä»¶å¤¹:\n\n```\nðŸ“ YOUR_PATH/\n  â”œâ”€â”€ ðŸ“models/\n  |   â”œâ”€â”€ ðŸ“ loras/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ checkpoints/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â”œâ”€â”€ ðŸ“ vae/\n  |   â”‚   â””â”€â”€ xxxxx.safetensors\n  |   â””â”€â”€ ðŸ“ controlnet/\n  |       â””â”€â”€ xxxxx.safetensors\n```\n\né‚£ä¹ˆä½ å¯ä»¥è¿›è¡Œå¦‚ä¸‹çš„é…ç½®æ¥è®© ComfyUI è¯†åˆ«åˆ°ä½ è®¾å¤‡ä¸Šçš„æ¨¡åž‹è·¯å¾„\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH\n    loras: models/loras/\n    checkpoints: models/checkpoints/\n    vae: models/vae/\n    controlnet: models/controlnet/\n```\n\næˆ–è€…ä½¿ç”¨\n\n```\nmy_custom_config:\n    base_path: YOUR_PATH/models/\n    loras: loras\n    checkpoints: checkpoints\n    vae: vae\n    controlnet: controlnet\n```\n\næˆ–è€…ä½ ä¹Ÿå¯ä»¥å‚è€ƒé»˜è®¤çš„ [extra\\_model\\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) æ¥é…ç½®ï¼Œä¿å­˜ä¹‹åŽï¼Œ éœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆã€‚ ä¸‹é¢æ˜¯å®Œæ•´çš„åŽŸå§‹çš„é…ç½®é…ç½®ç¤ºä¾‹:\n\n```\n#Rename this to extra_model_paths.yaml and ComfyUI will load it\n\n\n#config for a1111 ui\n#all you have to do is change the base_path to where yours is installed\na111:\n    base_path: path/to/stable-diffusion-webui/\n\n    checkpoints: models/Stable-diffusion\n    configs: models/Stable-diffusion\n    vae: models/VAE\n    loras: |\n         models/Lora\n         models/LyCORIS\n    upscale_models: |\n                  models/ESRGAN\n                  models/RealESRGAN\n                  models/SwinIR\n    embeddings: embeddings\n    hypernetworks: models/hypernetworks\n    controlnet: models/ControlNet\n\n#config for comfyui\n#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.\n\n#comfyui:\n#     base_path: path/to/comfyui/\n#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads\n#     #is_default: true\n#     checkpoints: models/checkpoints/\n#     clip: models/clip/\n#     clip_vision: models/clip_vision/\n#     configs: models/configs/\n#     controlnet: models/controlnet/\n#     diffusion_models: |\n#                  models/diffusion_models\n#                  models/unet\n#     embeddings: models/embeddings/\n#     loras: models/loras/\n#     upscale_models: models/upscale_models/\n#     vae: models/vae/\n\n#other_ui:\n#    base_path: path/to/ui\n#    checkpoints: models/checkpoints\n#    gligen: models/gligen\n#    custom_nodes: path/custom_nodes\n\n```\n\n### æ·»åŠ é¢å¤–è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\né™¤äº†æ·»åŠ å¤–éƒ¨æ¨¡åž‹ä¹‹å¤–ï¼Œä½ åŒæ ·å¯ä»¥æ·»åŠ ä¸åœ¨ ComfyUI é»˜è®¤è·¯å¾„ä¸‹çš„è‡ªå®šä¹‰èŠ‚ç‚¹è·¯å¾„\n\nä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„é…ç½®ç¤ºä¾‹ï¼ˆMac ç³»ç»Ÿï¼‰ï¼Œè¯·æ ¹æ®ä½ çš„å®žé™…æƒ…å†µè¿›è¡Œä¿®æ”¹ï¼Œå¹¶æ–°å¢žåˆ°å¯¹åº”çš„é…ç½®æ–‡ä»¶ä¸­ï¼Œä¿å­˜åŽéœ€è¦ **é‡å¯ ComfyUI** æ‰èƒ½ç”Ÿæ•ˆ:\n\n```\nmy_custom_nodes:\n  custom_nodes: /Users/your_username/Documents/extra_custom_nodes\n```"
},
{
  "url": "https://docs.comfy.org/zh-CN/installation/install_custom_node",
  "markdown": "# å¦‚ä½•åœ¨ ComfyUI ä¸­å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹ - ComfyUI\n\n## ä»€ä¹ˆæ˜¯è‡ªå®šä¹‰èŠ‚ç‚¹ï¼Ÿ\n\nè‡ªå®šä¹‰èŠ‚ç‚¹æ˜¯ComfyUIçš„æ‰©å±•æ’ä»¶ï¼Œèƒ½å¤Ÿå¢žåŠ æ–°åŠŸèƒ½ï¼Œå¦‚é«˜çº§å›¾åƒå¤„ç†ã€æœºå™¨å­¦ä¹ å¾®è°ƒã€é¢œè‰²è°ƒæ•´ç­‰ã€‚è¿™äº›èŠ‚ç‚¹ç”±ç¤¾åŒºå¼€å‘ï¼Œå¯æ˜¾è‘—æ‰©å±•ComfyUIçš„åŸºç¡€åŠŸèƒ½ã€‚\n\næ‰€æœ‰çš„è‡ªå®šä¹‰èŠ‚ç‚¹å®‰è£…éƒ½éœ€è¦å®Œæˆä¸‹é¢çš„ä¸¤ä¸ªæ­¥éª¤ï¼š\n\n1.  å…‹éš†èŠ‚ç‚¹ä»£ç åˆ° `ComfyUI/custom_nodes` ç›®å½•\n2.  å®‰è£…å¯¹åº”çš„ Python ä¾èµ–\n\nåœ¨æœ¬æ–‡ä¸­æˆ‘ä»¬å°†ä»‹ç»ä¸‰ç§å®‰è£…æ–¹æ³•ï¼Œä¸‹é¢æ˜¯å¯¹åº”çš„ä¼˜ç¼ºç‚¹å¯¹æ¯”ï¼Œç”±äºŽç›®å‰ [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) è¿˜æœªæ­£å¼åŠ å…¥åˆ°æ ¸å¿ƒä¾èµ–ä¸­ï¼Œä½†åœ¨æœªæ¥ ComfyUI Manager å°†ä¼šæˆä¸ºæ ¸å¿ƒä¾èµ–çš„ä¸€éƒ¨åˆ†ï¼Œä½†æœ¬éƒ¨åˆ†çš„æŒ‡å—ä»æ—§æä¾›äº†å…¶å®ƒå®‰è£…æ’ä»¶çš„æŒ‡å—,ä»¥ä¾¿æ»¡è¶³ä½ çš„éœ€æ±‚ã€‚\n\n| æ–¹æ³•  | ä¼˜ç‚¹  | ç¼ºç‚¹  |\n| --- | --- | --- |\n| **ComfyUI Manager** (æŽ¨è) | 1\\. è‡ªåŠ¨åŒ–å®‰è£…  <br>2\\. ä¾èµ–å¤„ç†  <br>3\\. å›¾å½¢ç•Œé¢ | ä¸åœ¨ registry ä¸­æ³¨å†Œçš„èŠ‚ç‚¹æ— æ³•é€šè¿‡ Manager ç›´æŽ¥æœç´¢åˆ° |\n| **Git å…‹éš†** | å¯ä»¥å®‰è£…ä¸åœ¨ registry ä¸­æ³¨å†Œçš„èŠ‚ç‚¹ | 1\\. éœ€è¦GitçŸ¥è¯†  <br>2\\. æ‰‹åŠ¨å¤„ç†ä¾èµ–  <br>3\\. å­˜åœ¨å®‰è£…é£Žé™© |\n| **ä»£ç ä»“åº“ ZIP ä¸‹è½½** | 1\\. æ— éœ€Git  <br>2\\. æ‰‹åŠ¨å¤„ç†ä¸€åˆ‡ | 1\\. éœ€è¦æ‰‹åŠ¨å¤„ç†ä¾èµ–  <br>2\\. æ— ç‰ˆæœ¬æŽ§åˆ¶  <br>3\\. å­˜åœ¨å®‰è£…é£Žé™© |\n\næç¤ºï¼š åœ¨å®‰è£…è‡ªå®šä¹‰èŠ‚ç‚¹å‰ï¼Œè¯·å…ˆæŸ¥çœ‹æ’ä»¶çš„ README æ–‡ä»¶ï¼Œäº†è§£æ’ä»¶çš„å®‰è£…æ–¹æ³•å’Œä½¿ç”¨æ–¹æ³•ï¼Œæœ‰äº›æ’ä»¶æœ‰å¯¹ç‰¹å®šçš„åŒ…å’ŒçŽ¯å¢ƒæœ‰è¦æ±‚ï¼Œæ¯”å¦‚å¯¹åº”çš„æ¨¡åž‹ã€ä¾èµ–çš„ç‰ˆæœ¬ã€å¸¸è§é—®é¢˜è§£å†³ç­‰\n\n## æ–¹æ³•ä¸€:ComfyUI Managerï¼ˆæŽ¨èï¼‰\n\n## æ–¹æ³•äºŒ:ä½¿ç”¨ Git è¿›è¡Œå®‰è£…æ‰‹åŠ¨å®‰è£…\n\né€‚ç”¨äºŽManagerä¸­æ‰¾ä¸åˆ°çš„æ–°èŠ‚ç‚¹æˆ–éœ€è¦ç‰¹å®šç‰ˆæœ¬æ—¶ï¼Œ éœ€è¦ä½ çš„ç³»ç»Ÿä¸­å·²ç»å®‰è£…å¥½äº† [Git](https://git-scm.com/)\n\n## æ–¹æ³•ä¸‰:ZIPä¸‹è½½å®‰è£…\n\né€‚ç”¨äºŽæ— æ³•ä½¿ç”¨ Git æˆ– Manager å®‰è£…çš„ç”¨æˆ·"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/openai/chat",
  "markdown": "# OpenAI Chat API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\nOpenAI æ˜¯ä¸€å®¶ä¸“æ³¨äºŽç”Ÿæˆå¼ AI çš„ç§‘æŠ€å…¬å¸ï¼Œæä¾›å¼ºå¤§çš„å¯¹è¯åŠŸèƒ½ã€‚ç›®å‰ ComfyUI å·²é›†æˆ OpenAI APIï¼Œä½ å¯ä»¥ç›´æŽ¥åœ¨ ComfyUI ä¸­ä½¿ç”¨ç›¸å…³èŠ‚ç‚¹æ¥å®Œæˆå¯¹è¯åŠŸèƒ½ã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å®Œæˆå¯¹åº”å¯¹è¯åŠŸèƒ½ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„ Json æ–‡ä»¶å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/openai/api_openai_chat.json)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![OpenAI Chat Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg)\n\nä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½ä½ éœ€è¦ AI çš„è§£è¯»å›¾ç‰‡\n2.  (å¯é€‰) å¦‚æžœéœ€è¦ä½ å¯ä»¥ä¿®æ”¹`OpenAI Chat Advanced Options` ä¸­çš„è®¾å®šï¼Œä»Žè€Œè®© AI æ¥æ‰§è¡Œç‰¹å®šä»»åŠ¡\n3.  åœ¨ `OpenAI Chat` èŠ‚ç‚¹ä½ å¯ä»¥ä¿®æ”¹ `Prompt` æ¥è®¾ç½®å¯¹è¯çš„æç¤ºè¯,æˆ–è€…ä¿®æ”¹ `model` æ¥é€‰æ‹©ä¸åŒçš„æ¨¡åž‹\n4.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå¯¹è¯ã€‚\n5.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Preview Any` èŠ‚ç‚¹ä¸­æŸ¥çœ‹å¯¹åº” AI è¿”å›žçš„å†…å®¹ã€‚\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\n*   ç›®å‰æ–‡ä»¶è¾“å…¥èŠ‚ç‚¹ `OpenAI Chat Input Files` éœ€è¦å…ˆå°†æ–‡ä»¶ä¸Šä¼ è‡³`ComfyUI/input/` ç›®å½•ä¸‹ï¼Œ æ­¤èŠ‚ç‚¹æ­£åœ¨æ”¹è¿›ï¼Œæˆ‘ä»¬ä¼šåœ¨æ›´æ–°åŽä¿®æ”¹æ¨¡æ¿\n*   å·¥ä½œæµä¸­æä¾›äº†ä½¿ç”¨ `Batch Images` æ¥è¾“å…¥çš„ç¤ºä¾‹ï¼Œå¦‚æžœä½ æœ‰å¤šå¼ å›¾ç‰‡éœ€è¦ AI è§£è¯»ï¼Œå¯å‚è€ƒæ­¥éª¤å›¾åœ¨ä½¿ç”¨å³é”®æ¥å°†å¯¹åº”çš„èŠ‚ç‚¹æ¨¡å¼è®¾ç½®ä¸º `æ€»æ˜¯ï¼ˆalwaysï¼‰` æ¥å¯ç”¨"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/overview",
  "markdown": "# ComfyUI è®¾ç½®æ¦‚è§ˆ - ComfyUI\n\nè¿™ä¸ªéƒ¨åˆ†æ˜¯å…³äºŽ ComfyUI å‰ç«¯è®¾ç½®èœå•ä¸­è¯¦ç»†çš„è®¾ç½®è¯´æ˜Žï¼Œ å¯¹äºŽæ‰€æœ‰çš„ç”¨æˆ·è®¾ç½®å°†ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `ComfyUI/user/default/comfy.settings.json` æ–‡ä»¶å¤¹ ä½ å¯ä»¥ä½¿ç”¨ `ctrl + ,` å¿«æ·é”®æ¥æ‰“å¼€è®¾ç½®é¢æ¿ï¼Œ ç„¶åŽç‚¹å‡»å¯¹åº”çš„è®¾ç½®é€‰é¡¹è¿›è¡Œè®¾ç½®ã€‚ ç”±äºŽè‡ªå®šä¹‰èŠ‚ç‚¹ä¹Ÿå¯ä»¥åœ¨èœå•ä¸­æ³¨å†Œå¯¹åº”çš„è®¾ç½®ç±»ç›®ï¼Œåœ¨æˆ‘ä»¬çš„å®˜æ–¹æ–‡æ¡£è¯´æ˜Žä¸­ç›®å‰ä»…åŒ…å«åŽŸç”Ÿçš„è®¾ç½®å†…å®¹ï¼Œå¦å¤–æœ‰éƒ¨åˆ†é€‰é¡¹è®¾ç½® **ä»…é’ˆå¯¹ ComfyUI æ¡Œé¢ç‰ˆ** æœ‰æ•ˆï¼Œæˆ‘ä»¬ä¹Ÿåœ¨å¯¹åº”é¡µé¢ä¸­åšäº†æ³¨é‡Šè¯´æ˜Žã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/stability-ai/stable-image-ultra",
  "markdown": "# Stability AI Stable Image Ultra API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\n[Stability Stable Image Ultra](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra) èŠ‚ç‚¹å…è®¸ä½ ä½¿ç”¨ Stability AI çš„ Stable Image Ultra æ¨¡åž‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºè¯æˆ–å‚è€ƒå›¾åƒåˆ›å»ºé«˜è´¨é‡ã€ç»†èŠ‚ä¸°å¯Œçš„å›¾åƒå†…å®¹ã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å¦‚ä½•ä½¿ç”¨å¯¹åº”èŠ‚ç‚¹æ¥è¿›è¡Œæ–‡ç”Ÿå›¾å’Œå›¾ç”Ÿå›¾çš„å·¥ä½œæµè®¾ç½®ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![Stability AI Stable Image Ultra æ–‡ç”Ÿå›¾å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_image_ultra_t2i.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Stability AI  Stable Image Ultra æ–‡ç”Ÿå›¾æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  (å¯é€‰)ä¿®æ”¹ `Stability AI Stable Image Ultra` èŠ‚ç‚¹ä¸­çš„ `prompt` å‚æ•°ï¼Œè¾“å…¥ä½ æƒ³è¦ç”Ÿæˆçš„å›¾åƒæè¿°ã€‚æç¤ºè¯è¶Šè¯¦ç»†ï¼Œç”Ÿæˆçš„å›¾åƒè´¨é‡å¾€å¾€è¶Šå¥½ã€‚ä½ å¯ä»¥ä½¿ç”¨`(è¯:æƒé‡)`æ ¼å¼æ¥æŽ§åˆ¶ç‰¹å®šè¯çš„æƒé‡ï¼Œä¾‹å¦‚ï¼š`å¤©ç©ºæ˜¯æ¸…çˆ½çš„(è“è‰²:0.3)å’Œ(ç»¿è‰²:0.8)`è¡¨ç¤ºå¤©ç©ºæ˜¯è“è‰²å’Œç»¿è‰²çš„ï¼Œä½†ç»¿è‰²æ›´ä¸ºçªå‡ºã€‚\n2.  (å¯é€‰)é€‰æ‹© `style_preset` å‚æ•°æ¥æŽ§åˆ¶å›¾åƒçš„è§†è§‰é£Žæ ¼ã€‚ä¸åŒçš„é¢„è®¾é£Žæ ¼ä¼šäº§ç”Ÿä¸åŒé£Žæ ¼ç‰¹ç‚¹çš„å›¾åƒï¼Œå¦‚â€cinematicâ€ï¼ˆç”µå½±æ„Ÿï¼‰ã€â€œanimeâ€ï¼ˆåŠ¨æ¼«é£Žæ ¼ï¼‰ç­‰ã€‚é€‰æ‹©â€Noneâ€åˆ™ä¸åº”ç”¨ä»»ä½•ç‰¹å®šé£Žæ ¼ã€‚\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç”Ÿæˆã€‚\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\n*   **æç¤ºè¯(Prompt)**ï¼šæç¤ºè¯æ˜¯ç”Ÿæˆè¿‡ç¨‹ä¸­æœ€é‡è¦çš„å‚æ•°ä¹‹ä¸€ï¼Œè¯¦ç»†ã€æ¸…æ™°çš„æè¿°ä¼šå¸¦æ¥æ›´å¥½çš„æ•ˆæžœã€‚å¯ä»¥åŒ…å«åœºæ™¯ã€ä¸»ä½“ã€é¢œè‰²ã€å…‰ç…§ã€é£Žæ ¼ç­‰å…ƒç´ ã€‚\n*   **é£Žæ ¼é¢„è®¾(Style Preset)**ï¼šæä¾›å¤šç§é¢„è®¾é£Žæ ¼ï¼Œå¦‚ç”µå½±æ„Ÿã€åŠ¨æ¼«é£Žã€æ•°å­—è‰ºæœ¯ç­‰ï¼Œèƒ½å¤Ÿå¿«é€Ÿå®šä¹‰å›¾åƒçš„æ•´ä½“é£Žæ ¼ã€‚\n*   **è´Ÿé¢æç¤ºè¯(Negative Prompt)**ï¼šç”¨äºŽæŒ‡å®šä¸å¸Œæœ›åœ¨ç”Ÿæˆå›¾åƒä¸­å‡ºçŽ°çš„å…ƒç´ ï¼Œå¯ä»¥å¸®åŠ©é¿å…å¸¸è§é—®é¢˜ï¼Œå¦‚é¢å¤–çš„è‚¢ä½“ã€æ‰­æ›²çš„é¢éƒ¨ç­‰ã€‚\n*   **Seed å‚æ•°**ï¼šå¯ä»¥ç”¨äºŽå¤çŽ°æˆ–å¾®è°ƒç”Ÿæˆç»“æžœï¼Œå¯¹äºŽåˆ›ä½œè¿‡ç¨‹ä¸­çš„è¿­ä»£å¾ˆæœ‰å¸®åŠ©ã€‚\n*   å½“å‰ `Load Image` èŠ‚ç‚¹ä¸º â€œç»•è¿‡ï¼ˆBypassï¼‰â€ æ¨¡å¼ï¼Œå¦‚éœ€å¯ç”¨å¯ä»¥å‚è€ƒæ­¥éª¤å›¾åœ¨å¯¹åº”èŠ‚ç‚¹ä¸Šå³é”®ç„¶åŽå°†â€œæ¨¡å¼ï¼ˆModeï¼‰â€è®¾ç½®ä¸ºâ€œæ€»æ˜¯ï¼ˆAlwaysï¼‰â€ æ¥å¯ç”¨è¾“å…¥,å³å¯è½¬ä¸ºå›¾ç”Ÿå›¾æ¨¡å¼\n\n## Stability AI Stable Image Ultra å›¾ç”Ÿå›¾å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![Stability Stable Image Ultra å›¾ç”Ÿå›¾å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/stable_image_ultra_i2i.png) ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡æˆ‘ä»¬å°†ç”¨äºŽè¾“å…¥å›¾ç‰‡ ![Stability Stable Image Ultra å›¾ç”Ÿå›¾å·¥ä½œæµè¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/input.png) \n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![Stability Stable Image Ultra å›¾ç”Ÿå›¾æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆå›¾ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  é€šè¿‡ `Load Image` èŠ‚ç‚¹åŠ è½½ä¸€å¼ å‚è€ƒå›¾åƒï¼Œè¯¥å›¾åƒå°†ä½œä¸ºç”Ÿæˆçš„åŸºç¡€ã€‚\n2.  (å¯é€‰)ä¿®æ”¹ `Stability Stable Image Ultra` èŠ‚ç‚¹ä¸­çš„ `prompt` å‚æ•°ï¼Œæè¿°ä½ å¸Œæœ›åœ¨å‚è€ƒå›¾åƒåŸºç¡€ä¸Šæ”¹å˜æˆ–å¢žå¼ºçš„å…ƒç´ ã€‚\n3.  (å¯é€‰)è°ƒæ•´ `image_denoise` å‚æ•°ï¼ˆèŒƒå›´0.0-1.0ï¼‰æ¥æŽ§åˆ¶å¯¹åŽŸå§‹å›¾åƒçš„ä¿®æ”¹ç¨‹åº¦ï¼š\n    *   å€¼è¶ŠæŽ¥è¿‘0.0ï¼Œç”Ÿæˆçš„å›¾åƒè¶ŠæŽ¥è¿‘è¾“å…¥çš„å‚è€ƒå›¾åƒ\n    *   å€¼è¶ŠæŽ¥è¿‘1.0ï¼Œç”Ÿæˆçš„å›¾åƒè¶ŠæŽ¥è¿‘çº¯æ–‡æœ¬ç”Ÿæˆçš„æ•ˆæžœ\n4.  (å¯é€‰)åŒæ ·å¯ä»¥è®¾ç½® `style_preset` å’Œå…¶ä»–å‚æ•°æ¥è¿›ä¸€æ­¥æŽ§åˆ¶ç”Ÿæˆæ•ˆæžœã€‚\n5.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç”Ÿæˆã€‚\n6.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n### 3\\. è¡¥å……è¯´æ˜Ž\n\n**å›¾åƒåŽ»å™ªå¼ºåº¦(Image Denoise)**ï¼šè¿™ä¸ªå‚æ•°å†³å®šäº†ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿ç•™åŽŸå§‹å›¾åƒç‰¹å¾çš„ç¨‹åº¦ï¼Œæ˜¯å›¾ç”Ÿå›¾æ¨¡å¼ä¸­æœ€å…³é”®çš„è°ƒèŠ‚å‚æ•°,ä¸‹å›¾æ˜¯ä¸åŒçš„åŽ»å™ªå¼ºåº¦ä¸‹ç”Ÿæˆçš„å›¾åƒæ•ˆæžœ ![Stability Stable Image Ultra å›¾ç”Ÿå›¾åŽ»å™ªå¼ºåº¦è¯´æ˜Ž](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg)\n\n*   **å‚è€ƒå›¾åƒé€‰æ‹©**ï¼šé€‰æ‹©å…·æœ‰æ¸…æ™°ä¸»ä½“å’Œè‰¯å¥½æž„å›¾çš„å›¾åƒé€šå¸¸èƒ½èŽ·å¾—æ›´å¥½çš„ç»“æžœã€‚\n*   **æç¤ºè¯æŠ€å·§**ï¼šåœ¨å›¾ç”Ÿå›¾æ¨¡å¼ä¸­ï¼Œæç¤ºè¯åº”è¯¥æ›´å¤šåœ°å…³æ³¨ä½ å¸Œæœ›æ”¹å˜æˆ–å¢žå¼ºçš„éƒ¨åˆ†ï¼Œè€Œä¸éœ€è¦æè¿°å·²ç»å­˜åœ¨äºŽå›¾åƒä¸­çš„æ‰€æœ‰å…ƒç´ ã€‚\n\n## ç›¸å…³èŠ‚ç‚¹è¯¦è§£\n\nä½ å¯æŸ¥é˜…ä¸‹é¢çš„æ–‡æ¡£äº†è§£å¯¹åº”èŠ‚ç‚¹çš„è¯¦ç»†å‚æ•°è®¾ç½®ç­‰\n\n[\n\n## Stability Stable Image Ultra èŠ‚ç‚¹æ–‡æ¡£\n\nStability Stable Image Ultra API èŠ‚ç‚¹è¯´æ˜Žæ–‡æ¡£\n\n\n\n](https://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/user",
  "markdown": "# ComfyUI è´¦å·ç®¡ç† - ComfyUI\n\nè´¦å·ç³»ç»Ÿæ˜¯ä¸ºäº†æ”¯æŒ `API Nodes` èŠ‚ç‚¹è€Œæ–°å¢žçš„ï¼Œ`API Nodes`æ”¯æŒäº†å¯¹é—­æºæ¨¡åž‹ API çš„è°ƒç”¨ï¼Œè¿™å¤§å¤§æ‰©å±•äº† ComfyUI çš„å¯èƒ½æ€§ï¼Œç”±äºŽå¯¹åº”çš„ API è°ƒç”¨éœ€è¦æ¶ˆè€— Tokenï¼Œæ‰€ä»¥æˆ‘ä»¬å¢žåŠ äº†å¯¹åº”çš„ç”¨æˆ·ç³»ç»Ÿã€‚ å½“å‰æˆ‘ä»¬æ”¯æŒä»¥ä¸‹å‡ ç§ç™»å½•æ–¹å¼ï¼š\n\n*   é‚®ç®±ç™»å½•\n*   Google ç™»å½•\n*   Github ç™»å½•\n*   API Key ç™»å½•(éžç™½åå•ç½‘ç«™æŽˆæƒä½¿ç”¨)\n\nç™»å½•è¦æ±‚ç›¸å…³åŠè¯´æ˜Žï¼Œæˆ‘ä»¬åœ¨æœ¬ç¯‡æ–‡æ¡£ä¸­ä¼šè¿›è¡Œå¯¹åº”è¯´æ˜Ž\n\nä½ å¯èƒ½è‡³å°‘éœ€è¦ä½¿ç”¨ [ComfyUI v0.3.0](https://github.com/comfyanonymous/ComfyUI/releases/tag/v0.3.30) ç‰ˆæœ¬æ‰èƒ½ä½¿ç”¨è´¦å·ç³»ç»Ÿ,ç¡®ä¿å¯¹åº”å‰ç«¯ç‰ˆæœ¬è‡³å°‘ä¸º`1.17.11`,æœ‰æ—¶å€™å‰ç«¯å¯èƒ½ä¼šå®‰è£…å¤±è´¥è€Œå¯¼è‡´å›žæ»šåˆ°æ—§ç‰ˆæœ¬ï¼Œæ‰€ä»¥è¯·åœ¨`è®¾ç½®` -> `å…³äºŽ` æŸ¥çœ‹å‰ç«¯ç‰ˆæœ¬æ˜¯å¦å¤§äºŽ`1.17.11` åœ¨éƒ¨åˆ†åœ°åŒºå¯èƒ½ä¼šå› ä¸ºç½‘ç»œé™åˆ¶æ— æ³•æ­£å¸¸è®¿é—®ç™»å½• API å¯¼è‡´ç™»å½•è¶…æ—¶æˆ–è€…å¤±è´¥ï¼Œ åœ¨ç™»å½•å‰è¯·**ç¡®ä¿ä½ çš„ç½‘ç»œçŽ¯å¢ƒä¸ä¼šè¢«é™åˆ¶å¯¹åº” API çš„è®¿é—®**ï¼Œä¿è¯èƒ½å¤Ÿæ­£å¸¸è®¿é—® Google æˆ–è€… Github ç­‰ç½‘ç«™ã€‚\n\n## ç½‘ç»œè¦æ±‚\n\nè¦ä½¿ç”¨APIï¼Œä½ å¿…é¡»å¤„äºŽå®‰å…¨çš„ç½‘ç»œçŽ¯å¢ƒä¸­ï¼š\n\n*   å…è®¸ä»Ž`127.0.0.1`æˆ–`localhost`è®¿é—®ã€‚\n*   ä¸æ”¯æŒä½¿ç”¨`--listen`å‚æ•°é€šè¿‡å±€åŸŸç½‘è®¿é—®APIèŠ‚ç‚¹ã€‚\n*   åœ¨æœªå¯ç”¨ SSL è¯ä¹¦å³éž `https` å¼€å¤´çš„ç«™ç‚¹ä½ å¯èƒ½æ— æ³•æˆåŠŸç™»å½•\n*   ä½ å¯èƒ½æ— æ³•åœ¨ä¸åœ¨æˆ‘ä»¬ç™½åå•çš„ç«™ç‚¹ä¸­ç™»å½•ï¼ˆå¯ä»¥é€šè¿‡ API Key ç™»å½•ï¼‰\n*   ç¡®ä¿ä½ èƒ½å¤Ÿæ­£å¸¸è¿žæŽ¥æˆ‘ä»¬çš„æœåŠ¡ï¼ˆåœ¨æŸäº›åœ°åŒºå¯èƒ½éœ€è¦ä»£ç†æ¥è®¿é—®ï¼‰ã€‚\n\n## å¦‚ä½•è¿›è¡Œç™»å½•\n\nåœ¨ `è®¾ç½®` -> `ç”¨æˆ·` ä¸­è¿›è¡Œç™»å½•ï¼š ![ComfyUI ç”¨æˆ·ç•Œé¢](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/user.jpg)\n\n## ç™»å½•æ–¹å¼\n\n![user-login](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/user-login.jpg) å¦‚æžœæ˜¯é¦–æ¬¡ç™»å½•ï¼Œè¯·é¦–å…ˆåˆ›å»ºä¸€ä¸ªè´¦æˆ·ã€‚\n\n## ä½¿ç”¨ API Key è¿›è¡Œç™»å½•\n\nç”±äºŽç›®å‰å¹¶éžæ‰€æœ‰ ComfyUI çš„ç›¸å…³éƒ¨ç½²éƒ½æ˜¯åœ¨æˆ‘ä»¬çš„åŸŸåæŽˆæƒç™½åå•ä¸­ï¼Œæ‰€ä»¥åœ¨é€šè¿‡éžç™½åå•çš„ç½‘ç«™ç™»å½•æ—¶ï¼Œåœ¨è¿‘æœŸï¼ˆ2025-05-10ï¼‰çš„æ›´æ–°ä¸­æˆ‘ä»¬æä¾›äº† API Key ç™»å½•ï¼Œä¸‹é¢æ˜¯ä½¿ç”¨ API Key ç™»å½•çš„ç›¸å…³æ­¥éª¤ï¼š\n\n## ç™»å½•åŽçŠ¶æ€\n\nç™»å½•åŽåœ¨ CofmyUI ç•Œé¢é¡¶éƒ¨èœå•æ æ˜¾ç¤ºç™»å½•æŒ‰é’®ï¼Œå¹¶å¯ä»¥é€šè¿‡è¯¥æŒ‰é’®æ‰“å¼€å¯¹åº”çš„ç™»å½•ç•Œé¢ï¼Œå¹¶å¯ä»¥åœ¨è®¾ç½®èœå•ä¸­é€€å‡ºå¯¹åº”çš„è´¦å· ![user-logged](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/user-logged.jpg) ![menu-user-logged](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/zh/interface/setting/menu-user-logged.jpg)\n\n## å¸¸è§é—®é¢˜"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/comfy",
  "markdown": "# Comfy è®¾ç½® - ComfyUI\n\n## API èŠ‚ç‚¹\n\n### æ˜¾ç¤º API èŠ‚ç‚¹å®šä»·å¾½ç« \n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨ API èŠ‚ç‚¹ä¸Šæ˜¾ç¤ºå®šä»·å¾½ç« ï¼Œå¸®åŠ©ç”¨æˆ·è¯†åˆ« API èŠ‚ç‚¹çš„ä½¿ç”¨æˆæœ¬\n\n![å¯ç”¨æ•ˆæžœ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/api_node_pricing_badge.jpg)\n\n> æ›´å¤šå…³äºŽ API èŠ‚ç‚¹è¯·å‚è€ƒ [API èŠ‚ç‚¹](https://docs.comfy.org/zh-CN/tutorials/api-nodes/overview)\n\n## å¼€å‘è€…æ¨¡å¼\n\n### å¯ç”¨å¼€å‘æ¨¡å¼é€‰é¡¹\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šå¯ç”¨å¼€å‘è€…æ¨¡å¼é€‰é¡¹ï¼ˆå¦‚APIä¿å­˜ç­‰ï¼‰\n\n## ç¼–è¾‘ä»¤ç‰Œæƒé‡\n\n### Ctrl+ä¸Š/ä¸‹ ç²¾åº¦\n\n*   **é»˜è®¤å€¼**ï¼š0.01\n*   **åŠŸèƒ½**ï¼šå½“ä½ åœ¨ä½¿ç”¨ç±»åž‹ CLIPTextEncode æˆ–è€…æ–‡æœ¬æ¡†è¾“å…¥ç±»çš„èŠ‚ç‚¹ç»„ä»¶æ—¶ï¼Œä½¿ç”¨ Ctrl+ä¸Š/ä¸‹ å¯ä»¥å¿«é€Ÿè°ƒæ•´æƒé‡ï¼Œè¿™ä¸ªé€‰é¡¹ä¼šæ”¹å˜æ¯æ¬¡è°ƒæ•´çš„æƒé‡å€¼\n\n## åŒºåŸŸè®¾ç½®ï¼ˆæœ¬åœ°åŒ–ï¼‰\n\n### è¯­è¨€\n\n*   **é€‰é¡¹**ï¼šä¸­æ–‡ (Chinese)ã€English (è‹±æ–‡)ã€æ—¥æœ¬èªž (æ—¥æ–‡)ã€í•œêµ­ì–´ (éŸ©æ–‡)ã€Ð ÑƒÑÑÐºÐ¸Ð¹ (ä¿„æ–‡)ã€EspaÃ±ol (è¥¿ç­ç‰™è¯­)ã€FranÃ§ais (æ³•è¯­)\n*   **é»˜è®¤å€¼**ï¼šè‡ªåŠ¨æ£€æµ‹æµè§ˆå™¨è¯­è¨€\n*   **åŠŸèƒ½**ï¼šä¿®æ”¹ ComfyUI ç•Œé¢æ˜¾ç¤ºè¯­è¨€\n\n## èœå•\n\n### ä½¿ç”¨æ–°èœå•\n\n*   **é»˜è®¤å€¼**ï¼šé¡¶éƒ¨\n*   **åŠŸèƒ½**ï¼šé€‰æ‹©èœå•ç•Œé¢å’Œä½ç½®ï¼Œç›®å‰ä»…æ”¯æŒé¡¶éƒ¨ã€åº•éƒ¨ã€ç¦ç”¨\n\nèœå•ç•Œé¢å°†ä¼šæ˜¾ç¤ºåœ¨å·¥ä½œç•Œé¢çš„é¡¶éƒ¨ ![é¡¶éƒ¨èœå•](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/UseNewMenu_top.jpg) \n\n## æ¨¡åž‹åº“\n\næ¨¡åž‹åº“æŒ‡çš„æ˜¯ ComfyUI ä¾§è¾¹èœå•æ ä¸­çš„æ¨¡åž‹ç®¡ç†åŠŸèƒ½ï¼Œä½ å¯ä»¥é€šè¿‡è¿™ä¸ªåŠŸèƒ½æ¥æŸ¥çœ‹ä½ åœ¨ `ComfyUI/models` åŠé¢å¤–é…ç½®çš„æ¨¡åž‹æ–‡ä»¶å¤¹ä¸­çš„æ¨¡åž‹ ![æ¨¡åž‹åº“](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/sidepanel/model_library.jpg)\n\n### æ¨¡åž‹åº“åç§°æ ¼å¼\n\n*   **é»˜è®¤å€¼**ï¼šæ ‡é¢˜\n*   **åŠŸèƒ½**ï¼šé€‰æ‹©åœ¨æ¨¡åž‹åº“æ ‘è§†å›¾ä¸­æ˜¾ç¤ºçš„åç§°æ ¼å¼ï¼Œç›®å‰ä»…æ”¯æŒæ–‡ä»¶åã€æ ‡é¢˜\n\n### è‡ªåŠ¨åŠ è½½æ‰€æœ‰æ¨¡åž‹æ–‡ä»¶å¤¹\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šæ˜¯å¦åœ¨ç‚¹å‡»æ¨¡åž‹åº“æ—¶è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰æ–‡ä»¶å¤¹ä¸‹çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¯ç”¨å¯èƒ½å¯¼è‡´åŠ è½½å»¶è¿Ÿï¼ˆéœ€è¦å¾ªçŽ¯éåŽ†æ‰€æœ‰æ–‡ä»¶å¤¹ï¼‰ï¼Œç¦ç”¨æ—¶åªæœ‰ç‚¹å‡»æ–‡ä»¶å¤¹åç§°æ‰ä¼šåŠ è½½å¯¹åº”æ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶\n\n## èŠ‚ç‚¹\n\nåœ¨ ComfyUI çš„è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šå¯¹ä¸€äº›èŠ‚ç‚¹è¿›è¡Œè°ƒæ•´ï¼Œä¹Ÿä¼šå¯ç”¨ä¸€äº›èŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹å¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‘ç”Ÿé‡å¤§å˜åŒ–æˆ–è¢«ç§»é™¤ï¼Œä½†æ˜¯ä¸ºäº†ä¿è¯å…¼å®¹æ€§ç±»ä¼¼å¼ƒç”¨çš„èŠ‚ç‚¹å¹¶æ²¡æœ‰è¢«ç§»é™¤ï¼Œä½ å¯ä»¥é€šè¿‡ä¸‹é¢çš„è®¾ç½®æ¥å¯ç”¨æ˜¯å¦æ˜¾ç¤º **å®žéªŒæ€§èŠ‚ç‚¹** å’Œ **å·²å¼ƒç”¨èŠ‚ç‚¹**\n\n### æ˜¾ç¤ºå·²å¼ƒç”¨èŠ‚ç‚¹\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨æœç´¢ä¸­æ˜¾ç¤ºå·²å¼ƒç”¨çš„èŠ‚ç‚¹ï¼Œå·²å¼ƒç”¨èŠ‚ç‚¹åœ¨UIä¸­é»˜è®¤éšè—ï¼Œä½†åœ¨çŽ°æœ‰å·¥ä½œæµä¸­ä»ç„¶æœ‰æ•ˆ\n\n![å·²å¼ƒç”¨èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/depr_node.jpg)\n\n### æ˜¾ç¤ºå®žéªŒæ€§èŠ‚ç‚¹\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨æœç´¢ä¸­æ˜¾ç¤ºå®žéªŒæ€§èŠ‚ç‚¹ï¼Œå®žéªŒæ€§èŠ‚ç‚¹è¯´ä¸€äº›æ–°çš„åŠŸèƒ½æ”¯æŒï¼Œä½†æœªå®Œå…¨ç¨³å®šï¼Œå¯èƒ½åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‘ç”Ÿå˜åŒ–æˆ–è¢«ç§»é™¤\n\n![å·²å¼ƒç”¨èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/beta_node.jpg)\n\n## èŠ‚ç‚¹æœç´¢æ¡†\n\n### èŠ‚ç‚¹å»ºè®®æ•°é‡\n\n*   **é»˜è®¤å€¼**ï¼š5\n*   **åŠŸèƒ½**ï¼šç”¨äºŽä¿®æ”¹ç›¸å…³èŠ‚ç‚¹ä¸Šä¸‹æ–‡èœå•ä¸­æŽ¨èçš„èŠ‚ç‚¹æ•°é‡ï¼Œæ•°å€¼è¶Šå¤§æ˜¾ç¤ºçš„ç›¸å…³æŽ¨èèŠ‚ç‚¹æ•°é‡è¶Šå¤š\n\n![èŠ‚ç‚¹å»ºè®®æ•°é‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_suggestions.jpg)\n\n### æ˜¾ç¤ºèŠ‚ç‚¹é¢‘çŽ‡\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨æœç´¢ç»“æžœä¸­æ˜¾ç¤ºèŠ‚ç‚¹ä½¿ç”¨é¢‘çŽ‡\n\n![èŠ‚ç‚¹é¢‘çŽ‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_frequency.png)\n\n### æ˜¾ç¤ºèŠ‚ç‚¹IDåç§°\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨æœç´¢ç»“æžœä¸­æ˜¾ç¤ºèŠ‚ç‚¹IDåç§°\n\n![èŠ‚ç‚¹IDåç§°](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/comfy/node_id_name.jpg)\n\n### æ˜¾ç¤ºèŠ‚ç‚¹ç±»åˆ«\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨æœç´¢ç»“æžœä¸­æ˜¾ç¤ºèŠ‚ç‚¹ç±»åˆ«ï¼Œå¸®åŠ©ç”¨æˆ·äº†è§£èŠ‚ç‚¹çš„åˆ†ç±»ä¿¡æ¯\n\n### èŠ‚ç‚¹é¢„è§ˆ\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨æœç´¢ç»“æžœä¸­æ˜¾ç¤ºèŠ‚ç‚¹é¢„è§ˆï¼Œæ–¹ä¾¿ä½ å¿«é€Ÿé¢„è§ˆèŠ‚ç‚¹\n\n### èŠ‚ç‚¹æœç´¢æ¡†\n\n*   **é»˜è®¤å€¼**ï¼šé»˜è®¤\n*   **åŠŸèƒ½**ï¼š é€‰æ‹©èŠ‚ç‚¹æœç´¢æ¡†çš„å®žçŽ°æ–¹å¼ï¼ˆå®žéªŒæ€§åŠŸèƒ½ï¼‰ï¼Œå¦‚æžœé€‰æ‹© `litegraphï¼ˆæ—§ç‰ˆï¼‰` åˆ™ä¼šåˆ‡æ¢åˆ°æ—©æœŸçš„ ComfyUI çš„æœç´¢æ¡†\n\n## èŠ‚ç‚¹ç»„ä»¶\n\n### ç»„ä»¶æŽ§åˆ¶æ¨¡å¼\n\n*   **é€‰é¡¹**ï¼šä¹‹å‰ã€ä¹‹åŽ\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶èŠ‚ç‚¹ç»„ä»¶å€¼çš„æ›´æ–°æ—¶æœºæ˜¯åœ¨å·¥ä½œæµè¿è¡Œä¹‹å‰è¿˜æ˜¯æ»žåŽï¼Œæ¯”å¦‚æ›´æ–° seed ç§å­çš„å€¼\n\n### æ–‡æœ¬æ¡†ç»„ä»¶æ‹¼å†™æ£€æŸ¥\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ–‡æœ¬åŸŸå°éƒ¨ä»¶æ˜¯å¦å¯ç”¨æ‹¼å†™æ£€æŸ¥, åœ¨æ–‡æœ¬è¾“å…¥æ—¶æä¾›æ‹¼å†™æ£€æŸ¥åŠŸèƒ½, è¿™ä¸€åŠŸèƒ½æ˜¯é€šè¿‡æµè§ˆå™¨çš„ spellcheck å±žæ€§å®žçŽ°çš„\n\n## é˜Ÿåˆ—\n\n### é˜Ÿåˆ—åŽ†å²å¤§å°\n\n*   **é»˜è®¤å€¼**ï¼š100\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶ä¾§è¾¹æ é˜Ÿåˆ—åŽ†å²é¢æ¿é‡Œè®°å½•çš„é˜Ÿåˆ—åŽ†å²å¤§å°ï¼Œæ•°å€¼è¶Šå¤§è®°å½•çš„é˜Ÿåˆ—åŽ†å²è¶Šå¤šï¼Œæ•°é‡å¤šå¤§æ—¶åŠ è½½é¡µé¢æ—¶ä¹Ÿä¼šå ç”¨æ›´å¤šå†…å­˜\n\n## æ‰§è¡ŒæŒ‰é’®\n\n### æ‰¹å¤„ç†è®¡æ•°é™åˆ¶\n\n*   **é»˜è®¤å€¼**ï¼š100\n*   **åŠŸèƒ½**ï¼šè®¾ç½®å•æ¬¡ç‚¹å‡»æ·»åŠ åˆ°é˜Ÿåˆ—çš„æœ€å¤§ä»»åŠ¡æ•°é‡ï¼Œé˜²æ­¢æ„å¤–æ·»åŠ è¿‡å¤šä»»åŠ¡åˆ°é˜Ÿåˆ—\n\n## éªŒè¯å’Œå¼€å‘è€…è®¾ç½®\n\n### éªŒè¯èŠ‚ç‚¹å®šä¹‰\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨å¯åŠ¨æ—¶éªŒè¯æ‰€æœ‰èŠ‚ç‚¹å®šä¹‰ï¼ˆæ…¢ï¼‰ï¼Œä»…æŽ¨èç»™èŠ‚ç‚¹å¼€å‘è€…ä½¿ç”¨ï¼Œå½“å¯ç”¨æ—¶ç³»ç»Ÿä¼šä½¿ç”¨ Zod æ¨¡å¼å¯¹æ¯ä¸ªèŠ‚ç‚¹å®šä¹‰è¿›è¡Œä¸¥æ ¼éªŒè¯ï¼Œè¿™ä¸€åŠŸèƒ½ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜å’Œæ—¶é—´\n*   **é”™è¯¯å¤„ç†**ï¼šéªŒè¯å¤±è´¥çš„èŠ‚ç‚¹å®šä¹‰ä¼šè¢«è·³è¿‡ï¼Œå¹¶åœ¨æŽ§åˆ¶å°è¾“å‡ºè­¦å‘Šä¿¡æ¯\n\n### æ ¡éªŒå·¥ä½œæµ\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šç¡®ä¿å·¥ä½œæµçš„ç»“æž„å’Œè¿žæŽ¥æ­£ç¡®æ€§, å¦‚æžœå¯ç”¨ï¼Œç³»ç»Ÿä¼šè°ƒç”¨ `useWorkflowValidation().validateWorkflow()` å¯¹å·¥ä½œæµæ•°æ®è¿›è¡ŒéªŒè¯\n*   **éªŒè¯è¿‡ç¨‹**ï¼šéªŒè¯è¿‡ç¨‹åŒ…å«ä¸¤ä¸ªæ­¥éª¤ï¼š\n    *   æ¨¡å¼éªŒè¯: ä½¿ç”¨ Zod æ¨¡å¼éªŒè¯å·¥ä½œæµç»“æž„\n    *   é“¾æŽ¥ä¿®å¤: æ£€æŸ¥å¹¶ä¿®å¤èŠ‚ç‚¹é—´çš„è¿žæŽ¥é—®é¢˜\n*   **é”™è¯¯å¤„ç†**ï¼šéªŒè¯å¤±è´¥æ—¶ä¼šæ˜¾ç¤ºé”™è¯¯æç¤ºï¼Œä½†ä¸ä¼šé˜»æ­¢å·¥ä½œæµåŠ è½½\n\n## çª—å£\n\n### å…³é—­çª—å£æ—¶æ˜¾ç¤ºç¡®è®¤\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šå½“å­˜åœ¨å·²ä¿®æ”¹ä½†æœªä¿å­˜çš„å·¥ä½œæµæ—¶ï¼ŒæŽ§åˆ¶å…³é—­æµè§ˆå™¨çª—å£æˆ–æ ‡ç­¾é¡µæ—¶æ˜¯å¦æ˜¾ç¤ºç¡®è®¤ï¼Œé˜²æ­¢æ„å¤–å…³é—­çª—å£å¯¼è‡´æœªä¿å­˜çš„å·¥ä½œæµä¸¢å¤±\n\n## å·¥ä½œæµ\n\n### æŒä¹…åŒ–å·¥ä½œæµçŠ¶æ€å¹¶åœ¨é¡µé¢ï¼ˆé‡æ–°ï¼‰åŠ è½½æ—¶æ¢å¤\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨é¡µé¢ï¼ˆé‡æ–°ï¼‰åŠ è½½æ—¶æ¢å¤å·¥ä½œæµçŠ¶æ€ï¼Œåœ¨é¡µé¢åˆ·æ–°åŽä¿æŒå·¥ä½œæµå†…å®¹\n\n### è‡ªåŠ¨ä¿å­˜\n\n*   **é»˜è®¤å€¼**ï¼šå…³é—­\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶å·¥ä½œæµçš„è‡ªåŠ¨ä¿å­˜è¡Œä¸ºï¼Œè‡ªåŠ¨ä¿å­˜å·¥ä½œæµæ›´æ”¹ï¼Œé¿å…æ•°æ®ä¸¢å¤±\n\n### è‡ªåŠ¨ä¿å­˜å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰\n\n*   **é»˜è®¤å€¼**ï¼š1000\n*   **åŠŸèƒ½**ï¼šè®¾ç½®è‡ªåŠ¨ä¿å­˜çš„å»¶è¿Ÿæ—¶é—´ï¼Œä»…åœ¨è‡ªåŠ¨ä¿å­˜è®¾ç½®ä¸ºâ€å»¶è¿ŸåŽâ€æ—¶ç”Ÿæ•ˆ\n\n### åˆ é™¤å·¥ä½œæµæ—¶æ˜¾ç¤ºç¡®è®¤\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶åœ¨ä¾§è¾¹æ åˆ é™¤å·¥ä½œæµæ—¶æ˜¯å¦æ˜¾ç¤ºç¡®è®¤å¯¹è¯æ¡†ï¼Œé˜²æ­¢æ„å¤–åˆ é™¤é‡è¦å·¥ä½œæµ\n\n### åœ¨å·¥ä½œæµä¸­ä¿å­˜å’Œæ¢å¤è§†å›¾ä½ç½®åŠç¼©æ”¾\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨å·¥ä½œæµä¸­ä¿å­˜å’Œæ¢å¤ç”»å¸ƒä½ç½®å’Œç¼©æ”¾çº§åˆ«ï¼Œåœ¨é‡æ–°æ‰“å¼€å·¥ä½œæµæ—¶æ¢å¤ä¹‹å‰çš„è§†å›¾çŠ¶æ€\n\n### å·²æ‰“å¼€å·¥ä½œæµä½ç½®\n\n*   **é€‰é¡¹**ï¼šä¾§è¾¹æ ã€é¡¶éƒ¨æ ã€é¡¶éƒ¨æ ï¼ˆç¬¬äºŒè¡Œï¼‰\n*   **é»˜è®¤å€¼**ï¼šé¡¶éƒ¨æ \n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ‰“å¼€çš„å·¥ä½œæµæ ‡ç­¾çš„æ˜¾ç¤ºä½ç½®ï¼Œç›®å‰ä»…æ”¯æŒä¾§è¾¹æ ã€é¡¶éƒ¨æ ã€é¡¶éƒ¨æ ï¼ˆç¬¬äºŒè¡Œï¼‰\n\n### ä¿å­˜å·¥ä½œæµæ—¶æç¤ºæ–‡ä»¶å\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶ä¿å­˜å·¥ä½œæµæ—¶æ˜¯å¦æç¤ºè¾“å…¥æ–‡ä»¶åï¼Œå…è®¸ç”¨æˆ·è‡ªå®šä¹‰å·¥ä½œæµæ–‡ä»¶å\n\n### ä¿å­˜å·¥ä½œæµæ—¶æŽ’åºèŠ‚ç‚¹ID\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šå†³å®šä¿å­˜å·¥ä½œæµæ—¶æ˜¯å¦å¯¹èŠ‚ç‚¹IDè¿›è¡ŒæŽ’åºï¼Œä½¿å·¥ä½œæµæ–‡ä»¶æ ¼å¼æ›´è§„èŒƒï¼Œä¾¿äºŽç‰ˆæœ¬æŽ§åˆ¶\n\n### æ˜¾ç¤ºç¼ºå¤±èŠ‚ç‚¹è­¦å‘Š\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦æ˜¾ç¤ºå·¥ä½œæµä¸­ç¼ºå¤±èŠ‚ç‚¹çš„è­¦å‘Šï¼Œå¸®åŠ©ç”¨æˆ·è¯†åˆ«å·¥ä½œæµä¸­ä¸å¯ç”¨çš„èŠ‚ç‚¹\n\n### æ˜¾ç¤ºç¼ºå¤±æ¨¡åž‹è­¦å‘Š\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼š æˆ‘ä»¬æ”¯æŒåœ¨å·¥ä½œæµæ–‡ä»¶ä¸­å¯¹ widget çš„å€¼æ·»åŠ æ¨¡åž‹é“¾æŽ¥ä¿¡æ¯ï¼Œç”¨äºŽåŠ è½½æ¨¡åž‹æ–‡ä»¶æ—¶çš„æç¤ºï¼Œå½“å¯ç”¨æ—¶å¦‚æžœä½ æœ¬åœ°æ²¡æœ‰å¯¹åº”çš„æ¨¡åž‹æ–‡ä»¶åˆ™ä¼šæ˜¾ç¤ºå·¥ä½œæµä¸­ç¼ºå¤±æ¨¡åž‹çš„è­¦å‘Š\n\n### æ¸…é™¤å·¥ä½œæµæ—¶éœ€è¦ç¡®è®¤\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ¸…é™¤å·¥ä½œæµæ—¶æ˜¯å¦æ˜¾ç¤ºç¡®è®¤å¯¹è¯æ¡†ï¼Œé˜²æ­¢æ„å¤–æ¸…é™¤å·¥ä½œæµå†…å®¹\n\n### ä¿å­˜èŠ‚ç‚¹IDåˆ°å·¥ä½œæµ\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨ä¿å­˜å·¥ä½œæµæ—¶ä¿å­˜èŠ‚ç‚¹IDï¼Œä½¿å·¥ä½œæµæ–‡ä»¶æ ¼å¼æ›´è§„èŒƒï¼Œä¾¿äºŽç‰ˆæœ¬æŽ§åˆ¶"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/lite-graph",
  "markdown": "# ComfyUI ç”»é¢(LiteGraph)è®¾ç½® - ComfyUI\n\nLiteGraph æ˜¯ ComfyUI çš„åº•å±‚å›¾å½¢æ¸²æŸ“å¼•æ“Žï¼Œè¿™ä¸ªåˆ†ç±»ä¸‹çš„è®¾ç½®ä¸»è¦æŽ§åˆ¶ç”»å¸ƒã€èŠ‚ç‚¹ã€é“¾æŽ¥ç­‰å›¾å½¢ç•Œé¢çš„è¡Œä¸ºå’Œå¤–è§‚ã€‚\n\n## ç”»å¸ƒç›¸å…³è®¾ç½®\n\n### æ˜¾ç¤ºé€‰æ‹©å·¥å…·ç®±\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šé€‰æ‹©å·¥å…·ç®±æ˜¯é€‰ä¸­èŠ‚ç‚¹åŽåœ¨èŠ‚ç‚¹ä¸Šæµ®åŠ¨æ˜¾ç¤ºçš„å¿«æ·æ“ä½œå·¥å…·æ ï¼Œæä¾›äº†å¸¸ç”¨çš„å¿«æ·æ“ä½œå¦‚éƒ¨åˆ†è¿è¡Œã€å›ºå®šã€åˆ é™¤ã€é¢œè‰²ä¿®æ”¹ç­‰ç­‰\n\n![æ˜¾ç¤ºé€‰æ‹©å·¥å…·ç®±](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/selection-toolbox.jpg)\n\n### ä½Žè´¨é‡æ¸²æŸ“ç¼©æ”¾é˜ˆå€¼\n\n*   **é»˜è®¤å€¼**ï¼š0.6\n*   **èŒƒå›´**ï¼š0.1 - 1.0\n*   **åŠŸèƒ½**ï¼š åœ¨æ¸²æŸ“ç•Œé¢æ—¶ï¼Œç‰¹åˆ«æ˜¯å½“å·¥ä½œæµç‰¹åˆ«å¤æ‚åŠæ•´ä¸ªç”»å¸ƒç‰¹åˆ«å¤§æ—¶ï¼Œå¯¹åº”å…ƒç´ çš„å‰ç«¯æ¸²æŸ“ä¼šæ¶ˆè€—ç‰¹åˆ«å¤šçš„å†…å­˜è€Œé€ æˆå¡é¡¿ï¼Œé€šè¿‡è°ƒä½Žæ­¤é˜ˆå€¼ï¼Œå¯ä»¥æŽ§åˆ¶å…ƒç´ åœ¨ç¼©æ”¾åˆ°ç‰¹å®šç™¾åˆ†æ¯”æ—¶è¿›å…¥ä½Žè´¨é‡æ¸²æŸ“æ¨¡å¼ï¼Œä»Žè€Œé™ä½Žå†…å­˜æ¶ˆè€—ï¼Œå¯¹åº”ä¸åŒæ¸²æŸ“æ¨¡å¼å¦‚ä¸‹å›¾\n\n![ä½Žè´¨é‡æ¸²æŸ“](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/render-mode.jpg)\n\n### æœ€å¤§FPS\n\n*   **é»˜è®¤å€¼**ï¼š0ï¼ˆä½¿ç”¨å±å¹•åˆ·æ–°çŽ‡ï¼‰\n*   **èŒƒå›´**ï¼š0 - 120\n*   **åŠŸèƒ½**ï¼šé™åˆ¶ç”»å¸ƒçš„æ¸²æŸ“å¸§çŽ‡ï¼Œ0è¡¨ç¤ºä½¿ç”¨å±å¹•åˆ·æ–°çŽ‡ï¼Œè¶Šé«˜çš„ FPS ä¼šè®©ç”»é¢ï¼ˆCanvasï¼‰ æ¸²æŸ“è¶Šæµç•…ï¼Œä½†åŒæ—¶ä¹Ÿä¼šæ¶ˆè€—æ›´å¤šæ€§èƒ½ï¼Œä½†è¿‡å°æ—¶åˆ™ä¼šæœ‰è¶Šæ˜Žæ˜¾çš„å¡é¡¿æ„Ÿã€‚\n\n### å§‹ç»ˆå¸é™„åˆ°ç½‘æ ¼\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šåœ¨æ­¤é€‰é¡¹æ²¡æœ‰å¯ç”¨æ—¶ï¼Œä½ å¯ä»¥æŒ‰ä½ `Shift` é”®æ¥ä½¿èŠ‚ç‚¹è¾¹ç¼˜å’Œç½‘æ ¼å¯¹é½ï¼Œåœ¨å¯ç”¨åŽåˆ™æ— éœ€æŒ‰ä½ `Shift` é”®å³å¯è‡ªåŠ¨å¯¹é½ç½‘æ ¼\n\n### å¸é™„ç½‘æ ¼å¤§å°\n\n*   **èŒƒå›´**ï¼š1 - 500\n*   **åŠŸèƒ½**ï¼šåœ¨å¯ç”¨è‡ªåŠ¨å¸é™„æˆ–è€…æŒ‰ä½ `Shift` é”®è¿›è¡ŒèŠ‚ç‚¹çš„ç§»åŠ¨æ—¶ï¼Œè¿™ä¸ªå‚æ•°ä¼šå†³å®šå¸é™„çš„ç½‘æ ¼å¤§å°ï¼Œé»˜è®¤å€¼ä¸º 10ï¼Œä½ å¯ä»¥æ ¹æ®ä½ çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚\n\n### å¯ç”¨å¿«é€Ÿç¼©æ”¾å¿«æ·é”®\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šå¯ç”¨ `Ctrl + Shift + é¼ æ ‡å·¦é”®æ‹–æ‹½` çš„å¿«é€Ÿç¼©æ”¾åŠŸèƒ½ï¼Œæä¾›æ›´å¿«é€Ÿçš„ç¼©æ”¾æ“ä½œæ–¹å¼\n\n### æ˜¾ç¤ºå›¾å½¢ç”»å¸ƒèœå•\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦æ˜¾ç¤ºå³ä¸‹è§’çš„ç”»å¸ƒèœå•\n\nç”»å¸ƒèœå•ä½äºŽæ•´ä¸ª ComfyUI ç•Œé¢çš„å³ä¸‹è§’ï¼ŒåŒ…å«äº†ç”»å¸ƒçš„ç¼©æ”¾ã€ä¸´æ—¶éšè—æ‰€æœ‰è¿žçº¿ã€å¿«é€Ÿç¼©æ”¾å·¥ä½œæµåˆ°é€‚åº”ç”»å¸ƒç­‰æ“ä½œï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º ![æ˜¾ç¤ºå›¾å½¢ç”»å¸ƒèœå•](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/canvas_menu.jpg)\n\n### ç”»å¸ƒç¼©æ”¾é€Ÿåº¦\n\n*   **é»˜è®¤å€¼**ï¼š1.1\n*   **èŒƒå›´**ï¼š1.01 - 2.5\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶ç”»å¸ƒç¼©æ”¾çš„é€Ÿåº¦ï¼Œè°ƒæ•´é¼ æ ‡æ»šè½®ç¼©æ”¾çš„æ•æ„Ÿåº¦\n\n### åœ¨å·¦ä¸‹è§’æ˜¾ç¤ºç”»å¸ƒä¿¡æ¯ï¼ˆfpsç­‰ï¼‰\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦æ˜¾ç¤ºç”»å¸ƒå³é”®èœå•ï¼Œå¯ç”¨/ç¦ç”¨ç”»å¸ƒçš„ä¸Šä¸‹æ–‡èœå•\n\n![å¿«é€Ÿç¼©æ”¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/canvas-info.jpg)\n\n## ä¸Šä¸‹æ–‡èœå•\n\n### æ”¾å¤§æ—¶ç¼©æ”¾èŠ‚ç‚¹ç»„åˆéƒ¨ä»¶èœå•ï¼ˆåˆ—è¡¨ï¼‰\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨æ”¾å¤§æ—¶æ˜¾ç¤ºèŠ‚ç‚¹ç»„åˆéƒ¨ä»¶èœå•ï¼ˆåˆ—è¡¨ï¼‰ï¼Œå…è®¸ç”¨æˆ·é€‰æ‹©èŠ‚ç‚¹ç»„åˆéƒ¨ä»¶\n\n## ç”»é¢\n\n### è¿žçº¿æ¸²æŸ“æ¨¡å¼\n\n*   **é»˜è®¤å€¼**ï¼š2ï¼ˆSplineæ ·æ¡çº¿ï¼‰\n*   **é€‰é¡¹**ï¼šç›´çº¿ã€çº¿æ€§ã€æ ·æ¡çº¿ã€éšè—\n*   **åŠŸèƒ½**ï¼šè®¾ç½®è¿žçº¿çš„æ¸²æŸ“æ ·å¼ï¼ŒæŽ§åˆ¶èŠ‚ç‚¹é—´è¿žçº¿çš„è§†è§‰æ ·å¼\n\n![è¿žçº¿æ¸²æŸ“æ¨¡å¼](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/link-render-mode.jpg)\n\n## ç»„\n\nè¿™éƒ¨åˆ†çš„è®¾ç½®ä¸»è¦å’ŒèŠ‚ç‚¹ç»„åŠŸèƒ½ç›¸å…³ ![èŠ‚ç‚¹ç»„](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-group.png)\n\n### åŒå‡»ç»„æ ‡é¢˜ä»¥ç¼–è¾‘\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦å¯ä»¥åŒå‡»èŠ‚ç‚¹æ ‡é¢˜è¿›è¡Œç¼–è¾‘ï¼Œå…è®¸ç”¨æˆ·é‡å‘½åèŠ‚ç‚¹ï¼Œå›¾ä¸­æ ‡æ³¨ä¸º `1` çš„éƒ¨åˆ†\n\n### åˆ†ç»„é€‰ä¸­èŠ‚ç‚¹å¡«å……\n\n*   **é»˜è®¤å€¼**ï¼š10\n*   **èŒƒå›´**ï¼š0 - 100\n*   **åŠŸèƒ½**ï¼šè®¾ç½®åˆ†ç»„é€‰ä¸­èŠ‚ç‚¹æ—¶çš„å†…è¾¹è·ï¼ŒæŽ§åˆ¶åˆ†ç»„æ¡†ä¸ŽèŠ‚ç‚¹é—´çš„é—´è·ï¼Œå›¾ä¸­æ ‡æ³¨ä¸º `2` ç®­å¤´æ ‡æ³¨éƒ¨åˆ†\n\n## è¿žçº¿\n\n### é“¾æŽ¥ä¸­ç‚¹æ ‡è®°\n\n*   **é»˜è®¤å€¼**ï¼šCircleï¼ˆåœ†å½¢ï¼‰\n*   **é€‰é¡¹**ï¼šæ— ã€åœ†å½¢ã€ç®­å¤´\n*   **åŠŸèƒ½**ï¼šè®¾ç½®é“¾æŽ¥ä¸­ç‚¹çš„æ ‡è®°æ ·å¼ï¼Œåœ¨é“¾æŽ¥ä¸­ç‚¹æ˜¾ç¤ºæ–¹å‘æŒ‡ç¤º\n\n![è¿žçº¿æ¸²æŸ“æ¨¡å¼](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/link-midpoint.jpg)\n\n## é‡Šæ”¾é“¾æŽ¥\n\nè¿™éƒ¨åˆ†çš„èœå•ç›®å‰ä¸»è¦æŽ§åˆ¶å½“é“¾æŽ¥è¿žçº¿é‡Šæ”¾æ—¶çš„ç›¸å…³æ“ä½œï¼Œç›®å‰ä¸¤ä¸ªç›¸å…³æ“ä½œä¸ºï¼š **é‡Šæ”¾åŽä¼šå‡ºçŽ°å’Œå½“å‰è¾“å…¥ / è¾“å‡ºç›¸å…³çš„èŠ‚ç‚¹æŽ¨èåˆ—è¡¨**\n\n**é‡Šæ”¾åŽä¼šå¯åŠ¨æœç´¢æ¡†**\n\n### é“¾æŽ¥é‡Šæ”¾åŠ¨ä½œï¼ˆShifté”®ï¼‰\n\n*   **é»˜è®¤å€¼**ï¼š æœç´¢æ¡†\n*   **é€‰é¡¹**ï¼š ä¸Šä¸‹æ–‡èœå•ã€æœç´¢æ¡†ã€æ— æ“ä½œ\n*   **åŠŸèƒ½**ï¼šè®¾ç½®æŒ‰ä½Shifté”®é‡Šæ”¾é“¾æŽ¥æ—¶çš„åŠ¨ä½œï¼ŒæŒ‰ä½Shifté‡Šæ”¾é“¾æŽ¥æ—¶çš„ç‰¹æ®Šè¡Œä¸º\n\n### é“¾æŽ¥é‡Šæ”¾åŠ¨ä½œï¼ˆæ— ä¿®é¥°é”®ï¼‰\n\n*   **é»˜è®¤å€¼**ï¼š ä¸Šä¸‹æ–‡èœå•\n*   **é€‰é¡¹**ï¼š ä¸Šä¸‹æ–‡èœå•ã€æœç´¢æ¡†ã€æ— æ“ä½œ\n*   **åŠŸèƒ½**ï¼šè®¾ç½®é‡Šæ”¾é“¾æŽ¥æ—¶çš„é»˜è®¤åŠ¨ä½œï¼ŒæŽ§åˆ¶æ‹–æ‹½é“¾æŽ¥åŽé‡Šæ”¾æ—¶çš„è¡Œä¸º\n\n## èŠ‚ç‚¹\n\n### å§‹ç»ˆæ”¶ç¼©æ–°èŠ‚ç‚¹\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦åœ¨åˆ›å»ºæ–°èŠ‚ç‚¹æ—¶è‡ªåŠ¨æ”¶ç¼©ï¼Œä»Žè€Œè®©èŠ‚ç‚¹èƒ½å¤Ÿå§‹ç»ˆæ˜¾ç¤ºæœ€å°çš„å°ºå¯¸ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´æ·»åŠ æ—¶æœ‰äº›æ–‡æœ¬æ˜¾ç¤ºä¼šè¢«æˆªæ–­ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒæ•´èŠ‚ç‚¹å¤§å°\n\n### å¯ç”¨DOMå…ƒç´ è£å‰ªï¼ˆå¯ç”¨å¯èƒ½ä¼šé™ä½Žæ€§èƒ½ï¼‰\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šå¯ç”¨DOMå…ƒç´ è£å‰ªï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰ï¼Œä¼˜åŒ–æ¸²æŸ“ä½†å¯èƒ½é™ä½Žæ€§èƒ½\n\n### ä¸­é”®å•å‡»åˆ›å»ºæ–°çš„è½¬æŽ¥ç‚¹\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šä¸­é”®ç‚¹å‡»æ—¶åˆ›å»ºæ–°çš„é‡è·¯ç”±èŠ‚ç‚¹ï¼Œå¿«é€Ÿåˆ›å»ºç”¨äºŽæ•´ç†è¿žçº¿çš„é‡è·¯ç”±èŠ‚ç‚¹\n\n### åˆ é™¤èŠ‚ç‚¹æ—¶ä¿ç•™è¿žçº¿\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šåˆ é™¤ä¸­é—´èŠ‚ç‚¹æ—¶è‡ªåŠ¨ç»•è¿‡è¿žæŽ¥ï¼Œåˆ é™¤èŠ‚ç‚¹æ—¶å°è¯•é‡æ–°è¿žæŽ¥å…¶è¾“å…¥è¾“å‡ºé“¾æŽ¥\n\n### å¸é™„é«˜äº®èŠ‚ç‚¹\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæ‹–æ‹½é“¾æŽ¥åˆ°èŠ‚ç‚¹æ—¶é«˜äº®æ˜¾ç¤ºèŠ‚ç‚¹ï¼Œæä¾›è§†è§‰åé¦ˆï¼Œæ˜¾ç¤ºå¯è¿žæŽ¥çš„èŠ‚ç‚¹,å¯ç”¨åŽæ•ˆæžœå¦‚ä¸‹å›¾ï¼Œå¯¹åº”é“¾æŽ¥çš„ä¸€ä¾§ä¼šæ˜¾ç¤ºé«˜äº®çš„æ ·å¼\n\n![å¸é™„é«˜äº®èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/highlights-node.jpg)\n\n### è¿žçº¿è‡ªåŠ¨å¸é™„åˆ°èŠ‚ç‚¹æŽ¥å£\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæ‹–æ‹½é“¾æŽ¥åˆ°èŠ‚ç‚¹ä¸Šæ—¶è‡ªåŠ¨å¸é™„åˆ°å¯ç”¨æ’æ§½ï¼Œç®€åŒ–è¿žæŽ¥æ“ä½œï¼Œè‡ªåŠ¨æ‰¾åˆ°åˆé€‚çš„è¾“å…¥æ’æ§½\n\n### å¯ç”¨å·¥å…·æç¤º\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šåœ¨éƒ¨åˆ†èŠ‚ç‚¹ä¿¡æ¯ä¸­ä¼šåŒ…å«ä¸€äº›å·¥å…·æç¤ºï¼ŒåŒ…å«äº†ä¸€äº›å‚æ•°è¯´æ˜Žç­‰ï¼Œå½“å¯ç”¨åŽä¼šåœ¨é¼ æ ‡æ‚¬åœæ—¶æ˜¾ç¤ºè¿™äº›å·¥å…·æç¤ºï¼Œå¦‚ä¸‹å›¾\n\n![å·¥å…·æç¤º](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/tooltips.jpg)\n\n### å·¥å…·æç¤ºå»¶è¿Ÿ\n\n*   **é»˜è®¤å€¼**ï¼š500\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶å·¥å…·æç¤ºçš„å»¶è¿Ÿæ—¶é—´ï¼Œå•ä½ä¸ºæ¯«ç§’ï¼Œè®¾ç½®ä¸º0è¡¨ç¤ºç«‹å³æ˜¾ç¤ºå·¥å…·æç¤º\n\n### èŠ‚ç‚¹åˆ¶ä½œå‘¨æœŸæ ‡ç­¾\n\n*   **é»˜è®¤å€¼**ï¼šShowAllï¼ˆæ˜¾ç¤ºå…¨éƒ¨ï¼‰\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶èŠ‚ç‚¹ç”Ÿå‘½å‘¨æœŸæ ‡è®°çš„æ˜¾ç¤ºï¼Œæ˜¾ç¤ºèŠ‚ç‚¹çš„çŠ¶æ€ä¿¡æ¯\n\n### èŠ‚ç‚¹IDæ ‡ç­¾\n\n*   **é»˜è®¤å€¼**ï¼šNoneï¼ˆä¸æ˜¾ç¤ºï¼‰\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶èŠ‚ç‚¹IDæ ‡è®°çš„æ˜¾ç¤ºï¼Œæ˜¾ç¤ºèŠ‚ç‚¹çš„å”¯ä¸€æ ‡è¯†ç¬¦\n\n![èŠ‚ç‚¹IDæ ‡ç­¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-id-badge.jpg)\n\n### èŠ‚ç‚¹æºæ ‡ç­¾\n\n*   **é€‰é¡¹**ï¼š\n    *   Noneï¼ˆä¸æ˜¾ç¤ºï¼‰\n    *   HideBuiltInï¼ˆéšè—å†…ç½®ï¼‰\n    *   ShowAllï¼ˆæ˜¾ç¤ºå…¨éƒ¨ï¼‰\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶èŠ‚ç‚¹æºæ ‡è®°çš„æ˜¾ç¤ºæ¨¡å¼ï¼Œæ˜¾ç¤ºèŠ‚ç‚¹æ¥æºä¿¡æ¯,å¯¹åº”çš„æ˜¾ç¤ºæ•ˆæžœå¦‚ä¸‹å›¾ï¼Œå¦‚æžœæ˜¾ç¤ºå…¨éƒ¨åˆ™ä¼šæ˜¾ç¤ºè‡ªå®šä¹‰èŠ‚ç‚¹å’Œå†…ç½®èŠ‚ç‚¹çš„æ ‡ç­¾ï¼Œæ–¹ä¾¿ä½ åˆ¤æ–­å¯¹åº”çš„èŠ‚ç‚¹æ¥æºï¼Œå¯¹åº”å°ç‹ç‹¸æ ‡å¿—ä¸º ComfyUI å†…ç½®èŠ‚ç‚¹\n\n![èŠ‚ç‚¹æºæ ‡ç­¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/node-source-badge.jpg)\n\n### åŒå‡»èŠ‚ç‚¹æ ‡é¢˜ä»¥ç¼–è¾‘\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦å¯ä»¥åŒå‡»èŠ‚ç‚¹æ ‡é¢˜è¿›è¡Œç¼–è¾‘ï¼Œå…è®¸ç”¨æˆ·é‡å‘½åèŠ‚ç‚¹\n\n## èŠ‚ç‚¹ç»„ä»¶\n\n### æµ®ç‚¹ç»„ä»¶å››èˆäº”å…¥çš„å°æ•°ä½æ•° \\[0 = è‡ªåŠ¨\\]\n\n*   **é»˜è®¤å€¼**ï¼š0ï¼ˆè‡ªåŠ¨ï¼‰\n*   **èŒƒå›´**ï¼š0 - 6\n*   **åŠŸèƒ½**ï¼šè®¾ç½®æµ®ç‚¹å°éƒ¨ä»¶å››èˆäº”å…¥çš„å°æ•°ä½æ•°ï¼Œ0è¡¨ç¤ºè‡ªåŠ¨ï¼Œéœ€è¦é¡µé¢é‡æ–°åŠ è½½\n\n### ç¦ç”¨é»˜è®¤æµ®ç‚¹ç»„ä»¶å››èˆäº”å…¥\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦ç¦ç”¨é»˜è®¤çš„æµ®ç‚¹å°éƒ¨ä»¶å››èˆäº”å…¥ï¼Œéœ€è¦é¡µé¢é‡æ–°åŠ è½½ï¼Œå½“èŠ‚ç‚¹åŽç«¯è®¾ç½®äº†å››èˆäº”å…¥æ—¶æ— æ³•ç¦ç”¨\n\n### ç¦ç”¨èŠ‚ç‚¹ç»„ä»¶æ»‘å—\n\n*   **é»˜è®¤å€¼**ï¼šç¦ç”¨\n*   **åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦ç¦ç”¨èŠ‚ç‚¹å°éƒ¨ä»¶ä¸­çš„æ»‘å—æŽ§ä»¶ï¼Œå¼ºåˆ¶ä½¿ç”¨æ–‡æœ¬è¾“å…¥è€Œéžæ»‘å—\n\n### é¢„è§ˆå›¾åƒæ ¼å¼\n\n*   **é»˜è®¤å€¼**ï¼šç©ºå­—ç¬¦ä¸²ï¼ˆä½¿ç”¨åŽŸæ ¼å¼ï¼‰\n*   **åŠŸèƒ½**ï¼šè®¾ç½®å›¾åƒå°éƒ¨ä»¶ä¸­é¢„è§ˆå›¾åƒçš„æ ¼å¼ï¼Œè½¬æ¢ä¸ºè½»é‡çº§æ ¼å¼å¦‚ webpã€jpeg ç­‰\n\n### åœ¨å›¾åƒé¢„è§ˆä¸‹æ–¹æ˜¾ç¤ºå®½åº¦Ã—é«˜åº¦\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šåœ¨å›¾åƒé¢„è§ˆä¸‹æ–¹æ˜¾ç¤ºå®½åº¦Ã—é«˜åº¦ä¿¡æ¯ï¼Œæ˜¾ç¤ºå›¾åƒçš„å°ºå¯¸ä¿¡æ¯\n\n![åœ¨å›¾åƒé¢„è§ˆä¸‹æ–¹æ˜¾ç¤ºå®½åº¦Ã—é«˜åº¦](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/show-size.jpg)\n\n## æŒ‡é’ˆ\n\n### å¯ç”¨è§¦æŽ§æ¿æ‰‹åŠ¿\n\n*   **é»˜è®¤å€¼**ï¼šå¯ç”¨\n*   **åŠŸèƒ½**ï¼šæ­¤è®¾ç½®ä¸ºç”»å¸ƒå¯ç”¨è§¦æŽ§æ¿æ¨¡å¼ï¼Œå…è®¸ä½¿ç”¨åŒæŒ‡æåˆç¼©æ”¾å’Œæ‹–åŠ¨ã€‚\n\n### åŒå‡»é—´éš”ï¼ˆæœ€å¤§ï¼‰\n\n*   **é»˜è®¤å€¼**ï¼š300\n*   **åŠŸèƒ½**ï¼šåŒå‡»çš„ä¸¤æ¬¡ç‚¹å‡»ä¹‹é—´çš„æœ€å¤§æ—¶é—´(æ¯«ç§’)ã€‚å¢žåŠ æ­¤å€¼æœ‰åŠ©äºŽè§£å†³åŒå‡»æœ‰æ—¶æœªè¢«è¯†åˆ«çš„é—®é¢˜ã€‚\n\n### æŒ‡é’ˆç‚¹å‡»æ¼‚ç§»å»¶è¿Ÿ\n\n*   **é»˜è®¤å€¼**ï¼š150\n*   **åŠŸèƒ½**ï¼šæŒ‰ä¸‹æŒ‡é’ˆæŒ‰é’®åŽï¼Œå¿½ç•¥æŒ‡é’ˆç§»åŠ¨çš„æœ€å¤§æ—¶é—´(æ¯«ç§’)ã€‚æœ‰åŠ©äºŽé˜²æ­¢åœ¨ç‚¹å‡»æ—¶æ„å¤–ç§»åŠ¨é¼ æ ‡ã€‚\n\n### æŒ‡é’ˆç‚¹å‡»æ¼‚ç§»ï¼ˆè·ç¦»ï¼‰\n\n*   **é»˜è®¤å€¼**ï¼š6\n*   **åŠŸèƒ½**ï¼šå¦‚æžœæŒ‡é’ˆåœ¨æŒ‰ä½æŒ‰é’®æ—¶ç§»åŠ¨è¶…è¿‡æ­¤è·ç¦»ï¼Œåˆ™è§†ä¸ºæ‹–åŠ¨(è€Œä¸æ˜¯ç‚¹å‡»)ã€‚æœ‰åŠ©äºŽé˜²æ­¢åœ¨ç‚¹å‡»æ—¶æ„å¤–ç§»åŠ¨é¼ æ ‡\n\n## é‡æ–°è·¯ç”±\n\n### é‡æ–°è·¯ç”±æ ·æ¡åç§»\n\n*   **é»˜è®¤å€¼**ï¼š20\n*   **åŠŸèƒ½**ï¼šç”¨äºŽç¡®å®šé‡è·¯ç”±èŠ‚ç‚¹ä¸¤ä¾§çš„æ›²çº¿çš„å¹³æ»‘ç¨‹åº¦ï¼Œå€¼è¶Šå¤§ï¼Œæ›²çº¿è¶Šå¹³æ»‘ï¼Œå€¼è¶Šå°ï¼Œæ›²çº¿è¶Šå°–é”\n\n![é‡æ–°è·¯ç”±æ ·æ¡åç§»](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/lite-graph/reroute-spline-offset.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/comfy-desktop",
  "markdown": "# ComfyUI æ¡Œé¢åº”ç”¨é€šç”¨è®¾ç½® - ComfyUI\n\n## å¸¸è§„\n\n**çª—å£æ ·å¼**\n\n*   **é»˜è®¤å€¼**: default\n*   **è¯´æ˜Ž**: æŽ§åˆ¶åº”ç”¨çª—å£çš„æ ‡é¢˜æ æ ·å¼\n\n**è‡ªåŠ¨æ›´æ–°æ£€æŸ¥**\n\n*   **é»˜è®¤å€¼**: å¯ç”¨\n*   **è¯´æ˜Ž**: è‡ªåŠ¨æ£€æŸ¥ ComfyUI æ¡Œé¢ç‰ˆçš„æ›´æ–°ï¼Œåœ¨æ›´æ–°å¯åŒæ—¶å°†ä¼šæé†’ä½ è¿›è¡Œæ›´æ–°\n\n**å‘é€åŒ¿åä½¿ç”¨æƒ…å†µç»Ÿè®¡**\n\n*   **é»˜è®¤å€¼**: å¯ç”¨\n*   **è¯´æ˜Ž**: å‘é€åŒ¿åä½¿ç”¨æƒ…å†µç»Ÿè®¡æ•°æ®ï¼Œå¸®åŠ©æ”¹è¿›è½¯ä»¶ ï¼Œå¯¹åº”çš„è®¾ç½®ä¿®æ”¹éœ€è¦é‡å¯æ‰èƒ½ç”Ÿæ•ˆ\n\n## UVï¼ˆåŒ…ç®¡ç†å™¨ï¼‰\n\nè¿™ä¸ªéƒ¨åˆ†ä¸»è¦é’ˆå¯¹ä¸­å›½åœ°åŒºç”¨æˆ·è®¾ç½®ä½¿ç”¨ï¼Œå› ä¸º Desktop ä½¿ç”¨çš„è®¸å¤šåŽŸå§‹é•œåƒéƒ½æ˜¯ä¸­å›½å¢ƒå¤–çš„ï¼Œæ‰€ä»¥å¯¹å›½å†…ç”¨æˆ·è®¿é—®ä¸ä¸€å®šå‹å¥½ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œè®¾ç½®ä½ è‡ªå·±çš„é•œåƒæºï¼Œä»¥æé«˜è®¿é—®é€Ÿåº¦ï¼Œä¿è¯å¯¹åº”åŒ…å¯ä»¥æ­£å¸¸è®¿é—®ä¸‹è½½ã€‚ **Python å®‰è£…é•œåƒ**\n\n*   **é»˜è®¤å€¼**: ç©ºï¼ˆä½¿ç”¨é»˜è®¤æºï¼‰\n*   **è¯´æ˜Ž**:\n    *   ç®¡ç†çš„ Python å®‰è£…åŒ…ä»Ž Astral python-build-standalone é¡¹ç›®ä¸‹è½½\n    *   å¯è®¾ç½®é•œåƒ URL ä½¿ç”¨ä¸åŒçš„ Python å®‰è£…æº\n    *   æä¾›çš„ URL å°†æ›¿æ¢é»˜è®¤çš„ GitHub ä¸‹è½½åœ°å€\n    *   æ”¯æŒä½¿ç”¨ file:// åè®®ä»Žæœ¬åœ°ç›®å½•è¯»å–åˆ†å‘åŒ…\n*   **éªŒè¯**: è‡ªåŠ¨æ£€æŸ¥é•œåƒå¯è¾¾æ€§\n\n**PyPI å®‰è£…é•œåƒ**\n\n*   **é»˜è®¤å€¼**: ç©ºï¼ˆä½¿ç”¨é»˜è®¤æºï¼‰\n*   **è¯´æ˜Ž**: é»˜è®¤çš„ pip åŒ…å®‰è£…é•œåƒæº\n\n**Torchå®‰è£…é•œåƒ**\n\n*   **é»˜è®¤å€¼**: ç©ºï¼ˆä½¿ç”¨é»˜è®¤æºï¼‰\n*   **è¯´æ˜Ž**: PyTorch ä¸“ç”¨çš„ pip å®‰è£…é•œåƒæº"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fbuilt-in-nodes%2Fapi-node%2Fimage%2Frecraft%2Frecraft-style-digital-illustration",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/mask-editor",
  "markdown": "# ComfyUI é®ç½©ç¼–è¾‘å™¨è®¾ç½® - ComfyUI\n\n## ç”»ç¬”è°ƒæ•´\n\n### ç”»ç¬”è°ƒæ•´é€Ÿåº¦å€å¢žå™¨\n\n*   **åŠŸèƒ½**: æŽ§åˆ¶è°ƒæ•´æ—¶ç”»ç¬”å¤§å°å’Œç¡¬åº¦å˜åŒ–çš„é€Ÿåº¦\n*   **è¯´æ˜Ž**: æ›´é«˜çš„å€¼æ„å‘³ç€æ›´å¿«çš„å˜åŒ–\n\n### å°†ç”»ç¬”è°ƒæ•´é”å®šåˆ°ä¸»è½´\n\n*   **åŠŸèƒ½**: å¯ç”¨åŽï¼Œç”»ç¬”è°ƒæ•´å°†ä»…æ ¹æ®æ‚¨ç§»åŠ¨çš„æ–¹å‘å½±å“å¤§å°æˆ–ç¡¬åº¦\n*   **è¯´æ˜Ž**: è¿™ä¸ªåŠŸèƒ½å¯ä»¥è®©ç”¨æˆ·æ›´ç²¾ç¡®åœ°æŽ§åˆ¶ç”»ç¬”å±žæ€§çš„è°ƒæ•´\n\n## æ–°ç¼–è¾‘å™¨\n\n### ä½¿ç”¨æ–°ç”»ç¬”ç¼–è¾‘å™¨\n\n*   **åŠŸèƒ½**: åˆ‡æ¢åˆ°æ–°çš„ç”»ç¬”ç¼–è¾‘å™¨ç•Œé¢\n*   **è¯´æ˜Ž**: å…è®¸ç”¨æˆ·åœ¨æ–°æ—§ç¼–è¾‘å™¨ç•Œé¢ä¹‹é—´åˆ‡æ¢\n\næ–°ç‰ˆæœ¬å…·æœ‰æ›´å¥½çš„ UI ç•Œé¢å’Œäº¤äº’ï¼ŒåŠŸèƒ½ä¼šæ›´åŠ å®Œæ•´ ![new](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/maskeditor/new-mask-editor.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/extension",
  "markdown": "# æ‰©å±•è®¾ç½® - ComfyUI\n\næ‰©å±•è®¾ç½®é¢æ¿æ˜¯ ComfyUI å‰ç«¯è®¾ç½®ç³»ç»Ÿä¸­çš„ä¸€ä¸ªç‰¹æ®Šç®¡ç†é¢æ¿ï¼Œä¸“é—¨ç”¨äºŽç®¡ç†å‰ç«¯æ‰©å±•æ’ä»¶çš„å¯ç”¨/ç¦ç”¨çŠ¶æ€ï¼ŒåŒºåˆ«äºŽè‡ªå®šä¹‰èŠ‚ç‚¹ï¼ˆCustom Nodeï¼‰ï¼Œè¿™ä¸ªé¢æ¿åªæ˜¯ç”¨äºŽç®¡ç†è‡ªå®šä¹‰èŠ‚ç‚¹æ³¨å†Œçš„å‰ç«¯æ‰©å±•ï¼Œè€Œä¸æ˜¯ç¦ç”¨è‡ªå®šä¹‰èŠ‚ç‚¹ã€‚ ![extension](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/extension/extension.jpg) è¿™äº›å‰ç«¯æ‰©å±•æ’ä»¶æ˜¯ç”¨äºŽå¢žå¼º ComfyUI çš„ä½“éªŒï¼Œæ¯”å¦‚æä¾›å¿«æ·é”®ã€è®¾ç½®ã€UI ç»„ä»¶ã€èœå•é¡¹ç­‰åŠŸèƒ½ã€‚ æ‰©å±•çŠ¶æ€æ›´æ”¹åŽéœ€è¦é‡æ–°åŠ è½½é¡µé¢æ‰èƒ½ç”Ÿæ•ˆï¼š\n\n## Extension è®¾ç½®é¢æ¿åŠŸèƒ½\n\n### 1\\. æ‰©å±•åˆ—è¡¨ç®¡ç†\n\næ˜¾ç¤ºæ‰€æœ‰å·²æ³¨å†Œçš„æ‰©å±•ï¼ŒåŒ…æ‹¬ï¼š\n\n*   æ‰©å±•åç§°\n*   æ ¸å¿ƒæ‰©å±•æ ‡è¯†ï¼ˆæ˜¾ç¤º â€œCoreâ€ æ ‡ç­¾ï¼‰\n*   å¯ç”¨/ç¦ç”¨çŠ¶æ€\n\n### 2\\. æœç´¢åŠŸèƒ½\n\næä¾›æœç´¢æ¡†å¿«é€ŸæŸ¥æ‰¾ç‰¹å®šæ‰©å±•ï¼š\n\n### 3\\. å¯ç”¨/ç¦ç”¨æŽ§åˆ¶\n\næ¯ä¸ªæ‰©å±•éƒ½æœ‰ç‹¬ç«‹çš„åˆ‡æ¢å¼€å…³ï¼š\n\n### 4\\. æ‰¹é‡æ“ä½œ\n\næä¾›å³é”®èœå•è¿›è¡Œæ‰¹é‡æ“ä½œï¼š\n\n*   å¯ç”¨æ‰€æœ‰æ‰©å±•\n*   ç¦ç”¨æ‰€æœ‰æ‰©å±•\n*   ç¦ç”¨ç¬¬ä¸‰æ–¹æ‰©å±•ï¼ˆä¿ç•™æ ¸å¿ƒæ‰©å±•ï¼‰\n\n## æ³¨æ„äº‹é¡¹\n\n*   æ‰©å±•çŠ¶æ€æ›´æ”¹éœ€è¦é‡æ–°åŠ è½½é¡µé¢æ‰èƒ½ç”Ÿæ•ˆ\n*   æŸäº›æ ¸å¿ƒæ‰©å±•æ— æ³•è¢«ç¦ç”¨\n*   ç³»ç»Ÿä¼šè‡ªåŠ¨ç¦ç”¨å·²çŸ¥æœ‰é—®é¢˜çš„æ‰©å±•\n*   æ‰©å±•è®¾ç½®ä¼šè‡ªåŠ¨ä¿å­˜åˆ°ç”¨æˆ·é…ç½®æ–‡ä»¶ä¸­\n\nè¿™ä¸ª Extension è®¾ç½®é¢æ¿æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªâ€å‰ç«¯æ’ä»¶ç®¡ç†å™¨â€ï¼Œè®©ç”¨æˆ·å¯ä»¥çµæ´»æŽ§åˆ¶ ComfyUI çš„åŠŸèƒ½æ¨¡å—ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/shortcuts",
  "markdown": "# ComfyUI çš„å¿«æ·é”®åŠè‡ªå®šä¹‰è®¾ç½® - ComfyUI\n\nç›®å‰ ComfyUI å·²ç»æ”¯æŒå¿«æ·é”®è‡ªå®šä¹‰ï¼Œä½ å¯ä»¥åœ¨ç‚¹å‡» `è®¾ç½®ï¼ˆé½¿è½®å›¾æ ‡ï¼‰` â€”> `å¿«æ·é”®` ä¸­è¿›è¡Œå¿«æ·é”®çš„è®¾ç½®ã€‚ ![ComfyUI å¿«æ·é”®è®¾ç½®](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/setting/keybinding.jpg) åœ¨å¯¹åº”èœå•ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°ç›®å‰ ComfyUI æ‰€æœ‰çš„å¿«æ·é”®è®¾ç½®ï¼Œç‚¹å‡»å¯¹åº”å‘½ä»¤ä¹‹å‰çš„`ç¼–è¾‘å›¾æ ‡`ï¼Œå°±å¯ä»¥å¯¹å¿«æ·é”®è¿›è¡Œè‡ªå®šä¹‰ã€‚\n\n| å¿«æ·é”® | å‘½ä»¤  |\n| --- | --- |\n| Ctrl + Enter | æ‰§è¡Œæç¤ºè¯ |\n| Ctrl + Shift + Enter | æ‰§è¡Œæç¤ºè¯ï¼ˆå‰ç«¯ï¼‰ |\n| Ctrl + Alt + Enter | ä¸­æ–­  |\n| Ctrl + Z / Ctrl + Y | æ’¤é”€/é‡åš |\n| Ctrl + S | ä¿å­˜å·¥ä½œæµ |\n| Ctrl + O | åŠ è½½å·¥ä½œæµ |\n| Ctrl + A | é€‰æ‹©æ‰€æœ‰èŠ‚ç‚¹ |\n| Alt + C | æŠ˜å /å±•å¼€é€‰å®šèŠ‚ç‚¹ |\n| Ctrl + M | é™éŸ³/å–æ¶ˆé™éŸ³é€‰å®šèŠ‚ç‚¹ |\n| Ctrl + B | å¿½ç•¥/å–æ¶ˆå¿½ç•¥é€‰å®šèŠ‚ç‚¹ |\n| Delete  <br>Backspace | åˆ é™¤é€‰å®šèŠ‚ç‚¹ |\n| Backspace | æ¸…é™¤å·¥ä½œæµ |\n| Space | æŒ‰ä½å¹¶ç§»åŠ¨å…‰æ ‡æ—¶ç§»åŠ¨ç”»å¸ƒ |\n| Ctrl + Click  <br>Shift + Click | å°†ç‚¹å‡»çš„èŠ‚ç‚¹æ·»åŠ åˆ°é€‰æ‹©ä¸­ |\n| Ctrl + C/Ctrl + V | å¤åˆ¶å¹¶ç²˜è´´é€‰å®šèŠ‚ç‚¹ï¼ˆä¸ä¿æŒä¸Žæœªé€‰å®šèŠ‚ç‚¹è¾“å‡ºçš„è¿žæŽ¥ï¼‰ |\n| Ctrl + C/Ctrl + Shift + V | å¤åˆ¶å¹¶ç²˜è´´é€‰å®šèŠ‚ç‚¹ï¼ˆä¿æŒæœªé€‰å®šèŠ‚ç‚¹è¾“å‡ºåˆ°ç²˜è´´èŠ‚ç‚¹è¾“å…¥çš„è¿žæŽ¥ï¼‰ |\n| Shift + Drag | åŒæ—¶ç§»åŠ¨å¤šä¸ªé€‰å®šèŠ‚ç‚¹ |\n| Ctrl + G | æ·»åŠ æ¡†åˆ°é€‰ä¸­èŠ‚ç‚¹ |\n| Ctrl + , | æ˜¾ç¤ºè®¾ç½®å¯¹è¯æ¡† |\n| Alt + = | æ”¾å¤§ï¼ˆç”»å¸ƒï¼‰ |\n| Alt + - | ç¼©å°ï¼ˆç”»å¸ƒï¼‰ |\n| .   | é€‚åº”è§†å›¾åˆ°é€‰ä¸­èŠ‚ç‚¹ |\n| P   | å›ºå®š/å–æ¶ˆå›ºå®šé€‰ä¸­é¡¹ |\n| Q   | åˆ‡æ¢æ‰§è¡Œé˜Ÿåˆ—ä¾§è¾¹æ  |\n| W   | åˆ‡æ¢å·¥ä½œæµä¾§è¾¹æ  |\n| N   | åˆ‡æ¢èŠ‚ç‚¹åº“ä¾§è¾¹æ  |\n| M   | åˆ‡æ¢æ¨¡åž‹åº“ä¾§è¾¹æ  |\n| Ctrl + \\` | åˆ‡æ¢æ—¥å¿—åº•éƒ¨é¢æ¿ |\n| F   | åˆ‡æ¢ç„¦ç‚¹æ¨¡å¼ï¼ˆå…¨å±ï¼‰ |\n| R   | åˆ·æ–°èŠ‚ç‚¹å®šä¹‰ |\n| åŒå‡»å·¦é”® | å¿«é€Ÿæœç´¢è¦æ·»åŠ çš„èŠ‚ç‚¹ |"
},
{
  "url": "https://docs.comfy.org/zh-CN/interface/settings/server-config",
  "markdown": "# æœåŠ¡å™¨é…ç½® - ComfyUI\n\n## ç½‘ç»œ\n\n### ä¸»æœºï¼šè¦ç›‘å¬çš„IPåœ°å€\n\n*   **ä¸»æœºåœ°å€ (listen)**:\n*   **åŠŸèƒ½**ï¼šè®¾ç½®æœåŠ¡å™¨ç»‘å®šçš„IPåœ°å€ã€‚é»˜è®¤ `127.0.0.1` è¡¨ç¤ºåªå…è®¸æœ¬åœ°è®¿é—®ï¼Œå¦‚æžœéœ€è¦å±€åŸŸç½‘è®¿é—®å¯è®¾ç½®ä¸º `0.0.0.0`\n\n### ç«¯å£ç«¯å£ï¼šè¦ç›‘å¬çš„ç«¯å£\n\n**åŠŸèƒ½**ï¼šæœåŠ¡å™¨ç›‘å¬çš„ç«¯å£å·ã€‚æ¡Œé¢ç‰ˆé»˜è®¤8000ç«¯å£ï¼ŒWebç‰ˆé€šå¸¸ä½¿ç”¨8188ç«¯å£\n\n### TTLS å¯†é’¥æ–‡ä»¶ï¼šHTTPS çš„ TLS å¯†é’¥æ–‡ä»¶è·¯å¾„\n\n**åŠŸèƒ½**ï¼šHTTPSåŠ å¯†æ‰€éœ€çš„ç§é’¥æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºŽå»ºç«‹å®‰å…¨è¿žæŽ¥\n\n### TLS è¯ä¹¦æ–‡ä»¶ï¼šHTTPS çš„ TLS è¯ä¹¦æ–‡ä»¶è·¯å¾„\n\n**åŠŸèƒ½**ï¼šHTTPSåŠ å¯†æ‰€éœ€çš„è¯ä¹¦æ–‡ä»¶è·¯å¾„ï¼Œä¸Žç§é’¥é…åˆä½¿ç”¨\n\n### æœ€å¤§ä¸Šä¼ å¤§å°\n\n*   **æœ€å¤§ä¸Šä¼ å¤§å° (max-upload-size)**:\n*   **åŠŸèƒ½**ï¼šé™åˆ¶å•ä¸ªæ–‡ä»¶ä¸Šä¼ çš„æœ€å¤§å°ºå¯¸ï¼Œå•ä½ä¸ºMBï¼Œé»˜è®¤100MBã€‚å½±å“å›¾ç‰‡ã€æ¨¡åž‹ç­‰æ–‡ä»¶çš„ä¸Šä¼ é™åˆ¶ é™åˆ¶å•ä¸ªæ–‡ä»¶ä¸Šä¼ çš„æœ€å¤§å°ºå¯¸ï¼Œå•ä½ä¸ºMBï¼Œé»˜è®¤100MBã€‚å½±å“å›¾ç‰‡ã€æ¨¡åž‹ç­‰æ–‡ä»¶çš„ä¸Šä¼ é™åˆ¶\n\n## CUDA\n\n### è¦ä½¿ç”¨çš„ CUDA è®¾å¤‡ç´¢å¼•\n\n**åŠŸèƒ½**ï¼šæŒ‡å®šä½¿ç”¨å“ªå—NVIDIAæ˜¾å¡ã€‚0è¡¨ç¤ºç¬¬ä¸€å—æ˜¾å¡ï¼Œ1è¡¨ç¤ºç¬¬äºŒå—ï¼Œä»¥æ­¤ç±»æŽ¨ã€‚å¯¹å¤šGPUç³»ç»Ÿå¾ˆé‡è¦\n\n### ä½¿ç”¨ CUDA malloc è¿›è¡Œå†…å­˜åˆ†é…\n\n**åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¯å¦ä½¿ç”¨CUDAçš„å†…å­˜åˆ†é…å™¨ã€‚å¯ä»¥æ”¹å–„æŸäº›æƒ…å†µä¸‹çš„å†…å­˜ç®¡ç†æ•ˆçŽ‡\n\n## æŽ¨ç†\n\n### å…¨å±€æµ®ç‚¹ç²¾åº¦\n\n**åŠŸèƒ½**ï¼šè®¾ç½®æ¨¡åž‹è®¡ç®—çš„æ•°å€¼ç²¾åº¦ã€‚FP16èŠ‚çœæ˜¾å­˜ä½†å¯èƒ½å½±å“è´¨é‡ï¼ŒFP32æ›´ç²¾ç¡®ä½†å ç”¨æ›´å¤šæ˜¾å­˜\n\n### UNET ç²¾åº¦\n\n**é€‰é¡¹**ï¼š\n\n*   `auto`ï¼šè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ç²¾åº¦\n*   `fp64`ï¼š64ä½æµ®ç‚¹ç²¾åº¦ï¼Œç²¾åº¦æœ€é«˜ä½†æ˜¾å­˜å ç”¨æœ€å¤§\n*   `fp32`ï¼š32ä½æµ®ç‚¹ç²¾åº¦ï¼Œæ ‡å‡†ç²¾åº¦\n*   `fp16`ï¼š16ä½æµ®ç‚¹ç²¾åº¦ï¼Œå¯èŠ‚çœæ˜¾å­˜\n*   `bf16`ï¼š16ä½brainæµ®ç‚¹ç²¾åº¦ï¼Œä»‹äºŽfp16å’Œfp32ä¹‹é—´\n*   `fp8_e4m3fn`ï¼š8ä½æµ®ç‚¹ç²¾åº¦(e4m3)ï¼Œæ˜¾å­˜å ç”¨æœ€å°\n*   `fp8_e5m2`ï¼š8ä½æµ®ç‚¹ç²¾åº¦(e5m2)ï¼Œæ˜¾å­˜å ç”¨æœ€å°\n\n**åŠŸèƒ½**ï¼šä¸“é—¨æŽ§åˆ¶æ‰©æ•£æ¨¡åž‹æ ¸å¿ƒç»„ä»¶UNETçš„è®¡ç®—ç²¾åº¦ã€‚æ›´é«˜çš„ç²¾åº¦å¯ä»¥æä¾›æ›´å¥½çš„å›¾åƒç”Ÿæˆè´¨é‡ï¼Œä½†ä¼šå ç”¨æ›´å¤šæ˜¾å­˜ã€‚è¾ƒä½Žçš„ç²¾åº¦å¯ä»¥æ˜¾è‘—èŠ‚çœæ˜¾å­˜ï¼Œä½†å¯èƒ½ä¼šå½±å“ç”Ÿæˆç»“æžœçš„è´¨é‡ã€‚\n\n### VAE ç²¾åº¦\n\n**é€‰é¡¹ä¸Žå»ºè®®**ï¼š\n\n*   `auto`ï¼šè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ç²¾åº¦ï¼ŒæŽ¨è8-12GBæ˜¾å­˜çš„ç”¨æˆ·ä½¿ç”¨\n*   `fp16`ï¼š16ä½æµ®ç‚¹ç²¾åº¦ï¼ŒæŽ¨è6GBåŠä»¥ä¸‹æ˜¾å­˜çš„ç”¨æˆ·ä½¿ç”¨ï¼Œå¯èŠ‚çœæ˜¾å­˜ä½†å¯èƒ½å½±å“è´¨é‡\n*   `fp32`ï¼š32ä½æµ®ç‚¹ç²¾åº¦ï¼ŒæŽ¨è16GBåŠä»¥ä¸Šæ˜¾å­˜ä¸”è¿½æ±‚æœ€ä½³è´¨é‡çš„ç”¨æˆ·ä½¿ç”¨\n*   `bf16`ï¼š16ä½brainæµ®ç‚¹ç²¾åº¦ï¼ŒæŽ¨èæ”¯æŒæ­¤æ ¼å¼çš„æ–°åž‹æ˜¾å¡ä½¿ç”¨ï¼Œå¯èŽ·å¾—æ›´å¥½çš„æ€§èƒ½å¹³è¡¡\n\n**åŠŸèƒ½**ï¼šæŽ§åˆ¶å˜åˆ†è‡ªç¼–ç å™¨(VAE)çš„è®¡ç®—ç²¾åº¦ï¼Œå½±å“å›¾åƒç¼–ç /è§£ç çš„è´¨é‡å’Œé€Ÿåº¦ã€‚æ›´é«˜çš„ç²¾åº¦å¯ä»¥æä¾›æ›´å¥½çš„å›¾åƒé‡å»ºè´¨é‡ï¼Œä½†ä¼šå ç”¨æ›´å¤šæ˜¾å­˜ã€‚è¾ƒä½Žçš„ç²¾åº¦å¯ä»¥èŠ‚çœæ˜¾å­˜ï¼Œä½†å¯èƒ½ä¼šå½±å“å›¾åƒçš„ç»†èŠ‚è¿˜åŽŸã€‚\n\n### æ–‡æœ¬ç¼–ç å™¨ç²¾åº¦\n\n**é€‰é¡¹**ï¼š\n\n*   `auto`ï¼šè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ç²¾åº¦\n*   `fp8_e4m3fn`ï¼š8ä½æµ®ç‚¹ç²¾åº¦(e4m3)ï¼Œæ˜¾å­˜å ç”¨æœ€å°\n*   `fp8_e5m2`ï¼š8ä½æµ®ç‚¹ç²¾åº¦(e5m2)ï¼Œæ˜¾å­˜å ç”¨æœ€å°\n*   `fp16`ï¼š16ä½æµ®ç‚¹ç²¾åº¦ï¼Œå¯èŠ‚çœæ˜¾å­˜\n*   `fp32`ï¼š32ä½æµ®ç‚¹ç²¾åº¦ï¼Œæ ‡å‡†ç²¾åº¦\n\n**åŠŸèƒ½**ï¼šæŽ§åˆ¶æ–‡æœ¬æç¤ºè¯ç¼–ç å™¨çš„è®¡ç®—ç²¾åº¦ï¼Œå½±å“æ–‡æœ¬ç†è§£çš„å‡†ç¡®æ€§å’Œæ˜¾å­˜å ç”¨ã€‚æ›´é«˜çš„ç²¾åº¦å¯ä»¥æä¾›æ›´å‡†ç¡®çš„æ–‡æœ¬ç†è§£ï¼Œä½†ä¼šå ç”¨æ›´å¤šæ˜¾å­˜ã€‚è¾ƒä½Žçš„ç²¾åº¦å¯ä»¥èŠ‚çœæ˜¾å­˜ï¼Œä½†å¯èƒ½ä¼šå½±å“æç¤ºè¯çš„è§£æžæ•ˆæžœã€‚\n\n## å†…å­˜\n\n### å¼ºåˆ¶ä½¿ç”¨ channels-last å†…å­˜æ ¼å¼\n\n**åŠŸèƒ½**ï¼šæ”¹å˜å†…å­˜ä¸­æ•°æ®çš„æŽ’åˆ—æ–¹å¼ï¼Œå¯èƒ½æå‡æŸäº›ç¡¬ä»¶ä¸Šçš„æ€§èƒ½\n\n### DirectML è®¾å¤‡ç´¢å¼•\n\n**åŠŸèƒ½**ï¼šåœ¨Windowsä¸Šä½¿ç”¨DirectMLåŠ é€Ÿæ—¶æŒ‡å®šè®¾å¤‡ï¼Œä¸»è¦ç”¨äºŽAMDæ˜¾å¡\n\n### ç¦ç”¨IPEXä¼˜åŒ–\n\n**åŠŸèƒ½**ï¼šå…³é—­Intel CPUä¼˜åŒ–ï¼Œä¸»è¦å½±å“Intelå¤„ç†å™¨çš„æ€§èƒ½\n\n### VRAM ç®¡ç†æ¨¡å¼\n\n**é€‰é¡¹**ï¼š\n\n*   `auto`ï¼šè‡ªåŠ¨ç®¡ç†æ˜¾å­˜ï¼Œæ ¹æ®æ¨¡åž‹å¤§å°å’Œéœ€æ±‚è‡ªåŠ¨åˆ†é…æ˜¾å­˜\n*   `lowvram`ï¼šä½Žæ˜¾å­˜æ¨¡å¼ï¼Œåªä½¿ç”¨æœ€ä½Žé™åº¦çš„æ˜¾å­˜ï¼Œå¯èƒ½ä¼šå½±å“ç”Ÿæˆè´¨é‡\n*   `normalvram`ï¼šæ ‡å‡†æ˜¾å­˜æ¨¡å¼ï¼Œå¹³è¡¡æ˜¾å­˜ä½¿ç”¨å’Œæ€§èƒ½\n*   `highvram`ï¼šé«˜æ˜¾å­˜æ¨¡å¼ï¼Œä½¿ç”¨è¾ƒå¤šæ˜¾å­˜ä»¥èŽ·å¾—æ›´å¥½æ€§èƒ½\n*   `novram`ï¼šä¸ä½¿ç”¨æ˜¾å­˜ï¼Œå®Œå…¨ä½¿ç”¨ç³»ç»Ÿå†…å­˜è¿è¡Œ\n*   `cpu`ï¼šä»…ä½¿ç”¨CPUè¿è¡Œï¼Œä¸ä½¿ç”¨æ˜¾å¡\n\n**åŠŸèƒ½**ï¼šæŽ§åˆ¶æ˜¾å­˜çš„ä½¿ç”¨ç­–ç•¥ï¼Œå¦‚è‡ªåŠ¨ç®¡ç†ã€ä½Žæ˜¾å­˜æ¨¡å¼ç­‰\n\n### ä¿ç•™VRAM\n\n**åŠŸèƒ½**ï¼šä¸ºæ“ä½œç³»ç»Ÿå’Œå…¶ä»–ç¨‹åºé¢„ç•™çš„æ˜¾å­˜é‡ï¼Œé˜²æ­¢ç³»ç»Ÿå¡æ­»\n\n### ç¦ç”¨æ™ºèƒ½å†…å­˜ç®¡ç†\n\n**åŠŸèƒ½**ï¼šå…³é—­è‡ªåŠ¨å†…å­˜ä¼˜åŒ–ï¼Œå¼ºåˆ¶å°†æ¨¡åž‹ç§»åˆ°ç³»ç»Ÿå†…å­˜ä»¥é‡Šæ”¾æ˜¾å­˜\n\n### CPU è¿è¡Œ VAE\n\n**åŠŸèƒ½**ï¼šå¼ºåˆ¶VAEåœ¨CPUä¸Šè¿è¡Œï¼Œå¯ä»¥èŠ‚çœæ˜¾å­˜ä½†ä¼šé™ä½Žå¤„ç†é€Ÿåº¦\n\n## é¢„è§ˆ\n\n### ç”¨äºŽæ½œç©ºé—´é¢„è§ˆçš„æ–¹æ³•\n\n**é€‰é¡¹**:\n\n*   `none`: ä¸æ˜¾ç¤ºé¢„è§ˆå›¾åƒ,ç”Ÿæˆè¿‡ç¨‹ä¸­åªæ˜¾ç¤ºè¿›åº¦æ¡\n*   `auto`: è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„é¢„è§ˆæ–¹æ³•,æ ¹æ®ç³»ç»Ÿæ€§èƒ½å’Œæ˜¾å­˜æƒ…å†µåŠ¨æ€è°ƒæ•´\n*   `latent2rgb`: ç›´æŽ¥å°†æ½œç©ºé—´æ•°æ®è½¬æ¢ä¸ºRGBå›¾åƒè¿›è¡Œé¢„è§ˆ,é€Ÿåº¦è¾ƒå¿«ä½†è´¨é‡ä¸€èˆ¬\n*   `taesd`: ä½¿ç”¨è½»é‡çº§çš„TAESDæ¨¡åž‹è¿›è¡Œé¢„è§ˆ,åœ¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡\n\n**åŠŸèƒ½**: æŽ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä¸­å¦‚ä½•é¢„è§ˆä¸­é—´ç»“æžœã€‚ä¸åŒçš„é¢„è§ˆæ–¹æ³•ä¼šå½±å“é¢„è§ˆçš„è´¨é‡å’Œæ€§èƒ½æ¶ˆè€—ã€‚é€‰æ‹©åˆé€‚çš„é¢„è§ˆæ–¹æ³•å¯ä»¥åœ¨é¢„è§ˆæ•ˆæžœå’Œç³»ç»Ÿèµ„æºå ç”¨ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚\n\n### é¢„è§ˆå›¾åƒå¤§å°\n\n**åŠŸèƒ½**ï¼šè®¾ç½®é¢„è§ˆå›¾åƒçš„åˆ†è¾¨çŽ‡ï¼Œå½±å“é¢„è§ˆæ¸…æ™°åº¦å’Œæ€§èƒ½ï¼Œå°ºå¯¸è¶Šå¤§ï¼Œé¢„è§ˆè´¨é‡è¶Šé«˜ï¼Œä½†ä¹Ÿä¼šå ç”¨æ›´å¤šæ˜¾å­˜\n\n### ç®—æ³•ä¼˜åŒ–\n\n*   **ç¡®å®šæ€§ç®—æ³• (deterministic)**: å¯ç”¨åŽä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ï¼Œç›¸åŒè¾“å…¥ä¼šäº§ç”Ÿç›¸åŒè¾“å‡ºï¼Œä½†è®¡ç®—é€Ÿåº¦è¾ƒæ…¢\n*   **å¿«é€Ÿæ¨¡å¼ (fast)**: å¯ç”¨å®žéªŒæ€§ä¼˜åŒ–ï¼Œå¯èƒ½æå‡é€Ÿåº¦ä½†å¯èƒ½å½±å“ç”Ÿæˆè´¨é‡\n\n## ç¼“å­˜\n\n### ç»å…¸ç¼“å­˜ç³»ç»Ÿ\n\n**åŠŸèƒ½**ï¼šä½¿ç”¨ä¼ ç»Ÿçš„ç¼“å­˜ç­–ç•¥ï¼Œæ›´ä¿å®ˆä½†ç¨³å®š\n\n### ä½¿ç”¨ LRU ç¼“å­˜ï¼Œæœ€å¤šç¼“å­˜ N ä¸ªèŠ‚ç‚¹ç»“æžœ\n\n**åŠŸèƒ½**ï¼šä½¿ç”¨æœ€è¿‘æœ€å°‘ä½¿ç”¨(Least Recently Used)ç®—æ³•çš„ç¼“å­˜ç³»ç»Ÿï¼Œå¯ä»¥ç¼“å­˜æŒ‡å®šæ•°é‡çš„èŠ‚ç‚¹è®¡ç®—ç»“æžœ **è¯´æ˜Ž**:\n\n*   é€šè¿‡è®¾ç½®ä¸€ä¸ªå…·ä½“çš„æ•°å­—æ¥æŽ§åˆ¶æœ€å¤§ç¼“å­˜æ•°é‡ï¼Œå¦‚ 10ã€50ã€100 ç­‰\n*   ç¼“å­˜å¯ä»¥é¿å…é‡å¤è®¡ç®—ç›¸åŒçš„èŠ‚ç‚¹æ“ä½œï¼Œæé«˜å·¥ä½œæµæ‰§è¡Œé€Ÿåº¦\n*   å½“ç¼“å­˜è¾¾åˆ°ä¸Šé™æ—¶ï¼Œä¼šè‡ªåŠ¨æ¸…é™¤æœ€ä¹…æœªä½¿ç”¨çš„ç»“æžœ\n*   ç¼“å­˜çš„ç»“æžœä¼šå ç”¨ç³»ç»Ÿå†…å­˜(RAM/VRAM)ï¼Œæ•°å€¼è¶Šå¤§å ç”¨è¶Šå¤š\n\n**ä½¿ç”¨å»ºè®®**:\n\n*   é»˜è®¤å€¼ä¸º nullï¼Œè¡¨ç¤ºä¸å¯ç”¨ LRU ç¼“å­˜\n*   æ ¹æ®ç³»ç»Ÿå†…å­˜å®¹é‡å’Œä½¿ç”¨éœ€æ±‚è®¾ç½®åˆé€‚çš„ç¼“å­˜æ•°é‡\n*   å¯¹äºŽç»å¸¸é‡å¤ä½¿ç”¨ç›¸åŒèŠ‚ç‚¹é…ç½®çš„å·¥ä½œæµï¼Œå»ºè®®å¯ç”¨æ­¤åŠŸèƒ½\n*   å¦‚æžœç³»ç»Ÿå†…å­˜å……è¶³ï¼Œå¯ä»¥è®¾ç½®è¾ƒå¤§çš„æ•°å€¼ä»¥èŽ·å¾—æ›´å¥½çš„æ€§èƒ½æå‡\n\n## æ³¨æ„åŠ›\n\n### äº¤å‰æ³¨æ„åŠ›æ–¹æ³•\n\n**é€‰é¡¹**:\n\n*   `auto`: è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•\n*   `split`: åˆ†å—è®¡ç®—æ³¨æ„åŠ›,å¯ä»¥èŠ‚çœæ˜¾å­˜ä½†é€Ÿåº¦è¾ƒæ…¢\n*   `quad`: ä½¿ç”¨å››åˆ†æ³¨æ„åŠ›ç®—æ³•,åœ¨é€Ÿåº¦å’Œæ˜¾å­˜ä½¿ç”¨ä¸Šå–å¾—å¹³è¡¡\n*   `pytorch`: ä½¿ç”¨PyTorchåŽŸç”Ÿæ³¨æ„åŠ›è®¡ç®—,é€Ÿåº¦è¾ƒå¿«ä½†æ˜¾å­˜å ç”¨å¤§\n\n**åŠŸèƒ½**: æŽ§åˆ¶æ¨¡åž‹è®¡ç®—æ³¨æ„åŠ›æ—¶ä½¿ç”¨çš„å…·ä½“ç®—æ³•ã€‚ä¸åŒçš„ç®—æ³•ä¼šåœ¨ç”Ÿæˆè´¨é‡ã€é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨ä¹‹é—´åšå‡ºä¸åŒçš„æƒè¡¡ã€‚é€šå¸¸å»ºè®®ä½¿ç”¨autoè‡ªåŠ¨é€‰æ‹©ã€‚\n\n*   **å¼ºåˆ¶upcast-attention (force-upcast-attention)**: å¼ºåˆ¶ä½¿ç”¨é«˜ç²¾åº¦è®¡ç®—æ³¨æ„åŠ›ï¼Œæå‡è´¨é‡ä½†å¢žåŠ æ˜¾å­˜ä½¿ç”¨\n*   **ç¦ç”¨upcast-attention (dont-upcast-attention)**: ç¦ç”¨é«˜ç²¾åº¦æ³¨æ„åŠ›è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜\n\n## å¸¸è§„\n\n### ç¦ç”¨xFormersä¼˜åŒ–\n\n**åŠŸèƒ½**ï¼šå…³é—­ xFormers åº“çš„ä¼˜åŒ–åŠŸèƒ½ã€‚xFormers æ˜¯ä¸€ä¸ªä¸“é—¨ä¼˜åŒ– Transformer æ¨¡åž‹æ³¨æ„åŠ›æœºåˆ¶çš„åº“ï¼Œé€šå¸¸å¯ä»¥æé«˜è®¡ç®—æ•ˆçŽ‡ã€å‡å°‘å†…å­˜ä½¿ç”¨å¹¶åŠ å¿«æŽ¨ç†é€Ÿåº¦ã€‚ç¦ç”¨æ­¤ä¼˜åŒ–åŽä¼šï¼š\n\n*   å›žé€€åˆ°æ ‡å‡†çš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•\n*   å¯èƒ½å¢žåŠ å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ—¶é—´\n*   åœ¨æŸäº›æƒ…å†µä¸‹æä¾›æ›´ç¨³å®šçš„è¿è¡ŒçŽ¯å¢ƒ\n\n**ä½¿ç”¨åœºæ™¯**ï¼š\n\n*   é‡åˆ°ä¸Ž xFormers ç›¸å…³çš„å…¼å®¹æ€§é—®é¢˜æ—¶\n*   éœ€è¦æ›´ç²¾ç¡®çš„è®¡ç®—ç»“æžœæ—¶ï¼ˆæŸäº›ä¼˜åŒ–å¯èƒ½å½±å“æ•°å€¼ç²¾åº¦ï¼‰\n*   åœ¨è°ƒè¯•æˆ–æŽ’æŸ¥é—®é¢˜æ—¶éœ€è¦ä½¿ç”¨æ ‡å‡†å®žçŽ°\n\n### æ¨¡åž‹æ–‡ä»¶çš„é»˜è®¤å“ˆå¸Œå‡½æ•°\n\n**é€‰é¡¹**:\n\n*   `sha256`: ä½¿ç”¨ SHA-256 ç®—æ³•è¿›è¡Œå“ˆå¸Œæ ¡éªŒ,å®‰å…¨æ€§é«˜ä½†è®¡ç®—è¾ƒæ…¢\n*   `sha1`: ä½¿ç”¨ SHA-1 ç®—æ³•,é€Ÿåº¦è¾ƒå¿«ä½†å®‰å…¨æ€§ç¨ä½Ž\n*   `sha512`: ä½¿ç”¨ SHA-512 ç®—æ³•,æä¾›æœ€é«˜å®‰å…¨æ€§ä½†è®¡ç®—æœ€æ…¢\n*   `md5`: ä½¿ç”¨ MD5 ç®—æ³•,é€Ÿåº¦æœ€å¿«ä½†å®‰å…¨æ€§è¾ƒä½Ž\n\n**åŠŸèƒ½**ï¼šè®¾ç½®æ¨¡åž‹æ–‡ä»¶æ ¡éªŒçš„å“ˆå¸Œç®—æ³•ï¼Œç”¨äºŽéªŒè¯æ–‡ä»¶å®Œæ•´æ€§ã€‚ä¸åŒçš„å“ˆå¸Œç®—æ³•åœ¨è®¡ç®—é€Ÿåº¦å’Œå®‰å…¨æ€§ä¹‹é—´æœ‰ä¸åŒçš„æƒè¡¡ã€‚é€šå¸¸å»ºè®®ä½¿ç”¨ sha256 ä½œä¸ºé»˜è®¤é€‰é¡¹ï¼Œå®ƒèƒ½åœ¨å®‰å…¨æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—è¾ƒå¥½çš„å¹³è¡¡ã€‚\n\n### ä½¿ pytorch åœ¨å¯ä»¥æ—¶ä½¿ç”¨è¾ƒæ…¢çš„ç¡®å®šæ€§ç®—æ³•\n\n**åŠŸèƒ½**: å¼ºåˆ¶ PyTorch åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ä½¿ç”¨ç¡®å®šæ€§ç®—æ³•ï¼Œä»¥æé«˜ç»“æžœçš„å¯é‡çŽ°æ€§ã€‚ **è¯´æ˜Ž**:\n\n*   å¯ç”¨åŽ PyTorch ä¼šä¼˜å…ˆä½¿ç”¨ç¡®å®šæ€§ç®—æ³•è€Œä¸æ˜¯æ›´å¿«çš„éžç¡®å®šæ€§ç®—æ³•\n*   ç›¸åŒçš„è¾“å…¥å°†äº§ç”Ÿç›¸åŒçš„è¾“å‡ºï¼Œæœ‰åŠ©äºŽè°ƒè¯•å’Œç»“æžœéªŒè¯\n*   ç¡®å®šæ€§ç®—æ³•é€šå¸¸æ¯”éžç¡®å®šæ€§ç®—æ³•è¿è¡Œæ›´æ…¢\n*   å³ä½¿å¯ç”¨æ­¤è®¾ç½®ï¼Œä¹Ÿä¸èƒ½å®Œå…¨ä¿è¯åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½èƒ½äº§ç”Ÿå®Œå…¨ç›¸åŒçš„å›¾åƒç»“æžœ\n\n**ä½¿ç”¨åœºæ™¯**:\n\n*   ç§‘å­¦ç ”ç©¶éœ€è¦ä¸¥æ ¼çš„ç»“æžœå¯é‡çŽ°æ€§\n*   è°ƒè¯•è¿‡ç¨‹ä¸­éœ€è¦ç¨³å®šçš„è¾“å‡ºç»“æžœ\n*   ç”Ÿäº§çŽ¯å¢ƒä¸­éœ€è¦ä¿è¯ç»“æžœä¸€è‡´æ€§\n\n### ä¸æ‰“å°æœåŠ¡å™¨è¾“å‡º\n\n**åŠŸèƒ½**ï¼šç¦æ­¢åœ¨æŽ§åˆ¶å°æ˜¾ç¤ºæœåŠ¡å™¨è¿è¡Œä¿¡æ¯ï¼Œä¿æŒç•Œé¢æ•´æ´ã€‚ **è¯´æ˜Ž**:\n\n*   å¯ç”¨åŽå°†ä¸æ˜¾ç¤º ComfyUI æœåŠ¡å™¨çš„æ—¥å¿—å’Œè¿è¡Œä¿¡æ¯\n*   å¯ä»¥å‡å°‘æŽ§åˆ¶å°çš„ä¿¡æ¯å¹²æ‰°ï¼Œä½¿ç•Œé¢æ›´åŠ æ¸…çˆ½\n*   åœ¨å¤§é‡æ—¥å¿—è¾“å‡ºæ—¶å¯èƒ½ç•¥å¾®æå‡ç³»ç»Ÿæ€§èƒ½\n*   é»˜è®¤ä¸ºå…³é—­çŠ¶æ€(false)ï¼Œå³é»˜è®¤æ˜¾ç¤ºæœåŠ¡å™¨è¾“å‡º\n\n**ä½¿ç”¨åœºæ™¯**:\n\n*   ç”Ÿäº§çŽ¯å¢ƒä¸­ä¸éœ€è¦æŸ¥çœ‹è°ƒè¯•ä¿¡æ¯æ—¶\n*   å¸Œæœ›ä¿æŒæŽ§åˆ¶å°ç•Œé¢æ•´æ´æ—¶\n*   ç³»ç»Ÿè¿è¡Œç¨³å®šæ— éœ€ç›‘æŽ§æ—¥å¿—æ—¶\n\n**æ³¨æ„**ï¼šåœ¨å¼€å‘å’Œè°ƒè¯•è¿‡ç¨‹ä¸­å»ºè®®ä¿æŒæ­¤é€‰é¡¹å…³é—­ï¼Œä»¥ä¾¿åŠæ—¶æŸ¥çœ‹æœåŠ¡å™¨çš„è¿è¡ŒçŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯ã€‚\n\n### ç¦ç”¨åœ¨æ–‡ä»¶ä¸­ä¿å­˜æç¤ºå…ƒæ•°æ®\n\n**åŠŸèƒ½**ï¼šä¸åœ¨ç”Ÿæˆçš„å›¾ç‰‡ä¸­ä¿å­˜å·¥ä½œæµä¿¡æ¯ï¼Œå‡å°‘æ–‡ä»¶å¤§å°ï¼Œä½†åŒæ—¶ä¹Ÿæ„å‘³ç€å¯¹åº”å·¥ä½œæµä¿¡æ¯çš„ç¼ºå¤±ï¼Œä½ æ— æ³•å†ä½¿ç”¨å·¥ä½œæµè¾“å‡ºçš„æ–‡ä»¶æ¥é‡çŽ°å¯¹åº”çš„ç”Ÿæˆç»“æžœ\n\n### ç¦ç”¨æ‰€æœ‰è‡ªå®šä¹‰èŠ‚ç‚¹\n\n**åŠŸèƒ½**ï¼šç¦æ­¢åŠ è½½æ‰€æœ‰ç¬¬ä¸‰æ–¹æ‰©å±•èŠ‚ç‚¹ï¼Œé€šå¸¸ç”¨äºŽåœ¨æŽ’æŸ¥é—®é¢˜æ—¶ä½¿ç”¨ï¼Œç”¨äºŽæ¥å®šä½å¯¹åº”çš„é”™è¯¯æ˜¯å¦ç”±äºŽç¬¬ä¸‰æ–¹æ‰©å±•èŠ‚ç‚¹å¯¼è‡´\n\n### æ—¥å¿—è¯¦ç»†çº§åˆ«\n\n**åŠŸèƒ½**ï¼šæŽ§åˆ¶æ—¥å¿—è¾“å‡ºçš„è¯¦ç»†ç¨‹åº¦ï¼Œç”¨äºŽè°ƒè¯•å’Œç›‘æŽ§ç³»ç»Ÿè¿è¡ŒçŠ¶æ€ã€‚ **é€‰é¡¹**:\n\n*   `CRITICAL`: ä»…è¾“å‡ºä¸¥é‡é”™è¯¯ä¿¡æ¯ï¼Œè¿™äº›é”™è¯¯å¯èƒ½å¯¼è‡´ç¨‹åºæ— æ³•ç»§ç»­è¿è¡Œ\n*   `ERROR`: è¾“å‡ºé”™è¯¯ä¿¡æ¯ï¼Œè¡¨ç¤ºæŸäº›åŠŸèƒ½æ— æ³•æ­£å¸¸å·¥ä½œ\n*   `WARNING`: è¾“å‡ºè­¦å‘Šä¿¡æ¯ï¼Œè¡¨ç¤ºå¯èƒ½å­˜åœ¨çš„é—®é¢˜ä½†ä¸å½±å“ä¸»è¦åŠŸèƒ½\n*   `INFO`: è¾“å‡ºä¸€èˆ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç³»ç»Ÿè¿è¡ŒçŠ¶æ€å’Œé‡è¦æ“ä½œè®°å½•\n*   `DEBUG`: è¾“å‡ºæœ€è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç³»ç»Ÿå†…éƒ¨è¿è¡Œçš„ç»†èŠ‚\n\n**è¯´æ˜Ž**:\n\n*   æ—¥å¿—çº§åˆ«ä»Žä¸Šåˆ°ä¸‹è¯¦ç»†ç¨‹åº¦é€’å¢ž\n*   æ¯ä¸ªçº§åˆ«éƒ½ä¼šåŒ…å«æ¯”å®ƒæ›´é«˜çº§åˆ«çš„æ‰€æœ‰æ—¥å¿—ä¿¡æ¯\n*   å»ºè®®åœ¨æ­£å¸¸ä½¿ç”¨æ—¶è®¾ç½®ä¸º INFO çº§åˆ«\n*   åœ¨æŽ’æŸ¥é—®é¢˜æ—¶å¯ä»¥è®¾ç½®ä¸º DEBUG çº§åˆ«ä»¥èŽ·å–æ›´å¤šä¿¡æ¯\n*   åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­å¯ä»¥è®¾ç½®ä¸º WARNING æˆ– ERROR çº§åˆ«ä»¥å‡å°‘æ—¥å¿—é‡\n\n## ç›®å½•\n\n### è¾“å…¥ç›®å½•\n\n**åŠŸèƒ½**ï¼šè®¾ç½®è¾“å…¥æ–‡ä»¶ï¼ˆå¦‚å›¾ç‰‡ã€æ¨¡åž‹ï¼‰çš„é»˜è®¤å­˜æ”¾è·¯å¾„\n\n### è¾“å‡ºç›®å½•\n\n**åŠŸèƒ½**ï¼šè®¾ç½®ç”Ÿæˆç»“æžœçš„ä¿å­˜è·¯å¾„"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/runway/image-generation",
  "markdown": "# Runway API èŠ‚ç‚¹ å›¾åƒç”Ÿæˆ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\nRunway æ˜¯ä¸€å®¶ä¸“æ³¨äºŽç”Ÿæˆå¼AIçš„ç§‘æŠ€å…¬å¸ï¼Œæä¾›å¼ºå¤§çš„å›¾åƒç”ŸæˆåŠŸèƒ½ã€‚å…¶æ¨¡åž‹æ”¯æŒé£Žæ ¼è¿ç§»ã€å›¾åƒæ‰©å±•å’Œç»†èŠ‚æŽ§åˆ¶ç­‰ç‰¹æ€§ã€‚ç›®å‰ ComfyUI å·²é›†æˆ Runway APIï¼Œä½ å¯ä»¥ç›´æŽ¥åœ¨ ComfyUI ä¸­ä½¿ç”¨ç›¸å…³èŠ‚ç‚¹è¿›è¡Œå›¾åƒç”Ÿæˆã€‚ æœ¬ç¯‡æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†å¼•å¯¼ä½ å®Œæˆä¸‹é¢çš„å·¥ä½œæµ:\n\n*   æ–‡ç”Ÿå›¾\n*   å‚è€ƒç”Ÿå›¾\n\n## Runway Image æ–‡ç”Ÿå›¾ å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![ComfyUI Runway Image Text to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/text_to_image.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Runway Image Text to Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Runway Text to Image` çš„ `prompt` ä¸­è¾“å…¥æç¤ºè¯\n2.  (å¯é€‰) è®¾ç½®è°ƒæ•´ `ratio` æ¥è®¾ç½®ä¸åŒçš„è¾“å‡ºæ¯”ä¾‹\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç”Ÿæˆã€‚\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼ˆå³é”®å¯ä»¥ä¿å­˜ï¼‰ï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n## Runway Image å‚è€ƒç”Ÿå›¾ å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµåŠè¾“å…¥å›¾åƒä¸‹è½½\n\nä¸‹é¢çš„å›¾ç‰‡çš„`metadata`ä¸­å·²ç»åŒ…å«å·¥ä½œæµä¿¡æ¯ï¼Œè¯·ä¸‹è½½å¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚ ![ComfyUI Runway Image Reference to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/runway_reference_to_image.png) ä¸‹è½½ä¸‹é¢çš„å›¾åƒç”¨äºŽè¾“å…¥ ![ComfyUI Runway Image Reference to Image Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/input.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Runway Image Reference to Image Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„å‚è€ƒç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„è¾“å…¥å›¾åƒ\n2.  åœ¨ `Runway Text to Image` çš„ `prompt` ä¸­è¾“å…¥æç¤ºè¯åŠè¿›è¡Œå°ºå¯¸è°ƒæ•´\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾åƒçš„ç”Ÿæˆã€‚\n4.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Image` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„å›¾åƒï¼ˆå³é”®å¯ä»¥ä¿å­˜ï¼‰ï¼Œå¯¹åº”çš„å›¾åƒä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹ã€‚\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcomfyui-server%2Fcomms_overview",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcomfyui-server%2Fexecution_model_inversion_guide",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fspecs%2Fnodedef_json",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/tripo/model-generation",
  "markdown": "# Tripo API èŠ‚ç‚¹æ¨¡åž‹ç”Ÿæˆ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\nTripo AI æ˜¯ä¸€å®¶ä¸“æ³¨äºŽç”Ÿæˆå¼ AI 3D å»ºæ¨¡çš„å…¬å¸ï¼Œå®ƒæä¾›ç”¨æˆ·å‹å¥½çš„å¹³å°å’Œ API æœåŠ¡ï¼Œèƒ½å¤Ÿå¿«é€Ÿåœ°å°†æ–‡æœ¬æç¤ºæˆ–2Då›¾åƒï¼ˆå•å¼ æˆ–å¤šå¼ ï¼‰è½¬æ¢æˆé«˜è´¨é‡çš„3Dæ¨¡åž‹ã€‚ ç›®å‰ ComfyUI å·²åŽŸç”Ÿé›†æˆäº†å¯¹åº” Tripo API ,çŽ°åœ¨ä½ å¯ä»¥åœ¨ ComfyUI ä¸­ä¾¿æ·åœ°ä½¿ç”¨ç›¸å…³èŠ‚ç‚¹æ¥è¿›è¡Œæ¨¡åž‹ç”Ÿæˆ ç›®å‰ ComfyUI çš„ API èŠ‚ç‚¹ä¸­å·²ç»æ”¯æŒ Tripo ä»¥ä¸‹æ¨¡åž‹ç”Ÿæˆèƒ½åŠ›ï¼š\n\n*   æ–‡ç”Ÿæ¨¡åž‹\n*   å›¾ç”Ÿæ¨¡åž‹\n*   å¤šè§†å›¾æ¨¡åž‹ç”Ÿæˆ\n*   éª¨éª¼ç»‘å®š\n*   éª¨éª¼åŠ¨ç”»\n\n## æ–‡ç”Ÿæ¨¡åž‹å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/api_tripo_text_to_model.json)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Tripo: Text to Model` èŠ‚ç‚¹çš„ `prompt` ä¸­è¾“å…¥æç¤ºè¯\n    *   modelï¼š å¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡åž‹ï¼Œç›®å‰ä»… v1.4 æ¨¡åž‹æ”¯æŒ `Tripo: Refine Draft model` çš„åŽç»­ä¼˜åŒ–\n    *   style: ä¸­å¯ä»¥è®¾ç½®ä¸åŒçš„é£Žæ ¼\n    *   texture\\_quality: å¯ä»¥è®¾ç½®ä¸åŒçš„çº¹ç†è´¨é‡\n2.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œæ¨¡åž‹çš„ç”Ÿæˆï¼Œå·¥ä½œæµå®ŒæˆåŽå¯¹åº”çš„æ¨¡åž‹ä¼šè‡ªåŠ¨ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹\n3.  åœ¨ `Preview 3D` èŠ‚ç‚¹ä¸­ç‚¹å‡»å±•å¼€èœå•\n4.  é€‰æ‹©`Export` å¯ä»¥ç›´æŽ¥å°†å¯¹åº”æ¨¡åž‹å¯¼å‡º\n\n## å›¾ç”Ÿæ¨¡åž‹å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/api_tripo_image_to_model.json)\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/panda.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„è¾“å…¥å›¾ç‰‡\n2.  åœ¨ `Tripo: Image to Model` èŠ‚ç‚¹ä¸­ä¿®æ”¹å¯¹åº”çš„å‚æ•°è®¾ç½®\n    *   modelï¼š å¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡åž‹ï¼Œç›®å‰ä»… v1.4 æ¨¡åž‹æ”¯æŒ `Tripo: Refine Draft model` çš„åŽç»­ä¼˜åŒ–\n    *   style: ä¸­å¯ä»¥è®¾ç½®ä¸åŒçš„é£Žæ ¼\n    *   texture\\_quality: å¯ä»¥è®¾ç½®ä¸åŒçš„çº¹ç†è´¨é‡\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œæ¨¡åž‹çš„ç”Ÿæˆï¼Œå·¥ä½œæµå®ŒæˆåŽå¯¹åº”çš„æ¨¡åž‹ä¼šè‡ªåŠ¨ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹\n4.  æ¨¡åž‹ä¸‹è½½è¯·å‚è€ƒæ–‡ç”Ÿå›¾éƒ¨åˆ†çš„è¯´æ˜Ž\n\n## å¤šè§†å›¾æ¨¡åž‹ç”Ÿæˆå·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµã€‚\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/api_tripo_multiview_to_model.json)\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![å‰è§†å›¾](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/front.jpg) ![èƒŒè§†å›¾](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/back.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Tripo Text to Model Step Guide](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆæœ€åŸºç¡€çš„æ–‡ç”Ÿå›¾å·¥ä½œæµè¿è¡Œï¼š\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åˆ†åˆ«åŠ è½½æä¾›çš„è¾“å…¥å›¾ç‰‡\n2.  åœ¨ `Tripo: Image to Model` èŠ‚ç‚¹ä¸­ä¿®æ”¹å¯¹åº”çš„å‚æ•°è®¾ç½®\n    *   modelï¼š å¯ä»¥é€‰æ‹©ä¸åŒçš„æ¨¡åž‹ï¼Œç›®å‰ä»… v1.4 æ¨¡åž‹æ”¯æŒ `Tripo: Refine Draft model` çš„åŽç»­ä¼˜åŒ–\n    *   style: ä¸­å¯ä»¥è®¾ç½®ä¸åŒçš„é£Žæ ¼\n    *   texture\\_quality: å¯ä»¥è®¾ç½®ä¸åŒçš„çº¹ç†è´¨é‡\n3.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œæ¨¡åž‹çš„ç”Ÿæˆï¼Œå·¥ä½œæµå®ŒæˆåŽå¯¹åº”çš„æ¨¡åž‹ä¼šè‡ªåŠ¨ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹\n4.  å…¶å®ƒè§†å›¾è¾“å…¥å¯ä»¥å‚è€ƒæ­¥éª¤å›¾ä¸­çš„ç¤ºæ„å°†å¯¹åº”èŠ‚ç‚¹çš„æ¨¡å¼è®¾ç½®ä¸º `æ€»æ˜¯ï¼ˆalwaysï¼‰` æ¥å¯ç”¨\n5.  æ¨¡åž‹ä¸‹è½½è¯·å‚è€ƒæ–‡ç”Ÿå›¾éƒ¨åˆ†çš„è¯´æ˜Ž\n\n## å¯¹åº”ä»»åŠ¡çš„åŽç»­ä»»åŠ¡å¤„ç†\n\nTripo çš„å¯¹åº”èŠ‚ç‚¹æä¾›äº†å¯¹äºŽåŒä¸€ä»»åŠ¡çš„åŽç»­å¤„ç†ï¼Œåªéœ€è¦åœ¨ç›¸å…³èŠ‚ç‚¹ä¸­è¾“å…¥å¯¹åº”çš„`model_task_id` å³å¯,æˆ‘ä»¬åœ¨ç›¸å…³æ¨¡æ¿ä¸­ä¹Ÿå·²æä¾›äº†å¯¹åº”çš„èŠ‚ç‚¹ï¼Œä½ ä¹Ÿå¯ä»¥æŒ‰éœ€é€šè¿‡ä¿®æ”¹å¯¹åº”èŠ‚ç‚¹æ¨¡å¼æ¥å¯ç”¨ ![Tripo ä»»åŠ¡å¤„ç†](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/tripo/other_nodes.jpg)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/cosmos/cosmos-predict2-video2world",
  "markdown": "# Cosmos Predict2 è§†é¢‘ç”Ÿæˆ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\nCosmos-Predict2 æ˜¯ç”± NVIDIA æŽ¨å‡ºçš„æ–°ä¸€ä»£ç‰©ç†ä¸–ç•ŒåŸºç¡€æ¨¡åž‹ï¼Œä¸“ä¸ºç‰©ç† AI åœºæ™¯ä¸‹çš„é«˜è´¨é‡è§†è§‰ç”Ÿæˆä¸Žé¢„æµ‹ä»»åŠ¡è®¾è®¡ã€‚ è¯¥æ¨¡åž‹å…·å¤‡æžé«˜çš„ç‰©ç†å‡†ç¡®æ€§ã€çŽ¯å¢ƒäº¤äº’æ€§å’Œç»†èŠ‚è¿˜åŽŸèƒ½åŠ›ï¼Œèƒ½å¤ŸçœŸå®žæ¨¡æ‹Ÿå¤æ‚çš„ç‰©ç†çŽ°è±¡ä¸ŽåŠ¨æ€åœºæ™¯ã€‚ Cosmos-Predict2 æ”¯æŒæ–‡æœ¬åˆ°å›¾åƒï¼ˆText2Imageï¼‰å’Œè§†é¢‘åˆ°ä¸–ç•Œï¼ˆVideo2Worldï¼‰ç­‰å¤šç§ç”Ÿæˆæ–¹å¼ï¼Œå¹¿æ³›åº”ç”¨äºŽå·¥ä¸šä»¿çœŸã€è‡ªåŠ¨é©¾é©¶ã€åŸŽå¸‚è§„åˆ’ã€ç§‘å­¦ç ”ç©¶ç­‰é¢†åŸŸï¼Œæ˜¯æŽ¨åŠ¨æ™ºèƒ½è§†è§‰ä¸Žç‰©ç†ä¸–ç•Œæ·±åº¦èžåˆçš„é‡è¦åŸºç¡€å·¥å…·ã€‚ GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2) huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959) æœ¬ç¯‡æŒ‡å—å°†å¼•å¯¼ä½ å®Œæˆåœ¨ ComfyUI ä¸­ **å›¾ç”Ÿè§†é¢‘** çš„å·¥ä½œæµ å¯¹äºŽæ–‡ç”Ÿå›¾éƒ¨åˆ†ï¼Œè¯·å‚è€ƒä¸‹é¢çš„éƒ¨åˆ†\n\n[\n\nä½¿ç”¨ Cosmos-Predict2 çš„è¿›è¡Œæ–‡ç”Ÿå›¾\n\n\n\n](https://docs.comfy.org/zh-CN/tutorials/image/cosmos/cosmos-predict2-t2i)\n\n## Cosmos Predict2 Video2World å·¥ä½œæµ\n\nå¯¹äºŽ 2B ç‰ˆæœ¬ï¼Œåœ¨æˆ‘ä»¬æµ‹è¯•ä½¿ç”¨æ—¶ï¼Œå¤§çº¦å ç”¨ 16GB çš„æ˜¾å­˜\n\n#### 1.ä¸‹è½½å·¥ä½œæµæ–‡ä»¶\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/cosmos/predict2/cosmos_predict2_2B_video2world_480p_16fps.json)\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥æ–‡ä»¶ï¼š ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/cosmos/predict2/input.png)\n\n### 2.æ‰‹åŠ¨æ¨¡åž‹å®‰è£…\n\n**Diffusion model**\n\n*   [cosmos\\_predict2\\_2B\\_video2world\\_480p\\_16fps.safetensors](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged/resolve/main/cosmos_predict2_2B_video2world_480p_16fps.safetensors)\n\nå…¶å®ƒæƒé‡è¯·è®¿é—® [Cosmos\\_Predict2\\_repackaged](https://huggingface.co/Comfy-Org/Cosmos_Predict2_repackaged) è¿›è¡Œä¸‹è½½ **Text encoder** [oldt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/cosmos_1.0_text_encoder_and_VAE_ComfyUI/resolve/main/text_encoders/oldt5_xxl_fp8_e4m3fn_scaled.safetensors) **VAE** [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors) æ–‡ä»¶ä¿å­˜ä½ç½®\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€ cosmos_predict2_2B_video2world_480p_16fps.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ oldt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚       â””â”€â”€  wan_2.1_vae.safetensors\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµè¿è¡Œ\n\n![å·¥ä½œæµä½¿ç”¨æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/cosmos/cosmos_predict2_2B_video2world_480p_16fps_step_guide.jpg) è¯·å‚ç…§å›¾ç‰‡åºå·è¿›è¡Œé€æ­¥ç¡®è®¤ï¼Œæ¥ä¿è¯å¯¹åº”å·¥ä½œæµçš„é¡ºåˆ©è¿è¡Œ\n\n1.  ç¡®ä¿ `Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `cosmos_predict2_2B_video2world_480p_16fps.safetensors`\n2.  ç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `oldt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  ç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors`\n4.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­ä¸Šä¼ æä¾›çš„è¾“å…¥å›¾ç‰‡\n5.  (å¯é€‰)å¦‚æžœéœ€è¦é¦–å°¾å¸§æŽ§åˆ¶ï¼Œå¯ä»¥ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + B` æ¥å¯ç”¨å°¾å¸§è¾“å…¥\n6.  (å¯é€‰) ä½ å¯ä»¥åœ¨ `ClipTextEncode` èŠ‚ç‚¹ä¸­ä¿®æ”¹æç¤ºè¯\n7.  (å¯é€‰) ä¿®æ”¹ `CosmosPredict2ImageToVideoLatent` èŠ‚ç‚¹ä¸­çš„å°ºå¯¸å’Œå¸§æ•°\n8.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n9.  ç”Ÿæˆå®ŒæˆåŽå¯¹åº”çš„è§†é¢‘ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `ComfyUI/output/` ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨ `save video` èŠ‚ç‚¹ä¸­é¢„è§ˆæˆ–è€…è°ƒæ•´ä¿å­˜ä½ç½®"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fcustom-nodes%2Fbackend%2Fmore_on_inputs",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcore-concepts%2Fcustom-nodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcore-concepts%2Flinks",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fdevelopment%2Fcore-concepts%2Fnodes",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/outpaint",
  "markdown": "# ComfyUI æ‰©å›¾ï¼ˆOutpaintï¼‰å·¥ä½œæµç¤ºä¾‹ - ComfyUI\n\næœ¬ç¯‡å°†å¼•å¯¼äº†è§£ AI ç»˜å›¾ä¸­æ‰©å›¾çš„æ¦‚å¿µï¼Œå¹¶åœ¨ ComfyUI ä¸­å®Œæˆæ‰©å›¾å·¥ä½œæµç”Ÿæˆã€‚æˆ‘ä»¬å°†æŽ¥è§¦ä»¥ä¸‹å†…å®¹ï¼š\n\n*   ä½¿ç”¨æ‰©å›¾å·¥ä½œæµå®Œæˆç”»é¢çš„æ‰©å±•\n*   äº†è§£å¹¶ä½¿ç”¨ ComfyUI ä¸­çš„æ‰©å›¾ç›¸å…³èŠ‚ç‚¹\n*   æŽŒæ¡æ‰©å›¾çš„åŸºæœ¬æ“ä½œæµç¨‹\n\n## å…³äºŽæ‰©å›¾\n\nåœ¨ AI å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°è¿™æ ·çš„éœ€æ±‚ï¼šå·²æœ‰çš„å›¾ç‰‡æž„å›¾å¾ˆå¥½ï¼Œä½†æ˜¯ç”»é¢èŒƒå›´å¤ªå°ï¼Œéœ€è¦æ‰©å±•ç”»å¸ƒæ¥èŽ·å¾—æ›´å¤§çš„åœºæ™¯ï¼Œè¿™æ—¶å€™å°±éœ€è¦ç”¨åˆ°æ‰©å›¾åŠŸèƒ½ã€‚ è¿™å°±åƒè®© **ç”»å®¶(AI ç»˜å›¾æ¨¡åž‹)** åœ¨å·²æœ‰çš„ç”»ä½œåŸºç¡€ä¸Šï¼Œå‘å¤–å»¶ä¼¸ç»˜åˆ¶æ›´å¤§çš„åœºæ™¯ã€‚æˆ‘ä»¬éœ€è¦å‘Šè¯‰ç”»å®¶ **éœ€è¦æ‰©å±•çš„æ–¹å‘å’ŒèŒƒå›´**ï¼Œç”»å®¶ä¼šæ ¹æ®å·²æœ‰çš„ç”»é¢å†…å®¹ï¼Œåˆç†åœ°å»¶ä¼¸å’Œæ‰©å±•åœºæ™¯ã€‚ åŸºæœ¬ä¸Šå®ƒè¦æ±‚çš„å†…å®¹ä¸Ž[å±€éƒ¨é‡ç»˜](https://docs.comfy.org/zh-CN/tutorials/basic/inpaint)ç›¸ä¼¼ï¼Œåªä¸è¿‡æˆ‘ä»¬ç”¨æ¥**æž„å»ºé®ç½©ï¼ˆMaskï¼‰çš„èŠ‚ç‚¹ä¸åŒ** æ‰©å›¾çš„åº”ç”¨åœºæ™¯åŒ…æ‹¬ï¼š\n\n*   **åœºæ™¯æ‰©å±•ï¼š** æ‰©å¤§åŽŸæœ‰ç”»é¢çš„åœºæ™¯èŒƒå›´ï¼Œå±•çŽ°æ›´å®Œæ•´çš„çŽ¯å¢ƒ\n*   **æž„å›¾è°ƒæ•´ï¼š** é€šè¿‡æ‰©å±•ç”»å¸ƒæ¥ä¼˜åŒ–æ•´ä½“æž„å›¾\n*   **å†…å®¹è¡¥å……ï¼š** ä¸ºåŽŸæœ‰ç”»é¢æ·»åŠ æ›´å¤šç›¸å…³çš„åœºæ™¯å…ƒç´ \n\n### å‡†å¤‡å·¥ä½œ\n\n#### 1\\. æ¨¡åž‹å®‰è£…\n\n*   [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)\n\n#### 2\\. è¾“å…¥å›¾ç‰‡\n\nè¯·å‡†å¤‡ä¸€å¼ ä½ æƒ³è¦è¿›è¡Œæ‰©å±•çš„å›¾ç‰‡ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢è¿™å¼ å›¾ç‰‡ä½œä¸ºç¤ºä¾‹ï¼š ![ComfyUIæ‰©å›¾è¾“å…¥å›¾ç‰‡](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/input.png)\n\n#### 3\\. æ‰©å›¾å·¥ä½œæµ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶å°†å…¶ **æ‹–å…¥** ComfyUI ç•Œé¢æˆ–ä½¿ç”¨èœå• **å·¥ä½œæµ(Workflow)** â€”> **æ‰“å¼€å·¥ä½œæµ(Open,å¿«æ·é”® `Ctrl + O`)** æ¥åŠ è½½è¿™ä¸ªæ‰©å›¾å·¥ä½œæµ ![ComfyUIæ‰©å›¾å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpaint.png)\n\n### æ‰©å›¾å·¥ä½œæµä½¿ç”¨è®²è§£\n\n![ComfyUI æ‰©å›¾å·¥ä½œæµç¤ºæ„å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpainting_workflow.jpg) æ‰©å›¾å·¥ä½œæµçš„å…³é”®æ­¥éª¤å¦‚ä¸‹ï¼š\n\n1.  è¯·åœ¨ `åŠ è½½æ¨¡åž‹(Load Checkpoint)` èŠ‚ç‚¹ä¸­åŠ è½½ä½ æœ¬åœ°å®‰è£…çš„æ¨¡åž‹æ–‡ä»¶\n2.  è¯·åœ¨ `åŠ è½½å›¾ç‰‡(Load Image)` èŠ‚ç‚¹ä¸­ç‚¹å‡» `Upload` æŒ‰é’®ä¸Šä¼ \n3.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\nåœ¨è¿™ä¸ªå·¥ä½œæµä¸­ä¸»è¦æ˜¯é€šè¿‡ `Pad Image for outpainting` èŠ‚ç‚¹æ¥æŽ§åˆ¶å›¾ç‰‡çš„æ‰©å±•æ–¹å‘å’ŒèŒƒå›´ï¼Œå…¶å®žè¿™ä¹Ÿæ˜¯ä¸€ä¸ª [å±€éƒ¨é‡ç»˜(Inpaint)](https://docs.comfy.org/zh-CN/tutorials/basic/inpaint) å·¥ä½œæµï¼Œåªä¸è¿‡æˆ‘ä»¬ç”¨æ¥æž„å»ºé®ç½©ï¼ˆMaskï¼‰çš„èŠ‚ç‚¹ä¸åŒã€‚\n\n### Pad Image for outpainting èŠ‚ç‚¹\n\n![Pad Image for outpainting èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/pad_image_for_outpainting.jpg) è¿™ä¸ªèŠ‚ç‚¹æŽ¥å—ä¸€ä¸ªè¾“å…¥å›¾ç‰‡ï¼Œå¹¶è¾“å‡ºä¸€å¼ æ‰©å±•è¿‡çš„å›¾åƒå’Œå¯¹åº”çš„é®ç½©ï¼ˆMaskï¼‰ï¼Œå…¶ä¸­é®ç½©ç”±äºŽå¯¹åº”çš„èŠ‚ç‚¹å‚æ•°æž„å»ºã€‚\n\n#### è¾“å…¥å‚æ•°\n\n| å‚æ•°åç§° | ä½œç”¨  |\n| --- | --- |\n| `image` | è¾“å…¥å›¾ç‰‡ |\n| `left` | å·¦ä¾§å¡«å……é‡ |\n| `top` | é¡¶éƒ¨å¡«å……é‡ |\n| `right` | å³ä¾§å¡«å……é‡ |\n| `bottom` | åº•éƒ¨å¡«å……é‡ |\n| `feathering` | æŽ§åˆ¶åŽŸå§‹å›¾åƒä¸Žæ·»åŠ çš„å¡«å……å†…å®¹ä¹‹é—´çš„è¿‡æ¸¡å¹³æ»‘åº¦ï¼Œè¶Šå¤§è¶Šå¹³æ»‘ |\n\n#### è¾“å‡ºå‚æ•°\n\n| å‚æ•°åç§° | ä½œç”¨  |\n| --- | --- |\n| `image` | è¾“å‡º`image`ä»£è¡¨å·²å¡«å……çš„å›¾åƒ |\n| `mask` | è¾“å‡º`mask`æŒ‡ç¤ºåŽŸå§‹å›¾åƒå’Œæ·»åŠ çš„å¡«å……åŒºåŸŸ |\n\n#### èŠ‚ç‚¹è¾“å‡ºå†…å®¹\n\nç»è¿‡ `Pad Image for outpainting` èŠ‚ç‚¹å¤„ç†åŽï¼Œè¾“å‡ºçš„å›¾ç‰‡å’Œè’™ç‰ˆé¢„è§ˆå¦‚ä¸‹ï¼š ![Pad Image for outpainting èŠ‚ç‚¹ç»“æžœ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg) ä½ å¯ä»¥çœ‹åˆ°å¯¹åº”çš„è¾“å‡ºç»“æžœ\n\n*   `Image` è¾“å‡ºçš„æ˜¯æ‰©å±•åŽçš„å›¾åƒ\n*   `Mask` è¾“å‡ºçš„æ˜¯æ ‡è®°äº†æ‰©å±•åŒºåŸŸçš„è’™ç‰ˆ"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/upscale",
  "markdown": "# ComfyUI å›¾åƒæ”¾å¤§å·¥ä½œæµ - ComfyUI\n\n## ä»€ä¹ˆæ˜¯å›¾åƒæ”¾å¤§\n\nå›¾åƒæ”¾å¤§ï¼ˆImage Upscalingï¼‰æ˜¯é€šè¿‡ç®—æ³•å°†ä½Žåˆ†è¾¨çŽ‡å›¾åƒè½¬æ¢ä¸ºé«˜åˆ†è¾¨çŽ‡å›¾åƒçš„è¿‡ç¨‹ã€‚ä¸Žä¼ ç»Ÿæ’å€¼æ”¾å¤§ä¸åŒï¼ŒAI æ”¾å¤§æ¨¡åž‹ï¼ˆå¦‚ ESRGANï¼‰èƒ½æ™ºèƒ½é‡å»ºç»†èŠ‚ï¼Œä¿æŒå›¾åƒè´¨é‡ã€‚ æ¯”å¦‚é»˜è®¤é€šè¿‡ SD1.5 æ¨¡åž‹å¯¹äºŽå¤§å°ºå¯¸çš„å›¾ç‰‡ç”Ÿæˆè¡¨çŽ°ä¸ä½³ï¼Œå¦‚æžœéœ€è¦é«˜åˆ†è¾¨çŽ‡ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå…ˆç”Ÿæˆå°å°ºå¯¸çš„å›¾åƒï¼Œç„¶åŽä½¿ç”¨å›¾åƒæ”¾å¤§æ¥æå‡å›¾ç‰‡çš„åˆ†è¾¨çŽ‡ã€‚ å½“ç„¶æœ¬æ–‡ä»‹ç»çš„åªæ˜¯è¯¸å¤š ComfyUI ä¸­å›¾åƒæ”¾å¤§æ–¹æ³•ä¸­çš„ä¸€ç§ï¼Œåœ¨è¿™ç¯‡è®²è§£ä¸­ï¼Œæˆ‘ä»¬å°†å¸¦ä½ å®Œæˆä»¥ä¸‹å†…å®¹ï¼š\n\n*   ä¸‹è½½å¹¶å®‰è£…æ”¾å¤§æ¨¡åž‹\n*   ä½¿ç”¨æ”¾å¤§æ¨¡åž‹è¿›è¡Œä¸€æ¬¡ç®€å•çš„æ”¾å¤§\n*   ç»“åˆæ–‡ç”Ÿå›¾å·¥ä½œæµï¼Œå®Œæˆå›¾åƒçš„æ”¾å¤§\n\n## ä¸‹è½½å¹¶å®‰è£…æ”¾å¤§æ¨¡åž‹\n\né¢å¤–éœ€è¦ä¸‹è½½ ESRGAN ç­‰æ”¾å¤§æ¨¡åž‹ï¼ˆå¿…é¡»ï¼‰ï¼š\n\n## ç®€å•æ”¾å¤§å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµåŠç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥åˆ° ComfyUI ä¸­ï¼ŒåŠ è½½ç®€å•ç‰ˆæœ¬æ”¾å¤§å·¥ä½œæµ ![æ”¾å¤§å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_workflow.png) è¯·ä¸‹è½½ä¸‹é¢è¿™å¼ å°å°ºå¯¸çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥ ![Upscale-input](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale-input.jpg) \n\n### 2\\. å·¥ä½œæµè®²è§£\n\n![æ”¾å¤§å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_simple_workflow.jpg)\n\n1.  åœ¨`åŠ è½½æ”¾å¤§æ¨¡åž‹(Load Upscale Model)`èŠ‚ç‚¹ä¸­é€‰æ‹©æˆ‘ä»¬ä¹‹å‰ä¸‹è½½çš„æ”¾å¤§æ¨¡åž‹\n2.  åœ¨`åŠ è½½å›¾ç‰‡(Load Image)`èŠ‚ç‚¹ä¸­é€‰æ‹©æˆ‘ä»¬ä¹‹å‰å‡†å¤‡çš„è¾“å…¥å›¾ç‰‡\n3.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\né€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®Œæˆä¸€ä¸ªå›¾ç‰‡çš„æ”¾å¤§ï¼Œä½ å¯ä»¥çœ‹åˆ°åœ¨è¿™ä¸ªå·¥ä½œæµä¸­ï¼Œæ ¸å¿ƒä¸»è¦åœ¨äºŽ `Load Upscale Model` å’Œ `Upscale Image(Using Model)` çš„ç»„åˆï¼Œä»–ä»¬é€šè¿‡æŽ¥æ”¶ä¸€ä¸ªå›¾åƒçš„è¾“å…¥ï¼Œç„¶åŽä½¿ç”¨æ”¾å¤§æ¨¡åž‹å°†å›¾åƒæ”¾å¤§ã€‚\n\n## ç»“åˆæ–‡ç”Ÿå›¾çš„æ”¾å¤§å·¥ä½œæµ\n\nåœ¨å®Œæˆäº†ç®€å•çš„æ”¾å¤§å·¥ä½œæµåŽï¼Œæˆ‘ä»¬å°±å¯ä»¥å°è¯•ç»“åˆ[æ–‡ç”Ÿå›¾](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)çš„å·¥ä½œæµæ¥å®Œæˆä¸€ä¸ªå®Œæ•´æ”¾å¤§å·¥ä½œçš„æµç¨‹ï¼Œå…³äºŽæ–‡ç”Ÿå›¾çš„åŸºç¡€éƒ¨åˆ†åŠç›¸å…³æ¨¡åž‹è¦æ±‚ï¼Œè¯·å‚è€ƒ[æ–‡ç”Ÿå›¾](https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image)çš„éƒ¨åˆ†çš„è¯´æ˜Žå®Œæˆã€‚ è¯·å°†ä¸‹é¢çš„å›¾ç‰‡ä¸‹è½½å¹¶ä¿å­˜åŽæ‹–å…¥åˆ° ComfyUI ä¸­ï¼ŒåŠ è½½ç»“åˆæ–‡ç”Ÿå›¾çš„æ”¾å¤§å·¥ä½œæµ ![ç»“åˆæ–‡ç”Ÿå›¾çš„æ”¾å¤§å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/esrgan_example.png) ä½ å¯ä»¥çœ‹åˆ°åœ¨è¿™ä¸ªå·¥ä½œæµé‡Œï¼Œå°±æ˜¯åœ¨æ–‡ç”Ÿå›¾å·¥ä½œæµä¹‹åŽæŠŠå¯¹åº”çš„å›¾ç‰‡è¾“å…¥åˆ°æ”¾å¤§å·¥ä½œæµä¸­å®Œæˆäº†å¯¹åº”å›¾ç‰‡çš„æ”¾å¤§ã€‚\n\n## å…¶å®ƒç›¸å…³è¡¥å……\n\n1.  **é“¾å¼æ”¾å¤§**ï¼šå¯¹äºŽéœ€è¦è¶…é«˜å€çŽ‡æ”¾å¤§çš„æƒ…å†µï¼Œå¯ä»¥ä¸²è”å¤šä¸ªæ”¾å¤§èŠ‚ç‚¹ï¼ˆå¦‚å…ˆ2xå†4xï¼‰\n2.  **æ··åˆæ”¾å¤§**ï¼šåœ¨ç”Ÿæˆå·¥ä½œæµåŽæŽ¥æ”¾å¤§èŠ‚ç‚¹ï¼Œå®žçŽ°â€ç”Ÿæˆ+å¢žå¼ºâ€ä¸€ä½“åŒ–æµç¨‹\n3.  **å¯¹æ¯”æµ‹è¯•**ï¼šä¸åŒæ¨¡åž‹å¯¹ç‰¹å®šç±»åž‹å›¾ç‰‡æ•ˆæžœå·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®åŒæ—¶æµ‹è¯•å¤šä¸ªæ¨¡åž‹"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/basic/multiple-loras",
  "markdown": "# ComfyUI åº”ç”¨å¤šä¸ª LoRA ç¤ºä¾‹ - ComfyUI\n\nåœ¨ [ComfyUI LoRA ä½¿ç”¨ç¤ºä¾‹](https://docs.comfy.org/zh-CN/tutorials/basic/lora) ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•åœ¨ ComfyUI ä¸­åŠ è½½å¹¶ä½¿ç”¨ LoRA æ¨¡åž‹ï¼Œä¹ŸæåŠäº†è¯¥èŠ‚ç‚¹æ”¯æŒé“¾å¼è¿žæŽ¥ã€‚ ![LoRA èŠ‚ç‚¹é“¾å¼è¿žæŽ¥](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png) åœ¨æœ¬ç¯‡ä¸­æˆ‘ä»¬å°†ä½¿ç”¨é“¾å¼è¿žæŽ¥`Load LoRA`èŠ‚ç‚¹çš„æ–¹å¼æ¥åŒæ—¶ä½¿ç”¨å¤šä¸ª LoRA æ¨¡åž‹ï¼Œåœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [blindbox\\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) å’Œ [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856) ä¸¤ä¸ª LoRA æ¨¡åž‹ã€‚ ä¸‹å›¾æ˜¯è¿™ä¸¤ä¸ª LoRA æ¨¡åž‹åœ¨åŒæ ·å‚æ•°ä¸‹å•ç‹¬ä½¿ç”¨çš„æ•ˆæžœ ![ComfyUI ä¸­ LoRA æ¨¡åž‹å•ç‹¬ä½¿ç”¨æ•ˆæžœ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/compare.png) ä½†é€šè¿‡å¤šä¸ª LoRA æ¨¡åž‹é“¾å¼è¿žæŽ¥åŽï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€ç»ˆçš„æ•ˆæžœä¸­çœ‹åˆ°ä¸¤ç§é£Žæ ¼èžåˆåœ¨ä¸€èµ·çš„æ•ˆæžœ ![ComfyUI ä¸­å¤š LoRA æ¨¡åž‹åº”ç”¨ç¤ºä¾‹ç»“æžœ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)\n\n## ç›¸å…³æ¨¡åž‹å®‰è£…\n\nè¯·ä¸‹è½½ [dreamshaper\\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) å¹¶ä¿å­˜è‡³ `ComfyUI/models/checkpoints` ç›®å½• è¯·ä¸‹è½½ [blindbox\\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) å¹¶ä¿å­˜è‡³ `ComfyUI/models/loras` ç›®å½• è¯·ä¸‹è½½ [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model&format=SafeTensor&size=full&fp=fp16) å¹¶ä¿å­˜è‡³ `ComfyUI/models/loras` ç›®å½•\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - å¤š LoRA æ¨¡åž‹åº”ç”¨ç¤ºä¾‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png) \n\n## æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\nè¯·å‚ç…§ä¸‹å›¾æ­¥éª¤å®Œæˆï¼Œç¡®ä¿å·¥ä½œæµèƒ½å¤Ÿæ­£å¸¸è¿è¡Œ ![ComfyUI å·¥ä½œæµ - å¤š LoRA æ¨¡åž‹åº”ç”¨ç¤ºä¾‹æµç¨‹å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/flow_diagram.png)\n\n1.  ç¡®ä¿`Load Checkpoint`å¯ä»¥åŠ è½½ **dreamshaper\\_8.safetensors**\n2.  ç¡®ä¿`Load LoRA`å¯ä»¥åŠ è½½ **blindbox\\_V1Mix.safetensors**\n3.  ç¡®ä¿`Load LoRA`å¯ä»¥åŠ è½½ **MoXinV1.safetensors**\n4.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n\n## å¼€å§‹ä½ çš„å°è¯•\n\n1.  è¯•ç€è°ƒæ•´ä¸¤ä¸ª `Load LoRA` çš„ `strength_model` å‚æ•°ï¼Œæ¥ä¿®æ”¹ä¸åŒ LoRA æ¨¡åž‹å¯¹æœ€ç»ˆç”Ÿæˆå›¾ç‰‡çš„å½±å“\n2.  è®¿é—® [CivitAI](https://civitai.com/models) ç½‘ç«™ï¼Œä¸‹è½½å…¶å®ƒé£Žæ ¼çš„ LoRA æ¨¡åž‹ï¼Œç»„åˆå‡ºä½ æ»¡æ„çš„æ•ˆæžœ"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets",
  "markdown": "# ComfyUI ControlNet æ··åˆä½¿ç”¨ç¤ºä¾‹ - ComfyUI\n\nåœ¨ AI å›¾åƒç”Ÿæˆä¸­ï¼Œå•ä¸€çš„æŽ§åˆ¶æ¡ä»¶å¾€å¾€éš¾ä»¥æ»¡è¶³å¤æ‚åœºæ™¯çš„éœ€æ±‚ã€‚æ··åˆä½¿ç”¨å¤šä¸ª ControlNet å¯ä»¥åŒæ—¶æŽ§åˆ¶å›¾åƒçš„ä¸åŒåŒºåŸŸæˆ–ä¸åŒæ–¹é¢ï¼Œå®žçŽ°æ›´ç²¾ç¡®çš„å›¾åƒç”ŸæˆæŽ§åˆ¶ã€‚ åœ¨ä¸€äº›åœºæ™¯ä¸‹ï¼Œæ··åˆä½¿ç”¨ ControlNet å¯ä»¥åˆ©ç”¨ä¸åŒæŽ§åˆ¶æ¡ä»¶çš„ç‰¹æ€§ï¼Œæ¥è¾¾åˆ°æ›´ç²¾ç»†çš„æ¡ä»¶æŽ§åˆ¶ï¼š\n\n1.  **åœºæ™¯å¤æ‚æ€§**ï¼šå¤æ‚åœºæ™¯éœ€è¦å¤šç§æŽ§åˆ¶æ¡ä»¶å…±åŒä½œç”¨\n2.  **ç²¾ç»†æŽ§åˆ¶**ï¼šé€šè¿‡è°ƒæ•´æ¯ä¸ª ControlNet çš„å¼ºåº¦å‚æ•°ï¼Œå¯ä»¥ç²¾ç¡®æŽ§åˆ¶å„éƒ¨åˆ†çš„å½±å“ç¨‹åº¦\n3.  **äº’è¡¥æ•ˆæžœ**ï¼šä¸åŒç±»åž‹çš„ ControlNet å¯ä»¥äº’ç›¸è¡¥å……ï¼Œå¼¥è¡¥å•ä¸€æŽ§åˆ¶çš„å±€é™æ€§\n4.  **åˆ›æ„è¡¨è¾¾**ï¼šç»„åˆä¸åŒæŽ§åˆ¶å¯ä»¥äº§ç”Ÿç‹¬ç‰¹çš„åˆ›æ„æ•ˆæžœ\n\n### æ··åˆ ControlNet çš„ä½¿ç”¨æ–¹æ³•\n\nå½“æˆ‘ä»¬æ··åˆä½¿ç”¨å¤šä¸ª ControlNet æ—¶ï¼Œæ¯ä¸ª ControlNet ä¼šæ ¹æ®å…¶åº”ç”¨çš„åŒºåŸŸå¯¹å›¾åƒç”Ÿæˆè¿‡ç¨‹æ–½åŠ å½±å“ã€‚ComfyUI é€šè¿‡ `Apply ControlNet` èŠ‚ç‚¹çš„é“¾å¼è¿žæŽ¥æ–¹å¼ï¼Œå…è®¸å¤šä¸ª ControlNet æ¡ä»¶æŒ‰é¡ºåºå åŠ åº”ç”¨æ··åˆæŽ§åˆ¶æ¡ä»¶ï¼š ![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)\n\nåœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **Pose ControlNet** å’Œ **Scribble ControlNet** çš„ç»„åˆæ¥ç”Ÿæˆä¸€å¼ åŒ…å«å¤šä¸ªå…ƒç´ çš„åœºæ™¯ï¼šå·¦ä¾§ç”± Pose ControlNet æŽ§åˆ¶çš„äººç‰©å’Œå³ä¾§ç”± Scribble ControlNet æŽ§åˆ¶çš„çŒ«å’ªæ»‘æ¿è½¦ã€‚\n\n### 1\\. ControlNet æ··åˆä½¿ç”¨å·¥ä½œæµç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - Mixing ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets.png) \n\nç”¨äºŽè¾“å…¥çš„ pose å›¾ç‰‡ï¼ˆæŽ§åˆ¶å·¦ä¾§äººç‰©å§¿æ€ï¼‰: ![ComfyUI å·¥ä½œæµ - Mixing ControlNet è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input.png) ç”¨äºŽè¾“å…¥çš„ scribble å›¾ç‰‡ï¼ˆæŽ§åˆ¶å³ä¾§çŒ«å’ªå’Œæ»‘æ¿è½¦ï¼‰: ![ComfyUI å·¥ä½œæµ - Mixing ControlNet è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input_scribble.png)\n\n### 2\\. æ‰‹åŠ¨æ¨¡åž‹å®‰è£…\n\n*   [awpainting\\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model&format=SafeTensor&size=full&fp=fp16)\n*   [control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)\n*   [control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ awpainting_v14.safetensors\nâ”‚   â”œâ”€â”€ controlnet/\nâ”‚   â”‚   â””â”€â”€ control_v11p_sd15_scribble_fp16.safetensors\nâ”‚   â”‚   â””â”€â”€ control_v11p_sd15_openpose_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ vae-ft-mse-840000-ema-pruned.safetensors\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI å·¥ä½œæµ - Mixing ControlNet æµç¨‹å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg) æŒ‰ç…§å›¾ç‰‡ä¸­çš„æ•°å­—æ ‡è®°ï¼Œæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n\n1.  ç¡®ä¿`Load Checkpoint`å¯ä»¥åŠ è½½ **awpainting\\_v14.safetensors**\n2.  ç¡®ä¿`Load VAE`å¯ä»¥åŠ è½½ **vae-ft-mse-840000-ema-pruned.safetensors**\n\nç¬¬ä¸€ç»„ ControlNet ä½¿ç”¨ Openpose æ¨¡åž‹: 3. ç¡®ä¿`Load ControlNet Model`åŠ è½½ **control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors** 4. åœ¨`Load Image`ä¸­ç‚¹å‡»`Upload` ä¸Šä¼ ä¹‹å‰æä¾›çš„ pose å›¾ç‰‡ ç¬¬äºŒç»„ ControlNet ä½¿ç”¨ Scribble æ¨¡åž‹: 5. ç¡®ä¿`Load ControlNet Model`åŠ è½½ **control\\_v11p\\_sd15\\_scribble\\_fp16.safetensors** 6. åœ¨`Load Image`ä¸­ç‚¹å‡»`Upload` ä¸Šä¼ ä¹‹å‰æä¾›çš„ scribble å›¾ç‰‡ 7. ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n\n## å·¥ä½œæµè®²è§£\n\n#### å¼ºåº¦å¹³è¡¡\n\nå½“æŽ§åˆ¶å›¾åƒä¸åŒåŒºåŸŸæ—¶ï¼Œå¼ºåº¦å‚æ•°çš„å¹³è¡¡å°¤ä¸ºé‡è¦ï¼š\n\n*   å¦‚æžœä¸€ä¸ªåŒºåŸŸçš„ ControlNet å¼ºåº¦æ˜Žæ˜¾é«˜äºŽå¦ä¸€ä¸ªï¼Œå¯èƒ½å¯¼è‡´è¯¥åŒºåŸŸçš„æŽ§åˆ¶æ•ˆæžœè¿‡å¼ºè€ŒæŠ‘åˆ¶å¦ä¸€åŒºåŸŸ\n*   æŽ¨èä¸ºä¸åŒåŒºåŸŸçš„ ControlNet è®¾ç½®ç›¸ä¼¼çš„å¼ºåº¦å€¼ï¼Œä¾‹å¦‚éƒ½è®¾ä¸º 1.0\n\n#### æç¤ºè¯æŠ€å·§\n\nåœ¨åŒºåŸŸåˆ†æ²»æ··åˆä¸­ï¼Œæç¤ºè¯éœ€è¦åŒæ—¶åŒ…å«ä¸¤ä¸ªåŒºåŸŸçš„æè¿°ï¼š\n\n```\n\"A woman in red dress, a cat riding a scooter, detailed background, high quality\"\n```\n\nè¿™æ ·çš„æç¤ºè¯åŒæ—¶æ¶µç›–äº†äººç‰©å’ŒçŒ«å’ªæ»‘æ¿è½¦ï¼Œç¡®ä¿æ¨¡åž‹èƒ½å¤ŸåŒæ—¶å…³æ³¨ä¸¤ä¸ªæŽ§åˆ¶åŒºåŸŸã€‚\n\n## åŒä¸€ä¸»ä½“å¤šç»´æŽ§åˆ¶çš„æ··åˆåº”ç”¨\n\né™¤äº†æœ¬ä¾‹å±•ç¤ºçš„åŒºåŸŸåˆ†æ²»æ··åˆå¤–ï¼Œå¦ä¸€ç§å¸¸è§çš„æ··åˆæ–¹å¼æ˜¯å¯¹åŒä¸€ä¸»ä½“è¿›è¡Œå¤šç»´æŽ§åˆ¶ã€‚ä¾‹å¦‚ï¼š\n\n*   **Pose + Depth**ï¼šæŽ§åˆ¶äººç‰©å§¿åŠ¿åŠç©ºé—´æ„Ÿ\n*   **Pose + Canny**ï¼šæŽ§åˆ¶äººç‰©å§¿åŠ¿åŠè¾¹ç¼˜ç»†èŠ‚\n*   **Pose + Reference**ï¼šæŽ§åˆ¶äººç‰©å§¿åŠ¿ä½†å‚è€ƒç‰¹å®šé£Žæ ¼\n\nåœ¨è¿™ç§åº”ç”¨ä¸­ï¼Œå¤šä¸ª ControlNet çš„å‚è€ƒå›¾åº”è¯¥å¯¹å‡†åŒä¸€ä¸»ä½“ï¼Œå¹¶è°ƒæ•´å„è‡ªçš„å¼ºåº¦ç¡®ä¿é€‚å½“å¹³è¡¡ã€‚ é€šè¿‡ç»„åˆä¸åŒç±»åž‹çš„ ControlNet å¹¶æŒ‡å®šå…¶æŽ§åˆ¶åŒºåŸŸï¼Œä½ å¯ä»¥å¯¹ç”»é¢å…ƒç´ è¿›è¡Œç²¾ç¡®æŽ§åˆ¶ã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-text-to-image",
  "markdown": "# ComfyUI Flux æ–‡ç”Ÿå›¾å·¥ä½œç¤ºä¾‹ - ComfyUI\n\n ![Flux](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_example.png) Flux æ˜¯ç›®å‰æœ€å¤§çš„å¼€æºAIç»˜ç”»æ¨¡åž‹ä¹‹ä¸€ï¼Œæ‹¥æœ‰ 12B å‚æ•°ï¼ŒåŽŸå§‹æ–‡ä»¶å¤§å°çº¦ä¸º23GBã€‚å®ƒç”± [Black Forest Labs](https://blackforestlabs.ai/) å¼€å‘ï¼Œè¯¥å›¢é˜Ÿç”±å‰ Stable Diffusion å›¢é˜Ÿæˆå‘˜åˆ›ç«‹ã€‚ Flux ä»¥å…¶å“è¶Šçš„ç”»é¢è´¨é‡å’Œçµæ´»æ€§è€Œé—»åï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å›¾åƒã€‚ ç›®å‰ Flux.1 æ¨¡åž‹ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªç‰ˆæœ¬ï¼š\n\n*   **Flux.1 Proï¼š** æ•ˆæžœæœ€ä½³æ¨¡åž‹ï¼Œé—­æºæ¨¡åž‹ï¼Œä»…æ”¯æŒé€šè¿‡ API è°ƒç”¨ã€‚\n*   **[Flux.1 \\[dev\\]ï¼š](https://huggingface.co/black-forest-labs/FLUX.1-dev)** å¼€æºä½†ä»…é™éžå•†ä¸šä½¿ç”¨ï¼Œä»Ž Pro ç‰ˆæœ¬è’¸é¦è€Œæ¥ï¼Œæ•ˆæžœæŽ¥è¿‘Proç‰ˆã€‚\n*   \\*\\*[Flux.1 \\[schnell\\]ï¼š](https://huggingface.co/black-forest-labs/FLUX.1-schnell)\\*\\*é‡‡ç”¨ Apache2.0 è®¸å¯ï¼Œä»…éœ€4æ­¥å³å¯ç”Ÿæˆå›¾åƒï¼Œé€‚åˆä½Žé…ç½®ç¡¬ä»¶ã€‚\n\n**Flux.1 æ¨¡åž‹ç‰¹ç‚¹**\n\n*   **æ··åˆæž¶æž„ï¼š** ç»“åˆäº† Transformer ç½‘ç»œå’Œæ‰©æ•£æ¨¡åž‹çš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆæ•´åˆæ–‡æœ¬ä¸Žå›¾åƒä¿¡æ¯ï¼Œæå‡ç”Ÿæˆå›¾åƒä¸Žæç¤ºè¯çš„å¯¹é½ç²¾åº¦ï¼Œå¯¹å¤æ‚çš„æç¤ºè¯ä¾æ—§æœ‰éžå¸¸å¥½çš„è¿˜åŽŸèƒ½åŠ›ã€‚\n*   **å‚æ•°è§„æ¨¡ï¼š** Flux æ‹¥æœ‰ 12B å‚æ•°ï¼Œå¯æ•æ‰æ›´å¤æ‚çš„æ¨¡å¼å…³ç³»ï¼Œç”Ÿæˆæ›´é€¼çœŸã€å¤šæ ·åŒ–çš„å›¾åƒã€‚\n*   **æ”¯æŒå¤šç§é£Žæ ¼ï¼š** æ”¯æŒå¤šæ ·åŒ–çš„é£Žæ ¼ï¼Œå¯¹å„ç§ç±»åž‹çš„å›¾åƒéƒ½æœ‰éžå¸¸å¥½çš„è¡¨çŽ°èƒ½åŠ›ã€‚\n\nåœ¨æœ¬ç¯‡ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä½¿ç”¨ Flux.1 Dev å’Œ Flux.1 Schnell ä¸¤ä¸ªç‰ˆæœ¬è¿›è¡Œæ–‡ç”Ÿå›¾çš„ç¤ºä¾‹ï¼ŒåŒ…æ‹¬åŽŸå§‹å®Œæ•´ç‰ˆæ¨¡åž‹å’Œ FP8 Checkpoint ç®€åŒ–ç‰ˆæœ¬ã€‚\n\n*   **Flux å®Œæ•´ç‰ˆæœ¬ï¼š** æ•ˆæžœæœ€ä½³ï¼Œä½†éœ€è¦è¾ƒå¤§çš„æ˜¾å­˜èµ„æºï¼ˆæŽ¨è16GBä»¥ä¸Šï¼‰ï¼Œéœ€è¦å®‰è£…å¤šä¸ªæ¨¡åž‹æ–‡ä»¶ã€‚\n*   **Flux FP8 Checkpointï¼š** ä»…éœ€ä¸€ä¸ª fp8 ç‰ˆæœ¬çš„æ¨¡åž‹ï¼Œä½†æ˜¯è´¨é‡ç›¸å¯¹å®Œæ•´ç‰ˆä¼šæœ‰æ‰€é™ä½Žã€‚\n\n### Flux.1 Dev å®Œæ•´ç‰ˆæœ¬å·¥ä½œæµ\n\n#### 1\\. å·¥ä½œæµæ–‡ä»¶\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å·¥ä½œæµã€‚ ![Flux Dev åŽŸå§‹ç‰ˆæœ¬å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_t5fp16.png) \n\n#### 2\\. æ‰‹åŠ¨å®‰è£…æ¨¡åž‹\n\nè¯·ä¸‹è½½ä¸‹é¢çš„æ¨¡åž‹æ–‡ä»¶ï¼š\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) å½“ä½ çš„æ˜¾å­˜å¤§äºŽ 32GB æ—¶æŽ¨èä½¿ç”¨ã€‚\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)\n\næ–‡ä»¶ä¿å­˜ä½ç½®ï¼š\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ t5xxl_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ flux1-dev.safetensors\n```\n\n#### 3\\. æŒ‰æ­¥éª¤æ£€æŸ¥ç¡®ä¿å·¥ä½œæµå¯ä»¥æ­£å¸¸è¿è¡Œ\n\nè¯·å‚ç…§ä¸‹é¢çš„å›¾ç‰‡ï¼Œç¡®ä¿å„ä¸ªæ¨¡åž‹æ–‡ä»¶éƒ½å·²ç»åŠ è½½å®Œæˆ ![ComfyUI Flux Devå·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg)\n\n1.  ç¡®ä¿åœ¨`DualCLIPLoader`èŠ‚ç‚¹ä¸­ä¸‹é¢çš„æ¨¡åž‹å·²åŠ è½½ï¼š\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n2.  ç¡®ä¿åœ¨`Load Diffusion Model`èŠ‚ç‚¹åŠ è½½äº†`flux1-dev.safetensors`\n3.  ç¡®ä¿åœ¨`Load VAE`èŠ‚ç‚¹ä¸­åŠ è½½äº†`ae.safetensors`\n4.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n### Flux.1 Schnell å®Œæ•´ç‰ˆæœ¬å·¥ä½œæµ\n\n#### 1\\. å·¥ä½œæµæ–‡ä»¶\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å·¥ä½œæµã€‚ ![Flux Schnell ç‰ˆæœ¬å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_t5fp8.png)\n\n#### 2\\. æ‰‹åŠ¨å®‰è£…æ¨¡åž‹\n\nå®Œæ•´æ¨¡åž‹æ–‡ä»¶åˆ—è¡¨ï¼š\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)\n\næ–‡ä»¶ä¿å­˜ä½ç½®ï¼š\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚   â””â”€â”€ t5xxl_fp8_e4m3fn.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚       â””â”€â”€ flux1-schnell.safetensors\n```\n\n#### 3\\. æŒ‰æ­¥éª¤æ£€æŸ¥ç¡®ä¿å·¥ä½œæµå¯ä»¥æ­£å¸¸è¿è¡Œ\n\n![Flux Schnell ç‰ˆæœ¬å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg)\n\n1.  ç¡®ä¿åœ¨`DualCLIPLoader`èŠ‚ç‚¹ä¸­ä¸‹é¢çš„æ¨¡åž‹å·²åŠ è½½ï¼š\n    *   clip\\_name1: t5xxl\\_fp8\\_e4m3fn.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n2.  ç¡®ä¿åœ¨`Load Diffusion Model`èŠ‚ç‚¹åŠ è½½äº†`flux1-schnell.safetensors`\n3.  ç¡®ä¿åœ¨`Load VAE`èŠ‚ç‚¹ä¸­åŠ è½½äº†`ae.safetensors`\n4.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n## Fp8 Checkpoint ç‰ˆæ–‡ç”Ÿå›¾ç¤ºä¾‹\n\nfp8 ç‰ˆæœ¬æ˜¯å¯¹ flux1 åŽŸç‰ˆ fp16 ç‰ˆæœ¬çš„é‡åŒ–ç‰ˆæœ¬ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šè¿™ä¸ªç‰ˆæœ¬çš„è´¨é‡ä¼šä½ŽäºŽ fp16 ç‰ˆæœ¬ï¼Œä½†åŒæ—¶å®ƒéœ€è¦çš„æ˜¾å­˜ä¹Ÿä¼šæ›´å°‘ï¼Œè€Œä¸”ä½ ä»…éœ€è¦å®‰è£…ä¸€ä¸ªæ¨¡åž‹æ–‡ä»¶å³å¯å°è¯•è¿è¡Œã€‚\n\n### Flux.1 Dev fp8 Checkpoint ç‰ˆå·¥ä½œæµ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å·¥ä½œæµã€‚ ![Flux Dev fp8 Checkpoint ç‰ˆæœ¬å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_fp8.png) è¯·ä¸‹è½½ [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true)å¹¶ä¿å­˜è‡³ `ComfyUI/models/Checkpoints/` ç›®å½•ä¸‹ã€‚ ç¡®ä¿å¯¹åº”çš„ `Load Checkpoint` èŠ‚ç‚¹åŠ è½½äº† `flux1-dev-fp8.safetensors`ï¼Œå³å¯æµ‹è¯•è¿è¡Œã€‚\n\n### Flux.1 Schnell fp8 Checkpoint ç‰ˆå·¥ä½œæµ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å·¥ä½œæµã€‚ ![Flux Schnell fp8 Checkpoint ç‰ˆæœ¬å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_fp8.png) è¯·ä¸‹è½½[flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true)å¹¶ä¿å­˜è‡³ `ComfyUI/models/Checkpoints/` ç›®å½•ä¸‹ã€‚ ç¡®ä¿å¯¹åº”çš„ `Load Checkpoint` èŠ‚ç‚¹åŠ è½½äº† `flux1-schnell-fp8.safetensors`ï¼Œå³å¯æµ‹è¯•è¿è¡Œã€‚"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-fill-dev",
  "markdown": "# ComfyUI Flux.1 fill dev ç¤ºä¾‹\n\n![Flux.1 fill dev](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-fill-dev-demo.jpeg)\n\nFlux.1 fill dev æ˜¯ [Black Forest Labs](https://blackforestlabs.ai/) æŽ¨å‡ºçš„ â€‹[FLUX.1 Tools å¥—ä»¶](https://blackforestlabs.ai/flux-1-tools/) ä¸­çš„æ ¸å¿ƒå·¥å…·ä¹‹ä¸€ï¼Œä¸“ä¸ºå›¾åƒä¿®å¤å’Œæ‰©å±•è®¾è®¡ã€‚ Flux.1 fill dev çš„æ ¸å¿ƒç‰¹ç‚¹ï¼š\n\n*   å¼ºå¤§çš„å›¾åƒé‡ç»˜(Inpainting)å’Œæ‰©ç»˜(Outpainting)èƒ½åŠ›ï¼Œç”Ÿæˆæ•ˆæžœä»…æ¬¡äºŽå•†ä¸šç‰ˆçš„ FLUX.1 Fill \\[pro\\]ã€‚\n*   å‡ºè‰²çš„æç¤ºè¯ç†è§£å’Œè·Ÿéšèƒ½åŠ›ï¼Œèƒ½å¤Ÿç²¾ç¡®æ•æ‰ç”¨æˆ·æ„å›¾å¹¶ä¸ŽåŽŸå›¾ä¿æŒé«˜åº¦ä¸€è‡´æ€§ã€‚\n*   é‡‡ç”¨å…ˆè¿›çš„å¼•å¯¼è’¸é¦è®­ç»ƒæŠ€æœ¯ï¼Œä½¿æ¨¡åž‹åœ¨ä¿æŒé«˜è´¨é‡è¾“å‡ºçš„åŒæ—¶æ›´åŠ é«˜æ•ˆã€‚\n*   å‹å¥½çš„è®¸å¯æ¡æ¬¾ï¼Œç”Ÿæˆçš„è¾“å‡ºå¯ç”¨äºŽä¸ªäººã€ç§‘å­¦å’Œå•†ä¸šç›®çš„ï¼Œå…·ä½“è¯·å‚è§ [FLUX.1 \\[dev\\] éžå•†ä¸šè®¸å¯è¯](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)ã€‚\n\næ¨¡åž‹å¼€æºåœ°å€ï¼š[FLUX.1 \\[dev\\]](https://huggingface.co/black-forest-labs/FLUX.1-dev) æœ¬æ–‡å°†åŸºäºŽ Flux.1 fill dev æ¨¡åž‹æ¥å®Œæˆ Inpainting å’Œ Outpainting çš„å·¥ä½œæµï¼Œ å¦‚æžœä½ ä¸å¤ªäº†è§£ Inpainting å’Œ Outpainting çš„å·¥ä½œæµå¯ä»¥å‚è€ƒ [ComfyUI å¸ƒå±€é‡ç»˜ç¤ºä¾‹](https://docs.comfy.org/zh-CN/tutorials/basic/inpaint) å’Œ [ComfyUI æ‰©å›¾ç¤ºä¾‹](https://docs.comfy.org/zh-CN/tutorials/basic/outpaint)ï¼Œéƒ¨åˆ†çš„ç›¸å…³è¯´æ˜Žã€‚\n\n## Flux.1 Fill dev å·¥ä½œæµæ¨¡åž‹å®‰è£…\n\nåœ¨å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆå®Œæˆ Flux.1 Fill dev æ¨¡åž‹æ–‡ä»¶çš„å®‰è£…ï¼Œ inpainting å’Œ outpainting çš„å·¥ä½œæµä¸­ä¼šä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¦‚æžœä½ ä¹‹å‰ä½¿ç”¨è¿‡å®Œæ•´ç‰ˆæœ¬çš„ [Flux.1 æ–‡ç”Ÿå›¾å·¥ä½œæµ](https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-text-to-image)ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸ªéƒ¨åˆ†ä½ ä»…éœ€è¦ä¸‹è½½ **flux1-fill-dev.safetensors** è¿™ä¸ªæ¨¡åž‹æ–‡ä»¶ã€‚ ä¸è¿‡ç”±äºŽä¸‹è½½å¯¹åº”æ¨¡åž‹éœ€è¦åŒæ„å¯¹åº”çš„ä½¿ç”¨åè®®ï¼Œæ‰€ä»¥è¯·è®¿é—® [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev)é¡µé¢ï¼Œç¡®ä¿ä½ å‚ç…§ä¸‹å›¾åŒæ„äº†å¯¹åº”çš„åè®®ã€‚ ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_fill_dev_agreement.jpg) å®Œæ•´æ¨¡åž‹åˆ—è¡¨ï¼š\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)\n*   [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)\n*   [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)\n\næ–‡ä»¶ä¿å­˜ä½ç½®ï¼š\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚    â”œâ”€â”€ clip_l.safetensors\nâ”‚   â”‚    â””â”€â”€ t5xxl_fp16.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚    â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ diffusion_models/\nâ”‚        â””â”€â”€ flux1-fill-dev.safetensors\n```\n\n## Flux.1 Fill dev inpainting å·¥ä½œæµ\n\n### 1\\. Inpainting å·¥ä½œæµåŠç›¸å…³ç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![ComfyUI Flux.1 inpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint.png) è¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![ComfyUI Flux.1 inpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input.png) \n\n### 2\\. å‚ç…§å›¾ç‰‡åºå·æ£€æŸ¥å®Œæˆå·¥ä½œæµè¿è¡Œ\n\n![ComfyUI Flux.1 Fill dev Inpainting å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_inpaint.jpg)\n\n1.  ç¡®ä¿åœ¨`Load Diffusion Model`èŠ‚ç‚¹åŠ è½½äº†`flux1-fill-dev.safetensors`\n2.  ç¡®ä¿åœ¨`DualCLIPLoader`èŠ‚ç‚¹ä¸­ä¸‹é¢çš„æ¨¡åž‹å·²åŠ è½½ï¼š\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n3.  ç¡®ä¿åœ¨`Load VAE`èŠ‚ç‚¹ä¸­åŠ è½½äº†`ae.safetensors`\n4.  åœ¨`Load Image`èŠ‚ç‚¹ä¸­ä¸Šä¼ äº†æ–‡æ¡£ä¸­æä¾›çš„è¾“å…¥å›¾ç‰‡ï¼Œå¦‚æžœä½ ä½¿ç”¨çš„æ˜¯ä¸å¸¦è’™ç‰ˆçš„ç‰ˆæœ¬ï¼Œè®°å¾—ä½¿ç”¨é®ç½©ç¼–è¾‘å™¨å®Œæˆè’™ç‰ˆçš„ç»˜åˆ¶\n5.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n## Flux.1 Fill dev Outpainting å·¥ä½œæµ\n\n### 1\\. Outpainting å·¥ä½œæµ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![ComfyUI Flux.1 outpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint.png) è¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![ComfyUI Flux.1 outpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint_input.png) \n\n### 2\\. å‚ç…§å›¾ç‰‡åºå·æ£€æŸ¥å®Œæˆå·¥ä½œæµè¿è¡Œ\n\n![ComfyUI Flux.1 Fill dev Outpainting å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_outpaint.jpg)\n\n1.  ç¡®ä¿åœ¨`Load Diffusion Model`èŠ‚ç‚¹åŠ è½½äº†`flux1-fill-dev.safetensors`\n2.  ç¡®ä¿åœ¨`DualCLIPLoader`èŠ‚ç‚¹ä¸­ä¸‹é¢çš„æ¨¡åž‹å·²åŠ è½½ï¼š\n    *   clip\\_name1: t5xxl\\_fp16.safetensors\n    *   clip\\_name2: clip\\_l.safetensors\n3.  ç¡®ä¿åœ¨`Load VAE`èŠ‚ç‚¹ä¸­åŠ è½½äº†`ae.safetensors`\n4.  åœ¨`Load Image`èŠ‚ç‚¹ä¸­ä¸Šä¼ äº†æ–‡æ¡£ä¸­æä¾›çš„è¾“å…¥å›¾ç‰‡\n5.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/api-nodes/moonvalley/moonvalley-video-generation",
  "markdown": "# Moonvalley API èŠ‚ç‚¹ ComfyUI å®˜æ–¹ç¤ºä¾‹\n\n![Legacy Browser](https://s1.hdslb.com/bfs/static/player/img/h5.png)\n\næ‚¨å½“å‰çš„æµè§ˆå™¨ä¸æ”¯æŒ HTML5 æ’­æ”¾å™¨\n\nè¯·æ›´æ¢æµè§ˆå™¨å†è¯•è¯•å“¦~\n\nMoonvalley Marey Realism v1.5 æ˜¯ä¸“ä¸ºå½±è§†çº§åˆ›ä½œæ‰“é€ çš„ AI è§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹ **å®Œå…¨ä½¿ç”¨å•†ä¸šæŽˆæƒå†…å®¹è®­ç»ƒ**ï¼Œç¡®ä¿ **ç‰ˆæƒæ— å¿§ï¼Œå•†ç”¨å®‰å…¨**ã€‚\n\n## äº§å“äº®ç‚¹\n\n*   æžå¼ºçš„æç¤ºè¯ç†è§£åŠ›: ç²¾å‡†è¿˜åŽŸå¤æ‚æç¤ºè¯æŒ‡ä»¤;\n*   åŽŸç”Ÿ 1080p é«˜æ¸…ç”»è´¨: è®­ç»ƒæ•°æ®é›†åŸºäºŽ **1080P** è§†é¢‘è®­ç»ƒï¼Œè¾“å‡ºç”»é¢ç»†è…»ã€‚\n*   çœŸå®žç‰©ç†ä¸ŽåŠ¨æ€è¡¨çŽ°: å¯¹ç‰©ç†è¿åŠ¨æ¨¡åž‹ã€è‡ªç„¶åŠ¨æ€è¿›è¡Œç²¾å‡†æ¨¡æ‹Ÿï¼Œå¸¦æ¥ä¸“ä¸šçº§åˆ«çš„çœŸå®žæ„Ÿã€‚\n*   å¤æ‚åœºæ™¯åˆ†å±‚ä¸Žé«˜çº§å…‰å½±æ•ˆæžœ: æ”¯æŒå¤æ‚åœºæ™¯çš„å‰ä¸­åŽæ™¯åˆ†å±‚ï¼Œæ™ºèƒ½ç†è§£ç©ºé—´å…³ç³»\n*   åŠ¨ä½œè¿ç§»å’Œå§¿æ€è¿ç§»ç­‰ç”Ÿäº§çº§æŽ§åˆ¶åŠŸèƒ½: è‡ªåŠ¨ç”Ÿæˆå¤åˆåœºæ™¯çš„çœŸå®žå…‰ç…§ã€‚\n\nç›®å‰ Moonvalley ç›¸å…³ API èŠ‚ç‚¹ï¼Œå·²åœ¨ ComfyUI ä¸­åŽŸç”Ÿæ”¯æŒï¼Œä½ å¯ä»¥åœ¨ ComfyUI ä¸­ä½¿ç”¨ å¯¹åº”çš„ æ–‡ç”Ÿè§†é¢‘ã€å›¾ç”Ÿè§†é¢‘ã€è§†é¢‘è½¬ç»˜ç­‰èƒ½åŠ›ã€‚\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_text_to_video.json)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![æ–‡æœ¬ç”Ÿè§†é¢‘ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg)\n\n1.  è¾“å…¥æ­£å‘æç¤ºè¯ï¼ˆæƒ³è¦å‡ºçŽ°åœ¨ç”»é¢ä¸­çš„å†…å®¹ï¼‰\n2.  è¾“å…¥è´Ÿå‘æç¤ºè¯ï¼ˆä¸æƒ³è¦å‡ºçŽ°åœ¨ç”»é¢ä¸­çš„å†…å®¹ï¼‰\n3.  ä¿®æ”¹è§†é¢‘è¾“å‡ºåˆ†è¾¨çŽ‡\n4.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘çš„ç”Ÿæˆ\n5.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Video` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„è§†é¢‘ï¼Œå¯¹åº”çš„è§†é¢‘ä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹\n\n## Moonvalley å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\n[\n\nä¸‹è½½ Json æ ¼å¼å·¥ä½œæµæ–‡ä»¶\n\n](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video.json)\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥å›¾ç‰‡ ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video_input.webp)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg)\n\n1.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½è¾“å…¥å›¾åƒ\n2.  è¾“å…¥æ­£å‘æç¤ºè¯ï¼ˆæƒ³è¦å‡ºçŽ°åœ¨ç”»é¢ä¸­çš„å†…å®¹ï¼‰\n3.  è¾“å…¥è´Ÿå‘æç¤ºè¯ï¼ˆä¸æƒ³è¦å‡ºçŽ°åœ¨ç”»é¢ä¸­çš„å†…å®¹ï¼‰\n4.  ä¿®æ”¹è§†é¢‘è¾“å‡ºåˆ†è¾¨çŽ‡\n5.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘çš„ç”Ÿæˆ\n6.  ç­‰å¾… API è¿”å›žç»“æžœåŽï¼Œä½ å¯åœ¨ `Save Video` èŠ‚ç‚¹ä¸­æŸ¥çœ‹ç”Ÿæˆçš„è§†é¢‘ï¼Œå¯¹åº”çš„è§†é¢‘ä¹Ÿä¼šè¢«ä¿å­˜è‡³ `ComfyUI/output/` ç›®å½•ä¸‹\n\n#### 0 ä¸ªè¡¨æƒ…\n\nåœ¨æ­¤é¡µé¢\n\n*   [äº§å“äº®ç‚¹](#%E4%BA%A7%E5%93%81%E4%BA%AE%E7%82%B9)\n*   [Moonvalley æ–‡ç”Ÿè§†é¢‘å·¥ä½œæµ](#moonvalley-%E6%96%87%E7%94%9F%E8%A7%86%E9%A2%91%E5%B7%A5%E4%BD%9C%E6%B5%81)\n*   [1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD)\n*   [2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ](#2-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C)\n*   [Moonvalley å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ](#moonvalley-%E5%9B%BE%E7%94%9F%E8%A7%86%E9%A2%91%E5%B7%A5%E4%BD%9C%E6%B5%81)\n*   [1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD-2)\n*   [2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ](#2-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C-2)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/image/omnigen/omnigen2",
  "markdown": "# ComfyUI OmniGen2 åŽŸç”Ÿå·¥ä½œæµç¤ºä¾‹ - ComfyUI\n\nOmniGen2 æ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”é«˜æ•ˆçš„ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡åž‹ï¼Œæ€»å‚æ•°é‡çº¦ **7B**ï¼ˆ3B æ–‡æœ¬æ¨¡åž‹ + 4B å›¾åƒç”Ÿæˆæ¨¡åž‹ï¼‰ã€‚ä¸Ž OmniGen v1 ä¸åŒï¼ŒOmniGen2 é‡‡ç”¨åˆ›æ–°çš„åŒè·¯å¾„ Transformer æž¶æž„ï¼Œå…·æœ‰å®Œå…¨ç‹¬ç«‹çš„æ–‡æœ¬è‡ªå›žå½’æ¨¡åž‹å’Œå›¾åƒæ‰©æ•£æ¨¡åž‹ï¼Œå®žçŽ°å‚æ•°è§£è€¦å’Œä¸“é—¨ä¼˜åŒ–ã€‚\n\n### æ¨¡åž‹äº®ç‚¹\n\n*   **è§†è§‰ç†è§£**ï¼šç»§æ‰¿äº† Qwen-VL-2.5 åŸºç¡€æ¨¡åž‹å¼ºå¤§çš„å›¾åƒå†…å®¹è§£é‡Šå’Œåˆ†æžèƒ½åŠ›\n*   **æ–‡ç”Ÿå›¾ç”Ÿæˆ**ï¼šä»Žæ–‡æœ¬æç¤ºåˆ›å»ºé«˜ä¿çœŸåº¦å’Œç¾Žè§‚çš„å›¾åƒ\n*   **æŒ‡ä»¤å¼•å¯¼çš„å›¾åƒç¼–è¾‘**ï¼šæ‰§è¡Œå¤æ‚çš„ã€åŸºäºŽæŒ‡ä»¤çš„å›¾åƒä¿®æ”¹ï¼Œåœ¨å¼€æºæ¨¡åž‹ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½\n*   **ä¸Šä¸‹æ–‡ç”Ÿæˆ**ï¼šå¤šåŠŸèƒ½çš„èƒ½åŠ›ï¼Œå¯ä»¥å¤„ç†å’Œçµæ´»ç»“åˆå¤šæ ·åŒ–çš„è¾“å…¥ï¼ˆåŒ…æ‹¬äººç‰©ã€å‚è€ƒå¯¹è±¡å’Œåœºæ™¯ï¼‰ï¼Œäº§ç”Ÿæ–°é¢–ä¸”è¿žè´¯çš„è§†è§‰è¾“å‡º\n\n### æŠ€æœ¯ç‰¹æ€§\n\n*   **åŒè·¯å¾„æž¶æž„**ï¼šåŸºäºŽ Qwen 2.5 VLï¼ˆ3Bï¼‰æ–‡æœ¬ç¼–ç å™¨ + ç‹¬ç«‹æ‰©æ•£ Transformerï¼ˆ4Bï¼‰\n*   **Omni-RoPE ä½ç½®ç¼–ç **ï¼šæ”¯æŒå¤šå›¾åƒç©ºé—´å®šä½å’Œèº«ä»½åŒºåˆ†\n*   **å‚æ•°è§£è€¦è®¾è®¡**ï¼šé¿å…æ–‡æœ¬ç”Ÿæˆå¯¹å›¾åƒè´¨é‡çš„è´Ÿé¢å½±å“\n*   æ”¯æŒå¤æ‚çš„æ–‡æœ¬ç†è§£å’Œå›¾åƒç†è§£\n*   å¯æŽ§çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘\n*   ä¼˜ç§€çš„ç»†èŠ‚ä¿æŒèƒ½åŠ›\n*   ç»Ÿä¸€æž¶æž„æ”¯æŒå¤šç§å›¾åƒç”Ÿæˆä»»åŠ¡\n*   æ–‡å­—ç”Ÿæˆèƒ½åŠ›ï¼šå¯ä»¥åœ¨å›¾åƒä¸­ç”Ÿæˆæ¸…æ™°çš„æ–‡å­—å†…å®¹\n\n## OmniGen2 æ¨¡åž‹ä¸‹è½½\n\nç”±äºŽæœ¬æ–‡æ¶‰åŠä¸åŒå·¥ä½œæµï¼Œå¯¹åº”çš„æ¨¡åž‹æ–‡ä»¶åŠå®‰è£…ä½ç½®å¦‚ä¸‹ï¼Œå¯¹åº”å·¥ä½œæµä¸­ä¹Ÿå·²åŒ…å«äº†æ¨¡åž‹æ–‡ä»¶ä¸‹è½½ä¿¡æ¯ï¼š **Diffusion Modelsï¼‰**\n\n*   [omnigen2\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/omnigen2_fp16.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/resolve/main/split_files/vae/ae.safetensors)\n\n**Text Encodersï¼‰**\n\n*   [qwen\\_2.5\\_vl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_fp16.safetensors)\n\næ–‡ä»¶ä¿å­˜ä½ç½®ï¼š\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ omnigen2_fp16.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ text_encoders/\nâ”‚       â””â”€â”€ qwen_2.5_vl_fp16.safetensors\n```\n\n## ComfyUI OmniGen2 æ–‡ç”Ÿå›¾å·¥ä½œæµ\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\n![æ–‡ç”Ÿå›¾å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_t2i.png)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµè¿è¡Œ\n\n![å·¥ä½œæµä½¿ç”¨æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg) è¯·å‚ç…§å›¾ç‰‡åºå·è¿›è¡Œé€æ­¥ç¡®è®¤ï¼Œæ¥ä¿è¯å¯¹åº”å·¥ä½œæµçš„é¡ºåˆ©è¿è¡Œï¼š\n\n1.  **åŠ è½½ä¸»æ¨¡åž‹**ï¼šç¡®ä¿ `Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `omnigen2_fp16.safetensors`\n2.  **åŠ è½½æ–‡æœ¬ç¼–ç å™¨**ï¼šç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `qwen_2.5_vl_fp16.safetensors`\n3.  **åŠ è½½ VAE**ï¼šç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `ae.safetensors`\n4.  **è®¾ç½®å›¾åƒå°ºå¯¸**ï¼šåœ¨ `EmptySD3LatentImage` èŠ‚ç‚¹è®¾ç½®ç”Ÿæˆå›¾ç‰‡çš„å°ºå¯¸ï¼ˆæŽ¨è 1024x1024ï¼‰\n5.  **è¾“å…¥æç¤ºè¯**ï¼š\n    *   åœ¨ç¬¬ä¸€ä¸ª `CLipTextEncode` èŠ‚ç‚¹ä¸­è¾“å…¥æ­£å‘æç¤ºè¯ï¼ˆæƒ³è¦å‡ºçŽ°åœ¨å›¾åƒä¸­çš„å†…å®¹ï¼‰\n    *   åœ¨ç¬¬äºŒä¸ª `CLipTextEncode` èŠ‚ç‚¹ä¸­è¾“å…¥è´Ÿå‘æç¤ºè¯ï¼ˆä¸æƒ³è¦å‡ºçŽ°åœ¨å›¾åƒä¸­çš„å†…å®¹ï¼‰\n6.  **å¼€å§‹ç”Ÿæˆ**ï¼šç‚¹å‡» `Queue Prompt` æŒ‰é’®ï¼Œæˆ–ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œæ–‡ç”Ÿå›¾\n7.  **æŸ¥çœ‹ç»“æžœ**ï¼šç”Ÿæˆå®ŒæˆåŽå¯¹åº”çš„å›¾ç‰‡ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `ComfyUI/output/` ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨ `SaveImage` èŠ‚ç‚¹ä¸­é¢„è§ˆ\n\n## ComfyUI OmniGen2 å›¾ç‰‡ç¼–è¾‘å·¥ä½œæµ\n\nOmniGen2 æœ‰ä¸°å¯Œçš„å›¾åƒç¼–è¾‘èƒ½åŠ›ï¼Œå¹¶ä¸”æ”¯æŒä¸ºå›¾åƒæ·»åŠ æ–‡æœ¬\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\n ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_image_edit.png) ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒä½œä¸ºè¾“å…¥å›¾ç‰‡ã€‚ ![è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/input_fairy.png) \n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµè¿è¡Œ\n\n![å·¥ä½œæµä½¿ç”¨æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg)\n\n1.  **åŠ è½½ä¸»æ¨¡åž‹**ï¼šç¡®ä¿ `Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `omnigen2_fp16.safetensors`\n2.  **åŠ è½½æ–‡æœ¬ç¼–ç å™¨**ï¼šç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `qwen_2.5_vl_fp16.safetensors`\n3.  **åŠ è½½ VAE**ï¼šç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `ae.safetensors`\n4.  **ä¸Šä¼ å›¾åƒ**ï¼šåœ¨ `Load Image` èŠ‚ç‚¹ä¸­ä¸Šä¼ æä¾›çš„å›¾ç‰‡\n5.  **è¾“å…¥æç¤ºè¯**ï¼š\n    *   åœ¨ç¬¬ä¸€ä¸ª `CLipTextEncode` èŠ‚ç‚¹ä¸­è¾“å…¥æ­£å‘æç¤ºè¯ï¼ˆæƒ³è¦å‡ºçŽ°åœ¨å›¾åƒä¸­çš„å†…å®¹ï¼‰\n    *   åœ¨ç¬¬äºŒä¸ª `CLipTextEncode` èŠ‚ç‚¹ä¸­è¾“å…¥è´Ÿå‘æç¤ºè¯ï¼ˆä¸æƒ³è¦å‡ºçŽ°åœ¨å›¾åƒä¸­çš„å†…å®¹ï¼‰\n6.  **å¼€å§‹ç”Ÿæˆ**ï¼šç‚¹å‡» `Queue Prompt` æŒ‰é’®ï¼Œæˆ–ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œæ–‡ç”Ÿå›¾\n7.  **æŸ¥çœ‹ç»“æžœ**ï¼šç”Ÿæˆå®ŒæˆåŽå¯¹åº”çš„å›¾ç‰‡ä¼šè‡ªåŠ¨ä¿å­˜åˆ° `ComfyUI/output/` ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨ `SaveImage` èŠ‚ç‚¹ä¸­é¢„è§ˆ\n\n### 3\\. å·¥ä½œæµè¡¥å……è¯´æ˜Ž\n\n*   å¦‚æžœä½ æƒ³è¦å¯ç”¨ç¬¬äºŒå¼ å›¾åƒè¾“å…¥ ï¼Œä½ å¯ä»¥å°†å·¥ä½œæµä¸­çŠ¶æ€ä¸ºç²‰ç´«è‰²çš„èŠ‚ç‚¹ä½¿ç”¨å¿«æ·é”® **Ctrl + B** æ¥å¯ç”¨å¯¹åº”çš„èŠ‚ç‚¹è¾“å…¥\n*   å¦‚æžœä½ æƒ³è¦è‡ªå®šä¹‰å°ºå¯¸ ï¼Œå¯ä»¥åˆ é™¤é“¾æŽ¥ `EmptySD3LatentImage` èŠ‚ç‚¹çš„ `Get image size` èŠ‚ç‚¹ï¼Œå¹¶è¾“å…¥è‡ªå®šä¹‰å°ºå¯¸"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fcontrolnet%2Fdepth-t2i-adapter",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2F3d%2Fhunyuan3D-2",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/controlnet/pose-controlnet-2-pass",
  "markdown": "# ComfyUI Pose ControlNet ä½¿ç”¨ç¤ºä¾‹ - ComfyUI\n\n## OpenPose ç®€ä»‹\n\n[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) æ˜¯ç”±å¡è€åŸºæ¢…éš†å¤§å­¦ï¼ˆCMUï¼‰å¼€å‘çš„å¼€æºå®žæ—¶å¤šäººå§¿æ€ä¼°è®¡ç³»ç»Ÿï¼Œæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦æŠ€æœ¯çªç ´ã€‚è¯¥ç³»ç»Ÿèƒ½å¤ŸåŒæ—¶æ£€æµ‹å›¾åƒä¸­å¤šä¸ªäººçš„ï¼š\n\n*   **äººä½“éª¨æž¶**ï¼š18ä¸ªå…³é”®ç‚¹ï¼ŒåŒ…æ‹¬å¤´éƒ¨ã€è‚©è†€ã€æ‰‹è‚˜ã€æ‰‹è…•ã€é«‹éƒ¨ã€è†ç›–å’Œè„šè¸ç­‰\n*   **é¢éƒ¨è¡¨æƒ…**ï¼š70ä¸ªé¢éƒ¨å…³é”®ç‚¹ï¼Œç”¨äºŽæ•æ‰å¾®è¡¨æƒ…å’Œé¢éƒ¨è½®å»“\n*   **æ‰‹éƒ¨ç»†èŠ‚**ï¼š21ä¸ªæ‰‹éƒ¨å…³é”®ç‚¹ï¼Œç²¾ç¡®è¡¨è¾¾æ‰‹æŒ‡å§¿åŠ¿å’Œæ‰‹åŠ¿\n*   **è„šéƒ¨å§¿æ€**ï¼š6ä¸ªè„šéƒ¨å…³é”®ç‚¹ï¼Œè®°å½•ç«™ç«‹å§¿åŠ¿å’ŒåŠ¨ä½œç»†èŠ‚\n\n![OpenPose ç¤ºä¾‹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/openpose_example.jpg) åœ¨ AI å›¾åƒç”Ÿæˆé¢†åŸŸï¼ŒOpenPose ç”Ÿæˆçš„éª¨éª¼ç»“æž„å›¾ä½œä¸º ControlNet çš„æ¡ä»¶è¾“å…¥ï¼Œèƒ½å¤Ÿç²¾ç¡®æŽ§åˆ¶ç”Ÿæˆäººç‰©çš„å§¿åŠ¿ã€åŠ¨ä½œå’Œè¡¨æƒ…ï¼Œè®©æˆ‘ä»¬èƒ½å¤ŸæŒ‰ç…§é¢„æœŸçš„å§¿æ€å’ŒåŠ¨ä½œç”Ÿæˆé€¼çœŸçš„äººç‰©å›¾åƒï¼Œæžå¤§æé«˜äº† AI ç”Ÿæˆå†…å®¹çš„å¯æŽ§æ€§å’Œå®žç”¨ä»·å€¼ã€‚ ç‰¹åˆ«é’ˆå¯¹æ—©æœŸ Stable diffusion 1.5 ç³»åˆ—çš„æ¨¡åž‹ï¼Œé€šè¿‡ OpenPose ç”Ÿæˆçš„éª¨éª¼å›¾ï¼Œå¯ä»¥æœ‰æ•ˆé¿å…äººç‰©åŠ¨ä½œã€è‚¢ä½“ã€è¡¨æƒ…ç•¸å˜çš„é—®é¢˜ã€‚\n\n### 1\\. Pose ControlNet å·¥ä½œæµç´ æ\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å·¥ä½œæµå›¾ç‰‡,å¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å·¥ä½œæµ ![ComfyUI å·¥ä½œæµ - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä¼šå°†å®ƒä½œä¸ºè¾“å…¥ ![ComfyUI Pose è¾“å…¥å›¾ç‰‡](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)\n\n### 2\\. æ‰‹åŠ¨æ¨¡åž‹å®‰è£…\n\n*   [control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)\n*   [majicmixRealistic\\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model&format=SafeTensor&size=pruned&fp=fp16)\n*   [japaneseStyleRealistic\\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model&format=SafeTensor&size=pruned&fp=fp16)\n*   [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ checkpoints/\nâ”‚   â”‚   â””â”€â”€ majicmixRealistic_v7.safetensors\nâ”‚   â”‚   â””â”€â”€ japaneseStyleRealistic_v20.safetensors\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€ vae-ft-mse-840000-ema-pruned.safetensors\nâ”‚   â””â”€â”€ controlnet/\nâ”‚       â””â”€â”€ control_v11p_sd15_openpose_fp16.safetensors\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI å·¥ä½œæµ - Pose ControlNet æµç¨‹å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg) æŒ‰ç…§å›¾ç‰‡ä¸­çš„æ•°å­—æ ‡è®°ï¼Œæ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š\n\n1.  ç¡®ä¿`Load Checkpoint`å¯ä»¥åŠ è½½ **majicmixRealistic\\_v7.safetensors**\n2.  ç¡®ä¿`Load VAE`å¯ä»¥åŠ è½½ **vae-ft-mse-840000-ema-pruned.safetensors**\n3.  ç¡®ä¿`Load ControlNet Model`å¯ä»¥åŠ è½½ **control\\_v11p\\_sd15\\_openpose\\_fp16.safetensors**\n4.  åœ¨`Load Image`èŠ‚ç‚¹ä¸­ç‚¹å‡»é€‰æ‹©æŒ‰é’®ï¼Œä¸Šä¼ ä¹‹å‰æä¾›çš„å§¿æ€è¾“å…¥å›¾ç‰‡ï¼Œæˆ–è€…ä½¿ç”¨ä½ è‡ªå·±çš„OpenPoseéª¨éª¼å›¾\n5.  ç¡®ä¿`Load Checkpoint`å¯ä»¥åŠ è½½ **japaneseStyleRealistic\\_v20.safetensors**\n6.  ç‚¹å‡»`Queue`æŒ‰é’®æˆ–ä½¿ç”¨å¿«æ·é”®`Ctrl(cmd) + Enter(å›žè½¦)`æ¥æ‰§è¡Œå›¾ç‰‡çš„ç”Ÿæˆ\n\n## Pose ControlNet äºŒæ¬¡å›¾ç”Ÿå›¾å·¥ä½œæµè®²è§£\n\næœ¬å·¥ä½œæµé‡‡ç”¨äºŒæ¬¡å›¾ç”Ÿå›¾ï¼ˆ2-passï¼‰çš„æ–¹å¼ï¼Œå°†å›¾åƒç”Ÿæˆåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š\n\n### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€å§¿æ€å›¾åƒç”Ÿæˆ\n\nåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨**majicmixRealistic\\_v7**æ¨¡åž‹ç»“åˆPose ControlNetç”Ÿæˆåˆæ­¥çš„äººç‰©å§¿æ€å›¾åƒï¼š\n\n1.  é¦–å…ˆé€šè¿‡`Load Checkpoint`åŠ è½½majicmixRealistic\\_v7æ¨¡åž‹\n2.  é€šè¿‡`Load ControlNet Model`åŠ è½½å§¿æ€æŽ§åˆ¶æ¨¡åž‹\n3.  è¾“å…¥çš„å§¿æ€å›¾è¢«é€å…¥`Apply ControlNet`èŠ‚ç‚¹ä¸Žæ­£å‘å’Œè´Ÿå‘æç¤ºè¯æ¡ä»¶ç»“åˆ\n4.  ç¬¬ä¸€ä¸ª`KSampler`èŠ‚ç‚¹ï¼ˆé€šå¸¸ä½¿ç”¨20-30æ­¥ï¼‰ç”ŸæˆåŸºç¡€çš„äººç‰©å§¿æ€å›¾åƒ\n5.  é€šè¿‡`VAE Decode`è§£ç å¾—åˆ°ç¬¬ä¸€é˜¶æ®µçš„åƒç´ ç©ºé—´å›¾åƒ\n\nè¿™ä¸ªé˜¶æ®µä¸»è¦å…³æ³¨æ­£ç¡®çš„äººç‰©å§¿æ€ã€å§¿åŠ¿å’ŒåŸºæœ¬ç»“æž„ï¼Œç¡®ä¿ç”Ÿæˆçš„äººç‰©ç¬¦åˆè¾“å…¥çš„éª¨éª¼å§¿æ€ã€‚\n\n### ç¬¬äºŒé˜¶æ®µï¼šé£Žæ ¼ä¼˜åŒ–ä¸Žç»†èŠ‚å¢žå¼º\n\nåœ¨ç¬¬äºŒé˜¶æ®µï¼Œå°†ç¬¬ä¸€é˜¶æ®µçš„è¾“å‡ºå›¾åƒä½œä¸ºå‚è€ƒï¼Œä½¿ç”¨**japaneseStyleRealistic\\_v20**æ¨¡åž‹è¿›è¡Œé£Žæ ¼åŒ–å’Œç»†èŠ‚å¢žå¼ºï¼š\n\n1.  ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„å›¾åƒé€šè¿‡`Upscale latent`èŠ‚ç‚¹åˆ›å»ºçš„æ›´å¤§åˆ†è¾¨çŽ‡çš„æ½œåœ¨ç©ºé—´\n2.  ç¬¬äºŒä¸ª`Load Checkpoint`åŠ è½½japaneseStyleRealistic\\_v20æ¨¡åž‹ï¼Œè¿™ä¸ªæ¨¡åž‹ä¸“æ³¨äºŽç»†èŠ‚å’Œé£Žæ ¼\n3.  ç¬¬äºŒä¸ª`KSampler`èŠ‚ç‚¹ä½¿ç”¨è¾ƒä½Žçš„`denoise`å¼ºåº¦ï¼ˆé€šå¸¸0.4-0.6ï¼‰è¿›è¡Œç»†åŒ–ï¼Œä¿ç•™ç¬¬ä¸€é˜¶æ®µçš„åŸºç¡€ç»“æž„\n4.  æœ€ç»ˆé€šè¿‡ç¬¬äºŒä¸ª`VAE Decode`å’Œ`Save Image`èŠ‚ç‚¹è¾“å‡ºæ›´é«˜è´¨é‡ã€æ›´å¤§åˆ†è¾¨çŽ‡çš„å›¾åƒ\n\nè¿™ä¸ªé˜¶æ®µä¸»è¦å…³æ³¨é£Žæ ¼ç»Ÿä¸€æ€§ã€ç»†èŠ‚ä¸°å¯Œåº¦å’Œæå‡æ•´ä½“ç”»é¢è´¨é‡ã€‚\n\n## äºŒæ¬¡å›¾ç”Ÿå›¾çš„ä¼˜åŠ¿\n\nä¸Žå•æ¬¡ç”Ÿæˆç›¸æ¯”ï¼ŒäºŒæ¬¡å›¾ç”Ÿå›¾æ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š\n\n1.  **æ›´é«˜åˆ†è¾¨çŽ‡**ï¼šé€šè¿‡äºŒæ¬¡å¤„ç†å¯ä»¥ç”Ÿæˆè¶…å‡ºå•æ¬¡ç”Ÿæˆèƒ½åŠ›çš„é«˜åˆ†è¾¨çŽ‡å›¾åƒ\n2.  **é£Žæ ¼æ··åˆ**ï¼šå¯ä»¥ç»“åˆä¸åŒæ¨¡åž‹çš„ä¼˜åŠ¿ï¼Œå¦‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å†™å®žæ¨¡åž‹ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨é£Žæ ¼åŒ–æ¨¡åž‹\n3.  **æ›´å¥½çš„ç»†èŠ‚**ï¼šç¬¬äºŒé˜¶æ®µå¯ä»¥ä¸“æ³¨äºŽä¼˜åŒ–ç»†èŠ‚ï¼Œè€Œä¸å¿…æ‹…å¿ƒæ•´ä½“ç»“æž„\n4.  **ç²¾ç¡®æŽ§åˆ¶**ï¼šå§¿æ€æŽ§åˆ¶åœ¨ç¬¬ä¸€é˜¶æ®µå®ŒæˆåŽï¼Œç¬¬äºŒé˜¶æ®µå¯ä»¥ä¸“æ³¨äºŽé£Žæ ¼å’Œç»†èŠ‚çš„å®Œå–„\n5.  **é™ä½ŽGPUè´Ÿæ‹…**ï¼šåˆ†ä¸¤æ¬¡ç”Ÿæˆå¯ä»¥åœ¨æœ‰é™çš„GPUèµ„æºä¸‹ç”Ÿæˆé«˜è´¨é‡å¤§å›¾\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/image/hidream/hidream-i1",
  "markdown": "# ComfyUI åŽŸç”Ÿç‰ˆæœ¬ HiDream-I1 æ–‡ç”Ÿå›¾å·¥ä½œæµç¤ºä¾‹ - ComfyUI\n\n![HiDream-I1 æ¼”ç¤º](https://raw.githubusercontent.com/HiDream-ai/HiDream-I1/main/assets/demo.jpg) HiDream-I1 æ˜¯æ™ºè±¡æœªæ¥(HiDream-ai)äºŽ2025å¹´4æœˆ7æ—¥æ­£å¼å¼€æºçš„æ–‡ç”Ÿå›¾æ¨¡åž‹ã€‚è¯¥æ¨¡åž‹æ‹¥æœ‰17Bå‚æ•°è§„æ¨¡ï¼Œé‡‡ç”¨ [MIT è®¸å¯è¯](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE) å‘å¸ƒï¼Œæ”¯æŒç”¨äºŽä¸ªäººé¡¹ç›®ã€ç§‘å­¦ç ”ç©¶ä»¥åŠå•†ç”¨ï¼Œç›®å‰åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¯¥æ¨¡åž‹è¡¨çŽ°ä¼˜å¼‚ã€‚\n\n## æ¨¡åž‹ç‰¹ç‚¹\n\n**æ··åˆæž¶æž„è®¾è®¡** é‡‡ç”¨â€‹â€‹æ‰©æ•£æ¨¡åž‹ï¼ˆDiTï¼‰â€‹â€‹ä¸Žâ€‹â€‹æ··åˆä¸“å®¶ç³»ç»Ÿï¼ˆMoEï¼‰â€‹â€‹çš„ç»“åˆæž¶æž„ï¼š\n\n*   ä¸»ä½“åŸºäºŽDiffusion Transformerï¼ˆDiTï¼‰ï¼Œé€šè¿‡åŒæµMMDiTæ¨¡å—å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå•æµDiTæ¨¡å—ä¼˜åŒ–å…¨å±€ä¸€è‡´æ€§ã€‚\n*   åŠ¨æ€è·¯ç”±æœºåˆ¶çµæ´»åˆ†é…è®¡ç®—èµ„æºï¼Œæå‡å¤æ‚åœºæ™¯å¤„ç†èƒ½åŠ›ï¼Œåœ¨è‰²å½©è¿˜åŽŸã€è¾¹ç¼˜å¤„ç†ç­‰ç»†èŠ‚ä¸Šè¡¨çŽ°ä¼˜å¼‚ã€‚\n\n**å¤šæ¨¡æ€æ–‡æœ¬ç¼–ç å™¨é›†æˆ** æ•´åˆå››ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼š\n\n*   OpenCLIP ViT-bigGã€OpenAI CLIP ViT-Lï¼ˆè§†è§‰è¯­ä¹‰å¯¹é½ï¼‰\n*   T5-XXLï¼ˆé•¿æ–‡æœ¬è§£æžï¼‰\n*   Llama-3.1-8B-Instructï¼ˆæŒ‡ä»¤ç†è§£ï¼‰ è¿™ä¸€ç»„åˆä½¿å…¶åœ¨é¢œè‰²ã€æ•°é‡ã€ç©ºé—´å…³ç³»ç­‰å¤æ‚è¯­ä¹‰è§£æžä¸Šè¾¾åˆ°SOTAæ°´å¹³ï¼Œä¸­æ–‡æç¤ºè¯æ”¯æŒæ˜¾è‘—ä¼˜äºŽåŒç±»å¼€æºæ¨¡åž‹ã€‚\n\n**åŽŸå§‹æ¨¡åž‹ç‰ˆæœ¬** æ™ºè±¡æœªæ¥(HiDream-ai)æä¾›äº†ä¸‰ä¸ªç‰ˆæœ¬çš„ HiDream-I1 æ¨¡åž‹ï¼Œä»¥æ»¡è¶³ä¸åŒåœºæ™¯çš„éœ€æ±‚ï¼Œä¸‹é¢æ˜¯åŽŸå§‹çš„æ¨¡åž‹ä»“åº“é“¾æŽ¥ï¼š\n\n*   å®Œæ•´ç‰ˆæœ¬ï¼š[ðŸ¤— HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full) æŽ¨ç†æ­¥æ•°ä¸º 50\n*   è’¸é¦å¼€å‘ç‰ˆæœ¬ï¼š[ðŸ¤— HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev) æŽ¨ç†æ­¥æ•°ä¸º 28\n*   è’¸é¦å¿«é€Ÿç‰ˆæœ¬ï¼š[ðŸ¤— HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) æŽ¨ç†æ­¥æ•°ä¸º 16\n\n## å…³äºŽæœ¬ç¯‡å·¥ä½œæµç¤ºä¾‹\n\næˆ‘ä»¬å°†åœ¨æœ¬ç¯‡ç¤ºä¾‹ä¸­ä½¿ç”¨ ComfyOrg çš„ repackaged çš„ç‰ˆæœ¬ï¼Œä½ å¯ä»¥åœ¨ [HiDream-I1\\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) ä»“åº“ä¸­æ‰¾åˆ°æˆ‘ä»¬å°†åœ¨æœ¬ç¯‡ç¤ºä¾‹ä¸­ä½¿ç”¨çš„æ‰€æœ‰æ¨¡åž‹æ–‡ä»¶ã€‚\n\nå¯¹åº”ä¸åŒ ComfyUI åŽŸç”Ÿç‰ˆæœ¬ HiDream-I1 å·¥ä½œæµçš„æ¨¡åž‹è¦æ±‚åŸºæœ¬ä¸Šæ˜¯ç›¸åŒçš„ï¼Œåªæœ‰ä½¿ç”¨è¿‡çš„ [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) æ–‡ä»¶ä¸åŒã€‚ å¦‚æžœä½ ä¸çŸ¥é“å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬ï¼Œè¯·å‚è€ƒä»¥ä¸‹å»ºè®®ï¼š\n\n*   **HiDream-I1-Full** å¯ä»¥ç”Ÿæˆè´¨é‡æœ€é«˜çš„å›¾åƒ\n*   **HiDream-I1-Dev** åœ¨ç”Ÿæˆè¾ƒé«˜è´¨é‡çš„å›¾åƒçš„åŒæ—¶ï¼Œåˆå…¼é¡¾é€Ÿåº¦\n*   **HiDream-I1-Fast** åªéœ€è¦ 16 æ­¥å°±å¯ä»¥ç”Ÿæˆå›¾åƒï¼Œé€‚åˆéœ€è¦å®žæ—¶è¿­ä»£çš„åœºæ™¯\n\nå¯¹äºŽ **dev** å’Œ **fast** ç‰ˆæœ¬å¹¶ä¸éœ€è¦è´Ÿå‘æç¤ºè¯ï¼Œæ‰€ä»¥è¯·åœ¨é‡‡æ ·æ—¶è®¾ç½®`cfg` å‚æ•°ä¸º `1.0`ï¼Œæˆ‘ä»¬å¯¹åº”å‚æ•°è®¾ç½®å·²åœ¨ç›¸å…³å·¥ä½œæµä¸­å¤‡æ³¨ã€‚\n\n### æ¨¡åž‹å®‰è£…\n\nä¸‹é¢çš„æ¨¡åž‹æ–‡ä»¶æ˜¯æˆ‘ä»¬ä¼šå…±ç”¨çš„æ¨¡åž‹æ–‡ä»¶ï¼Œè¯·ç‚¹å‡»å¯¹åº”çš„é“¾æŽ¥è¿›è¡Œä¸‹è½½ï¼Œå¹¶å‚ç…§æ¨¡åž‹æ–‡ä»¶ä¿å­˜ä½ç½®è¿›è¡Œä¿å­˜ï¼Œå¯¹åº”çš„ **diffusion models** æ¨¡åž‹æˆ‘ä»¬ä¼šåœ¨å¯¹åº”å·¥ä½œæµä¸­å¼•å¯¼ä½ è¿›è¡Œä¸‹è½½ã€‚ **text\\_encoders**ï¼š\n\n*   [clip\\_l\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)\n*   [clip\\_g\\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)\n*   [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) è¿™ä¸ªæ¨¡åž‹åœ¨è®¸å¤šçš„å·¥ä½œæµä¸­éƒ½æœ‰ä½¿ç”¨è¿‡ï¼Œä½ å¯èƒ½å·²ç»ä¸‹è½½äº†è¿™ä¸ªæ–‡ä»¶ã€‚\n*   [llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) è¿™ä¸ªæ˜¯ Flux çš„ VAE æ¨¡åž‹ï¼Œå¦‚æžœä½ ä¹‹å‰ä½¿ç”¨è¿‡ Flux çš„å·¥ä½œæµï¼Œä½ å¯èƒ½å·²ç»ä¸‹è½½äº†è¿™ä¸ªæ–‡ä»¶ã€‚\n\n**diffusion models** è¿™éƒ¨åˆ†æˆ‘ä»¬å°†åœ¨å¯¹åº”å·¥ä½œæµä¸­å…·ä½“å¼•å¯¼ä¸‹è½½å¯¹åº”çš„æ¨¡åž‹æ–‡ä»¶ã€‚ æ¨¡åž‹æ–‡ä»¶ä¿å­˜ä½ç½®\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ text_encoders/\nâ”‚   â”‚   â”œâ”€â”€â”€ clip_l_hidream.safetensors\nâ”‚   â”‚   â”œâ”€â”€â”€ clip_g_hidream.safetensors\nâ”‚   â”‚   â”œâ”€â”€â”€ t5xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â”‚   â””â”€â”€â”€ llama_3.1_8b_instruct_fp8_scaled.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensors\nâ”‚   â””â”€â”€ ðŸ“‚ diffusion_models/\nâ”‚       â””â”€â”€ ...               # å°†åœ¨å¯¹åº”ç‰ˆæœ¬çš„å·¥ä½œæµä¸­å¼•å¯¼ä½ è¿›è¡Œå®‰è£…            \n```\n\n### HiDream-I1 full ç‰ˆæœ¬å·¥ä½œæµ\n\n#### 1\\. æ¨¡åž‹æ–‡ä»¶ä¸‹è½½\n\nè¯·æ ¹æ®ä½ çš„ç¡¬ä»¶æƒ…å†µé€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬ï¼Œç‚¹å‡»é“¾æŽ¥å¹¶ä¸‹è½½å¯¹åº”çš„æ¨¡åž‹æ–‡ä»¶ä¿å­˜åˆ° `ComfyUI/models/diffusion_models/` æ–‡ä»¶å¤¹ä¸‹ã€‚\n\n*   FP8 ç‰ˆæœ¬ï¼š[hidream\\_i1\\_full\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) éœ€è¦ 16GB ä»¥ä¸Šçš„æ˜¾å­˜\n*   å®Œæ•´ç‰ˆæœ¬ï¼š[hidream\\_i1\\_full\\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) éœ€è¦ 27GB ä»¥ä¸Šçš„æ˜¾å­˜\n\n#### 2\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![HiDream-I1 full ç‰ˆæœ¬å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_full.png) \n\n#### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![HiDream-I1 full ç‰ˆæœ¬æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg) æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n1.  ç¡®ä¿`Load Diffusion Model` èŠ‚ç‚¹ä¸­ä½¿ç”¨çš„æ˜¯ `hidream_i1_full_fp8.safetensors` æ–‡ä»¶\n2.  ç¡®ä¿`QuadrupleCLIPLoader` ä¸­å››ä¸ªå¯¹åº”çš„ text encoder è¢«æ­£ç¡®åŠ è½½\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  ç¡®ä¿`Load VAE` èŠ‚ç‚¹ä¸­ä½¿ç”¨çš„æ˜¯ `ae.safetensors` æ–‡ä»¶\n4.  å¯¹äºŽ **full** ç‰ˆæœ¬ä½ éœ€è¦è®¾ç½® `ModelSamplingSD3` ä¸­çš„ `shift` å‚æ•°ä¸º `3.0`\n5.  å¯¹äºŽ `Ksampler` èŠ‚ç‚¹ï¼Œä½ éœ€è¦è¿›è¡Œä»¥ä¸‹è®¾ç½®\n    *   `steps` è®¾ç½®ä¸º `50`\n    *   `cfg` è®¾ç½®ä¸º `5.0`\n    *   (å¯é€‰) `sampler` è®¾ç½®ä¸º `lcm`\n    *   (å¯é€‰) `scheduler` è®¾ç½®ä¸º `normal`\n6.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\n### HiDream-I1 dev ç‰ˆæœ¬å·¥ä½œæµ\n\n#### 1\\. æ¨¡åž‹æ–‡ä»¶ä¸‹è½½\n\nè¯·æ ¹æ®ä½ çš„ç¡¬ä»¶æƒ…å†µé€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬ï¼Œç‚¹å‡»é“¾æŽ¥å¹¶ä¸‹è½½å¯¹åº”çš„æ¨¡åž‹æ–‡ä»¶ä¿å­˜åˆ° `ComfyUI/models/diffusion_models/` æ–‡ä»¶å¤¹ä¸‹ã€‚\n\n*   FP8 ç‰ˆæœ¬ï¼š[hidream\\_i1\\_dev\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) éœ€è¦ 16GB ä»¥ä¸Šçš„æ˜¾å­˜\n*   å®Œæ•´ç‰ˆæœ¬ï¼š[hidream\\_i1\\_dev\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) éœ€è¦ 27GB ä»¥ä¸Šçš„æ˜¾å­˜\n\n#### 2\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![HiDream-I1 dev ç‰ˆæœ¬å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_dev.png)\n\n#### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n ![HiDream-I1 dev ç‰ˆæœ¬æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg) æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n1.  ç¡®ä¿`Load Diffusion Model` èŠ‚ç‚¹ä¸­ä½¿ç”¨çš„æ˜¯ `hidream_i1_dev_fp8.safetensors` æ–‡ä»¶\n2.  ç¡®ä¿`QuadrupleCLIPLoader` ä¸­å››ä¸ªå¯¹åº”çš„ text encoder è¢«æ­£ç¡®åŠ è½½\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  ç¡®ä¿`Load VAE` èŠ‚ç‚¹ä¸­ä½¿ç”¨çš„æ˜¯ `ae.safetensors` æ–‡ä»¶\n4.  å¯¹äºŽ **dev** ç‰ˆæœ¬ä½ éœ€è¦è®¾ç½® `ModelSamplingSD3` ä¸­çš„ `shift` å‚æ•°ä¸º `6.0`\n5.  å¯¹äºŽ `Ksampler` èŠ‚ç‚¹ï¼Œä½ éœ€è¦è¿›è¡Œä»¥ä¸‹è®¾ç½®\n    *   `steps` è®¾ç½®ä¸º `28`\n    *   (é‡è¦) `cfg` è®¾ç½®ä¸º `1.0`\n    *   (å¯é€‰) `sampler` è®¾ç½®ä¸º `lcm`\n    *   (å¯é€‰) `scheduler` è®¾ç½®ä¸º `normal`\n6.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\n### HiDream-I1 fast ç‰ˆæœ¬å·¥ä½œæµ\n\n#### 1\\. æ¨¡åž‹æ–‡ä»¶ä¸‹è½½\n\nè¯·æ ¹æ®ä½ çš„ç¡¬ä»¶æƒ…å†µé€‰æ‹©åˆé€‚çš„ç‰ˆæœ¬ï¼Œç‚¹å‡»é“¾æŽ¥å¹¶ä¸‹è½½å¯¹åº”çš„æ¨¡åž‹æ–‡ä»¶ä¿å­˜åˆ° `ComfyUI/models/diffusion_models/` æ–‡ä»¶å¤¹ä¸‹ã€‚\n\n*   FP8 ç‰ˆæœ¬ï¼š[hidream\\_i1\\_fast\\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) éœ€è¦ 16GB ä»¥ä¸Šçš„æ˜¾å­˜\n*   å®Œæ•´ç‰ˆæœ¬ï¼š[hidream\\_i1\\_fast\\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_bf16.safetensors?download=true) éœ€è¦ 27GB ä»¥ä¸Šçš„æ˜¾å­˜\n\n#### 2\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nè¯·ä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![HiDream-I1 fast ç‰ˆæœ¬å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_fast.png)\n\n#### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![HiDream-I1 fast ç‰ˆæœ¬æ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg) æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n1.  ç¡®ä¿`Load Diffusion Model` èŠ‚ç‚¹ä¸­ä½¿ç”¨çš„æ˜¯ `hidream_i1_fast_fp8.safetensors` æ–‡ä»¶\n2.  ç¡®ä¿`QuadrupleCLIPLoader` ä¸­å››ä¸ªå¯¹åº”çš„ text encoder è¢«æ­£ç¡®åŠ è½½\n    *   clip\\_l\\_hidream.safetensors\n    *   clip\\_g\\_hidream.safetensors\n    *   t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors\n    *   llama\\_3.1\\_8b\\_instruct\\_fp8\\_scaled.safetensors\n3.  ç¡®ä¿`Load VAE` èŠ‚ç‚¹ä¸­ä½¿ç”¨çš„æ˜¯ `ae.safetensors` æ–‡ä»¶\n4.  å¯¹äºŽ **fast** ç‰ˆæœ¬ä½ éœ€è¦è®¾ç½® `ModelSamplingSD3` ä¸­çš„ `shift` å‚æ•°ä¸º `3.0`\n5.  å¯¹äºŽ `Ksampler` èŠ‚ç‚¹ï¼Œä½ éœ€è¦è¿›è¡Œä»¥ä¸‹è®¾ç½®\n    *   `steps` è®¾ç½®ä¸º `16`\n    *   (é‡è¦) `cfg` è®¾ç½®ä¸º `1.0`\n    *   (å¯é€‰) `sampler` è®¾ç½®ä¸º `lcm`\n    *   (å¯é€‰) `scheduler` è®¾ç½®ä¸º `normal`\n6.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ\n\n## ä½¿ç”¨å»ºè®®\n\n*   è™½ç„¶ HiDream-I1 æ”¯æŒä¸­æ–‡æç¤ºè¯ï¼Œä½†å»ºè®®è¿˜æ˜¯ä¼˜å…ˆä½¿ç”¨è‹±æ–‡æç¤ºè¯æ¥ä¿è¯å‡†ç¡®æ€§\n*   ä½ å¯ä»¥ä½¿ç”¨ fast ç‰ˆæœ¬æ¥å¿«é€Ÿç”Ÿæˆç¤ºä¾‹éªŒè¯ï¼Œç„¶åŽå†ç”¨å®Œæ•´ç‰ˆæœ¬çš„æ¨¡åž‹æ¥ç”Ÿæˆè¾ƒé«˜è´¨é‡çš„å›¾åƒ\n\n## å…¶å®ƒç›¸å…³èµ„æº\n\n### GGUF ç‰ˆæœ¬æ¨¡åž‹\n\n*   [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)\n*   [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)\n\nä½ éœ€è¦ä½¿ç”¨ City96 çš„ [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) ä¸­çš„ `Unet Loader (GGUF)`èŠ‚ç‚¹æ›¿æ¢æŽ‰ `Load Diffusion Model` èŠ‚ç‚¹æ¥ä½¿ç”¨ GGUF ç‰ˆæœ¬æ¨¡åž‹ã€‚\n\n*   [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF)\n\n### NF4 ç‰ˆæœ¬æ¨¡åž‹\n\n*   [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)\n*   ä½¿ç”¨ [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) èŠ‚ç‚¹æ¥ä½¿ç”¨ NF4 ç‰ˆæœ¬æ¨¡åž‹ã€‚\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-kontext-dev",
  "markdown": "# ComfyUI Flux Kontext Dev åŽŸç”Ÿå·¥ä½œæµç¤ºä¾‹\n\n![Legacy Browser](https://s1.hdslb.com/bfs/static/player/img/h5.png)\n\næ‚¨å½“å‰çš„æµè§ˆå™¨ä¸æ”¯æŒ HTML5 æ’­æ”¾å™¨\n\nè¯·æ›´æ¢æµè§ˆå™¨å†è¯•è¯•å“¦~\n\nFLUX.1 Kontext æ˜¯ Black Forest Labs æŽ¨å‡ºçš„çªç ´æ€§å¤šæ¨¡æ€å›¾åƒç¼–è¾‘æ¨¡åž‹ï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒåŒæ—¶è¾“å…¥ï¼Œèƒ½å¤Ÿæ™ºèƒ½ç†è§£å›¾åƒä¸Šä¸‹æ–‡å¹¶æ‰§è¡Œç²¾ç¡®ç¼–è¾‘ã€‚å…¶å¼€å‘ç‰ˆæ˜¯ä¸€ä¸ªæ‹¥æœ‰ 120 äº¿å‚æ•°çš„å¼€æºæ‰©æ•£å˜åŽ‹å™¨æ¨¡åž‹ï¼Œå…·æœ‰å‡ºè‰²çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å’Œè§’è‰²ä¸€è‡´æ€§ä¿æŒï¼Œå³ä½¿ç»è¿‡å¤šæ¬¡è¿­ä»£ç¼–è¾‘ï¼Œä¹Ÿèƒ½ç¡®ä¿äººç‰©ç‰¹å¾ã€æž„å›¾å¸ƒå±€ç­‰å…³é”®å…ƒç´ ä¿æŒç¨³å®šã€‚ ä¸Ž FLUX.1 Kontext å¥—ä»¶å…·å¤‡ç›¸åŒçš„æ ¸å¿ƒèƒ½åŠ›ï¼š è§’è‰²ä¸€è‡´æ€§ï¼šåœ¨å¤šä¸ªåœºæ™¯å’ŒçŽ¯å¢ƒä¸­ä¿ç•™å›¾åƒçš„ç‹¬ç‰¹å…ƒç´ ï¼Œä¾‹å¦‚å›¾ç‰‡ä¸­çš„å‚è€ƒè§’è‰²æˆ–ç‰©ä½“ã€‚ å±€éƒ¨ç¼–è¾‘ï¼šå¯¹å›¾åƒä¸­çš„ç‰¹å®šå…ƒç´ è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ä¿®æ”¹ï¼Œè€Œä¸å½±å“å…¶ä»–éƒ¨åˆ†ã€‚ é£Žæ ¼å‚è€ƒï¼šæ ¹æ®æ–‡æœ¬æç¤ºï¼Œåœ¨ä¿ç•™å‚è€ƒå›¾åƒç‹¬ç‰¹é£Žæ ¼çš„åŒæ—¶ç”Ÿæˆæ–°é¢–åœºæ™¯ã€‚ äº¤äº’é€Ÿåº¦ï¼šå›¾åƒç”Ÿæˆå’Œç¼–è¾‘çš„å»¶è¿Ÿæžå°ã€‚ è™½ç„¶ä¹‹å‰å‘å¸ƒçš„ API ç‰ˆæœ¬æä¾›äº†æœ€é«˜çš„ä¿çœŸåº¦å’Œé€Ÿåº¦ï¼Œä½† FLUX.1 Kontext \\[Dev\\] å®Œå…¨åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œï¼Œä¸ºå¸Œæœ›è¿›è¡Œå®žéªŒçš„å¼€å‘è€…ã€ç ”ç©¶äººå‘˜å’Œé«˜çº§ç”¨æˆ·æä¾›äº†æ— ä¸Žä¼¦æ¯”çš„çµæ´»æ€§ã€‚\n\n### ç‰ˆæœ¬è¯´æ˜Ž\n\n*   **\\[FLUX.1 Kontext \\[pro\\]** - å•†ä¸šç‰ˆæœ¬ï¼Œä¸“æ³¨å¿«é€Ÿè¿­ä»£ç¼–è¾‘\n*   **FLUX.1 Kontext \\[max\\]** - å®žéªŒç‰ˆæœ¬ï¼Œæ›´å¼ºçš„æç¤ºéµå¾ªèƒ½åŠ›\n*   **FLUX.1 Kontext \\[dev\\]** - å¼€æºç‰ˆæœ¬ï¼ˆæœ¬æ•™ç¨‹ä½¿ç”¨ï¼‰ï¼Œ12Bå‚æ•°ï¼Œä¸»è¦ç”¨äºŽç ”ç©¶\n\nç›®å‰åœ¨ ComfyUI ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨æ‰€æœ‰çš„è¿™äº›ç‰ˆæœ¬ï¼Œå…¶ä¸­ [Pro åŠ Max ç‰ˆæœ¬](https://docs.comfy.org/zh-CN/tutorials/api-nodes/black-forest-labs/flux-1-kontext) å¯ä»¥é€šè¿‡ API èŠ‚ç‚¹æ¥è¿›è¡Œè°ƒç”¨ï¼Œè€Œ Dev ç‰ˆæœ¬å¼€æºç‰ˆæœ¬è¯·å‚è€ƒæœ¬ç¯‡æŒ‡å—ä¸­çš„è¯´æ˜Žã€‚\n\n## å·¥ä½œæµè¯´æ˜Ž\n\nç›®å‰åœ¨æœ¬ç¯‡æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ¶‰åŠäº†ä¸¤ç±»å·¥ä½œæµï¼Œæœ¬è´¨ä¸Šä»–ä»¬å…¶å®žæ˜¯ç›¸åŒçš„ï¼Œ\n\n*   ä½¿ç”¨äº†ç»„èŠ‚ç‚¹ **FLUX.1 Kontext Image Edit** çš„å·¥ä½œæµï¼Œä½¿å¾—æ•´ä¸ªç•Œé¢å’Œå·¥ä½œæµå¤ç”¨èµ·æ¥å˜å¾—ç®€å•\n*   è€Œå¦ä¸€ä¸ªå·¥ä½œæµæ²¡æœ‰ä½¿ç”¨ç»„èŠ‚ç‚¹ï¼Œæ˜¯å®Œæ•´çš„åŽŸå§‹å·¥ä½œæµã€‚\n\nä½¿ç”¨ç»„èŠ‚ç‚¹çš„ä¸»è¦ä¼˜ç‚¹æ˜¯å·¥ä½œæµç®€æ´ï¼Œä½ å¯ä»¥å¤ç”¨ç»„èŠ‚ç‚¹æ¥å®žçŽ°å¤æ‚çš„å·¥ä½œæµï¼Œå¿«é€Ÿå¤ç”¨èŠ‚ç‚¹ç»„ï¼Œå¦å¤–åœ¨æ–°ç‰ˆæœ¬çš„å‰ç«¯ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿä¸º Flux.1 Kontext Dev å¢žåŠ äº†ä¸€ä¸ªå¿«é€Ÿæ·»åŠ ç»„èŠ‚ç‚¹çš„åŠŸèƒ½ï¼š ![å¿«é€Ÿæ·»åŠ ç»„èŠ‚ç‚¹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/selcetion_toolbox_edit.jpg)\n\n## æ¨¡åž‹ä¸‹è½½\n\nä¸ºäº†ä½¿æœ¬ç¯‡æŒ‡å—çš„å·¥ä½œæµèƒ½å¤Ÿé¡ºåˆ©è¿è¡Œï¼Œä½ å…ˆéœ€è¦ä¸‹è½½ä¸‹é¢çš„æ¨¡åž‹æ–‡ä»¶,ä½ ä¹Ÿå¯ä»¥ç›´æŽ¥åŠ è½½å¯¹åº”å·¥ä½œæµä¸‹ç›´æŽ¥èŽ·å–æ¨¡åž‹çš„ä¸‹è½½é“¾æŽ¥ï¼Œå¯¹åº”çš„å·¥ä½œæµå·²ç»åŒ…å«äº†æ¨¡åž‹æ–‡ä»¶çš„ä¸‹è½½ä¿¡æ¯ã€‚ **Diffusion Model**\n\n*   [flux1-dev-kontext\\_fp8\\_scaled.safetensors](https://huggingface.co/Comfy-Org/flux1-kontext-dev_ComfyUI/resolve/main/split_files/diffusion_models/flux1-dev-kontext_fp8_scaled.safetensors)\n\n**VAE**\n\n*   [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/blob/main/split_files/vae/ae.safetensors)\n\n**Text Encoder**\n\n*   [clip\\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/clip_l.safetensors)\n*   [t5xxl\\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors) æˆ– [t5xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors)\n\næ¨¡åž‹ä¿å­˜ä½ç½®\n\n```\nðŸ“‚ ComfyUI/\nâ”œâ”€â”€ ðŸ“‚ models/\nâ”‚   â”œâ”€â”€ ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€ flux1-dev-kontext_fp8_scaled.safetensors\nâ”‚   â”œâ”€â”€ ðŸ“‚ vae/\nâ”‚   â”‚   â””â”€â”€ ae.safetensor\nâ”‚   â””â”€â”€ ðŸ“‚ text_encoders/\nâ”‚       â”œâ”€â”€ clip_l.safetensors\nâ”‚       â””â”€â”€ t5xxl_fp16.safetensors æˆ–è€… t5xxl_fp8_e4m3fn_scaled.safetensors\n```\n\n## Flux.1 Kontext Dev Basic å·¥ä½œæµ\n\nè¿™ä¸ªå·¥ä½œæµæ˜¯æ­£å¸¸çš„å·¥ä½œæµï¼Œä¸è¿‡ä½¿ç”¨äº† `Load Image(from output)` èŠ‚ç‚¹æ¥åŠ è½½éœ€è¦ç¼–è¾‘çš„å›¾åƒå¯ä»¥è®©ä½ æ›´æ–¹ä¾¿åœ°èŽ·å–åˆ°ç¼–è¾‘åŽçš„å›¾åƒï¼Œä»Žè€Œè¿›è¡Œå¤šè½®æ¬¡ç¼–è¾‘\n\n### 1\\. å·¥ä½œæµåŠè¾“å…¥å›¾ç‰‡ä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµ ![ComfyUI Flux.1 Kontext Pro Image API èŠ‚ç‚¹ å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/flux_1_kontext_dev_basic.png) **è¾“å…¥å›¾ç‰‡** ![ComfyUI Flux Kontext åŽŸç”Ÿå·¥ä½œæµè¾“å…¥](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/rabbit.jpg)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n ![å·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆå›¾å·¥ä½œæµçš„è¿è¡Œï¼š\n\n1.  åœ¨ `Load Diffusion Model` èŠ‚ç‚¹ä¸­åŠ è½½ `flux1-dev-kontext_fp8_scaled.safetensors` æ¨¡åž‹\n2.  åœ¨ `DualCLIP Load` èŠ‚ç‚¹ä¸­ç¡®ä¿ï¼š `clip_l.safetensors` åŠ `t5xxl_fp16.safetensors` æˆ– `t5xxl_fp8_e4m3fn_scaled.safetensors` å·²ç»åŠ è½½\n3.  åœ¨ `Load VAE` èŠ‚ç‚¹ä¸­ç¡®ä¿åŠ è½½ `ae.safetensors` æ¨¡åž‹\n4.  åœ¨ `Load Image(from output)` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„è¾“å…¥å›¾åƒ\n5.  åœ¨ `CLIP Text Encode` èŠ‚ç‚¹ä¸­ä¿®æ”¹æç¤ºè¯ï¼Œä»…æ”¯æŒè‹±æ–‡\n6.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n## Flux.1 Kontext Dev Grouped å·¥ä½œæµ\n\nè¿™ä¸ªå·¥ä½œæµæ˜¯ä½¿ç”¨ç»„èŠ‚ç‚¹ **FLUX.1 Kontext Image Edit** çš„å·¥ä½œæµï¼Œä½¿å¾—æ•´ä¸ªç•Œé¢å’Œå·¥ä½œæµå¤ç”¨èµ·æ¥å˜å¾—ç®€å• åŒæ—¶è¿™ä¸ªç¤ºä¾‹ä¹Ÿä½¿ç”¨äº†ä¸¤ä¸ªå›¾åƒè¿›è¡Œè¾“å…¥é€šè¿‡ `Image Stitch` èŠ‚ç‚¹å°†ä¸¤ä¸ªå›¾åƒæ‹¼æŽ¥æˆä¸€ä¸ªå›¾åƒï¼Œå¹¶ä½¿ç”¨ Flux.1 Kontext è¿›è¡Œç¼–è¾‘ã€‚\n\n### 1\\. å·¥ä½œæµåŠè¾“å…¥å›¾ç‰‡ä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­åŠ è½½å¯¹åº”å·¥ä½œæµ ![ComfyUI Flux Kontext åŽŸç”Ÿå·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/flux_1_kontext_dev_grouped.png) **è¾“å…¥å›¾ç‰‡** ![ComfyUI Flux Kontext åŽŸç”Ÿå·¥ä½œæµè¾“å…¥](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/doll_1.webp) ![ComfyUI Flux Kontext åŽŸç”Ÿå·¥ä½œæµè¾“å…¥](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/doll_2.webp)\n\n### 2\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n ![å·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_1_kontext_dev_grouped_step_guide.jpg) ä½ å¯å‚è€ƒå›¾ç‰‡ä¸­çš„åºå·æ¥å®Œæˆå›¾å·¥ä½œæµçš„è¿è¡Œï¼š\n\n1.  åœ¨ `Load VAE` èŠ‚ç‚¹ä¸­åŠ è½½ `ae.safetensors` æ¨¡åž‹\n2.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„ç¬¬ä¸€ä¸ªè¾“å…¥å›¾åƒ\n3.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸­åŠ è½½æä¾›çš„ç¬¬äºŒä¸ªè¾“å…¥å›¾åƒ\n4.  ç”±äºŽå…¶å®ƒæ¨¡åž‹å’Œç›¸å…³èŠ‚ç‚¹éƒ½è¢«ç»„èŠ‚ç‚¹æ‰“åŒ…ï¼Œä½ éœ€è¦æŒ‰ç…§æ­¥éª¤å›¾ä¸­çš„å‚è€ƒåŒæ ·ç¡®ä¿å¯¹åº”çš„æ¨¡åž‹å·²ç»æ­£ç¡®åŠ è½½ï¼Œå¹¶ä¹¦å†™æç¤ºè¯\n5.  ç‚¹å‡» `Queue` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥è¿è¡Œå·¥ä½œæµ\n\n## æ–°å¢žçš„ Flux.1 Kontext Dev é€‰æ‹©å·¥å…·ç®±åŠŸèƒ½\n\næ­¤æ¬¡ä¸ºäº†æ–¹ä¾¿ç”¨æˆ·ä½¿ç”¨ Flux.1 Kontext è¿›è¡Œç¼–è¾‘ï¼Œæˆ‘ä»¬æ–°å¢žäº†é€‰æ‹©å·¥å…·ç®±åŠŸèƒ½ï¼Œç”¨æˆ·å¯ä»¥æ›´åŠ æ–¹ä¾¿åœ°å¿«é€Ÿæ·»åŠ  `FLUX.1 Kontext Image Edit` ç»„èŠ‚ç‚¹ï¼Œå…·ä½“å¯ä»¥æŸ¥çœ‹ä¸‹é¢çš„è§†é¢‘æ¼”ç¤ºï¼Œå½“ä½ é€‰ä¸­ `Load Image` èŠ‚ç‚¹æ—¶ï¼Œå°±å¯ä»¥åœ¨é€‰æ‹©å·¥å…·ç®±ä¸­æ‰¾åˆ°æ–°å¢žçš„ç¼–è¾‘æŒ‰é’®\n\n![Legacy Browser](https://s1.hdslb.com/bfs/static/player/img/h5.png)\n\næ‚¨å½“å‰çš„æµè§ˆå™¨ä¸æ”¯æŒ HTML5 æ’­æ”¾å™¨\n\nè¯·æ›´æ¢æµè§ˆå™¨å†è¯•è¯•å“¦~\n\n## Flux Kontext æç¤ºè¯æŠ€å·§\n\n### 1\\. åŸºç¡€ä¿®æ”¹\n\n*   ç®€å•ç›´æŽ¥ï¼š`\"Change the car color to red\"`\n*   ä¿æŒé£Žæ ¼ï¼š`\"Change to daytime while maintaining the same style of the painting\"`\n\n### 2\\. é£Žæ ¼è½¬æ¢\n\n**åŽŸåˆ™ï¼š**\n\n*   æ˜Žç¡®å‘½åé£Žæ ¼ï¼š`\"Transform to Bauhaus art style\"`\n*   æè¿°ç‰¹å¾ï¼š`\"Transform to oil painting with visible brushstrokes, thick paint texture\"`\n*   ä¿ç•™æž„å›¾ï¼š`\"Change to Bauhaus style while maintaining the original composition\"`\n\n### 3\\. è§’è‰²ä¸€è‡´æ€§\n\n**æ¡†æž¶ï¼š**\n\n*   å…·ä½“æè¿°ï¼š`\"The woman with short black hair\"`è€Œéž`\"she\"`\n*   ä¿ç•™ç‰¹å¾ï¼š`\"while maintaining the same facial features, hairstyle, and expression\"`\n*   åˆ†æ­¥ä¿®æ”¹ï¼šå…ˆæ”¹èƒŒæ™¯ï¼Œå†æ”¹åŠ¨ä½œ\n\n### 4\\. æ–‡æœ¬ç¼–è¾‘\n\n*   ä½¿ç”¨å¼•å·ï¼š`\"Replace 'joy' with 'BFL'\"`\n*   ä¿æŒæ ¼å¼ï¼š`\"Replace text while maintaining the same font style\"`\n\n## å¸¸è§é—®é¢˜è§£å†³\n\n### è§’è‰²å˜åŒ–è¿‡å¤§\n\nâŒ é”™è¯¯ï¼š`\"Transform the person into a Viking\"` âœ… æ­£ç¡®ï¼š`\"Change the clothes to be a viking warrior while preserving facial features\"`\n\n### æž„å›¾ä½ç½®æ”¹å˜\n\nâŒ é”™è¯¯ï¼š`\"Put him on a beach\"` âœ… æ­£ç¡®ï¼š`\"Change the background to a beach while keeping the person in the exact same position, scale, and pose\"`\n\n### é£Žæ ¼åº”ç”¨ä¸å‡†ç¡®\n\nâŒ é”™è¯¯ï¼š`\"Make it a sketch\"` âœ… æ­£ç¡®ï¼š`\"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture\"`\n\n## æ ¸å¿ƒåŽŸåˆ™\n\n1.  **å…·ä½“æ˜Žç¡®** - ä½¿ç”¨ç²¾ç¡®æè¿°ï¼Œé¿å…æ¨¡ç³Šè¯æ±‡\n2.  **åˆ†æ­¥ç¼–è¾‘** - å¤æ‚ä¿®æ”¹åˆ†ä¸ºå¤šä¸ªç®€å•æ­¥éª¤\n3.  **æ˜Žç¡®ä¿ç•™** - è¯´æ˜Žå“ªäº›è¦ä¿æŒä¸å˜\n4.  **åŠ¨è¯é€‰æ‹©** - ç”¨â€changeâ€ã€â€œreplaceâ€è€Œéžâ€transformâ€\n\n## æœ€ä½³å®žè·µæ¨¡æ¿\n\n**å¯¹è±¡ä¿®æ”¹ï¼š** `\"Change [object] to [new state], keep [content to preserve] unchanged\"` **é£Žæ ¼è½¬æ¢ï¼š** `\"Transform to [specific style], while maintaining [composition/character/other] unchanged\"` **èƒŒæ™¯æ›¿æ¢ï¼š** `\"Change the background to [new background], keep the subject in the exact same position and pose\"` **æ–‡æœ¬ç¼–è¾‘ï¼š** `\"Replace '[original text]' with '[new text]', maintain the same font style\"`\n\n> **è®°ä½ï¼š** è¶Šå…·ä½“è¶Šå¥½ï¼ŒKontext æ“…é•¿ç†è§£è¯¦ç»†æŒ‡ä»¤å¹¶ä¿æŒä¸€è‡´æ€§ã€‚\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/wan-video",
  "markdown": "# ComfyUI Wan2.1 Video ç¤ºä¾‹ - ComfyUI\n\nWan2.1 Video ç³»åˆ—ä¸ºé˜¿é‡Œå·´å·´äºŽ 2025å¹´2æœˆå¼€æºçš„è§†é¢‘ç”Ÿæˆæ¨¡åž‹ï¼Œå…¶å¼€æºåè®®ä¸º [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file)ï¼Œæä¾› 14Bï¼ˆ140äº¿å‚æ•°ï¼‰å’Œ 1.3Bï¼ˆ13äº¿å‚æ•°ï¼‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œè¦†ç›–æ–‡ç”Ÿè§†é¢‘ï¼ˆT2Vï¼‰ã€å›¾ç”Ÿè§†é¢‘ï¼ˆI2Vï¼‰ç­‰å¤šé¡¹ä»»åŠ¡ã€‚ è¯¥æ¨¡åž‹ä¸ä»…åœ¨æ€§èƒ½ä¸Šè¶…è¶ŠçŽ°æœ‰å¼€æºæ¨¡åž‹ï¼Œæ›´é‡è¦çš„æ˜¯å…¶è½»é‡çº§ç‰ˆæœ¬ä»…éœ€ 8GB æ˜¾å­˜å³å¯è¿è¡Œï¼Œå¤§å¤§é™ä½Žäº†ä½¿ç”¨é—¨æ§›ã€‚\n\n*   [Wan2.1 ä»£ç ä»“åº“](https://github.com/Wan-Video/Wan2.1)\n*   [Wan2.1 ç›¸å…³æ¨¡åž‹ä»“åº“](https://huggingface.co/Wan-AI)\n\n## Wan2.1 ComfyUI åŽŸç”Ÿï¼ˆnativeï¼‰å·¥ä½œæµç¤ºä¾‹\n\n## æ¨¡åž‹å®‰è£…\n\næœ¬ç¯‡æŒ‡å—æ¶‰åŠçš„æ‰€æœ‰æ¨¡åž‹ä½ éƒ½å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files)æ‰¾åˆ°, ä¸‹é¢æ˜¯æœ¬ç¯‡ç¤ºä¾‹ä¸­å°†ä¼šä½¿ç”¨åˆ°çš„å…±ç”¨çš„æ¨¡åž‹ï¼Œä½ å¯ä»¥æå‰è¿›è¡Œä¸‹è½½ï¼š ä»Ž**Text encoders** é€‰æ‹©ä¸€ä¸ªç‰ˆæœ¬è¿›è¡Œä¸‹è½½ï¼Œ\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\næ–‡ä»¶ä¿å­˜ä½ç½®\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ diffusion_models/\nâ”‚   â”œâ”€â”€ ...                  # æˆ‘ä»¬åœ¨å¯¹åº”çš„å·¥ä½œæµä¸­è¿›è¡Œè¡¥å……è¯´æ˜Ž\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors\nâ”‚   â””â”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€  wan_2.1_vae.safetensors\nâ”‚   â””â”€â”€ clip_vision/\nâ”‚       â””â”€â”€  clip_vision_h.safetensors   \n```\n\nåœ¨å¼€å§‹å·¥ä½œæµå‰è¯·ä¸‹è½½ [wan2.1\\_t2v\\_1.3B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true)ï¼Œå¹¶ä¿å­˜åˆ° `ComfyUI/models/diffusion_models/` ç›®å½•ä¸‹ã€‚\n\n> å¦‚æžœä½ éœ€è¦å…¶å®ƒçš„ t2v ç²¾åº¦ç‰ˆæœ¬ï¼Œè¯·è®¿é—®[è¿™é‡Œ](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models)è¿›è¡Œä¸‹è½½\n\n### 1\\. å·¥ä½œæµæ–‡ä»¶ä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![Wan2.1 æ–‡ç”Ÿè§†é¢‘å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_t2v_1.3b.webp)\n\n### 2\\. æŒ‰æµç¨‹å®Œæˆå·¥ä½œæµè¿è¡Œ\n\n![ComfyUI Wan2.1 å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg)\n\n1.  ç¡®ä¿`Load Diffusion Model`èŠ‚ç‚¹åŠ è½½äº† `wan2.1_t2v_1.3B_fp16.safetensors` æ¨¡åž‹\n2.  ç¡®ä¿`Load CLIP`èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors` æ¨¡åž‹\n3.  ç¡®ä¿`Load VAE`èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors` æ¨¡åž‹\n4.  ï¼ˆå¯é€‰ï¼‰å¯ä»¥åœ¨`EmptyHunyuanLatentVideo` èŠ‚ç‚¹è®¾ç½®äº†è§†é¢‘çš„å°ºå¯¸ï¼Œå¦‚æžœæœ‰éœ€è¦ä½ å¯ä»¥ä¿®æ”¹\n5.  ï¼ˆå¯é€‰ï¼‰å¦‚æžœä½ éœ€è¦ä¿®æ”¹æç¤ºè¯ï¼ˆæ­£å‘åŠè´Ÿå‘ï¼‰è¯·åœ¨åºå·`5` çš„ `CLIP Text Encoder` èŠ‚ç‚¹ä¸­è¿›è¡Œä¿®æ”¹\n6.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n\n## Wan2.1 å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ\n\n**ç”±äºŽ Wan Video å°† 480P å’Œ 720P çš„æ¨¡åž‹åˆ†å¼€** ï¼Œæ‰€ä»¥åœ¨æœ¬ç¯‡ä¸­æˆ‘ä»¬å°†éœ€è¦åˆ†åˆ«å¯¹ä¸¤ä¸­æ¸…æ™°åº¦çš„è§†é¢‘åšå‡ºç¤ºä¾‹ï¼Œé™¤äº†å¯¹åº”æ¨¡åž‹ä¸åŒä¹‹å¤–ï¼Œä»–ä»¬è¿˜æœ‰äº›è®¸çš„å‚æ•°å·®å¼‚\n\n### 480P ç‰ˆæœ¬\n\n#### 1\\. å·¥ä½œæµåŠè¾“å…¥å›¾ç‰‡\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­æ¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![Wan2.1 å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ 14B 480P Workflow è¾“å…¥å›¾ç‰‡ç¤ºä¾‹](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_480P.webp) æˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥ï¼š ![Wan2.1 å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ 14B 480P Workflow è¾“å…¥å›¾ç‰‡ç¤ºä¾‹](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/flux_dev_example.png)\n\n#### 2\\. æ¨¡åž‹ä¸‹è½½\n\nè¯·ä¸‹è½½[wan2.1\\_i2v\\_480p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true)ï¼Œå¹¶ä¿å­˜åˆ° `ComfyUI/models/diffusion_models/` ç›®å½•ä¸‹\n\n#### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Wan2.1 å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg)\n\n1.  ç¡®ä¿`Load Diffusion Model`èŠ‚ç‚¹åŠ è½½äº† `wan2.1_i2v_480p_14B_fp16.safetensors` æ¨¡åž‹\n2.  ç¡®ä¿`Load CLIP`èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors` æ¨¡åž‹\n3.  ç¡®ä¿`Load VAE`èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors` æ¨¡åž‹\n4.  ç¡®ä¿`Load CLIP Vision`èŠ‚ç‚¹åŠ è½½äº† `clip_vision_h.safetensors` æ¨¡åž‹\n5.  åœ¨`Load Image`èŠ‚ç‚¹ä¸­ä¸Šä¼ æˆ‘ä»¬æä¾›çš„è¾“å…¥å›¾ç‰‡\n6.  ï¼ˆå¯é€‰ï¼‰åœ¨`CLIP Text Encoder`èŠ‚ç‚¹ä¸­è¾“å…¥ä½ æƒ³è¦ç”Ÿæˆçš„è§†é¢‘æè¿°å†…å®¹ï¼Œ\n7.  ï¼ˆå¯é€‰ï¼‰åœ¨`WanImageToVideo` èŠ‚ç‚¹ä¸­è®¾ç½®äº†è§†é¢‘çš„å°ºå¯¸ï¼Œå¦‚æžœæœ‰éœ€è¦ä½ å¯ä»¥ä¿®æ”¹\n8.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n\n### 720P ç‰ˆæœ¬\n\n#### 1\\. å·¥ä½œæµåŠè¾“å…¥å›¾ç‰‡\n\nä¸‹è½½ä¸‹é¢çš„å›¾ç‰‡ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­æ¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ ![Wan2.1 å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ 14B 720P Workflow è¾“å…¥å›¾ç‰‡ç¤ºä¾‹](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_720P.webp) æˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„å›¾ç‰‡ä½œä¸ºè¾“å…¥ï¼š ![Wan2.1 å›¾ç”Ÿè§†é¢‘å·¥ä½œæµ 14B 720P Workflow è¾“å…¥å›¾ç‰‡ç¤ºä¾‹](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/magician.png)\n\n#### 2\\. æ¨¡åž‹ä¸‹è½½\n\nè¯·ä¸‹è½½[wan2.1\\_i2v\\_720p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true)ï¼Œå¹¶ä¿å­˜åˆ° `ComfyUI/models/diffusion_models/` ç›®å½•ä¸‹\n\n#### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![ComfyUI Wan2.1 å·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg)\n\n1.  ç¡®ä¿`Load Diffusion Model`èŠ‚ç‚¹åŠ è½½äº† `wan2.1_i2v_720p_14B_fp16.safetensors` æ¨¡åž‹\n2.  ç¡®ä¿`Load CLIP`èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors` æ¨¡åž‹\n3.  ç¡®ä¿`Load VAE`èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors` æ¨¡åž‹\n4.  ç¡®ä¿`Load CLIP Vision`èŠ‚ç‚¹åŠ è½½äº† `clip_vision_h.safetensors` æ¨¡åž‹\n5.  åœ¨`Load Image`èŠ‚ç‚¹ä¸­ä¸Šä¼ æˆ‘ä»¬æä¾›çš„è¾“å…¥å›¾ç‰‡\n6.  ï¼ˆå¯é€‰ï¼‰åœ¨`CLIP Text Encoder`èŠ‚ç‚¹ä¸­è¾“å…¥ä½ æƒ³è¦ç”Ÿæˆçš„è§†é¢‘æè¿°å†…å®¹ï¼Œ\n7.  ï¼ˆå¯é€‰ï¼‰åœ¨`WanImageToVideo` èŠ‚ç‚¹ä¸­è®¾ç½®äº†è§†é¢‘çš„å°ºå¯¸ï¼Œå¦‚æžœæœ‰éœ€è¦ä½ å¯ä»¥ä¿®æ”¹\n8.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n\n#### 0 ä¸ªè¡¨æƒ…"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/wan-ati",
  "markdown": "# Wan ATI ComfyUI åŽŸç”Ÿå·¥ä½œæµæ•™ç¨‹ - ComfyUI\n\n**ATIï¼ˆAny Trajectory Instructionï¼‰** æ˜¯ç”±å­—èŠ‚è·³åŠ¨å›¢é˜Ÿæå‡ºçš„å¯æŽ§è§†é¢‘ç”Ÿæˆæ¡†æž¶ã€‚ATI åŸºäºŽ Wan2.1 å®žçŽ°ï¼Œæ”¯æŒé€šè¿‡ä»»æ„è½¨è¿¹æŒ‡ä»¤å¯¹è§†é¢‘ä¸­çš„ç‰©ä½“ã€å±€éƒ¨åŒºåŸŸåŠæ‘„åƒæœºè¿åŠ¨è¿›è¡Œç»Ÿä¸€æŽ§åˆ¶ã€‚ é¡¹ç›®åœ°å€ï¼š[https://github.com/bytedance/ATI](https://github.com/bytedance/ATI)\n\n## ä¸»è¦ç‰¹æ€§\n\n*   **ç»Ÿä¸€è¿åŠ¨æŽ§åˆ¶**ï¼šæ”¯æŒç‰©ä½“ã€å±€éƒ¨ã€æ‘„åƒæœºç­‰å¤šç§è¿åŠ¨ç±»åž‹çš„è½¨è¿¹æŽ§åˆ¶ã€‚\n*   **äº¤äº’å¼è½¨è¿¹ç¼–è¾‘å™¨**ï¼šå¯è§†åŒ–å·¥å…·ï¼Œç”¨æˆ·å¯åœ¨å›¾ç‰‡ä¸Šè‡ªç”±ç»˜åˆ¶ã€ç¼–è¾‘è¿åŠ¨è½¨è¿¹ã€‚\n*   **å…¼å®¹ Wan2.1**ï¼šåŸºäºŽ Wan2.1 å®˜æ–¹å®žçŽ°ï¼ŒçŽ¯å¢ƒå’Œæ¨¡åž‹ç»“æž„å…¼å®¹ã€‚\n*   **ä¸°å¯Œçš„å¯è§†åŒ–å·¥å…·**ï¼šæ”¯æŒè¾“å…¥è½¨è¿¹ã€è¾“å‡ºè§†é¢‘åŠè½¨è¿¹å¯è§†åŒ–ã€‚\n\n### 1\\. å·¥ä½œæµä¸‹è½½\n\nä¸‹è½½ä¸‹é¢çš„è§†é¢‘å¹¶æ‹–å…¥ ComfyUI ä¸­ï¼Œä»¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ\n\næˆ‘ä»¬å°†ä½¿ç”¨ä¸‹é¢çš„ç´ æä½œä¸ºè¾“å…¥: ![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/ati/input.jpg) \n\n### 2\\. æ¨¡åž‹ä¸‹è½½\n\nå¦‚æžœä½ æ²¡æœ‰æˆåŠŸä¸‹è½½å·¥ä½œæµä¸­çš„æ¨¡åž‹æ–‡ä»¶ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨ä¸‹é¢çš„é“¾æŽ¥æ‰‹åŠ¨ä¸‹è½½ **Diffusion Model**\n\n*   [Wan2\\_1-I2V-ATI-14B\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**Text encoders** Chose one of following model\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**clip\\_vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)\n\nFile save location\n\n```\nComfyUI/\nâ”œâ”€â”€â”€ðŸ“‚ models/\nâ”‚   â”œâ”€â”€â”€ðŸ“‚ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors\nâ”‚   â”œâ”€â”€â”€ðŸ“‚ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors # or other version\nâ”‚   â”œâ”€â”€â”€ðŸ“‚ clip_vision/\nâ”‚   â”‚   â””â”€â”€â”€ clip_vision_h.safetensors\nâ”‚   â””â”€â”€â”€ðŸ“‚ vae/\nâ”‚       â””â”€â”€  wan_2.1_vae.safetensors\n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ\n\n![å·¥ä½œæµæ­¥éª¤å›¾](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan_ati_guide.jpg) è¯·å‚ç…§å›¾ç‰‡åºå·è¿›è¡Œé€æ­¥ç¡®è®¤ï¼Œæ¥ä¿è¯å¯¹åº”å·¥ä½œæµçš„é¡ºåˆ©è¿è¡Œ\n\n1.  ç¡®ä¿`Load Diffusion Model`èŠ‚ç‚¹åŠ è½½äº† `Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors` æ¨¡åž‹\n2.  ç¡®ä¿`Load CLIP`èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors` æ¨¡åž‹\n3.  ç¡®ä¿`Load VAE`èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors` æ¨¡åž‹\n4.  ç¡®ä¿`Load CLIP Vision`èŠ‚ç‚¹åŠ è½½äº† `clip_vision_h.safetensors` æ¨¡åž‹\n5.  åœ¨ `Load Image` èŠ‚ç‚¹ä¸Šä¼ æä¾›çš„è¾“å…¥å›¾ç‰‡\n6.  è½¨è¿¹ç¼–è¾‘ï¼š ç›®å‰ ComfyUI ä¸­è¿˜æœªæœ‰å¯¹åº”çš„è½¨è¿¹ç¼–è¾‘å™¨ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„é“¾æŽ¥æ¥å®Œæˆè½¨è¿¹ç¼–è¾‘\n    *   [åœ¨çº¿è½¨è¿¹ç¼–è¾‘å·¥å…·](https://comfyui-wiki.github.io/Trajectory-Annotation-Tool/)\n7.  å¦‚æžœä½ éœ€è¦ä¿®æ”¹æç¤ºè¯ï¼ˆæ­£å‘åŠè´Ÿå‘ï¼‰è¯·åœ¨åºå·`5` çš„ `CLIP Text Encoder` èŠ‚ç‚¹ä¸­è¿›è¡Œä¿®æ”¹\n8.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ\n\n#### 0 ä¸ªè¡¨æƒ…\n\nåœ¨æ­¤é¡µé¢\n\n*   [ä¸»è¦ç‰¹æ€§](#%E4%B8%BB%E8%A6%81%E7%89%B9%E6%80%A7)\n*   [WAN ATI è½¨è¿¹æŽ§åˆ¶å·¥ä½œæµç¤ºä¾‹](#wan-ati-%E8%BD%A8%E8%BF%B9%E6%8E%A7%E5%88%B6%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A4%BA%E4%BE%8B)\n*   [1\\. å·¥ä½œæµä¸‹è½½](#1-%E5%B7%A5%E4%BD%9C%E6%B5%81%E4%B8%8B%E8%BD%BD)\n*   [2\\. æ¨¡åž‹ä¸‹è½½](#2-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD)\n*   [3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµçš„è¿è¡Œ](#3-%E6%8C%89%E6%AD%A5%E9%AA%A4%E5%AE%8C%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%9A%84%E8%BF%90%E8%A1%8C)"
},
{
  "url": "https://docs.comfy.org/zh-CN/tutorials/video/wan/wan-flf",
  "markdown": "# ComfyUI Wan2.1 FLF2V åŽŸç”Ÿç¤ºä¾‹ - ComfyUI\n\nWan FLF2Vï¼ˆé¦–å°¾å¸§è§†é¢‘ç”Ÿæˆï¼‰æ˜¯ç”±é˜¿é‡Œé€šä¹‰ä¸‡ç›¸å›¢é˜ŸæŽ¨å‡ºçš„å¼€æºè§†é¢‘ç”Ÿæˆæ¨¡åž‹ã€‚å…¶å¼€æºåè®®ä¸º [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file)ã€‚ ç”¨æˆ·åªéœ€æä¾›èµ·å§‹å¸§å’Œç»“æŸå¸§ä¸¤å¼ å›¾åƒï¼Œæ¨¡åž‹å³å¯è‡ªåŠ¨ç”Ÿæˆä¸­é—´è¿‡æ¸¡å¸§ï¼Œè¾“å‡ºä¸€æ®µé€»è¾‘è¿žè´¯ã€è‡ªç„¶æµç•…çš„720pé«˜æ¸…è§†é¢‘ã€‚ **æ ¸å¿ƒæŠ€æœ¯äº®ç‚¹**\n\n1.  **é¦–å°¾å¸§ç²¾å‡†æŽ§åˆ¶**ï¼šé¦–å°¾å¸§åŒ¹é…åº¦è¾¾98%ï¼Œé€šè¿‡èµ·å§‹å’Œç»“æŸç”»é¢å®šä¹‰è§†é¢‘è¾¹ç•Œï¼Œæ¨¡åž‹æ™ºèƒ½å¡«å……ä¸­é—´åŠ¨æ€å˜åŒ–ï¼Œå®žçŽ°åœºæ™¯è½¬æ¢å’Œç‰©ä½“å½¢æ€æ¼”å˜ç­‰æ•ˆæžœã€‚\n2.  **ç¨³å®šæµç•…è§†é¢‘ç”Ÿæˆ**ï¼šé‡‡ç”¨CLIPè¯­ä¹‰ç‰¹å¾å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œè§†é¢‘æŠ–åŠ¨çŽ‡æ¯”åŒç±»æ¨¡åž‹é™ä½Ž37%ï¼Œç¡®ä¿è½¬åœºè‡ªç„¶æµç•…ã€‚\n3.  **å¤šåŠŸèƒ½åˆ›ä½œèƒ½åŠ›**ï¼šæ”¯æŒä¸­è‹±æ–‡å­—å¹•åŠ¨æ€åµŒå…¥ã€äºŒæ¬¡å…ƒ/å†™å®ž/å¥‡å¹»ç­‰å¤šé£Žæ ¼ç”Ÿæˆï¼Œé€‚åº”ä¸åŒåˆ›ä½œéœ€æ±‚ã€‚\n4.  **720pé«˜æ¸…è¾“å‡º**ï¼šç›´æŽ¥ç”Ÿæˆ1280Ã—720åˆ†è¾¨çŽ‡è§†é¢‘ï¼Œæ— éœ€åŽå¤„ç†ï¼Œé€‚ç”¨äºŽç¤¾äº¤åª’ä½“å’Œå•†ä¸šåº”ç”¨ã€‚\n5.  **å¼€æºç”Ÿæ€æ”¯æŒ**ï¼šæ¨¡åž‹æƒé‡ã€ä»£ç åŠè®­ç»ƒæ¡†æž¶å…¨é¢å¼€æºï¼Œæ”¯æŒä¸»æµAIå¹³å°éƒ¨ç½²ã€‚\n\n**æŠ€æœ¯åŽŸç†ä¸Žæž¶æž„**\n\n1.  **DiTæž¶æž„**ï¼šåŸºäºŽæ‰©æ•£æ¨¡åž‹å’ŒDiffusion Transformeræž¶æž„ï¼Œç»“åˆFull Attentionæœºåˆ¶ä¼˜åŒ–æ—¶ç©ºä¾èµ–å»ºæ¨¡ï¼Œç¡®ä¿è§†é¢‘è¿žè´¯æ€§ã€‚\n2.  **ä¸‰ç»´å› æžœå˜åˆ†ç¼–ç å™¨**ï¼šWan-VAEæŠ€æœ¯å°†é«˜æ¸…ç”»é¢åŽ‹ç¼©è‡³1/128å°ºå¯¸ï¼ŒåŒæ—¶ä¿ç•™ç»†å¾®åŠ¨æ€ç»†èŠ‚ï¼Œæ˜¾è‘—é™ä½Žæ˜¾å­˜éœ€æ±‚ã€‚\n3.  **ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥**ï¼šä»Ž480Påˆ†è¾¨çŽ‡å¼€å§‹é¢„è®­ç»ƒï¼Œé€æ­¥æå‡è‡³720Pï¼Œé€šè¿‡åˆ†é˜¶æ®µä¼˜åŒ–å¹³è¡¡ç”Ÿæˆè´¨é‡ä¸Žè®¡ç®—æ•ˆçŽ‡ã€‚\n\n**ç›¸å…³é“¾æŽ¥**\n\n*   **GitHubä»£ç ä»“åº“**ï¼š[GitHub](https://github.com/Wan-Video/Wan2.1)\n*   **Hugging Faceæ¨¡åž‹é¡µ**ï¼š[Hugging Face](https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P)\n*   **ModelScopeï¼ˆé­”æ­ç¤¾åŒºï¼‰**ï¼š[ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)\n\n### 1\\. ä¸‹è½½å·¥ä½œæµæ–‡ä»¶åŠç›¸å…³è¾“å…¥æ–‡ä»¶\n\nè¯·ä¸‹è½½ä¸‹é¢çš„ WebP ä¿å­˜ä¸‹é¢çš„ WebP æ–‡ä»¶ï¼Œå¹¶æ‹–å…¥ ComfyUI ä¸­æ¥åŠ è½½å¯¹åº”çš„å·¥ä½œæµ,å¯¹åº”å·¥ä½œæµå·²åµŒå…¥å¯¹åº”çš„æ¨¡åž‹ä¸‹è½½æ–‡ä»¶ä¿¡æ¯ã€‚ ![Wan2.1 FLF2V 720P f16 å·¥ä½œæµ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/wan2.1_flf2v_720_f16.webp) è¯·ä¸‹è½½ä¸‹é¢çš„ä¸¤å¼ å›¾ç‰‡ï¼Œæˆ‘ä»¬å°†ä¼šä½œä¸ºä½œä¸ºè§†é¢‘çš„èµ·å§‹å¸§å’Œç»“æŸå¸§ ![start_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/start_image.png) ![end_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/end_image.png)\n\n### 2.æ‰‹åŠ¨æ¨¡åž‹å®‰è£…\n\næœ¬ç¯‡æŒ‡å—æ¶‰åŠçš„æ‰€æœ‰æ¨¡åž‹ä½ éƒ½å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files)æ‰¾åˆ°ã€‚ **diffusion\\_models** æ ¹æ®ä½ çš„ç¡¬ä»¶æƒ…å†µé€‰æ‹©ä¸€ä¸ªç‰ˆæœ¬è¿›è¡Œä¸‹è½½ï¼ŒFP8 ç‰ˆæœ¬å¯¹æ˜¾å­˜è¦æ±‚ä½Žä¸€äº›\n\n*   FP16:[wan2.1\\_flf2v\\_720p\\_14B\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp16.safetensors?download=true)\n*   FP8:[wan2.1\\_flf2v\\_720p\\_14B\\_fp8\\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors)\n\nä»Ž**Text encoders** é€‰æ‹©ä¸€ä¸ªç‰ˆæœ¬è¿›è¡Œä¸‹è½½ï¼Œ\n\n*   [umt5\\_xxl\\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)\n*   [umt5\\_xxl\\_fp8\\_e4m3fn\\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)\n\n**VAE**\n\n*   [wan\\_2.1\\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)\n\n**CLIP Vision**\n\n*   [clip\\_vision\\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)\n\næ–‡ä»¶ä¿å­˜ä½ç½®\n\n```\nComfyUI/\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ diffusion_models/\nâ”‚   â”‚   â””â”€â”€â”€ wan2.1_flf2v_720p_14B_fp16.safetensors          # æˆ–è€… FP8 ç‰ˆæœ¬\nâ”‚   â”œâ”€â”€ text_encoders/\nâ”‚   â”‚   â””â”€â”€â”€ umt5_xxl_fp8_e4m3fn_scaled.safetensors           # æˆ–è€…ä½ é€‰æ‹©çš„ç‰ˆæœ¬\nâ”‚   â”œâ”€â”€ vae/\nâ”‚   â”‚   â””â”€â”€  wan_2.1_vae.safetensors\nâ”‚   â””â”€â”€ clip_vision/\nâ”‚       â””â”€â”€  clip_vision_h.safetensors   \n```\n\n### 3\\. æŒ‰æ­¥éª¤å®Œæˆå·¥ä½œæµè¿è¡Œ\n\n![Wan2.1 FLF2V 720P åŽŸç”Ÿå·¥ä½œæµæ­¥éª¤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg)\n\n1.  ç¡®ä¿ `Load Diffusion Model` èŠ‚ç‚¹åŠ è½½äº† `wan2.1_flf2v_720p_14B_fp16.safetensors` æˆ–è€… `wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors`\n2.  ç¡®ä¿ `Load CLIP` èŠ‚ç‚¹åŠ è½½äº† `umt5_xxl_fp8_e4m3fn_scaled.safetensors`\n3.  ç¡®ä¿ `Load VAE` èŠ‚ç‚¹åŠ è½½äº† `wan_2.1_vae.safetensors`\n4.  ç¡®ä¿ `Load CLIP Vision` èŠ‚ç‚¹åŠ è½½äº† `clip_vision_h.safetensors`\n5.  åœ¨ `Start_image` èŠ‚ç‚¹ä¸Šä¼ èµ·å§‹å¸§\n6.  åœ¨ `End_image` èŠ‚ç‚¹ä¸Šä¼ ç»“æŸå¸§\n7.  ï¼ˆå¯é€‰ï¼‰ä¿®æ”¹ æ­£å‘å’Œè´Ÿå‘çš„æç¤ºè¯ï¼ˆPromptï¼‰ä½¿ç”¨ä¸­è‹±æ–‡éƒ½å¯ä»¥\n8.  ï¼ˆ**é‡è¦**ï¼‰åœ¨ `WanFirstLastFrameToVideo` ä¿®æ”¹å¯¹åº”è§†é¢‘çš„å°ºå¯¸æˆ‘ä»¬é»˜è®¤ä½¿ç”¨äº† 720 \\* 1280 çš„å°ºå¯¸æ¥ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ª 720P çš„å°ºå¯¸æ¥ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ª720Pçš„æ¨¡åž‹ï¼Œæ‰€ä»¥ä½¿ç”¨è¾ƒå°çš„å°ºå¯¸ä¼šæ— æ³•èŽ·å¾—è¾ƒå¥½çš„ç»“æžœã€‚\n9.  ç‚¹å‡» `Run` æŒ‰é’®ï¼Œæˆ–è€…ä½¿ç”¨å¿«æ·é”® `Ctrl(cmd) + Enter(å›žè½¦)` æ¥æ‰§è¡Œè§†é¢‘ç”Ÿæˆ"
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fvideo%2Fwan%2Ffun-camera",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fapi-nodes%2Fideogram%2Fideogram-v3",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Fcustom-nodes%2Fjs%2Fjavascript_topbar_menu",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fapi-nodes%2Frunway%2Fimage-generation",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fvideo%2Fhunyuan-video",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fapi-nodes%2Fmoonvalley%2Fmoonvalley-video-generation",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fcontrolnet%2Fpose-controlnet-2-pass",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fimage%2Fhidream%2Fhidream-i1",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fflux%2Fflux-1-kontext-dev",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fvideo%2Fwan%2Fwan-video",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
},
{
  "url": "https://docs.comfy.org/api/oauth/authorize?redirect_uri=https%3A%2F%2Fdocs.comfy.org%2Fzh-CN%2Ftutorials%2Fvideo%2Fwan%2Fwan-ati",
  "markdown": "Error 500\n\n## Page not found!\n\nAn unexpected error occurred. Please [contact support](mailto:support@mintlify.com) to get help."
}]